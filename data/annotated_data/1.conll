-DOCSTART- -X- O
WhitenedCSE -X- _ B-MethodName
: -X- _ O
Whitening -X- _ B-MethodName
- -X- _ I-MethodName
based -X- _ I-MethodName
Contrastive -X- _ I-MethodName
Learning -X- _ I-MethodName
of -X- _ I-MethodName
Sentence -X- _ I-MethodName
Embeddings -X- _ I-MethodName

This -X- _ O
paper -X- _ O
presents -X- _ O
a -X- _ O
whitening -X- _ B-MethodName
- -X- _ I-MethodName
based -X- _ I-MethodName
contrastive -X- _ I-MethodName
learning -X- _ I-MethodName
method -X- _ I-MethodName
for -X- _ I-MethodName
sentence -X- _ I-MethodName
embedding -X- _ I-MethodName
learning -X- _ I-MethodName
( -X- _ O
WhitenedCSE -X- _ B-MethodName
) -X- _ O
, -X- _ O
which -X- _ O
combines -X- _ O
contrastive -X- _ O
learning -X- _ O
with -X- _ O
a -X- _ O
novel -X- _ O
shuffled -X- _ B-MethodName
group -X- _ I-MethodName
whitening -X- _ I-MethodName
. -X- _ O
Generally -X- _ O
, -X- _ O
contrastive -X- _ O
learning -X- _ O
pulls -X- _ O
distortions -X- _ O
of -X- _ O
a -X- _ O
single -X- _ O
sample -X- _ O
( -X- _ O
i.e. -X- _ O
, -X- _ O
positive -X- _ O
samples -X- _ O
) -X- _ O
close -X- _ O
and -X- _ O
push -X- _ O
negative -X- _ O
samples -X- _ O
far -X- _ O
away -X- _ O
, -X- _ O
correspondingly -X- _ O
facilitating -X- _ O
the -X- _ O
alignment -X- _ O
and -X- _ O
uniformity -X- _ O
in -X- _ O
the -X- _ O
feature -X- _ O
space -X- _ O
. -X- _ O
A -X- _ O
popular -X- _ O
alternative -X- _ O
to -X- _ O
the -X- _ O
" -X- _ O
pushing -X- _ O
" -X- _ O
operation -X- _ O
is -X- _ O
whitening -X- _ O
the -X- _ O
feature -X- _ O
space -X- _ O
, -X- _ O
which -X- _ O
scatters -X- _ O
all -X- _ O
the -X- _ O
samples -X- _ O
for -X- _ O
uniformity -X- _ B-MetricName
. -X- _ O
Since -X- _ O
the -X- _ O
whitening -X- _ O
and -X- _ O
the -X- _ O
contrastive -X- _ O
learning -X- _ O
have -X- _ O
large -X- _ O
redundancy -X- _ O
w.r.t -X- _ O
. -X- _ O
the -X- _ O
uniformity -X- _ B-MetricName
, -X- _ O
they -X- _ O
are -X- _ O
usually -X- _ O
used -X- _ O
separately -X- _ O
and -X- _ O
do -X- _ O
not -X- _ O
easily -X- _ O
work -X- _ O
together -X- _ O
. -X- _ O
For -X- _ O
the -X- _ O
first -X- _ O
time -X- _ O
, -X- _ O
this -X- _ O
paper -X- _ O
integrates -X- _ O
whitening -X- _ O
into -X- _ O
the -X- _ O
contrastive -X- _ O
learning -X- _ O
scheme -X- _ O
and -X- _ O
facilitates -X- _ O
two -X- _ O
benefits -X- _ O
. -X- _ O
1 -X- _ O
) -X- _ O
Better -X- _ O
uniformity -X- _ B-MetricName
. -X- _ O
We -X- _ O
find -X- _ O
that -X- _ O
these -X- _ O
two -X- _ O
approaches -X- _ O
are -X- _ O
not -X- _ O
totally -X- _ O
redundant -X- _ O
but -X- _ O
actually -X- _ O
have -X- _ O
some -X- _ O
complementarity -X- _ O
due -X- _ O
to -X- _ O
different -X- _ O
uniformity -X- _ B-MetricName
mechanism -X- _ O
. -X- _ O
2 -X- _ O
) -X- _ O
Better -X- _ O
alignment -X- _ B-MetricName
. -X- _ O
We -X- _ O
randomly -X- _ O
divide -X- _ O
the -X- _ O
feature -X- _ O
into -X- _ O
multiple -X- _ O
groups -X- _ O
along -X- _ O
the -X- _ O
channel -X- _ O
axis -X- _ O
and -X- _ O
perform -X- _ O
whitening -X- _ O
independently -X- _ O
within -X- _ O
each -X- _ O
group -X- _ O
. -X- _ O
By -X- _ O
shuffling -X- _ O
the -X- _ O
group -X- _ O
division -X- _ O
, -X- _ O
we -X- _ O
derive -X- _ O
multiple -X- _ O
distortions -X- _ O
of -X- _ O
a -X- _ O
single -X- _ O
sample -X- _ O
and -X- _ O
thus -X- _ O
increase -X- _ O
the -X- _ O
positive -X- _ O
sample -X- _ O
diversity -X- _ O
. -X- _ O
Consequently -X- _ O
, -X- _ O
using -X- _ O
multiple -X- _ O
positive -X- _ O
samples -X- _ O
with -X- _ O
enhanced -X- _ O
diversity -X- _ O
further -X- _ O
improves -X- _ O
contrastive -X- _ O
learning -X- _ O
due -X- _ O
to -X- _ O
better -X- _ O
alignment -X- _ B-MetricName
. -X- _ O
Extensive -X- _ O
experiments -X- _ O
on -X- _ O
seven -X- _ O
semantic -X- _ O
textual -X- _ O
similarity -X- _ O
tasks -X- _ O
show -X- _ O
our -X- _ O
method -X- _ O
achieves -X- _ O
consistent -X- _ O
improvement -X- _ O
over -X- _ O
the -X- _ O
contrastive -X- _ O
learning -X- _ O
baseline -X- _ O
and -X- _ O
sets -X- _ O
new -X- _ O
states -X- _ O
of -X- _ O
the -X- _ O
art -X- _ O
, -X- _ O
e.g. -X- _ O
, -X- _ O
78.78 -X- _ B-MetricValue
% -X- _ I-MetricValue
( -X- _ O
+2.53 -X- _ B-MetricValue
% -X- _ I-MetricValue
based -X- _ O
on -X- _ O
BERT -X- _ B-MethodName
base -X- _ I-MethodName
) -X- _ O
Spearman -X- _ B-MetricName
correlation -X- _ I-MetricName
on -X- _ O
STS -X- _ B-TaskName
tasks -X- _ O
. -X- _ O
1 -X- _ O

Introduction -X- _ O

This -X- _ O
paper -X- _ O
considers -X- _ O
self -X- _ B-TaskName
- -X- _ I-TaskName
supervised -X- _ I-TaskName
sentence -X- _ I-TaskName
representation -X- _ I-TaskName
( -X- _ I-TaskName
embedding -X- _ I-TaskName
) -X- _ I-TaskName
learning -X- _ I-TaskName
. -X- _ O
It -X- _ O
is -X- _ O
a -X- _ O
fundamental -X- _ O
task -X- _ O
in -X- _ O
language -X- _ O
processing -X- _ O
( -X- _ O
NLP -X- _ O
) -X- _ O
and -X- _ O
can -X- _ O
†Corresponding -X- _ O
author -X- _ O
. -X- _ O

1 -X- _ O
Our -X- _ O
code -X- _ O
will -X- _ O
be -X- _ O
available -X- _ O
at -X- _ O
https -X- _ O
: -X- _ O
/ -X- _ O
/ -X- _ O
github.com -X- _ O
/ -X- _ O
SupstarZh -X- _ O
/ -X- _ O
WhitenedCSE -X- _ B-MethodName
. -X- _ O
Meanwhile -X- _ O
, -X- _ O
in -X- _ O
( -X- _ O
d -X- _ O
) -X- _ O
, -X- _ O
the -X- _ O
positive -X- _ O
samples -X- _ O
after -X- _ O
SGW -X- _ B-MethodName
( -X- _ O
red -X- _ O
) -X- _ O
obtain -X- _ O
higher -X- _ O
diversity -X- _ O
than -X- _ O
the -X- _ O
original -X- _ O
bert -X- _ O
features -X- _ O
( -X- _ O
green -X- _ O
) -X- _ O
. -X- _ O
Using -X- _ O
these -X- _ O
diverse -X- _ O
positive -X- _ O
samples -X- _ O
for -X- _ O
contrastive -X- _ O
learning -X- _ O
, -X- _ O
the -X- _ O
proposed -X- _ O
WhitenedCSE -X- _ B-MethodName
achieves -X- _ O
better -X- _ O
alignment -X- _ B-MetricName
. -X- _ O

benefit -X- _ O
a -X- _ O
wide -X- _ O
range -X- _ O
of -X- _ O
downstream -X- _ O
tasks -X- _ O
( -X- _ O
Qiao -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2016 -X- _ O
; -X- _ O
Le -X- _ O
and -X- _ O
Mikolov -X- _ O
, -X- _ O
2014 -X- _ O
; -X- _ O
Lan -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
; -X- _ O
Logeswaran -X- _ O
and -X- _ O
Lee -X- _ O
, -X- _ O
2018 -X- _ O
) -X- _ O
. -X- _ O
Two -X- _ O
characteristics -X- _ O
matter -X- _ O
for -X- _ O
sentence -X- _ B-TaskName
embeddings -X- _ I-TaskName
, -X- _ O
i.e. -X- _ O
, -X- _ O
uniformity -X- _ B-MetricName
( -X- _ O
of -X- _ O
the -X- _ O
overall -X- _ O
feature -X- _ O
distribution -X- _ O
) -X- _ O
and -X- _ O
alignment -X- _ B-MetricName
( -X- _ O
of -X- _ O
the -X- _ O
positive -X- _ O
samples -X- _ O
) -X- _ O
, -X- _ O
according -X- _ O
to -X- _ O
a -X- _ O
common -X- _ O
sense -X- _ O
in -X- _ O
deep -X- _ O
representation -X- _ O
learning -X- _ O
( -X- _ O
Wang -X- _ O
and -X- _ O
Isola -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
. -X- _ O
Alignment -X- _ B-MetricName
expects -X- _ O
minimal -X- _ O
distance -X- _ O
between -X- _ O
positive -X- _ O
pairs -X- _ O
, -X- _ O
while -X- _ O
uniformity -X- _ B-MetricName
expects -X- _ O
the -X- _ O
features -X- _ O
are -X- _ O
uniformly -X- _ O
distributed -X- _ O
in -X- _ O
the -X- _ O
representation -X- _ O
space -X- _ O
in -X- _ O
overall -X- _ O
. -X- _ O
From -X- _ O
this -X- _ O
viewpoint -X- _ O
, -X- _ O
the -X- _ O
popular -X- _ O
masked -X- _ O
language -X- _ O
modeling -X- _ O
( -X- _ O
MLM -X- _ O
) -X- _ O
( -X- _ O
Devlin -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2018 -X- _ O
; -X- _ O
Liu -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
; -X- _ O
Brown -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020b -X- _ O
; -X- _ O
Reimers -X- _ O
and -X- _ O
Gurevych -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
is -X- _ O
not -X- _ O
an -X- _ O
optimal -X- _ O
choice -X- _ O
for -X- _ O
sentence -X- _ B-TaskName
embedding -X- _ I-TaskName
: -X- _ O
MLM -X- _ O
methods -X- _ O
do -X- _ O
not -X- _ O
explicitly -X- _ O
enforce -X- _ O
the -X- _ O
objective -X- _ O
of -X- _ O
uniformity -X- _ B-MetricName
and -X- _ O
alignment -X- _ B-MetricName
and -X- _ O
thus -X- _ O
do -X- _ O
not -X- _ O
quite -X- _ O
fit -X- _ O
the -X- _ O
objective -X- _ O
of -X- _ O
sentence -X- _ B-TaskName
representation -X- _ I-TaskName
learning -X- _ I-TaskName
. -X- _ O

To -X- _ O
improve -X- _ O
the -X- _ O
uniformity -X- _ B-MetricName
as -X- _ O
well -X- _ O
as -X- _ O
the -X- _ O
alignment -X- _ B-MetricName
, -X- _ O
there -X- _ O
are -X- _ O
two -X- _ O
popular -X- _ O
approaches -X- _ O
, -X- _ O
i.e. -X- _ O
, -X- _ O
contrastive -X- _ O
learning -X- _ O
and -X- _ O
post -X- _ O
- -X- _ O
processing -X- _ O
. -X- _ O
1 -X- _ O
) -X- _ O
The -X- _ O
contrastive -X- _ O
learning -X- _ O
methods -X- _ O
( -X- _ O
Yan -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021 -X- _ O
; -X- _ O
Kim -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021 -X- _ O
; -X- _ O
pulls -X- _ O
similar -X- _ O
sentences -X- _ O
close -X- _ O
to -X- _ O
each -X- _ O
other -X- _ O
and -X- _ O
pushes -X- _ O
dissimilar -X- _ O
sentences -X- _ O
far -X- _ O
- -X- _ O
away -X- _ O
in -X- _ O
the -X- _ O
latent -X- _ O
feature -X- _ O
space -X- _ O
. -X- _ O
Pulling -X- _ O
similar -X- _ O
sentences -X- _ O
close -X- _ O
directly -X- _ O
enforces -X- _ O
alignment -X- _ B-MetricName
, -X- _ O
while -X- _ O
pushing -X- _ O
dissimilar -X- _ O
sentences -X- _ O
apart -X- _ O
implicitly -X- _ O
enforces -X- _ O
uniformity -X- _ B-MetricName
( -X- _ O
Wang -X- _ O
and -X- _ O
Isola -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
. -X- _ O
2 -X- _ O
) -X- _ O
In -X- _ O
contrast -X- _ O
, -X- _ O
the -X- _ O
postprocessing -X- _ O
methods -X- _ O
mainly -X- _ O
focus -X- _ O
on -X- _ O
improving -X- _ O
the -X- _ O
uniformity -X- _ B-MetricName
. -X- _ O
They -X- _ O
use -X- _ O
normalizing -X- _ O
flows -X- _ O
( -X- _ O
Li -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
or -X- _ O
whitening -X- _ O
operation -X- _ O
( -X- _ O
Su -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O
) -X- _ O
to -X- _ O
project -X- _ O
the -X- _ O
already -X- _ O
- -X- _ O
learned -X- _ O
representations -X- _ O
into -X- _ O
an -X- _ O
isotropic -X- _ O
space -X- _ O
. -X- _ O
In -X- _ O
other -X- _ O
words -X- _ O
, -X- _ O
these -X- _ O
methods -X- _ O
scatter -X- _ O
all -X- _ O
the -X- _ O
samples -X- _ O
into -X- _ O
the -X- _ O
feature -X- _ O
space -X- _ O
and -X- _ O
thus -X- _ O
improve -X- _ O
the -X- _ O
uniformity -X- _ B-MetricName
. -X- _ O

In -X- _ O
this -X- _ O
paper -X- _ O
, -X- _ O
we -X- _ O
propose -X- _ O
a -X- _ O
whitening -X- _ B-MethodName
- -X- _ I-MethodName
based -X- _ I-MethodName
contrastive -X- _ I-MethodName
learning -X- _ I-MethodName
method -X- _ I-MethodName
for -X- _ I-MethodName
sentence -X- _ I-MethodName
representation -X- _ I-MethodName
learning -X- _ I-MethodName
( -X- _ O
WhitenedCSE -X- _ B-MethodName
) -X- _ O
. -X- _ O
For -X- _ O
the -X- _ O
first -X- _ O
time -X- _ O
, -X- _ O
we -X- _ O
integrate -X- _ O
whitening -X- _ O
into -X- _ O
the -X- _ O
contrastive -X- _ O
learning -X- _ O
scheme -X- _ O
and -X- _ O
demonstrate -X- _ O
substantial -X- _ O
improvement -X- _ O
. -X- _ O
Specifically -X- _ O
, -X- _ O
WhitenedCSE -X- _ B-MethodName
combines -X- _ O
contrastive -X- _ O
learning -X- _ O
with -X- _ O
a -X- _ O
novel -X- _ O
Shuffled -X- _ B-MethodName
Group -X- _ I-MethodName
Whitening -X- _ I-MethodName
( -X- _ O
SGW -X- _ B-MethodName
) -X- _ O
. -X- _ O
Given -X- _ O
a -X- _ O
backbone -X- _ O
feature -X- _ O
, -X- _ O
SGW -X- _ B-MethodName
randomly -X- _ O
divides -X- _ O
the -X- _ O
feature -X- _ O
into -X- _ O
multiple -X- _ O
groups -X- _ O
along -X- _ O
the -X- _ O
channel -X- _ O
axis -X- _ O
and -X- _ O
perform -X- _ O
whitening -X- _ O
independently -X- _ O
within -X- _ O
each -X- _ O
group -X- _ O
. -X- _ O
The -X- _ O
whitened -X- _ O
features -X- _ O
are -X- _ O
then -X- _ O
fed -X- _ O
into -X- _ O
the -X- _ O
contrastive -X- _ O
loss -X- _ O
for -X- _ O
optimization -X- _ O
. -X- _ O

Although -X- _ O
the -X- _ O
canonical -X- _ O
whitening -X- _ O
( -X- _ O
or -X- _ O
group -X- _ O
whitening -X- _ O
) -X- _ O
is -X- _ O
only -X- _ O
beneficial -X- _ O
for -X- _ O
uniformity -X- _ B-MetricName
, -X- _ O
SGW -X- _ B-MethodName
in -X- _ O
WhitenedCSE -X- _ B-MethodName
improves -X- _ O
not -X- _ O
only -X- _ O
the -X- _ O
uniformity -X- _ B-MetricName
but -X- _ O
also -X- _ O
the -X- _ O
alignment -X- _ B-MetricName
. -X- _ O
We -X- _ O
explain -X- _ O
these -X- _ O
two -X- _ O
benefits -X- _ O
in -X- _ O
details -X- _ O
as -X- _ O
below -X- _ O
: -X- _ O

• -X- _ O
Better -X- _ O
uniformity -X- _ B-MetricName
. -X- _ O
We -X- _ O
notice -X- _ O
that -X- _ O
the -X- _ O
pushing -X- _ O
effect -X- _ O
in -X- _ O
contrastive -X- _ O
learning -X- _ O
and -X- _ O
the -X- _ O
scattering -X- _ O
effect -X- _ O
in -X- _ O
the -X- _ O
whitening -X- _ O
have -X- _ O
large -X- _ O
redundancy -X- _ O
to -X- _ O
each -X- _ O
other -X- _ O
, -X- _ O
because -X- _ O
they -X- _ O
both -X- _ O
facilitate -X- _ O
the -X- _ O
uniformity -X- _ B-MetricName
. -X- _ O
This -X- _ O
redundancy -X- _ O
is -X- _ O
arguably -X- _ O
the -X- _ O
reason -X- _ O
why -X- _ O
no -X- _ O
prior -X- _ O
literature -X- _ O
tries -X- _ O
to -X- _ O
combine -X- _ O
them -X- _ O
. -X- _ O
Under -X- _ O
this -X- _ O
background -X- _ O
, -X- _ O
our -X- _ O
finding -X- _ O
i.e. -X- _ O
, -X- _ O
these -X- _ O
two -X- _ O
approaches -X- _ O
are -X- _ O
not -X- _ O
totally -X- _ O
redundant -X- _ O
but -X- _ O
actually -X- _ O
have -X- _ O
some -X- _ O
complementarity -X- _ O
is -X- _ O
non -X- _ O
- -X- _ O
trivial -X- _ O
. -X- _ O
We -X- _ O
think -X- _ O
such -X- _ O
complemenetarity -X- _ O
is -X- _ O
because -X- _ O
these -X- _ O
two -X- _ O
approaches -X- _ O
have -X- _ O
different -X- _ O
uniformity -X- _ B-MetricName
mechanism -X- _ O
and -X- _ O
will -X- _ O
discuss -X- _ O
the -X- _ O
differences -X- _ O
in -X- _ O
Section -X- _ O
3.2.3 -X- _ O
. -X- _ O
In -X- _ O
Fig -X- _ O
. -X- _ O
1 -X- _ O
, -X- _ O
we -X- _ O
observe -X- _ O
that -X- _ O
while -X- _ O
the -X- _ O
contrastive -X- _ O
learning -X- _ O
( -X- _ O
Fig -X- _ O
. -X- _ O
1 -X- _ O
( -X- _ O
b -X- _ O
) -X- _ O
) -X- _ O
already -X- _ O
improves -X- _ O
the -X- _ O
uniformity -X- _ B-MetricName
over -X- _ O
the -X- _ O
original -X- _ O
bert -X- _ O
features -X- _ O
( -X- _ O
Fig -X- _ O
. -X- _ O
1 -X- _ O
( -X- _ O
a -X- _ O
) -X- _ O
) -X- _ O
, -X- _ O
applying -X- _ O
whitening -X- _ O
( -X- _ O
Fig -X- _ O
. -X- _ O
1 -X- _ O
( -X- _ O
c -X- _ O
) -X- _ O
) -X- _ O
brings -X- _ O
another -X- _ O
round -X- _ O
of -X- _ O
uniformity -X- _ B-MetricName
improvement -X- _ O
. -X- _ O

• -X- _ O
Better -X- _ O
alignment -X- _ B-MetricName
. -X- _ O
In -X- _ O
the -X- _ O
proposed -X- _ O
Whitened -X- _ B-MethodName
- -X- _ I-MethodName
CSE -X- _ I-MethodName
, -X- _ O
SGW -X- _ B-MethodName
is -X- _ O
featured -X- _ O
for -X- _ O
its -X- _ O
shuffled -X- _ O
grouping -X- _ O
operation -X- _ O
, -X- _ O
i.e. -X- _ O
, -X- _ O
randomly -X- _ O
dividing -X- _ O
a -X- _ O
backbone -X- _ O
feature -X- _ O
into -X- _ O
multiple -X- _ O
groups -X- _ O
before -X- _ O
whitening -X- _ O
. -X- _ O
Therefore -X- _ O
, -X- _ O
given -X- _ O
a -X- _ O
same -X- _ O
backbone -X- _ O
feature -X- _ O
, -X- _ O
we -X- _ O
may -X- _ O
repeat -X- _ O
SGW -X- _ B-MethodName
multiple -X- _ O
times -X- _ O
to -X- _ O
get -X- _ O
different -X- _ O
grouping -X- _ O
results -X- _ O
, -X- _ O
and -X- _ O
then -X- _ O
different -X- _ O
whitened -X- _ O
features -X- _ O
. -X- _ O
These -X- _ O
" -X- _ O
duplicated -X- _ O
" -X- _ O
features -X- _ O
are -X- _ O
different -X- _ O
from -X- _ O
each -X- _ O
other -X- _ O
and -X- _ O
thus -X- _ O
increase -X- _ O
the -X- _ O
diversity -X- _ O
of -X- _ O
positive -X- _ O
samples -X- _ O
, -X- _ O
as -X- _ O
shown -X- _ O
in -X- _ O
Fig -X- _ O
. -X- _ O
1 -X- _ O
( -X- _ O
d -X- _ O
) -X- _ O
. -X- _ O
Using -X- _ O
these -X- _ O
diverse -X- _ O
positive -X- _ O
samples -X- _ O
for -X- _ O
contrastive -X- _ O
learning -X- _ O
, -X- _ O
WhitenedCSE -X- _ B-MethodName
improves -X- _ O
the -X- _ O
alignment -X- _ B-MetricName
. -X- _ O

Another -X- _ O
important -X- _ O
advantage -X- _ O
of -X- _ O
SGW -X- _ B-MethodName
is -X- _ O
: -X- _ O
since -X- _ O
it -X- _ O
is -X- _ O
applied -X- _ O
onto -X- _ O
the -X- _ O
backbone -X- _ O
features -X- _ O
, -X- _ O
it -X- _ O
incurs -X- _ O
very -X- _ O
slight -X- _ O
computational -X- _ O
overhead -X- _ O
for -X- _ O
generating -X- _ O
additional -X- _ O
positive -X- _ O
samples -X- _ O
. -X- _ O
This -X- _ O
high -X- _ O
efficiency -X- _ O
allows -X- _ O
WhitenedCSE -X- _ B-MethodName
to -X- _ O
increase -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
positive -X- _ O
samples -X- _ O
( -X- _ O
more -X- _ O
than -X- _ O
common -X- _ O
setting -X- _ O
of -X- _ O
2 -X- _ O
) -X- _ O
in -X- _ O
a -X- _ O
mini -X- _ O
- -X- _ O
batch -X- _ O
with -X- _ O
little -X- _ O
cost -X- _ O
. -X- _ O
Ablation -X- _ O
study -X- _ O
shows -X- _ O
that -X- _ O
the -X- _ O
enlarged -X- _ O
positive -X- _ O
- -X- _ O
sample -X- _ O
number -X- _ O
brings -X- _ O
a -X- _ O
further -X- _ O
benefit -X- _ O
. -X- _ O

Our -X- _ O
contributions -X- _ O
are -X- _ O
summarized -X- _ O
as -X- _ O
follows -X- _ O
: -X- _ O

( -X- _ O
1 -X- _ O
) -X- _ O
We -X- _ O
propose -X- _ O
WhitenedCSE -X- _ B-MethodName
for -X- _ O
the -X- _ O
selfsupervised -X- _ B-TaskName
sentence -X- _ I-TaskName
representation -X- _ I-TaskName
learning -X- _ I-TaskName
task -X- _ O
. -X- _ O
WhitenedCSE -X- _ B-MethodName
combines -X- _ O
the -X- _ O
contrastive -X- _ O
learning -X- _ O
with -X- _ O
a -X- _ O
novel -X- _ O
Shuffled -X- _ B-MethodName
Group -X- _ I-MethodName
Whitening -X- _ I-MethodName
( -X- _ O
SGW -X- _ B-MethodName
) -X- _ O
. -X- _ O

( -X- _ O
2 -X- _ O
) -X- _ O
We -X- _ O
show -X- _ O
that -X- _ O
through -X- _ O
SGW -X- _ B-MethodName
, -X- _ O
WhitenedCSE -X- _ B-MethodName
improves -X- _ O
not -X- _ O
only -X- _ O
the -X- _ O
uniformity -X- _ B-MetricName
but -X- _ O
also -X- _ O
the -X- _ O
alignment -X- _ B-MetricName
. -X- _ O
Moreover -X- _ O
, -X- _ O
SGW -X- _ B-MethodName
enables -X- _ O
efficient -X- _ O
multipositive -X- _ O
training -X- _ O
, -X- _ O
which -X- _ O
is -X- _ O
also -X- _ O
beneficial -X- _ O
. -X- _ O

( -X- _ O
3 -X- _ O
) -X- _ O
We -X- _ O
evaluate -X- _ O
our -X- _ O
method -X- _ O
on -X- _ O
seven -X- _ O
semantic -X- _ B-TaskName
textual -X- _ I-TaskName
similarity -X- _ I-TaskName
tasks -X- _ O
and -X- _ O
seven -X- _ O
transfer -X- _ B-TaskName
tasks -X- _ O
. -X- _ O
Experimental -X- _ O
results -X- _ O
show -X- _ O
that -X- _ O
WhitenedCSE -X- _ B-MethodName
brings -X- _ O
consistent -X- _ O
improvement -X- _ O
over -X- _ O
the -X- _ O
contrastive -X- _ O
learning -X- _ O
baseline -X- _ O
and -X- _ O
sets -X- _ O
new -X- _ O
states -X- _ O
of -X- _ O
the -X- _ O
art -X- _ O
. -X- _ O

Related -X- _ O
Work -X- _ O

Sentence -X- _ B-TaskName
Representation -X- _ I-TaskName
Learning -X- _ I-TaskName

As -X- _ O
a -X- _ O
fundamental -X- _ O
task -X- _ O
in -X- _ O
natural -X- _ O
language -X- _ O
processing -X- _ O
, -X- _ O
sentence -X- _ B-TaskName
representation -X- _ I-TaskName
learning -X- _ I-TaskName
has -X- _ O
been -X- _ O
extensively -X- _ O
studied -X- _ O
. -X- _ O
Early -X- _ O
works -X- _ O
mainly -X- _ O
based -X- _ O
on -X- _ O
bag -X- _ O
- -X- _ O
of -X- _ O
- -X- _ O
words -X- _ O
( -X- _ O
Wu -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2010 -X- _ O
; -X- _ O
Tsai -X- _ O
, -X- _ O
2012 -X- _ O
) -X- _ O
or -X- _ O
context -X- _ O
prediction -X- _ O
tasks -X- _ O
( -X- _ O
Kiros -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2015 -X- _ O
; -X- _ O
Hill -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2016 -X- _ O
) -X- _ O
, -X- _ O
etc -X- _ O
. -X- _ O
Recently -X- _ O
, -X- _ O
with -X- _ O
the -X- _ O
advent -X- _ O
of -X- _ O
pretrained -X- _ O
language -X- _ O
model -X- _ O
( -X- _ O
Devlin -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2018 -X- _ O
; -X- _ O
Liu -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
; -X- _ O
Brown -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020a -X- _ O
) -X- _ O
, -X- _ O
many -X- _ O
works -X- _ O
tend -X- _ O
to -X- _ O
directly -X- _ O
use -X- _ O
PLMs -X- _ O
, -X- _ O
such -X- _ O
as -X- _ O
BERT -X- _ O
( -X- _ O
Devlin -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2018 -X- _ O
) -X- _ O
, -X- _ O
to -X- _ O
generate -X- _ O
sentence -X- _ O
representations -X- _ O
. -X- _ O
However -X- _ O
, -X- _ O
some -X- _ O
studies -X- _ O
( -X- _ O
Ethayarajh -X- _ O
, -X- _ O
2019 -X- _ O
; -X- _ O
Yan -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O
found -X- _ O
that -X- _ O
directly -X- _ O
use -X- _ O
the -X- _ O
[ -X- _ O
CLS -X- _ O
] -X- _ O
representation -X- _ O
or -X- _ O
the -X- _ O
average -X- _ O
pooling -X- _ O
of -X- _ O
token -X- _ O
embeddings -X- _ O
at -X- _ O
the -X- _ O
last -X- _ O
layer -X- _ O
will -X- _ O
suffer -X- _ O
from -X- _ O
anisotropy -X- _ O
problem -X- _ O
, -X- _ O
i.e. -X- _ O
, -X- _ O
the -X- _ O
learned -X- _ O
embeddings -X- _ O
are -X- _ O
collasped -X- _ O
into -X- _ O
a -X- _ O
small -X- _ O
area -X- _ O
. -X- _ O
To -X- _ O
alleviate -X- _ O
this -X- _ O
problem -X- _ O
, -X- _ O
BERTflow -X- _ O
( -X- _ O
Li -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
adopts -X- _ O
a -X- _ O
standardized -X- _ O
flow -X- _ O
transformation -X- _ O
while -X- _ O
BERT -X- _ O
- -X- _ O
Whitening -X- _ O
( -X- _ O
Su -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O
adopts -X- _ O
a -X- _ O
whitening -X- _ O
transformation -X- _ O
, -X- _ O
both -X- _ O
of -X- _ O
them -X- _ O
transform -X- _ O
the -X- _ O
representation -X- _ O
space -X- _ O
to -X- _ O
a -X- _ O
smooth -X- _ O
and -X- _ O
isotropic -X- _ O
space -X- _ O
. -X- _ O
Most -X- _ O
recently -X- _ O
, -X- _ O
contrastive -X- _ O
learning -X- _ O
( -X- _ O
Chen -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020 -X- _ O
; -X- _ O
has -X- _ O
become -X- _ O
a -X- _ O
powerful -X- _ O
tool -X- _ O
to -X- _ O
obtain -X- _ O
the -X- _ O
sentence -X- _ O
representations -X- _ O
. -X- _ O

Contrastive -X- _ O
Learning -X- _ O

Contrastive -X- _ O
learning -X- _ O
( -X- _ O
Chen -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020 -X- _ O
; -X- _ O
has -X- _ O
achieved -X- _ O
great -X- _ O
success -X- _ O
in -X- _ O
sentence -X- _ O
representation -X- _ O
learning -X- _ O
tasks -X- _ O
Yan -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021 -X- _ O
; -X- _ O
Kim -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021 -X- _ O
; -X- _ O
. -X- _ O
It -X- _ O
pulls -X- _ O
semantically -X- _ O
similar -X- _ O
samples -X- _ O
together -X- _ O
, -X- _ O
and -X- _ O
pushes -X- _ O
the -X- _ O
dissimilar -X- _ O
samples -X- _ O
away -X- _ O
, -X- _ O
which -X- _ O
can -X- _ O
be -X- _ O
formulated -X- _ O
as -X- _ O
: -X- _ O

L -X- _ O
cl -X- _ O
= -X- _ O
−log -X- _ O
e -X- _ O
sim -X- _ O
( -X- _ O
h -X- _ O
i -X- _ O
, -X- _ O
h -X- _ O
* -X- _ O
i -X- _ O
) -X- _ O
/ -X- _ O
τ -X- _ O
n -X- _ O
j=1 -X- _ O
e -X- _ O
sim -X- _ O
( -X- _ O
h -X- _ O
i -X- _ O
, -X- _ O
h -X- _ O
* -X- _ O
j -X- _ O
) -X- _ O
/ -X- _ O
τ -X- _ O
( -X- _ O
1 -X- _ O
) -X- _ O

where -X- _ O
τ -X- _ O
is -X- _ O
a -X- _ O
temperature -X- _ O
hyperparameter -X- _ O
, -X- _ O
h -X- _ O
* -X- _ O
i -X- _ O
, -X- _ O
h -X- _ O
* -X- _ O
j -X- _ O
are -X- _ O
the -X- _ O
positive -X- _ O
sample -X- _ O
and -X- _ O
negative -X- _ O
samples -X- _ O
respectively -X- _ O
. -X- _ O
Recently -X- _ O
, -X- _ O
alignment -X- _ O
and -X- _ O
uniformity -X- _ O
( -X- _ O
Wang -X- _ O
and -X- _ O
Isola -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
are -X- _ O
proposed -X- _ O
to -X- _ O
measure -X- _ O
the -X- _ O
quality -X- _ O
of -X- _ O
representations -X- _ O
. -X- _ O
Alignment -X- _ O
measures -X- _ O
whether -X- _ O
the -X- _ O
distance -X- _ O
between -X- _ O
positive -X- _ O
samples -X- _ O
is -X- _ O
close -X- _ O
, -X- _ O
while -X- _ O
uniformity -X- _ O
measures -X- _ O
the -X- _ O
dispersion -X- _ O
of -X- _ O
embedding -X- _ O
in -X- _ O
vector -X- _ O
space -X- _ O
. -X- _ O
A -X- _ O
typical -X- _ O
method -X- _ O
called -X- _ O
SimCSE -X- _ O
uses -X- _ O
dropout -X- _ O
as -X- _ O
a -X- _ O
feature -X- _ O
- -X- _ O
wise -X- _ O
data -X- _ O
augmentation -X- _ O
to -X- _ O
construct -X- _ O
the -X- _ O
positive -X- _ O
sample -X- _ O
, -X- _ O
and -X- _ O
randomly -X- _ O
sample -X- _ O
negatives -X- _ O
from -X- _ O
the -X- _ O
batch -X- _ O
, -X- _ O
which -X- _ O
can -X- _ O
achieve -X- _ O
a -X- _ O
great -X- _ O
balance -X- _ O
between -X- _ O
alignment -X- _ O
and -X- _ O
uniformity -X- _ O
. -X- _ O
Some -X- _ O
new -X- _ O
works -X- _ O
further -X- _ O
improved -X- _ O
the -X- _ O
quality -X- _ O
of -X- _ O
sentence -X- _ O
representations -X- _ O
based -X- _ O
on -X- _ O
SimCSE -X- _ O
, -X- _ O
such -X- _ O
as -X- _ O
ESimCSE -X- _ O
, -X- _ O
MixCSE -X- _ O
( -X- _ O
Zhang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2022a -X- _ O
) -X- _ O
and -X- _ O
VaSCL -X- _ O
( -X- _ O
Zhang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021b -X- _ O
) -X- _ O
, -X- _ O
each -X- _ O
of -X- _ O
them -X- _ O
proposed -X- _ O
a -X- _ O
new -X- _ O
data -X- _ O
augmentation -X- _ O
strategy -X- _ O
to -X- _ O
construct -X- _ O
the -X- _ O
positive -X- _ O
pair -X- _ O
. -X- _ O
Besides -X- _ O
, -X- _ O
DCLR -X- _ O
( -X- _ O
Zhou -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2022 -X- _ O
) -X- _ O
focus -X- _ O
on -X- _ O
optimizing -X- _ O
the -X- _ O
strategy -X- _ O
of -X- _ O
sampling -X- _ O
negatives -X- _ O
, -X- _ O
and -X- _ O
ArcCSE -X- _ O
( -X- _ O
Zhang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2022b -X- _ O
) -X- _ O
optimized -X- _ O
the -X- _ O
objective -X- _ O
function -X- _ O
, -X- _ O
etc -X- _ O
. -X- _ O
In -X- _ O
this -X- _ O
paper -X- _ O
, -X- _ O
we -X- _ O
find -X- _ O
that -X- _ O
contrastive -X- _ O
learning -X- _ O
can -X- _ O
be -X- _ O
further -X- _ O
combined -X- _ O
with -X- _ O
whitening -X- _ O
to -X- _ O
obtain -X- _ O
better -X- _ O
sentence -X- _ O
representations -X- _ O
. -X- _ O

Whitening -X- _ O
Transformation -X- _ O

In -X- _ O
computer -X- _ O
vision -X- _ O
, -X- _ O
recent -X- _ O
works -X- _ O
( -X- _ O
Ermolov -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021 -X- _ O
; -X- _ O
Zhang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021c -X- _ O
; -X- _ O
Hua -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O
use -X- _ O
whitening -X- _ O
transformation -X- _ O
as -X- _ O
an -X- _ O
alternative -X- _ O
method -X- _ O
to -X- _ O
the -X- _ O
" -X- _ O
pushing -X- _ O
negatives -X- _ O
away -X- _ O
" -X- _ O
operation -X- _ O
in -X- _ O
contrastive -X- _ O
learning -X- _ O
to -X- _ O
disperse -X- _ O
the -X- _ O
data -X- _ O
uniformly -X- _ O
throughout -X- _ O
the -X- _ O
spherical -X- _ O
space -X- _ O
( -X- _ O
i.e. -X- _ O
, -X- _ O
the -X- _ O
feature -X- _ O
space -X- _ O
) -X- _ O
, -X- _ O
and -X- _ O
then -X- _ O
pull -X- _ O
the -X- _ O
positive -X- _ O
samples -X- _ O
together -X- _ O
, -X- _ O
which -X- _ O
have -X- _ O
achieved -X- _ O
great -X- _ O
success -X- _ O
in -X- _ O
unsupervised -X- _ O
representation -X- _ O
learning -X- _ O
. -X- _ O

Whitening -X- _ O
( -X- _ O
aka -X- _ O
. -X- _ O
, -X- _ O
sphering -X- _ O
) -X- _ O
is -X- _ O
a -X- _ O
common -X- _ O
transformation -X- _ O
that -X- _ O
transforms -X- _ O
a -X- _ O
set -X- _ O
of -X- _ O
variables -X- _ O
into -X- _ O
a -X- _ O
new -X- _ O
set -X- _ O
of -X- _ O
isotropic -X- _ O
variables -X- _ O
, -X- _ O
and -X- _ O
makes -X- _ O
the -X- _ O
covariance -X- _ O
matrix -X- _ O
of -X- _ O
whitened -X- _ O
variables -X- _ O
equal -X- _ O
to -X- _ O
the -X- _ O
identity -X- _ O
matrix -X- _ O
. -X- _ O
In -X- _ O
natural -X- _ O
language -X- _ O
processing -X- _ O
, -X- _ O
Su -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2021 -X- _ O
) -X- _ O
use -X- _ O
whitening -X- _ O
as -X- _ O
a -X- _ O
post -X- _ O
- -X- _ O
processing -X- _ O
method -X- _ O
to -X- _ O
alleviate -X- _ O
the -X- _ O
anisotropic -X- _ O
problem -X- _ O
in -X- _ O
pretrained -X- _ O
language -X- _ O
models -X- _ O
. -X- _ O
In -X- _ O
this -X- _ O
paper -X- _ O
, -X- _ O
we -X- _ O
use -X- _ O
whitening -X- _ O
as -X- _ O
an -X- _ O
explicit -X- _ O
operation -X- _ O
to -X- _ O
further -X- _ O
improve -X- _ O
the -X- _ O
uniformity -X- _ O
of -X- _ O
the -X- _ O
representation -X- _ O
space -X- _ O
, -X- _ O
and -X- _ O
further -X- _ O
explore -X- _ O
the -X- _ O
potential -X- _ O
of -X- _ O
whitening -X- _ O
in -X- _ O
improving -X- _ O
alignment -X- _ O
, -X- _ O
so -X- _ O
as -X- _ O
to -X- _ O
obtain -X- _ O
a -X- _ O
better -X- _ O
sentence -X- _ O
representation -X- _ O
model -X- _ O
. -X- _ O

Methods -X- _ O

In -X- _ O
this -X- _ O
section -X- _ O
, -X- _ O
we -X- _ O
first -X- _ O
describe -X- _ O
the -X- _ O
overall -X- _ O
architecture -X- _ O
of -X- _ O
WhitenedCSE -X- _ B-MethodName
and -X- _ O
then -X- _ O
present -X- _ O
the -X- _ O
details -X- _ O
of -X- _ O
all -X- _ O
the -X- _ O
modules -X- _ O
, -X- _ O
including -X- _ O
the -X- _ O
shuffled -X- _ B-MethodName
group -X- _ I-MethodName
whitening -X- _ I-MethodName
module -X- _ O
and -X- _ O
the -X- _ O
new -X- _ O
contrastive -X- _ O
learning -X- _ O
module -X- _ O
. -X- _ O

General -X- _ O
Framework -X- _ O

As -X- _ O
shown -X- _ O
in -X- _ O
Fig -X- _ O
. -X- _ O
2 -X- _ O
, -X- _ O
WhitenedCSE -X- _ B-MethodName
has -X- _ O
three -X- _ O
major -X- _ O
components -X- _ O
: -X- _ O

• -X- _ O
An -X- _ O
BERT -X- _ O
- -X- _ O
like -X- _ O
encoder -X- _ O
, -X- _ O
which -X- _ O
we -X- _ O
use -X- _ O
to -X- _ O
extract -X- _ O
features -X- _ O
from -X- _ O
native -X- _ O
sentences -X- _ O
, -X- _ O
and -X- _ O
take -X- _ O
the -X- _ O
[ -X- _ O
CLS -X- _ O
] -X- _ O
token -X- _ O
as -X- _ O
our -X- _ O
native -X- _ O
sentence -X- _ O
representations -X- _ O
. -X- _ O

• -X- _ O
Shuffled -X- _ O
- -X- _ O
group -X- _ O
- -X- _ O
whitening -X- _ O
module -X- _ O
, -X- _ O
we -X- _ O
use -X- _ O
it -X- _ O
as -X- _ O
a -X- _ O
complementary -X- _ O
module -X- _ O
to -X- _ O
contrastive -X- _ O
learning -X- _ O
to -X- _ O
further -X- _ O
improve -X- _ O
the -X- _ O
uniformity -X- _ O
and -X- _ O
alignment -X- _ O
of -X- _ O
the -X- _ O
representation -X- _ O
space -X- _ O
. -X- _ O

• -X- _ O
Multi -X- _ O
- -X- _ O
positives -X- _ O
contrastive -X- _ O
module -X- _ O
, -X- _ O
in -X- _ O
this -X- _ O
module -X- _ O
, -X- _ O
we -X- _ O
pull -X- _ O
distortions -X- _ O
of -X- _ O
the -X- _ O
representations -X- _ O
close -X- _ O
and -X- _ O
push -X- _ O
the -X- _ O
negative -X- _ O
samples -X- _ O
away -X- _ O
in -X- _ O
the -X- _ O
latent -X- _ O
feature -X- _ O
space -X- _ O
. -X- _ O

Specifically -X- _ O
, -X- _ O
given -X- _ O
a -X- _ O
batch -X- _ O
of -X- _ O
sentences -X- _ O
X -X- _ O
, -X- _ O
WhitenedCSE -X- _ B-MethodName
use -X- _ O
the -X- _ O
feature -X- _ O
encoder -X- _ O
f -X- _ O
θ -X- _ O
( -X- _ O
x -X- _ O
i -X- _ O
, -X- _ O
γ -X- _ O
) -X- _ O
to -X- _ O
map -X- _ O
them -X- _ O
to -X- _ O
a -X- _ O
higher -X- _ O
dimensional -X- _ O
space -X- _ O
, -X- _ O
where -X- _ O
γ -X- _ O
is -X- _ O
a -X- _ O
random -X- _ O
mask -X- _ O
for -X- _ O
dropout -X- _ O
, -X- _ O
then -X- _ O
we -X- _ O
take -X- _ O
the -X- _ O
[ -X- _ O
CLS -X- _ O
] -X- _ O
output -X- _ O
as -X- _ O
the -X- _ O
native -X- _ O
sentence -X- _ O
representations -X- _ O
. -X- _ O
After -X- _ O
this -X- _ O
, -X- _ O
we -X- _ O
feed -X- _ O
the -X- _ O
na- -X- _ O
In -X- _ O
the -X- _ O
middle -X- _ O
column -X- _ O
, -X- _ O
WhitenedCSE -X- _ B-MethodName
consists -X- _ O
of -X- _ O
three -X- _ O
components -X- _ O
, -X- _ O
i.e. -X- _ O
, -X- _ O
1 -X- _ O
) -X- _ O
An -X- _ O
BERTlike -X- _ O
encoder -X- _ O
for -X- _ O
generating -X- _ O
the -X- _ O
backbone -X- _ O
features -X- _ O
from -X- _ O
input -X- _ O
samples -X- _ O
, -X- _ O
2 -X- _ O
) -X- _ O
A -X- _ O
Shuffled -X- _ B-MethodName
Group -X- _ I-MethodName
Whitenning -X- _ I-MethodName
( -X- _ O
SGW -X- _ B-MethodName
) -X- _ O
module -X- _ O
for -X- _ O
scattering -X- _ O
the -X- _ O
backbone -X- _ O
features -X- _ O
and -X- _ O
augmenting -X- _ O
the -X- _ O
positive -X- _ O
feature -X- _ O
diversity -X- _ O
, -X- _ O
and -X- _ O
3 -X- _ O
) -X- _ O
A -X- _ O
multi -X- _ O
- -X- _ O
positive -X- _ O
contrastive -X- _ O
loss -X- _ O
for -X- _ O
optimizing -X- _ O
the -X- _ O
features -X- _ O
. -X- _ O
In -X- _ O
the -X- _ O
left -X- _ O
column -X- _ O
, -X- _ O
when -X- _ O
the -X- _ O
mini -X- _ O
- -X- _ O
batch -X- _ O
flows -X- _ O
through -X- _ O
these -X- _ O
three -X- _ O
components -X- _ O
sequentially -X- _ O
, -X- _ O
the -X- _ O
feature -X- _ O
space -X- _ O
undergoes -X- _ O
" -X- _ O
anisotropy -X- _ O
" -X- _ O
→ -X- _ O
" -X- _ O
good -X- _ O
uniformity -X- _ O
+ -X- _ O
augmented -X- _ O
positives -X- _ O
" -X- _ O
→ -X- _ O
" -X- _ O
pulling -X- _ O
close -X- _ O
the -X- _ O
positives -X- _ O
" -X- _ O
. -X- _ O
The -X- _ O
right -X- _ O
column -X- _ O
illustrates -X- _ O
the -X- _ O
SGW -X- _ B-MethodName
module -X- _ O
in -X- _ O
details -X- _ O
. -X- _ O
Specifically -X- _ O
, -X- _ O
SGW -X- _ B-MethodName
randomly -X- _ O
shuffles -X- _ O
the -X- _ O
backbone -X- _ O
feature -X- _ O
along -X- _ O
the -X- _ O
axis -X- _ O
and -X- _ O
then -X- _ O
divides -X- _ O
the -X- _ O
feature -X- _ O
into -X- _ O
multiple -X- _ O
groups -X- _ O
. -X- _ O
Afterwards -X- _ O
, -X- _ O
SGW -X- _ B-MethodName
whitens -X- _ O
each -X- _ O
group -X- _ O
independently -X- _ O
and -X- _ O
re -X- _ O
- -X- _ O
shuffles -X- _ O
the -X- _ O
whitened -X- _ O
feature -X- _ O
. -X- _ O
Given -X- _ O
a -X- _ O
single -X- _ O
backbone -X- _ O
feature -X- _ O
, -X- _ O
we -X- _ O
repeat -X- _ O
the -X- _ O
SGW -X- _ B-MethodName
process -X- _ O
several -X- _ O
times -X- _ O
so -X- _ O
as -X- _ O
to -X- _ O
generate -X- _ O
multiple -X- _ O
positive -X- _ O
features -X- _ O
. -X- _ O

tive -X- _ O
sentence -X- _ O
representations -X- _ O
to -X- _ O
the -X- _ O
shuffled -X- _ B-MethodName
- -X- _ I-MethodName
groupwhitening -X- _ I-MethodName
( -X- _ O
SGW -X- _ B-MethodName
) -X- _ O
module -X- _ O
, -X- _ O
in -X- _ O
this -X- _ O
module -X- _ O
we -X- _ O
randomly -X- _ O
dividing -X- _ O
each -X- _ O
sentence -X- _ O
representation -X- _ O
into -X- _ O
multiple -X- _ O
groups -X- _ O
along -X- _ O
the -X- _ O
axis -X- _ O
, -X- _ O
then -X- _ O
we -X- _ O
operate -X- _ O
group -X- _ O
whitening -X- _ O
on -X- _ O
each -X- _ O
group -X- _ O
. -X- _ O
We -X- _ O
repeat -X- _ O
SGW -X- _ B-MethodName
multiple -X- _ O
times -X- _ O
to -X- _ O
get -X- _ O
different -X- _ O
grouping -X- _ O
results -X- _ O
, -X- _ O
and -X- _ O
then -X- _ O
different -X- _ O
whitened -X- _ O
representations -X- _ O
. -X- _ O
These -X- _ O
" -X- _ O
duplicated -X- _ O
" -X- _ O
features -X- _ O
are -X- _ O
different -X- _ O
from -X- _ O
each -X- _ O
other -X- _ O
. -X- _ O
Finally -X- _ O
, -X- _ O
we -X- _ O
use -X- _ O
multi -X- _ O
- -X- _ O
positives -X- _ O
contrastive -X- _ O
loss -X- _ O
function -X- _ O
to -X- _ O
pull -X- _ O
one -X- _ O
representation -X- _ O
and -X- _ O
all -X- _ O
its -X- _ O
corresponding -X- _ O
augmentations -X- _ O
together -X- _ O
, -X- _ O
and -X- _ O
push -X- _ O
it -X- _ O
away -X- _ O
from -X- _ O
others -X- _ O
. -X- _ O
We -X- _ O
will -X- _ O
discuss -X- _ O
feasible -X- _ O
loss -X- _ O
function -X- _ O
in -X- _ O
Section -X- _ O
3.3 -X- _ O
, -X- _ O
and -X- _ O
present -X- _ O
our -X- _ O
final -X- _ O
form -X- _ O
of -X- _ O
loss -X- _ O
function -X- _ O
. -X- _ O

From -X- _ O
Whitening -X- _ O
to -X- _ O
SGW -X- _ B-MethodName

Preliminaries -X- _ O
for -X- _ O
Whitening -X- _ O

Given -X- _ O
a -X- _ O
batch -X- _ O
of -X- _ O
normalized -X- _ O
sentence -X- _ O
representations -X- _ O
Z -X- _ O
∈ -X- _ O
R -X- _ O
N -X- _ O
×d -X- _ O
, -X- _ O
the -X- _ O
whitening -X- _ O
transformation -X- _ O
can -X- _ O
be -X- _ O
formulated -X- _ O
as -X- _ O
: -X- _ O

H -X- _ O
= -X- _ O
Z -X- _ O
T -X- _ O
W -X- _ O
( -X- _ O
2 -X- _ O
) -X- _ O

where -X- _ O
H -X- _ O
∈ -X- _ O
R -X- _ O
d×N -X- _ O
is -X- _ O
the -X- _ O
whitened -X- _ O
embeddings -X- _ O
and -X- _ O
W -X- _ O
∈ -X- _ O
R -X- _ O
d×d -X- _ O
is -X- _ O
the -X- _ O
whitening -X- _ O
matrix -X- _ O
. -X- _ O
We -X- _ O
denote -X- _ O
the -X- _ O
covariance -X- _ O
matrix -X- _ O
of -X- _ O
ZZ -X- _ O
T -X- _ O
as -X- _ O
Σ. -X- _ O
the -X- _ O
goal -X- _ O
of -X- _ O
whitening -X- _ O
is -X- _ O
to -X- _ O
make -X- _ O
the -X- _ O
covariance -X- _ O
matrix -X- _ O
of -X- _ O
HH -X- _ O
T -X- _ O
equal -X- _ O
to -X- _ O
the -X- _ O
identity -X- _ O
matrix -X- _ O
I -X- _ O
, -X- _ O
i.e. -X- _ O
, -X- _ O
WΣW -X- _ O
T -X- _ O
= -X- _ O
I. -X- _ O
There -X- _ O
are -X- _ O
many -X- _ O
different -X- _ O
whitening -X- _ O
methods -X- _ O
, -X- _ O
such -X- _ O
as -X- _ O
PCA -X- _ O
( -X- _ O
Jégou -X- _ O
and -X- _ O
Chum -X- _ O
, -X- _ O
2012 -X- _ O
) -X- _ O
, -X- _ O
ZCA -X- _ B-MethodName
( -X- _ O
Bell -X- _ O
and -X- _ O
Sejnowski -X- _ O
, -X- _ O
1997 -X- _ O
) -X- _ O
, -X- _ O
etc -X- _ O
. -X- _ O
Group -X- _ B-MethodName
whitening -X- _ I-MethodName
use -X- _ O
ZCA -X- _ B-MethodName
as -X- _ O
its -X- _ O
whitening -X- _ O
method -X- _ O
to -X- _ O
prevent -X- _ O
the -X- _ O
stochastic -X- _ O
axis -X- _ O
swapping -X- _ O
( -X- _ O
Huang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2018 -X- _ O
) -X- _ O
. -X- _ O
2 -X- _ O
. -X- _ O

ZCA -X- _ B-MethodName
Whitening -X- _ I-MethodName
. -X- _ O
The -X- _ O
whitening -X- _ O
matrix -X- _ O
of -X- _ O
ZCA -X- _ B-MethodName
whitening -X- _ I-MethodName
transformation -X- _ O
can -X- _ O
be -X- _ O
formulated -X- _ O
as -X- _ O
: -X- _ O

W -X- _ O
ZCA -X- _ O
= -X- _ O
UΛ -X- _ O
−1 -X- _ O
/ -X- _ O
2 -X- _ O
U -X- _ O
T -X- _ O
( -X- _ O
3 -X- _ O

) -X- _ O

where -X- _ O
U -X- _ O
∈ -X- _ O
R -X- _ O
d×d -X- _ O
is -X- _ O
the -X- _ O
stack -X- _ O
of -X- _ O
eigenvector -X- _ O
of -X- _ O
cov -X- _ O
( -X- _ O
Z -X- _ O
, -X- _ O
Z -X- _ O
T -X- _ O
) -X- _ O
, -X- _ O
and -X- _ O
Λ -X- _ O
is -X- _ O
the -X- _ O
correspond -X- _ O
eigenvalue -X- _ O
matrix -X- _ O
. -X- _ O
U -X- _ O
and -X- _ O
Λ -X- _ O
are -X- _ O
obtained -X- _ O
by -X- _ O
matrix -X- _ O
decomposition -X- _ O
. -X- _ O
Therefore -X- _ O
, -X- _ O
Eq -X- _ O
. -X- _ O
2 -X- _ O
becomes -X- _ O
: -X- _ O

H -X- _ O
= -X- _ O
Z -X- _ O
T -X- _ O
UΛ -X- _ O
−1 -X- _ O
/ -X- _ O
2 -X- _ O
U -X- _ O
T -X- _ O
( -X- _ O
4 -X- _ O
) -X- _ O

Group -X- _ B-MethodName
Whitening -X- _ I-MethodName
. -X- _ O
Since -X- _ O
whitening -X- _ O
module -X- _ O
needs -X- _ O
a -X- _ O
large -X- _ O
batch -X- _ O
size -X- _ O
to -X- _ O
obtain -X- _ O
a -X- _ O
suitable -X- _ O
estimate -X- _ O
for -X- _ O
the -X- _ O
full -X- _ O
covariance -X- _ O
matrix -X- _ O
, -X- _ O
while -X- _ O
in -X- _ O
NLP -X- _ O
, -X- _ O
large -X- _ O
batch -X- _ O
size -X- _ O
can -X- _ O
be -X- _ O
detrimental -X- _ O
to -X- _ O
unsupervised -X- _ O
contrastive -X- _ O
learning -X- _ O
. -X- _ O
To -X- _ O
address -X- _ O
this -X- _ O
problem -X- _ O
, -X- _ O
we -X- _ O
use -X- _ O
group -X- _ B-MethodName
whitening -X- _ I-MethodName
( -X- _ O
Huang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2018 -X- _ O
) -X- _ O
, -X- _ O
which -X- _ O
controls -X- _ O
the -X- _ O
extent -X- _ O
of -X- _ O
whitening -X- _ O
by -X- _ O
decorrelating -X- _ O
smaller -X- _ O
groups -X- _ O
. -X- _ O
Specifically -X- _ O
, -X- _ O
give -X- _ O
a -X- _ O
sentence -X- _ O
representation -X- _ O
of -X- _ O
dimension -X- _ O
d -X- _ O
, -X- _ O
group -X- _ B-MethodName
whitening -X- _ I-MethodName
first -X- _ O
divide -X- _ O
it -X- _ O
into -X- _ O
k -X- _ O
groups -X- _ O
( -X- _ O
Z -X- _ O
0 -X- _ O
, -X- _ O
Z -X- _ O
1 -X- _ O
, -X- _ O
... -X- _ O
, -X- _ O
Z -X- _ O
k−1 -X- _ O
) -X- _ O
, -X- _ O
i.e. -X- _ O
, -X- _ O
Z -X- _ O
k -X- _ O
∈ -X- _ O
R -X- _ O
N -X- _ O
× -X- _ O
d -X- _ O
k -X- _ O
and -X- _ O
then -X- _ O
apply -X- _ O
whitening -X- _ O
on -X- _ O
each -X- _ O
group -X- _ O
. -X- _ O
That -X- _ O
is -X- _ O
: -X- _ O

H -X- _ O
= -X- _ O
concat -X- _ O
( -X- _ O
Z -X- _ O
i -X- _ O
W -X- _ O
ZCA -X- _ O
i -X- _ O
) -X- _ O
, -X- _ O
i -X- _ O
∈ -X- _ O
[ -X- _ O
0 -X- _ O
, -X- _ O
k -X- _ O
) -X- _ O
( -X- _ O
5 -X- _ O
) -X- _ O

Shuffled -X- _ B-MethodName
Group -X- _ I-MethodName
Whitening -X- _ I-MethodName

In -X- _ O
order -X- _ O
to -X- _ O
further -X- _ O
improve -X- _ O
the -X- _ O
quality -X- _ O
of -X- _ O
the -X- _ O
sentence -X- _ O
representation -X- _ O
model -X- _ O
, -X- _ O
we -X- _ O
proposed -X- _ O
shuffledgroup -X- _ B-MethodName
- -X- _ I-MethodName
whitening -X- _ I-MethodName
( -X- _ O
SGW -X- _ B-MethodName
) -X- _ O
. -X- _ O
We -X- _ O
randomly -X- _ O
divide -X- _ O
the -X- _ O
feature -X- _ O
into -X- _ O
multiple -X- _ O
groups -X- _ O
along -X- _ O
the -X- _ O
channel -X- _ O
axis -X- _ O
, -X- _ O
and -X- _ O
then -X- _ O
perform -X- _ O
ZCA -X- _ B-MethodName
whitening -X- _ I-MethodName
independently -X- _ O
within -X- _ O
each -X- _ O
group -X- _ O
. -X- _ O
After -X- _ O
whitening -X- _ O
, -X- _ O
we -X- _ O
do -X- _ O
a -X- _ O
reshuffled -X- _ O
operation -X- _ O
to -X- _ O
recover -X- _ O
features -X- _ O
to -X- _ O
their -X- _ O
original -X- _ O
arrangement -X- _ O
. -X- _ O
The -X- _ O
process -X- _ O
can -X- _ O
be -X- _ O
formulated -X- _ O
as -X- _ O
: -X- _ O

H -X- _ O
= -X- _ O
shuffled -X- _ O
−1 -X- _ O
( -X- _ O
GW -X- _ O
( -X- _ O
shuffled -X- _ O
( -X- _ O
Z -X- _ O
T -X- _ O
) -X- _ O
) -X- _ O
) -X- _ O
( -X- _ O
6 -X- _ O
) -X- _ O

This -X- _ O
can -X- _ O
bring -X- _ O
two -X- _ O
benefits -X- _ O
. -X- _ O
One -X- _ O
is -X- _ O
that -X- _ O
it -X- _ O
can -X- _ O
avoid -X- _ O
the -X- _ O
limitation -X- _ O
that -X- _ O
only -X- _ O
adjacent -X- _ O
features -X- _ O
can -X- _ O
be -X- _ O
put -X- _ O
into -X- _ O
the -X- _ O
same -X- _ O
group -X- _ O
, -X- _ O
so -X- _ O
as -X- _ O
to -X- _ O
better -X- _ O
decorrelation -X- _ O
and -X- _ O
then -X- _ O
achieve -X- _ O
better -X- _ O
uniformity -X- _ O
in -X- _ O
the -X- _ O
representation -X- _ O
space -X- _ O
. -X- _ O
Another -X- _ O
is -X- _ O
that -X- _ O
it -X- _ O
brings -X- _ O
a -X- _ O
disturbance -X- _ O
to -X- _ O
samples -X- _ O
, -X- _ O
we -X- _ O
can -X- _ O
use -X- _ O
it -X- _ O
as -X- _ O
a -X- _ O
data -X- _ O
augmentation -X- _ O
method -X- _ O
. -X- _ O
Specifically -X- _ O
, -X- _ O
we -X- _ O
repeat -X- _ O
SGW -X- _ B-MethodName
multiple -X- _ O
times -X- _ O
and -X- _ O
we -X- _ O
can -X- _ O
get -X- _ O
different -X- _ O
grouping -X- _ O
results -X- _ O
and -X- _ O
then -X- _ O
different -X- _ O
whitened -X- _ O
features -X- _ O
. -X- _ O
These -X- _ O
" -X- _ O
duplicated -X- _ O
" -X- _ O
features -X- _ O
are -X- _ O
different -X- _ O
from -X- _ O
each -X- _ O
other -X- _ O
, -X- _ O
and -X- _ O
thus -X- _ O
increase -X- _ O
the -X- _ O
diversity -X- _ O
of -X- _ O
postive -X- _ O
samples -X- _ O
. -X- _ O

Connection -X- _ O
to -X- _ O
contrastive -X- _ O
learning -X- _ O

We -X- _ O
find -X- _ O
that -X- _ O
whitening -X- _ O
and -X- _ O
contrastive -X- _ O
learning -X- _ O
are -X- _ O
not -X- _ O
totally -X- _ O
redundant -X- _ O
but -X- _ O
actually -X- _ O
have -X- _ O
some -X- _ O
complementarity -X- _ O
is -X- _ O
non -X- _ O
- -X- _ O
trivia -X- _ O
. -X- _ O
Specifically -X- _ O
, -X- _ O
whitening -X- _ O
decorrelates -X- _ O
features -X- _ O
through -X- _ O
matrix -X- _ O
decomposition -X- _ O
, -X- _ O
and -X- _ O
makes -X- _ O
the -X- _ O
variance -X- _ O
of -X- _ O
all -X- _ O
features -X- _ O
equal -X- _ O
to -X- _ O
1 -X- _ O
, -X- _ O
that -X- _ O
is -X- _ O
, -X- _ O
to -X- _ O
project -X- _ O
features -X- _ O
into -X- _ O
a -X- _ O
spherical -X- _ O
space -X- _ O
. -X- _ O
The -X- _ O
" -X- _ O
pushing -X- _ O
" -X- _ O
operation -X- _ O
in -X- _ O
contrastive -X- _ O
learning -X- _ O
is -X- _ O
to -X- _ O
approach -X- _ O
a -X- _ O
uniform -X- _ O
spherical -X- _ O
spatial -X- _ O
distribution -X- _ O
step -X- _ O
by -X- _ O
step -X- _ O
through -X- _ O
learning -X- _ O
/ -X- _ O
iteration -X- _ O
. -X- _ O
Therefore -X- _ O
, -X- _ O
conceptually -X- _ O
, -X- _ O
whitening -X- _ O
and -X- _ O
contrastive -X- _ O
learning -X- _ O
are -X- _ O
redundant -X- _ O
in -X- _ O
optimizing -X- _ O
the -X- _ O
uniformity -X- _ O
of -X- _ O
the -X- _ O
representation -X- _ O
space -X- _ O
. -X- _ O
However -X- _ O
, -X- _ O
contrastive -X- _ O
learning -X- _ O
achieves -X- _ O
uniformity -X- _ O
by -X- _ O
widening -X- _ O
the -X- _ O
distance -X- _ O
between -X- _ O
positive -X- _ O
samples -X- _ O
and -X- _ O
all -X- _ O
negative -X- _ O
samples -X- _ O
, -X- _ O
but -X- _ O
there -X- _ O
is -X- _ O
no -X- _ O
explicit -X- _ O
separation -X- _ O
between -X- _ O
negative -X- _ O
samples -X- _ O
. -X- _ O
Whitening -X- _ O
is -X- _ O
the -X- _ O
uniform -X- _ O
dispersion -X- _ O
of -X- _ O
the -X- _ O
entire -X- _ O
samples -X- _ O
, -X- _ O
so -X- _ O
there -X- _ O
is -X- _ O
complementarity -X- _ O
between -X- _ O
them -X- _ O
. -X- _ O
That -X- _ O
is -X- _ O
, -X- _ O
whitening -X- _ O
can -X- _ O
supplement -X- _ O
the -X- _ O
lack -X- _ O
of -X- _ O
contrastive -X- _ O
learning -X- _ O
for -X- _ O
the -X- _ O
" -X- _ O
pushing -X- _ O
" -X- _ O
operation -X- _ O
between -X- _ O
negative -X- _ O
samples -X- _ O
. -X- _ O

Multi -X- _ O
- -X- _ O
Positive -X- _ O
Contrastive -X- _ O
Loss -X- _ O

Since -X- _ O
we -X- _ O
get -X- _ O
multi -X- _ O
- -X- _ O
positive -X- _ O
samples -X- _ O
from -X- _ O
SGW -X- _ B-MethodName
module -X- _ O
, -X- _ O
however -X- _ O
, -X- _ O
the -X- _ O
original -X- _ O
contrastive -X- _ O
loss -X- _ O
in -X- _ O
Eq -X- _ O
. -X- _ O
1 -X- _ O
is -X- _ O
unable -X- _ O
to -X- _ O
handle -X- _ O
multiple -X- _ O
positives -X- _ O
. -X- _ O
We -X- _ O
provide -X- _ O
two -X- _ O
possible -X- _ O
options -X- _ O
of -X- _ O
contrastive -X- _ O
loss -X- _ O
which -X- _ O
can -X- _ O
adapt -X- _ O
multi -X- _ O
- -X- _ O
positives -X- _ O
. -X- _ O
Given -X- _ O
m -X- _ O
positive -X- _ O
samples -X- _ O
, -X- _ O
the -X- _ O
objective -X- _ O
function -X- _ O
can -X- _ O
be -X- _ O
formulated -X- _ O
as -X- _ O
: -X- _ O

L -X- _ O
1 -X- _ O
= -X- _ O
−λ -X- _ O
m -X- _ O
m -X- _ O
p=1 -X- _ O
log -X- _ O
e -X- _ O
sim -X- _ O
( -X- _ O
h -X- _ O
i -X- _ O
, -X- _ O
h -X- _ O
+ -X- _ O
i -X- _ O
, -X- _ O
p -X- _ O
) -X- _ O
/ -X- _ O
τ -X- _ O
N -X- _ O
j=1 -X- _ O
e -X- _ O
sim -X- _ O
( -X- _ O
h -X- _ O
i -X- _ O
, -X- _ O
h -X- _ O
+ -X- _ O
j -X- _ O
) -X- _ O
/ -X- _ O
τ -X- _ O
( -X- _ O
7 -X- _ O
) -X- _ O
L -X- _ O
2 -X- _ O
= -X- _ O
− -X- _ O
log -X- _ O
m -X- _ O
p=1 -X- _ O
λ -X- _ O
m -X- _ O
e -X- _ O
−sim -X- _ O
( -X- _ O
h -X- _ O
i -X- _ O
, -X- _ O
h -X- _ O
+ -X- _ O
i -X- _ O
, -X- _ O
p -X- _ O
) -X- _ O
/ -X- _ O
τ -X- _ O
N -X- _ O
j=1 -X- _ O
e -X- _ O
sim -X- _ O
( -X- _ O
h -X- _ O
i -X- _ O
, -X- _ O
h -X- _ O
+ -X- _ O
j -X- _ O
) -X- _ O
/ -X- _ O
τ -X- _ O
( -X- _ O
8 -X- _ O
) -X- _ O

where -X- _ O
λ -X- _ O
m -X- _ O
is -X- _ O
a -X- _ O
hyperparameter -X- _ O
, -X- _ O
it -X- _ O
controls -X- _ O
the -X- _ O
impact -X- _ O
of -X- _ O
each -X- _ O
positive -X- _ O
. -X- _ O
Eq -X- _ O
. -X- _ O
7 -X- _ O
puts -X- _ O
the -X- _ O
summation -X- _ O
over -X- _ O
positives -X- _ O
outside -X- _ O
of -X- _ O
the -X- _ O
log -X- _ O
while -X- _ O
Eq -X- _ O
. -X- _ O
8 -X- _ O
puts -X- _ O
the -X- _ O
sum -X- _ O
of -X- _ O
positives -X- _ O
inside -X- _ O
the -X- _ O
log -X- _ O
. -X- _ O
It -X- _ O
should -X- _ O
be -X- _ O
noted -X- _ O
that -X- _ O
in -X- _ O
Eq -X- _ O
. -X- _ O
8 -X- _ O
, -X- _ O
there -X- _ O
is -X- _ O
a -X- _ O
negative -X- _ O
sign -X- _ O
before -X- _ O
the -X- _ O
sum -X- _ O
of -X- _ O
positives -X- _ O
, -X- _ O
without -X- _ O
it -X- _ O
, -X- _ O
the -X- _ O
Eq -X- _ O
. -X- _ O
8 -X- _ O
will -X- _ O
conduct -X- _ O
hard -X- _ O
mining -X- _ O
, -X- _ O
which -X- _ O
means -X- _ O
the -X- _ O
maximum -X- _ O
of -X- _ O
m -X- _ O
p=1 -X- _ O
e -X- _ O
sim -X- _ O
( -X- _ O
h -X- _ O
i -X- _ O
, -X- _ O
h -X- _ O
+ -X- _ O
i -X- _ O
, -X- _ O
p -X- _ O
) -X- _ O
/ -X- _ O
τ -X- _ O
is -X- _ O
mainly -X- _ O
determined -X- _ O
by -X- _ O
max -X- _ O
( -X- _ O
e -X- _ O
−sim -X- _ O
( -X- _ O
h -X- _ O
i -X- _ O
, -X- _ O
h -X- _ O
+ -X- _ O
i -X- _ O
, -X- _ O
p -X- _ O
) -X- _ O
/ -X- _ O
τ -X- _ O
) -X- _ O
. -X- _ O
If -X- _ O
we -X- _ O
add -X- _ O
the -X- _ O
negative -X- _ O
sign -X- _ O
, -X- _ O
the -X- _ O
loss -X- _ O
function -X- _ O
will -X- _ O
be -X- _ O
committed -X- _ O
to -X- _ O
punish -X- _ O
the -X- _ O
items -X- _ O
with -X- _ O
less -X- _ O
similarity -X- _ O
, -X- _ O
which -X- _ O
is -X- _ O
good -X- _ O
for -X- _ O
bringing -X- _ O
all -X- _ O
positive -X- _ O
samples -X- _ O
closer -X- _ O
to -X- _ O
the -X- _ O
anchor -X- _ O
samples -X- _ O
. -X- _ O
In -X- _ O
our -X- _ O
framework -X- _ O
, -X- _ O
we -X- _ O
adopt -X- _ O
Eq -X- _ O
. -X- _ O
7 -X- _ O
as -X- _ O
our -X- _ O
final -X- _ O
loss -X- _ O
function -X- _ O
because -X- _ O
it -X- _ O
can -X- _ O
achieve -X- _ O
better -X- _ O
performance -X- _ O
. -X- _ O

Experiments -X- _ O

Experiment -X- _ O
Setup -X- _ O

In -X- _ O
this -X- _ O
section -X- _ O
, -X- _ O
We -X- _ O
evaluate -X- _ O
our -X- _ O
method -X- _ O
on -X- _ O
seven -X- _ O
Semantic -X- _ B-TaskName
Textual -X- _ I-TaskName
Similarity -X- _ I-TaskName
( -X- _ O
STS -X- _ B-TaskName
) -X- _ O
tasks -X- _ O
and -X- _ O
seven -X- _ O
transfer -X- _ B-TaskName
tasks -X- _ O
. -X- _ O
We -X- _ O
use -X- _ O
the -X- _ O
SentEval -X- _ B-DatasetName
( -X- _ O
Conneau -X- _ O
and -X- _ O
Kiela -X- _ O
, -X- _ O
2018 -X- _ O
) -X- _ O
toolkit -X- _ O
for -X- _ O
all -X- _ O
of -X- _ O
tasks -X- _ O
. -X- _ O

Datasets -X- _ O
. -X- _ O
Semantic -X- _ B-TaskName
Textual -X- _ I-TaskName
Similarity -X- _ I-TaskName
( -X- _ O
STS -X- _ B-TaskName
) -X- _ O
tasks -X- _ O
consist -X- _ O
of -X- _ O
seven -X- _ O
tasks -X- _ O
: -X- _ O
STS -X- _ B-DatasetName
2012 -X- _ O
- -X- _ O
2016 -X- _ O
( -X- _ O
Agirre -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2012 -X- _ O
( -X- _ O
Agirre -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
, -X- _ O
2013 -X- _ O
( -X- _ O
Agirre -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
, -X- _ O
2014 -X- _ O
( -X- _ O
Agirre -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
, -X- _ O
2015 -X- _ O
( -X- _ O
Agirre -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
, -X- _ O
2016 -X- _ O
STS -X- _ B-DatasetName
Benchmark -X- _ I-DatasetName
( -X- _ O
Cer -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
and -X- _ O
SICK -X- _ B-DatasetName
- -X- _ I-DatasetName
Relatedness -X- _ I-DatasetName
( -X- _ O
Marelli -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2014 -X- _ O
) -X- _ O
. -X- _ O
Each -X- _ O
sample -X- _ O
in -X- _ O
those -X- _ O
datasets -X- _ O
has -X- _ O
two -X- _ O
sentences -X- _ O
and -X- _ O
a -X- _ O
manually -X- _ O
annotated -X- _ O
similarity -X- _ O
score -X- _ O
from -X- _ O
0 -X- _ O
to -X- _ O
5 -X- _ O
to -X- _ O
measure -X- _ O
their -X- _ O
similarity -X- _ O
. -X- _ O
The -X- _ O
transfer -X- _ B-TaskName
tasks -X- _ O
include -X- _ O
MR -X- _ B-DatasetName
( -X- _ O
Pang -X- _ O
and -X- _ O
Lee -X- _ O
, -X- _ O
2005 -X- _ O
) -X- _ O
, -X- _ O
CR -X- _ B-DatasetName
( -X- _ O
Hu -X- _ O
and -X- _ O
Liu -X- _ O
, -X- _ O
2004 -X- _ O
) -X- _ O
, -X- _ O
SUBJ -X- _ B-DatasetName
( -X- _ O
Pang -X- _ O
and -X- _ O
Lee -X- _ O
, -X- _ O
2004 -X- _ O
) -X- _ O
, -X- _ O
MPQA -X- _ B-DatasetName
( -X- _ O
Wiebe -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2005 -X- _ O
) -X- _ O
, -X- _ O
SST-2 -X- _ B-DatasetName
( -X- _ O
Socher -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2013 -X- _ O
) -X- _ O
, -X- _ O
TREC -X- _ B-DatasetName
( -X- _ O
Voorhees -X- _ O
and -X- _ O
Tice -X- _ O
, -X- _ O
2000 -X- _ O
) -X- _ O
, -X- _ O
MRPC -X- _ B-DatasetName
( -X- _ O
Dolan -X- _ O
and -X- _ O
Brockett -X- _ O
, -X- _ O
2005 -X- _ O
) -X- _ O
. -X- _ O
In -X- _ O
this -X- _ O
tasks -X- _ O
, -X- _ O
we -X- _ O
use -X- _ O
a -X- _ O
logistic -X- _ O
regression -X- _ O
classifier -X- _ O
trained -X- _ O
on -X- _ O
top -X- _ O
of -X- _ O
the -X- _ O
frozen -X- _ O
sentence -X- _ O
embeddings -X- _ O
. -X- _ O

Baseline -X- _ O
and -X- _ O
competing -X- _ O
methods -X- _ O
. -X- _ O
We -X- _ O
compare -X- _ O
WhitenedCSE -X- _ B-MethodName
against -X- _ O
several -X- _ O
classic -X- _ O
methods -X- _ O
on -X- _ O
Semantic -X- _ B-TaskName
Textual -X- _ I-TaskName
Similarity -X- _ I-TaskName
datasets -X- _ O
, -X- _ O
i.e. -X- _ O
, -X- _ O
GloVe -X- _ B-MethodName
embeddings -X- _ O
( -X- _ O
Pennington -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2014 -X- _ O
) -X- _ O
, -X- _ O
average -X- _ O
BERT -X- _ B-MethodName
embeddings -X- _ O
from -X- _ O
the -X- _ O
last -X- _ O
layer -X- _ O
( -X- _ O
Devlin -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2018 -X- _ O
) -X- _ O
, -X- _ O
BERT -X- _ B-MethodName
- -X- _ I-MethodName
flow -X- _ I-MethodName
( -X- _ O
Li -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
, -X- _ O
BERTwhitening -X- _ B-MethodName
( -X- _ O
Su -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O
, -X- _ O
IS -X- _ B-MethodName
- -X- _ I-MethodName
BERT -X- _ I-MethodName
( -X- _ O
Zhang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
, -X- _ O
CT -X- _ B-MethodName
( -X- _ O
Carlsson -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O
, -X- _ O
ConSERT -X- _ B-MethodName
( -X- _ O
Yan -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O
, -X- _ O
SimCSE -X- _ B-MethodName
, -X- _ O
as -X- _ O
well -X- _ O
as -X- _ O
some -X- _ O
most -X- _ O
recent -X- _ O
state -X- _ O
- -X- _ O
of -X- _ O
- -X- _ O
the -X- _ O
- -X- _ O
art -X- _ O
methods -X- _ O
, -X- _ O
i.e. -X- _ O
, -X- _ O
MixCSE -X- _ B-MethodName
( -X- _ O
Zhang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2022a -X- _ O
) -X- _ O
, -X- _ O
ArcCSE -X- _ B-MethodName
( -X- _ O
Zhang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021a -X- _ O
) -X- _ O
, -X- _ O
DCLR -X- _ B-MethodName
( -X- _ O
Zhou -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2022 -X- _ O
) -X- _ O
. -X- _ O

Among -X- _ O
these -X- _ O
methods -X- _ O
, -X- _ O
SimCSE -X- _ B-MethodName
may -X- _ O
be -X- _ O
viewed -X- _ O
as -X- _ O
our -X- _ O
direct -X- _ O
baseline -X- _ O
, -X- _ O
because -X- _ O
WhitenedCSE -X- _ B-MethodName
may -X- _ O
be -X- _ O
viewed -X- _ O
as -X- _ O
being -X- _ O
transformed -X- _ O
from -X- _ O
SimCSE -X- _ B-MethodName
by -X- _ O
adding -X- _ O
the -X- _ O
SGW -X- _ B-MethodName
and -X- _ O
replacing -X- _ O
the -X- _ O
dual -X- _ O
- -X- _ O
positive -X- _ O
contrastive -X- _ O
loss -X- _ O
with -X- _ O
multi -X- _ O
- -X- _ O
positive -X- _ O
contrastive -X- _ O
loss -X- _ O
. -X- _ O

Therefore -X- _ O
, -X- _ O
when -X- _ O
conduct -X- _ O
ablation -X- _ O
study -X- _ O
, -X- _ O
we -X- _ O
use -X- _ O
SimCSE -X- _ B-MethodName
as -X- _ O
our -X- _ O
baseline -X- _ O
. -X- _ O

Implementation -X- _ O
details -X- _ O
. -X- _ O
We -X- _ O
use -X- _ O
the -X- _ O
output -X- _ O
of -X- _ O
the -X- _ O
MLP -X- _ O
layer -X- _ O
on -X- _ O
top -X- _ O
of -X- _ O
the -X- _ O
[ -X- _ O
CLS -X- _ O
] -X- _ O
as -X- _ O
the -X- _ O
our -X- _ O
sentence -X- _ O
representation -X- _ O
. -X- _ O
The -X- _ O
MLP -X- _ O
layer -X- _ O
is -X- _ O
consist -X- _ O
of -X- _ O
three -X- _ O
components -X- _ O
, -X- _ O
which -X- _ O
are -X- _ O
a -X- _ O
shuffled -X- _ O
group -X- _ O
whitening -X- _ O
module -X- _ O
, -X- _ O
a -X- _ O
768 -X- _ O
× -X- _ O
768 -X- _ O
linear -X- _ O
layer -X- _ O
and -X- _ O
a -X- _ O
activation -X- _ O
layer -X- _ O
. -X- _ O
Following -X- _ O
SimCSE -X- _ B-MethodName
, -X- _ O
we -X- _ O
use -X- _ O
1 -X- _ O
× -X- _ O
10 -X- _ O
6 -X- _ O
randomly -X- _ O
sampled -X- _ O
sentences -X- _ O
from -X- _ O
English -X- _ O
Wikipedia -X- _ O
as -X- _ O
our -X- _ O
training -X- _ O
corpus -X- _ O
. -X- _ O
We -X- _ O
start -X- _ O
from -X- _ O
pretrained -X- _ O
checkpoints -X- _ O
of -X- _ O
BERT -X- _ B-MethodName
( -X- _ O
Devlin -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2018 -X- _ O
) -X- _ O
and -X- _ O
RoBERTa -X- _ B-MethodName
( -X- _ O
Liu -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
. -X- _ O
At -X- _ O
training -X- _ O
time -X- _ O
, -X- _ O
we -X- _ O
set -X- _ O
the -X- _ O
learning -X- _ B-HyperparameterName
rate -X- _ I-HyperparameterName
as -X- _ O
3e-5 -X- _ B-HyperparameterValue
, -X- _ O
the -X- _ O
batch -X- _ B-HyperparameterName
size -X- _ I-HyperparameterName
as -X- _ O
64 -X- _ B-HyperparameterValue
. -X- _ O
We -X- _ O
train -X- _ O
our -X- _ O
model -X- _ O
for -X- _ O
1 -X- _ O
epoch -X- _ O
with -X- _ O
temperature -X- _ B-HyperparameterName
τ -X- _ O
= -X- _ O
0.05 -X- _ B-HyperparameterValue
. -X- _ O
For -X- _ O
BERT -X- _ B-MethodName
- -X- _ O
base -X- _ O
and -X- _ O
BERT -X- _ B-MethodName
- -X- _ O
large -X- _ O
, -X- _ O
we -X- _ O
set -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
group -X- _ B-HyperparameterName
size -X- _ I-HyperparameterName
as -X- _ O
384 -X- _ B-HyperparameterValue
, -X- _ O
for -X- _ O
RoBERTabase -X- _ B-MethodName
and -X- _ O
RoBERTa -X- _ B-MethodName
- -X- _ O
large -X- _ O
, -X- _ O
we -X- _ O
set -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
group -X- _ B-HyperparameterName
size -X- _ I-HyperparameterName
as -X- _ O
256 -X- _ B-HyperparameterValue
. -X- _ O
We -X- _ O
set -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
positives -X- _ O
as -X- _ O
3 -X- _ O
for -X- _ O
all -X- _ O
of -X- _ O
models -X- _ O
. -X- _ O
We -X- _ O
evaluate -X- _ O
the -X- _ O
model -X- _ O
every -X- _ O
125 -X- _ O
training -X- _ O
steps -X- _ O
on -X- _ O
the -X- _ O
development -X- _ O
set -X- _ O
of -X- _ O
STS -X- _ B-DatasetName
- -X- _ I-DatasetName
B -X- _ I-DatasetName
, -X- _ O
and -X- _ O
keep -X- _ O
the -X- _ O
best -X- _ O
checkpoint -X- _ O
for -X- _ O
evaluation -X- _ O
on -X- _ O
test -X- _ O
sets -X- _ O
. -X- _ O
We -X- _ O
conduct -X- _ O
our -X- _ O
experiments -X- _ O
on -X- _ O
two -X- _ O
3090 -X- _ O
GPUs -X- _ O
. -X- _ O

STS -X- _ B-TaskName
tasks -X- _ O

We -X- _ O
conduct -X- _ O
experiments -X- _ O
on -X- _ O
7 -X- _ O
semantic -X- _ B-TaskName
textual -X- _ I-TaskName
similarity -X- _ I-TaskName
( -X- _ O
STS -X- _ B-TaskName
) -X- _ O
tasks -X- _ O
, -X- _ O
and -X- _ O
use -X- _ O
SentEval -X- _ B-DatasetName
toolkit -X- _ O
( -X- _ O
Conneau -X- _ O
and -X- _ O
Kiela -X- _ O
, -X- _ O
2018 -X- _ O
) -X- _ O
Spearman -X- _ B-MetricName
's -X- _ I-MetricName
correlation -X- _ I-MetricName
coefficient -X- _ I-MetricName
as -X- _ O
our -X- _ O
evaluation -X- _ O
metrics -X- _ O
. -X- _ O
The -X- _ O
Spearman -X- _ B-MetricName
's -X- _ I-MetricName
correlation -X- _ I-MetricName
uses -X- _ O
a -X- _ O
monotonic -X- _ O
equation -X- _ O
to -X- _ O
evaluate -X- _ O
the -X- _ O
correlation -X- _ O
of -X- _ O
two -X- _ O
statistical -X- _ O
variables -X- _ O
, -X- _ O
it -X- _ O
varies -X- _ O
between -X- _ O
-1 -X- _ O
and -X- _ O
1 -X- _ O
with -X- _ O
0 -X- _ O
implying -X- _ O
no -X- _ O
correlation -X- _ O
, -X- _ O
and -X- _ O
the -X- _ O
closer -X- _ O
the -X- _ O
value -X- _ O
is -X- _ O
to -X- _ O
1 -X- _ O
, -X- _ O
the -X- _ O
closer -X- _ O
the -X- _ O
two -X- _ O
statistical -X- _ O
variables -X- _ O
are -X- _ O
to -X- _ O
positive -X- _ O
correlation -X- _ O
. -X- _ O
Tab -X- _ O
. -X- _ O
1 -X- _ O
shows -X- _ O
the -X- _ O
evaluation -X- _ O
results -X- _ O
on -X- _ O
7 -X- _ O
STS -X- _ B-TaskName
tasks -X- _ O
, -X- _ O
from -X- _ O
which -X- _ O
we -X- _ O
can -X- _ O
see -X- _ O
that -X- _ O
WhitenedCSE -X- _ B-MethodName
achieves -X- _ O
competitive -X- _ O
performance -X- _ O
. -X- _ O
Compared -X- _ O
with -X- _ O
SimCSE -X- _ B-MethodName
, -X- _ O
WhitenedCSE -X- _ B-MethodName
achieves -X- _ O
2.53 -X- _ B-MetricValue
and -X- _ O
1.56 -X- _ B-MetricValue
points -X- _ O
of -X- _ O
improvement -X- _ O
based -X- _ O
on -X- _ O
BERT -X- _ B-MethodName
base -X- _ O
and -X- _ O
BERT -X- _ B-MethodName
large -X- _ O
. -X- _ O
It -X- _ O
also -X- _ O
raise -X- _ O
the -X- _ O
performance -X- _ O
from -X- _ O
76.57 -X- _ B-MetricValue
% -X- _ I-MetricValue
to -X- _ O
78.22 -X- _ B-MetricValue
% -X- _ I-MetricValue
base -X- _ O
on -X- _ O
RoBERTa -X- _ B-MethodName
base -X- _ O
. -X- _ O
Compared -X- _ O
with -X- _ O
recent -X- _ O
works -X- _ O
, -X- _ O
WhitenedCSE -X- _ B-MethodName
also -X- _ O
achieves -X- _ O
the -X- _ O
best -X- _ O
performance -X- _ O
in -X- _ O
most -X- _ O
of -X- _ O
the -X- _ O
STS -X- _ B-TaskName
tasks -X- _ O
. -X- _ O

Transfer -X- _ B-TaskName
tasks -X- _ O

We -X- _ O
also -X- _ O
conduct -X- _ O
experiments -X- _ O
on -X- _ O
7 -X- _ O
transfer -X- _ B-TaskName
tasks -X- _ O
, -X- _ O
and -X- _ O
use -X- _ O
SentEval -X- _ B-DatasetName
toolkit -X- _ O
( -X- _ O
Conneau -X- _ O
and -X- _ O
Kiela -X- _ O
, -X- _ O
2018 -X- _ O
) -X- _ O
for -X- _ O
evaluation -X- _ O
. -X- _ O
For -X- _ O
each -X- _ O
task -X- _ O
, -X- _ O
we -X- _ O
train -X- _ O
a -X- _ O
logistic -X- _ O
regression -X- _ O
classifier -X- _ O
on -X- _ O
top -X- _ O
of -X- _ O
the -X- _ O
frozen -X- _ O
sentence -X- _ O
embeddings -X- _ O
and -X- _ O
test -X- _ O
the -X- _ O
accuracy -X- _ B-MetricName
on -X- _ O
the -X- _ O
downstream -X- _ O
task -X- _ O
. -X- _ O
In -X- _ O
our -X- _ O
experiment -X- _ O
settings -X- _ O
, -X- _ O
we -X- _ O
do -X- _ O
not -X- _ O
include -X- _ O
models -X- _ O
with -X- _ O
auxiliary -X- _ O
tasks -X- _ O
, -X- _ O
i.e. -X- _ O
, -X- _ O
masked -X- _ O
language -X- _ O
modeling -X- _ O
, -X- _ O
for -X- _ O
a -X- _ O
fair -X- _ O
comparison -X- _ O
. -X- _ O

Tab -X- _ O
. -X- _ O
2 -X- _ O
shows -X- _ O
the -X- _ O
evaluation -X- _ O
results -X- _ O
. -X- _ O
Comparied -X- _ O
with -X- _ O
the -X- _ O
SimCSE -X- _ B-MethodName
baseline -X- _ O
, -X- _ O
WhitenedCSE -X- _ B-MethodName
achieves -X- _ O
0.59 -X- _ B-MetricValue
and -X- _ O
0.33 -X- _ B-MetricValue
accuracy -X- _ B-MetricName
improvement -X- _ O
on -X- _ O
average -X- _ O
results -X- _ O
based -X- _ O
on -X- _ O
BERT -X- _ B-MethodName
base -X- _ O
and -X- _ O
BERT -X- _ B-MethodName
large -X- _ O
. -X- _ O
Compared -X- _ O
with -X- _ O
recent -X- _ O
works -X- _ O
, -X- _ O
WhitenedCSE -X- _ B-MethodName
also -X- _ O
achieves -X- _ O
the -X- _ O
best -X- _ O
performance -X- _ O
in -X- _ O
most -X- _ O
of -X- _ O
the -X- _ O
transfer -X- _ O
tasks -X- _ O
, -X- _ O
which -X- _ O
further -X- _ O
demonstrates -X- _ O
the -X- _ O
effectiveness -X- _ O
of -X- _ O
our -X- _ O
method -X- _ O
. -X- _ O

Alignment -X- _ O
and -X- _ O
Uniformity -X- _ O

In -X- _ O
order -X- _ O
to -X- _ O
further -X- _ O
quantify -X- _ O
the -X- _ O
improvement -X- _ O
in -X- _ O
uniformity -X- _ B-MetricName
and -X- _ O
alignment -X- _ B-MetricName
of -X- _ O
WhitenedCSE -X- _ B-MethodName
, -X- _ O
we -X- _ O
follow -X- _ O
SimCSE -X- _ B-MethodName
, -X- _ O
and -X- _ O
use -X- _ O
alignment -X- _ B-MetricName
loss -X- _ O
and -X- _ O
uniformity -X- _ B-MetricName
loss -X- _ O
( -X- _ O
Wang -X- _ O
and -X- _ O
Isola -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
to -X- _ O
measure -X- _ O
the -X- _ O
quality -X- _ O
of -X- _ O
representations -X- _ O
. -X- _ O
Alignment -X- _ O
is -X- _ O
used -X- _ O
to -X- _ O
measure -X- _ O
the -X- _ O
expected -X- _ O
distance -X- _ O
between -X- _ O
the -X- _ O
embeddings -X- _ O
of -X- _ O
the -X- _ O
positive -X- _ O
pairs -X- _ O
, -X- _ O
and -X- _ O
can -X- _ O
be -X- _ O
formulated -X- _ O
as -X- _ O
: -X- _ O

ℓ -X- _ O
align -X- _ O
= -X- _ O
E -X- _ O
( -X- _ O
x -X- _ O
, -X- _ O
x -X- _ O
+ -X- _ O
) -X- _ O
∼ppos -X- _ O
∥f -X- _ O
( -X- _ O
x -X- _ O
) -X- _ O
− -X- _ O
f -X- _ O
( -X- _ O
x -X- _ O
+ -X- _ O
) -X- _ O
∥ -X- _ O
2 -X- _ O
( -X- _ O
9 -X- _ O
) -X- _ O

while -X- _ O
uniformity -X- _ B-MetricName
measures -X- _ O
how -X- _ O
well -X- _ O
the -X- _ O
embeddings -X- _ O
are -X- _ O
uniformly -X- _ O
distributed -X- _ O
in -X- _ O
the -X- _ O
representation -X- _ O
space -X- _ O
: -X- _ O

ℓ -X- _ O
uniform -X- _ O
= -X- _ O
log -X- _ O
E -X- _ O
x -X- _ O
, -X- _ O
y -X- _ O
i.i.d -X- _ O
. -X- _ O
∼ -X- _ O
p -X- _ O
data -X- _ O
e -X- _ O
−2∥f -X- _ O
( -X- _ O
x -X- _ O
) -X- _ O
−f -X- _ O
( -X- _ O
y -X- _ O
) -X- _ O
∥ -X- _ O
2 -X- _ O
( -X- _ O
10 -X- _ O
) -X- _ O

We -X- _ O
calculate -X- _ O
the -X- _ O
alignment -X- _ B-MetricName
loss -X- _ O
and -X- _ O
uniformity -X- _ B-MetricName
loss -X- _ O
every -X- _ O
125 -X- _ O
training -X- _ O
steps -X- _ O
on -X- _ O
the -X- _ O
STS -X- _ B-DatasetName
- -X- _ I-DatasetName
B -X- _ I-DatasetName
development -X- _ O
set -X- _ O
. -X- _ O
From -X- _ O
Fig -X- _ O
. -X- _ O
3 -X- _ O
, -X- _ O
we -X- _ O
can -X- _ O
see -X- _ O
that -X- _ O
compared -X- _ O
with -X- _ O
SimCSE -X- _ B-MethodName
, -X- _ O
WhitenedCSE -X- _ B-MethodName
performs -X- _ O
better -X- _ O
both -X- _ O
on -X- _ O
the -X- _ O
alignment -X- _ B-MetricName
measure -X- _ O
and -X- _ O
the -X- _ O
uniformity -X- _ B-MetricName
measure -X- _ O
. -X- _ O
We -X- _ O
also -X- _ O
find -X- _ O
that -X- _ O
the -X- _ O
uniformity -X- _ B-MetricName
of -X- _ O
our -X- _ O
models -X- _ O
is -X- _ O
well -X- _ O
optimized -X- _ O
at -X- _ O
the -X- _ O
beginning -X- _ O
and -X- _ O
remains -X- _ O
stable -X- _ O
throughout -X- _ O
the -X- _ O
training -X- _ O
process -X- _ O
. -X- _ O
This -X- _ O
further -X- _ O
confirms -X- _ O
that -X- _ O
our -X- _ O
method -X- _ O
can -X- _ O
improve -X- _ O
the -X- _ O
quality -X- _ O
of -X- _ O
sentence -X- _ B-TaskName
representation -X- _ I-TaskName
more -X- _ O
effectively -X- _ O
. -X- _ O

Ablation -X- _ O
Analysis -X- _ O

In -X- _ O
this -X- _ O
section -X- _ O
, -X- _ O
we -X- _ O
further -X- _ O
investigate -X- _ O
the -X- _ O
effectiveness -X- _ O
of -X- _ O
our -X- _ O
proposed -X- _ O
model -X- _ O
WhitenedCSE -X- _ B-MethodName
. -X- _ O
For -X- _ O
all -X- _ O
experiments -X- _ O
, -X- _ O
we -X- _ O
use -X- _ O
BERT -X- _ B-MethodName
base -X- _ O
as -X- _ O
our -X- _ O
base -X- _ O
model -X- _ O
, -X- _ O
and -X- _ O
evaluate -X- _ O
WhitenedCSE -X- _ B-MethodName
on -X- _ O
the -X- _ O
STS -X- _ B-TaskName
tasks -X- _ O
unless -X- _ O
otherwise -X- _ O
specified -X- _ O
. -X- _ O

Shuffling -X- _ O
augments -X- _ O
the -X- _ O
positive -X- _ O
samples -X- _ O

We -X- _ O
prove -X- _ O
theoretically -X- _ O
and -X- _ O
practically -X- _ O
that -X- _ O
SGW -X- _ B-MethodName
can -X- _ O
be -X- _ O
regarded -X- _ O
as -X- _ O
an -X- _ O
effective -X- _ O
data -X- _ O
augmentation -X- _ O
. -X- _ O
We -X- _ O
know -X- _ O
different -X- _ O
whitening -X- _ O
transformations -X- _ O
will -X- _ O
get -X- _ O
different -X- _ O
whitened -X- _ O
results -X- _ O
, -X- _ O
but -X- _ O
all -X- _ O
of -X- _ O
them -X- _ O
are -X- _ O
representations -X- _ O
for -X- _ O
the -X- _ O
same -X- _ O
sample -X- _ O
, -X- _ O
so -X- _ O
they -X- _ O
can -X- _ O
be -X- _ O
regarded -X- _ O
as -X- _ O
positive -X- _ O
samples -X- _ O
for -X- _ O
each -X- _ O
other -X- _ O
. -X- _ O
In -X- _ O
WhitenedCSE -X- _ B-MethodName
, -X- _ O
we -X- _ O
operate -X- _ O
randomly -X- _ O
shuffled -X- _ O
on -X- _ O
feature -X- _ O
dimension -X- _ O
, -X- _ O
and -X- _ O
divide -X- _ O
the -X- _ O
representations -X- _ O
along -X- _ O
the -X- _ O
feature -X- _ O
dimension -X- _ O
into -X- _ O
k -X- _ O
groups -X- _ O
. -X- _ O
Since -X- _ O
each -X- _ O
time -X- _ O
we -X- _ O
use -X- _ O
a -X- _ O
different -X- _ O
permutation -X- _ O
, -X- _ O
we -X- _ O
can -X- _ O
get -X- _ O
a -X- _ O
different -X- _ O
representations -X- _ O
Z -X- _ O
and -X- _ O
the -X- _ O
corresponding -X- _ O
whitening -X- _ O
matrix -X- _ O
W -X- _ O
, -X- _ O
We -X- _ O
find -X- _ O
it -X- _ O
can -X- _ O
be -X- _ O
written -X- _ O
as -X- _ O
a -X- _ O
form -X- _ O
of -X- _ O
feature -X- _ O
- -X- _ O
wise -X- _ O
disturbance -X- _ O
Z -X- _ O
* -X- _ O
= -X- _ O
Z -X- _ O
+ -X- _ O
ϵ -X- _ O
: -X- _ O

Z -X- _ O
* -X- _ O
= -X- _ O
Z -X- _ O
+ -X- _ O
( -X- _ O
W -X- _ O
ZCA -X- _ O
− -X- _ O
1 -X- _ O
) -X- _ O
Z -X- _ O
= -X- _ O
Z -X- _ O
+ -X- _ O
( -X- _ O
UΛ -X- _ O
−1 -X- _ O
/ -X- _ O
2 -X- _ O
U -X- _ O
T -X- _ O
− -X- _ O
1 -X- _ O
) -X- _ O
Z -X- _ O
( -X- _ O
11 -X- _ O
) -X- _ O

Here -X- _ O
, -X- _ O
we -X- _ O
treat -X- _ O
( -X- _ O
UΛ -X- _ O
−1 -X- _ O
/ -X- _ O
2 -X- _ O
U -X- _ O
T -X- _ O
− -X- _ O
1 -X- _ O
) -X- _ O
Z -X- _ O
as -X- _ O
a -X- _ O
perturbation -X- _ O
ϵ -X- _ O
on -X- _ O
the -X- _ O
feature -X- _ O
dimension -X- _ O
. -X- _ O
Thus -X- _ O
, -X- _ O
in -X- _ O
Whitened -X- _ B-MethodName
- -X- _ I-MethodName
CSE -X- _ I-MethodName
, -X- _ O
we -X- _ O
use -X- _ O
it -X- _ O
as -X- _ O
a -X- _ O
data -X- _ O
augmentation -X- _ O
and -X- _ O
generate -X- _ O
more -X- _ O
diverse -X- _ O
positive -X- _ O
samples -X- _ O
. -X- _ O
From -X- _ O
Tab.4 -X- _ O
, -X- _ O
we -X- _ O
can -X- _ O
see -X- _ O
that -X- _ O
, -X- _ O
shuffling -X- _ O
plays -X- _ O
a -X- _ O
very -X- _ O
important -X- _ O
role -X- _ O
in -X- _ O
the -X- _ O
performance -X- _ O
of -X- _ O
the -X- _ O
model -X- _ O
. -X- _ O

The -X- _ O
importance -X- _ O
of -X- _ O
Group -X- _ B-MethodName
Whitening -X- _ I-MethodName

Recently -X- _ O
, -X- _ O
Su -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2021 -X- _ O
) -X- _ O
directly -X- _ O
apply -X- _ O
whitening -X- _ O
on -X- _ O
the -X- _ O
output -X- _ O
of -X- _ O
the -X- _ O
BERT -X- _ B-MethodName
and -X- _ O
have -X- _ O
achieved -X- _ O
remarkable -X- _ O
performance -X- _ O
at -X- _ O
the -X- _ O
time -X- _ O
. -X- _ O
This -X- _ O
lead -X- _ O
us -X- _ O
to -X- _ O
think -X- _ O
whether -X- _ O
whitening -X- _ O
can -X- _ O
be -X- _ O
directly -X- _ O
applied -X- _ O
to -X- _ O
the -X- _ O
output -X- _ O
of -X- _ O
contrastive -X- _ O
learning -X- _ O
model -X- _ O
to -X- _ O
further -X- _ O
improve -X- _ O
the -X- _ O
uniformity -X- _ O
of -X- _ O
the -X- _ O
model -X- _ O
representation -X- _ O
space -X- _ O
. -X- _ O
We -X- _ O
consider -X- _ O
two -X- _ O
different -X- _ O
whitening -X- _ O
methods -X- _ O
: -X- _ O
PCA -X- _ O
Whitening -X- _ O
and -X- _ O
ZCA -X- _ B-MethodName
Whitening -X- _ I-MethodName
. -X- _ O
The -X- _ O
difference -X- _ O
between -X- _ O
them -X- _ O
is -X- _ O
that -X- _ O
ZCA -X- _ B-MethodName
Whitening -X- _ I-MethodName
uses -X- _ O
an -X- _ O
additional -X- _ O
rotation -X- _ O
matrix -X- _ O
to -X- _ O
rotate -X- _ O
the -X- _ O
PCA -X- _ O
whitened -X- _ O
data -X- _ O
back -X- _ O
to -X- _ O
the -X- _ O
original -X- _ O
feature -X- _ O
space -X- _ O
, -X- _ O
which -X- _ O
can -X- _ O
make -X- _ O
the -X- _ O
transformed -X- _ O
data -X- _ O
closer -X- _ O
to -X- _ O
the -X- _ O
original -X- _ O
input -X- _ O
data -X- _ O
. -X- _ O

W -X- _ O
ZCA -X- _ O
= -X- _ O
U -X- _ O
rotate -X- _ O
W -X- _ O
P -X- _ O
CA -X- _ O
( -X- _ O
12 -X- _ O
) -X- _ O

We -X- _ O
use -X- _ O
the -X- _ O
in -X- _ O
- -X- _ O
batch -X- _ O
sentence -X- _ O
representations -X- _ O
to -X- _ O
calculate -X- _ O
the -X- _ O
mean -X- _ O
valuex -X- _ O
and -X- _ O
the -X- _ O
covariance -X- _ O
matrix -X- _ O
σ -X- _ O
, -X- _ O
and -X- _ O
use -X- _ O
the -X- _ O
momentum -X- _ O
to -X- _ O
estimate -X- _ O
the -X- _ O
overall -X- _ O
mean -X- _ O
value -X- _ O
µ -X- _ O
and -X- _ O
covariance -X- _ O
matrix -X- _ O
Σ -X- _ O
. -X- _ O

µ -X- _ O
n -X- _ O
= -X- _ O
βµ -X- _ O
n−1 -X- _ O
+ -X- _ O
( -X- _ O
1 -X- _ O
− -X- _ O
β -X- _ O
) -X- _ O
x -X- _ O
n−1 -X- _ O
Σ -X- _ O
n -X- _ O
= -X- _ O
βΣ -X- _ O
n−1 -X- _ O
+ -X- _ O
( -X- _ O
1 -X- _ O
− -X- _ O
β -X- _ O
) -X- _ O
σ -X- _ O
n−1 -X- _ O
( -X- _ O
13 -X- _ O
) -X- _ O

As -X- _ O
the -X- _ O
results -X- _ O
shown -X- _ O
in -X- _ O
the -X- _ O
Tab -X- _ O
. -X- _ O
3 -X- _ O
, -X- _ O
we -X- _ O
found -X- _ O
that -X- _ O
directly -X- _ O
applying -X- _ O
the -X- _ O
whitening -X- _ O
transformation -X- _ O
on -X- _ O
contrastive -X- _ O
learning -X- _ O
models -X- _ O
is -X- _ O
detrimental -X- _ O
to -X- _ O
the -X- _ O
performance -X- _ O
. -X- _ O
we -X- _ O
attribute -X- _ O
this -X- _ O
to -X- _ O
two -X- _ O
reasons -X- _ O
: -X- _ O
( -X- _ O
1 -X- _ O
) -X- _ O
small -X- _ O
batch -X- _ O
size -X- _ O
may -X- _ O
not -X- _ O
provide -X- _ O
enough -X- _ O
samples -X- _ O
to -X- _ O
obtain -X- _ O
a -X- _ O
suitable -X- _ O
estimate -X- _ O
for -X- _ O
the -X- _ O
full -X- _ O
covariance -X- _ O
matrix -X- _ O
. -X- _ O
( -X- _ O
2 -X- _ O
) -X- _ O
The -X- _ O
covariance -X- _ O
matrix -X- _ O
obtained -X- _ O
by -X- _ O
high -X- _ O
- -X- _ O
dimensional -X- _ O
features -X- _ O
is -X- _ O
not -X- _ O
necessarily -X- _ O
a -X- _ O
positive -X- _ O
definite -X- _ O
matrix -X- _ O
( -X- _ O
maybe -X- _ O
a -X- _ O
semi -X- _ O
- -X- _ O
positive -X- _ O
definite -X- _ O
matrix -X- _ O
) -X- _ O
, -X- _ O
which -X- _ O
may -X- _ O
leads -X- _ O
to -X- _ O
errors -X- _ O
in -X- _ O
matrix -X- _ O
decomposition -X- _ O
. -X- _ O
To -X- _ O
alleviate -X- _ O
this -X- _ O
problem -X- _ O
, -X- _ O
we -X- _ O
use -X- _ O
the -X- _ O
group -X- _ B-MethodName
whitening -X- _ I-MethodName
to -X- _ O
control -X- _ O
the -X- _ O
extent -X- _ O
of -X- _ O
whitening -X- _ O
. -X- _ O
From -X- _ O
Tab -X- _ O
. -X- _ O
3 -X- _ O
we -X- _ O
can -X- _ O
see -X- _ O
that -X- _ O
group -X- _ B-MethodName
whitening -X- _ I-MethodName
can -X- _ O
significantly -X- _ O
improve -X- _ O
the -X- _ O
performance -X- _ O
. -X- _ O

Hyperparameters -X- _ O
Analysis -X- _ O

For -X- _ O
hyperparameters -X- _ O
analysis -X- _ O
, -X- _ O
we -X- _ O
want -X- _ O
to -X- _ O
explore -X- _ O
the -X- _ O
sensitivity -X- _ B-MetricName
of -X- _ O
WhitenedCSE -X- _ B-MethodName
to -X- _ O
these -X- _ O
parameters -X- _ O
. -X- _ O
Concretely -X- _ O
, -X- _ O
we -X- _ O
study -X- _ O
the -X- _ O
impact -X- _ O
of -X- _ O
the -X- _ O
group -X- _ B-HyperparameterName
size -X- _ I-HyperparameterName
, -X- _ O
the -X- _ O
number -X- _ B-HyperparameterName
of -X- _ I-HyperparameterName
positive -X- _ I-HyperparameterName
samples -X- _ I-HyperparameterName
. -X- _ O
We -X- _ O
evaluate -X- _ O
our -X- _ O
model -X- _ O
with -X- _ O
varying -X- _ O
values -X- _ O
, -X- _ O
and -X- _ O
report -X- _ O
the -X- _ O
performances -X- _ O
on -X- _ O
the -X- _ O
seven -X- _ O
STS -X- _ B-TaskName
tasks -X- _ O
. -X- _ O

The -X- _ O
influence -X- _ O
of -X- _ O
group -X- _ B-HyperparameterName
size -X- _ I-HyperparameterName
. -X- _ O
In -X- _ O
WhitenedCSE -X- _ B-MethodName
, -X- _ O
we -X- _ O
divide -X- _ O
the -X- _ O
representation -X- _ O
into -X- _ O
k -X- _ B-HyperparameterName
groups -X- _ O
. -X- _ O
However -X- _ O
, -X- _ O
we -X- _ O
know -X- _ O
the -X- _ O
size -X- _ O
of -X- _ O
group -X- _ O
controls -X- _ O
the -X- _ O
degree -X- _ O
of -X- _ O
whitening -X- _ O
, -X- _ O
and -X- _ O
has -X- _ O
a -X- _ O
great -X- _ O
effect -X- _ O
on -X- _ O
the -X- _ O
effectiveness -X- _ O
of -X- _ O
WhitenedCSE -X- _ B-MethodName
, -X- _ O
so -X- _ O
we -X- _ O
carry -X- _ O
out -X- _ O
an -X- _ O
experiment -X- _ O
with -X- _ O
k -X- _ B-HyperparameterName
varying -X- _ O
from -X- _ O
32 -X- _ B-HyperparameterValue
to -X- _ O
384 -X- _ B-HyperparameterValue
. -X- _ O
As -X- _ O
shown -X- _ O
in -X- _ O
Tab -X- _ O
. -X- _ O
4 -X- _ O
, -X- _ O
we -X- _ O
can -X- _ O
see -X- _ O
that -X- _ O
the -X- _ O
best -X- _ O
performance -X- _ O
is -X- _ O
achieved -X- _ O
when -X- _ O
k -X- _ B-HyperparameterName
= -X- _ O
384 -X- _ B-HyperparameterValue
, -X- _ O
and -X- _ O
the -X- _ O
second -X- _ O
best -X- _ O
performance -X- _ O
is -X- _ O
achieved -X- _ O
when -X- _ O
k -X- _ B-HyperparameterName
= -X- _ O
128 -X- _ B-HyperparameterValue
. -X- _ O
When -X- _ O
k -X- _ B-HyperparameterName
takes -X- _ O
other -X- _ O
values -X- _ O
, -X- _ O
the -X- _ O
performance -X- _ O
will -X- _ O
drop -X- _ O
slightly -X- _ O
. -X- _ O

The -X- _ O
influence -X- _ O
of -X- _ O
positive -X- _ B-HyperparameterName
samples -X- _ I-HyperparameterName
number -X- _ I-HyperparameterName
. -X- _ O
Sampling -X- _ O
multi -X- _ O
- -X- _ O
positive -X- _ O
samples -X- _ O
can -X- _ O
significantly -X- _ O
enrich -X- _ O
semantic -X- _ O
diversity -X- _ O
, -X- _ O
we -X- _ O
want -X- _ O
to -X- _ O
explore -X- _ O
how -X- _ O
the -X- _ O
number -X- _ B-HyperparameterName
of -X- _ I-HyperparameterName
positive -X- _ I-HyperparameterName
samples -X- _ I-HyperparameterName
affect -X- _ O
the -X- _ O
performance -X- _ O
of -X- _ O
our -X- _ O
model -X- _ O
, -X- _ O
so -X- _ O
we -X- _ O
conduct -X- _ O
an -X- _ O
experiment -X- _ O
with -X- _ O
positive -X- _ O
number -X- _ O
m -X- _ B-HyperparameterName
varying -X- _ O
from -X- _ O
2 -X- _ B-HyperparameterValue
to -X- _ O
5 -X- _ B-HyperparameterValue
. -X- _ O
From -X- _ O
Tab -X- _ O
. -X- _ O
5 -X- _ O
, -X- _ O
we -X- _ O
can -X- _ O
see -X- _ O
that -X- _ O
when -X- _ O
m -X- _ B-HyperparameterName
= -X- _ O
3 -X- _ B-HyperparameterValue
, -X- _ O
our -X- _ O
model -X- _ O
achieve -X- _ O
the -X- _ O
best -X- _ O
performance -X- _ O
. -X- _ O
However -X- _ O
, -X- _ O
due -X- _ O
to -X- _ O
the -X- _ O
limitation -X- _ O
of -X- _ O
the -X- _ O
memory -X- _ O
size -X- _ O
, -X- _ O
we -X- _ O
can -X- _ O
not -X- _ O
exhaust -X- _ O
all -X- _ O
the -X- _ O
possibilities -X- _ O
, -X- _ O
but -X- _ O
we -X- _ O
found -X- _ O
that -X- _ O
when -X- _ O
m -X- _ B-HyperparameterName
≥ -X- _ O
2 -X- _ B-HyperparameterValue
, -X- _ O
the -X- _ O
performance -X- _ O
of -X- _ O
the -X- _ O
model -X- _ O
is -X- _ O
always -X- _ O
better -X- _ O
than -X- _ O
when -X- _ O
m -X- _ B-HyperparameterName
= -X- _ O
2 -X- _ B-HyperparameterValue
, -X- _ O
which -X- _ O
confirms -X- _ O
that -X- _ O
mult -X- _ O
- -X- _ O
positive -X- _ O
samples -X- _ O
can -X- _ O
bring -X- _ O
richer -X- _ O
semantics -X- _ O
, -X- _ O
allowing -X- _ O
the -X- _ O
model -X- _ O
to -X- _ O
learn -X- _ O
better -X- _ O
sentence -X- _ B-TaskName
representations -X- _ I-TaskName
. -X- _ O

The -X- _ O
influence -X- _ O
of -X- _ O
different -X- _ O
modules -X- _ O
. -X- _ O
In -X- _ O
Whitened -X- _ B-MethodName
- -X- _ I-MethodName
CSE -X- _ I-MethodName
, -X- _ O
using -X- _ O
the -X- _ O
proposed -X- _ O
shuffled -X- _ B-MethodName
- -X- _ I-MethodName
group -X- _ I-MethodName
- -X- _ I-MethodName
whitening -X- _ I-MethodName
method -X- _ O
is -X- _ O
beneficial -X- _ O
, -X- _ O
and -X- _ O
further -X- _ O
using -X- _ O
multi -X- _ O
- -X- _ O
hot -X- _ O
positive -X- _ O
samples -X- _ O
brings -X- _ O
additional -X- _ O
benefit -X- _ O
. -X- _ O
We -X- _ O
investigate -X- _ O
their -X- _ O
respective -X- _ O
contributions -X- _ O
to -X- _ O
Whitened -X- _ B-MethodName
- -X- _ I-MethodName
CSE -X- _ I-MethodName
in -X- _ O
Tab -X- _ O
. -X- _ O
6 -X- _ O
. -X- _ O
In -X- _ O
our -X- _ O
ablation -X- _ O
study -X- _ O
, -X- _ O
we -X- _ O
replace -X- _ O
the -X- _ O
whitening -X- _ O
method -X- _ O
with -X- _ O
ordinary -X- _ O
dropout -X- _ O
technique -X- _ O
, -X- _ O
and -X- _ O
still -X- _ O
retain -X- _ O
the -X- _ O
multi -X- _ O
- -X- _ O
hot -X- _ O
positive -X- _ O
sample -X- _ O
loss -X- _ O
. -X- _ O
Meanwhile -X- _ O
, -X- _ O
we -X- _ O
use -X- _ O
the -X- _ O
proposed -X- _ O
whitening -X- _ O
method -X- _ O
alone -X- _ O
, -X- _ O
and -X- _ O
keep -X- _ O
the -X- _ O
number -X- _ B-HyperparameterName
of -X- _ I-HyperparameterName
positive -X- _ I-HyperparameterName
samples -X- _ I-HyperparameterName
2 -X- _ B-HyperparameterValue
. -X- _ O
From -X- _ O
Tab -X- _ O
. -X- _ O
6 -X- _ O
, -X- _ O
we -X- _ O
find -X- _ O
that -X- _ O
multi -X- _ O
- -X- _ O
hot -X- _ O
positive -X- _ O
samples -X- _ O
based -X- _ O
on -X- _ O
dropout -X- _ O
only -X- _ O
brings -X- _ O
+0.12 -X- _ B-MetricValue
% -X- _ I-MetricValue
improvement -X- _ O
. -X- _ O
This -X- _ O
is -X- _ O
reasonable -X- _ O
because -X- _ O
when -X- _ O
the -X- _ O
data -X- _ O
augmentation -X- _ O
is -X- _ O
subtle -X- _ O
( -X- _ O
i.e. -X- _ O
, -X- _ O
the -X- _ O
dropout -X- _ O
) -X- _ O
, -X- _ O
using -X- _ O
extra -X- _ O
positive -X- _ O
samples -X- _ O
barely -X- _ O
increases -X- _ O
the -X- _ O
diversity -X- _ O
. -X- _ O
In -X- _ O
contrast -X- _ O
, -X- _ O
the -X- _ O
proposed -X- _ O
SGW -X- _ B-MethodName
gener- -X- _ O
ates -X- _ O
informative -X- _ O
data -X- _ O
augmentation -X- _ O
, -X- _ O
and -X- _ O
thus -X- _ O
well -X- _ O
accommodates -X- _ O
the -X- _ O
multi -X- _ O
- -X- _ O
hot -X- _ O
positive -X- _ O
samples -X- _ O
. -X- _ O

Conclusion -X- _ O

In -X- _ O
this -X- _ O
paper -X- _ O
, -X- _ O
we -X- _ O
proposed -X- _ O
WhitenedCSE -X- _ B-MethodName
, -X- _ O
a -X- _ O
whitening -X- _ B-MethodName
- -X- _ I-MethodName
based -X- _ I-MethodName
contrastive -X- _ I-MethodName
learning -X- _ I-MethodName
framework -X- _ I-MethodName
for -X- _ I-MethodName
unsupervised -X- _ I-MethodName
sentence -X- _ I-MethodName
representation -X- _ I-MethodName
learning -X- _ I-MethodName
. -X- _ O
We -X- _ O
proposed -X- _ O
a -X- _ O
novel -X- _ O
shuffled -X- _ B-MethodName
group -X- _ I-MethodName
whitening -X- _ I-MethodName
, -X- _ O
which -X- _ O
reinforces -X- _ O
the -X- _ O
contrastive -X- _ O
learning -X- _ O
effect -X- _ O
regarding -X- _ O
both -X- _ O
the -X- _ O
uniformity -X- _ B-MetricName
and -X- _ O
the -X- _ O
alignment -X- _ B-MetricName
. -X- _ O
Specifically -X- _ O
, -X- _ O
it -X- _ O
retains -X- _ O
the -X- _ O
role -X- _ O
of -X- _ O
whitening -X- _ O
in -X- _ O
dispersing -X- _ O
data -X- _ O
, -X- _ O
and -X- _ O
can -X- _ O
further -X- _ O
improve -X- _ O
uniformity -X- _ B-MetricName
on -X- _ O
the -X- _ O
basis -X- _ O
of -X- _ O
contrastive -X- _ O
learning -X- _ O
. -X- _ O
Additionally -X- _ O
, -X- _ O
it -X- _ O
shuffles -X- _ O
and -X- _ O
groups -X- _ O
features -X- _ O
on -X- _ O
channel -X- _ O
axis -X- _ O
, -X- _ O
and -X- _ O
performs -X- _ O
whitening -X- _ O
independently -X- _ O
within -X- _ O
each -X- _ O
group -X- _ O
. -X- _ O
This -X- _ O
kind -X- _ O
of -X- _ O
operation -X- _ O
can -X- _ O
be -X- _ O
regarded -X- _ O
as -X- _ O
a -X- _ O
disturbance -X- _ O
on -X- _ O
feature -X- _ O
dimension -X- _ O
. -X- _ O
We -X- _ O
obtain -X- _ O
multiple -X- _ O
positive -X- _ O
samples -X- _ O
through -X- _ O
this -X- _ O
operation -X- _ O
, -X- _ O
and -X- _ O
learn -X- _ O
the -X- _ O
invariance -X- _ O
to -X- _ O
this -X- _ O
disturbance -X- _ O
to -X- _ O
obtain -X- _ O
better -X- _ O
alignment -X- _ O
. -X- _ O
Experimental -X- _ O
results -X- _ O
on -X- _ O
seven -X- _ O
semantic -X- _ B-TaskName
textual -X- _ I-TaskName
similarity -X- _ I-TaskName
tasks -X- _ O
have -X- _ O
shown -X- _ O
that -X- _ O
our -X- _ O
approach -X- _ O
achieve -X- _ O
consistent -X- _ O
improvement -X- _ O
over -X- _ O
the -X- _ O
contrastive -X- _ O
learning -X- _ O
baseline -X- _ O
. -X- _ O

Limitations -X- _ O

In -X- _ O
this -X- _ O
paper -X- _ O
, -X- _ O
we -X- _ O
limit -X- _ O
the -X- _ O
proposed -X- _ O
WhitenedCSE -X- _ B-MethodName
for -X- _ O
sentence -X- _ B-TaskName
embedding -X- _ I-TaskName
learning -X- _ I-TaskName
. -X- _ O
Conceptually -X- _ O
, -X- _ O
WhitenedCSE -X- _ B-MethodName
is -X- _ O
potential -X- _ O
to -X- _ O
benefit -X- _ O
contrastive -X- _ O
learning -X- _ O
on -X- _ O
some -X- _ O
other -X- _ O
tasks -X- _ O
, -X- _ O
e.g. -X- _ O
, -X- _ O
self -X- _ O
- -X- _ O
supervised -X- _ O
image -X- _ O
representation -X- _ O
learning -X- _ O
and -X- _ O
self -X- _ O
- -X- _ O
supervised -X- _ O
vision -X- _ O
- -X- _ O
language -X- _ O
contrastive -X- _ O
learning -X- _ O
. -X- _ O
However -X- _ O
, -X- _ O
we -X- _ O
did -X- _ O
not -X- _ O
investigate -X- _ O
the -X- _ O
self -X- _ O
- -X- _ O
supervised -X- _ O
image -X- _ O
representation -X- _ O
learning -X- _ O
because -X- _ O
this -X- _ O
domain -X- _ O
is -X- _ O
currently -X- _ O
dominated -X- _ O
by -X- _ O
masked -X- _ O
image -X- _ O
modeling -X- _ O
. -X- _ O
We -X- _ O
will -X- _ O
consider -X- _ O
extending -X- _ O
WhitenedCSE -X- _ B-MethodName
for -X- _ O
visionlanguage -X- _ O
contrastive -X- _ O
learning -X- _ O
when -X- _ O
we -X- _ O
have -X- _ O
sufficient -X- _ O
training -X- _ O
resources -X- _ O
for -X- _ O
the -X- _ O
extraordinary -X- _ O
largescale -X- _ O
text -X- _ O
- -X- _ O
image -X- _ O
pairs -X- _ O
. -X- _ O
Since -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
pages -X- _ O
in -X- _ O
the -X- _ O
text -X- _ O
is -X- _ O
limited -X- _ O
and -X- _ O
our -X- _ O
model -X- _ O
does -X- _ O
not -X- _ O
have -X- _ O
significant -X- _ O
potential -X- _ O
risks -X- _ O
, -X- _ O
we -X- _ O
do -X- _ O
not -X- _ O
discuss -X- _ O
this -X- _ O
. -X- _ O

Ethics -X- _ O
Statement -X- _ O

