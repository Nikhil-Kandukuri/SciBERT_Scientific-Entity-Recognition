-DOCSTART- -X- O
VGNMN -X- _ B-MethodName
: -X- _ O
Video -X- _ B-MethodName
- -X- _ I-MethodName
grounded -X- _ I-MethodName
Neural -X- _ I-MethodName
Module -X- _ I-MethodName
Networks -X- _ I-MethodName
for -X- _ O
Video -X- _ B-TaskName
- -X- _ I-TaskName
Grounded -X- _ I-TaskName
Dialogue -X- _ I-TaskName
Systems -X- _ O

Introduction -X- _ O

Vision -X- _ O
- -X- _ O
language -X- _ O
tasks -X- _ O
have -X- _ O
been -X- _ O
studied -X- _ O
to -X- _ O
build -X- _ O
intelligent -X- _ O
systems -X- _ O
that -X- _ O
can -X- _ O
perceive -X- _ O
information -X- _ O
from -X- _ O
multiple -X- _ O
modalities -X- _ O
, -X- _ O
such -X- _ O
as -X- _ O
images -X- _ O
, -X- _ O
videos -X- _ O
, -X- _ O
and -X- _ O
text -X- _ O
. -X- _ O
Extended -X- _ O
from -X- _ O
image -X- _ O
- -X- _ O
grounded -X- _ O
tasks -X- _ O
, -X- _ O
e.g. -X- _ O
( -X- _ O
Antol -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2015 -X- _ O
) -X- _ O
, -X- _ O
recently -X- _ O
Jang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2017 -X- _ O
) -X- _ O
; -X- _ O
Lei -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2018 -X- _ O
) -X- _ O
propose -X- _ O
to -X- _ O
use -X- _ O
video -X- _ O
as -X- _ O
the -X- _ O
grounding -X- _ O
features -X- _ O
. -X- _ O
This -X- _ O
modification -X- _ O
poses -X- _ O
a -X- _ O
significant -X- _ O
challenge -X- _ O
to -X- _ O
previous -X- _ O
image -X- _ O
- -X- _ O
based -X- _ O
models -X- _ O
with -X- _ O
the -X- _ O
additional -X- _ O
temporal -X- _ O
variance -X- _ O
through -X- _ O
video -X- _ O
frames -X- _ O
. -X- _ O

Recently -X- _ O
further -X- _ O
develop -X- _ O
videogrounded -X- _ O
language -X- _ O
research -X- _ O
into -X- _ O
the -X- _ O
dialogue -X- _ O
domain -X- _ O
. -X- _ O
In -X- _ O
the -X- _ O
proposed -X- _ O
task -X- _ O
, -X- _ O
video -X- _ O
- -X- _ O
grounded -X- _ O
dialogues -X- _ O
, -X- _ O
the -X- _ O
dialogue -X- _ O
agent -X- _ O
is -X- _ O
required -X- _ O
to -X- _ O
answer -X- _ O
questions -X- _ O
about -X- _ O
a -X- _ O
video -X- _ O
over -X- _ O
multiple -X- _ O
dialogue -X- _ O
turns -X- _ O
. -X- _ O
Using -X- _ O
Figure -X- _ O
1 -X- _ O
as -X- _ O
an -X- _ O
example -X- _ O
, -X- _ O
to -X- _ O
answer -X- _ O
Video -X- _ O
Caption -X- _ O
: -X- _ O
a -X- _ O
boy -X- _ O
and -X- _ O
a -X- _ O
man -X- _ O
walk -X- _ O
to -X- _ O
the -X- _ O
room -X- _ O
. -X- _ O

The -X- _ O
boy -X- _ O
carries -X- _ O
his -X- _ O
backpack -X- _ O
while -X- _ O
the -X- _ O
man -X- _ O
â€¦ -X- _ O
Visual -X- _ O
: -X- _ O
... -X- _ O
Audio -X- _ O
: -X- _ O
... -X- _ O

Question -X- _ O

Dialogue -X- _ O
Understanding -X- _ O
Video -X- _ O
Understanding -X- _ O

Figure -X- _ O
1 -X- _ O
: -X- _ O
A -X- _ O
sample -X- _ O
video -X- _ O
- -X- _ O
grounded -X- _ O
dialogue -X- _ O
with -X- _ O
a -X- _ O
demonstration -X- _ O
of -X- _ O
a -X- _ O
reasoning -X- _ O
process -X- _ O
questions -X- _ O
correctly -X- _ O
, -X- _ O
a -X- _ O
dialogue -X- _ O
agent -X- _ O
has -X- _ O
to -X- _ O
resolve -X- _ O
references -X- _ O
in -X- _ O
dialogue -X- _ O
context -X- _ O
, -X- _ O
e.g. -X- _ O
" -X- _ O
he -X- _ O
" -X- _ O
and -X- _ O
" -X- _ O
it -X- _ O
" -X- _ O
, -X- _ O
and -X- _ O
identify -X- _ O
the -X- _ O
original -X- _ O
entity -X- _ O
, -X- _ O
e.g. -X- _ O
" -X- _ O
a -X- _ O
boy -X- _ O
" -X- _ O
and -X- _ O
" -X- _ O
a -X- _ O
backpack -X- _ O
" -X- _ O
. -X- _ O
Besides -X- _ O
, -X- _ O
the -X- _ O
agent -X- _ O
also -X- _ O
needs -X- _ O
to -X- _ O
identify -X- _ O
the -X- _ O
actions -X- _ O
of -X- _ O
these -X- _ O
entities -X- _ O
, -X- _ O
e.g. -X- _ O
" -X- _ O
carrying -X- _ O
a -X- _ O
backpack -X- _ O
" -X- _ O
to -X- _ O
retrieve -X- _ O
information -X- _ O
from -X- _ O
the -X- _ O
video -X- _ O
. -X- _ O

Current -X- _ O
state -X- _ O
- -X- _ O
of -X- _ O
- -X- _ O
the -X- _ O
- -X- _ O
art -X- _ O
approaches -X- _ O
to -X- _ O
videogrounded -X- _ O
dialogue -X- _ O
tasks -X- _ O
, -X- _ O
e.g. -X- _ O
( -X- _ O
Le -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019b -X- _ O
; -X- _ O
Fan -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
have -X- _ O
achieved -X- _ O
remarkable -X- _ O
performance -X- _ O
through -X- _ O
the -X- _ O
use -X- _ O
of -X- _ O
deep -X- _ O
neural -X- _ O
networks -X- _ O
to -X- _ O
retrieve -X- _ O
grounding -X- _ O
video -X- _ O
signals -X- _ O
based -X- _ O
on -X- _ O
language -X- _ O
inputs -X- _ O
. -X- _ O
However -X- _ O
, -X- _ O
these -X- _ O
approaches -X- _ O
often -X- _ O
assume -X- _ O
the -X- _ O
reasoning -X- _ O
structure -X- _ O
, -X- _ O
including -X- _ O
resolving -X- _ O
references -X- _ O
of -X- _ O
entities -X- _ O
and -X- _ O
detecting -X- _ O
the -X- _ O
corresponding -X- _ O
actions -X- _ O
to -X- _ O
retrieve -X- _ O
visual -X- _ O
cues -X- _ O
, -X- _ O
is -X- _ O
implicitly -X- _ O
learned -X- _ O
. -X- _ O
An -X- _ O
explicit -X- _ O
reasoning -X- _ O
structure -X- _ O
becomes -X- _ O
more -X- _ O
beneficial -X- _ O
as -X- _ O
the -X- _ O
tasks -X- _ O
complicate -X- _ O
in -X- _ O
two -X- _ O
scenarios -X- _ O
: -X- _ O
video -X- _ O
with -X- _ O
complex -X- _ O
spatial -X- _ O
and -X- _ O
temporal -X- _ O
dynamics -X- _ O
, -X- _ O
and -X- _ O
language -X- _ O
inputs -X- _ O
with -X- _ O
sophisticated -X- _ O
semantic -X- _ O
dependencies -X- _ O
, -X- _ O
e.g. -X- _ O
questions -X- _ O
positioned -X- _ O
in -X- _ O
a -X- _ O
dialogue -X- _ O
context -X- _ O
. -X- _ O
These -X- _ O
scenarios -X- _ O
often -X- _ O
challenge -X- _ O
researchers -X- _ O
to -X- _ O
interpret -X- _ O
model -X- _ O
hidden -X- _ O
layers -X- _ O
, -X- _ O
identify -X- _ O
errors -X- _ O
, -X- _ O
and -X- _ O
assess -X- _ O
model -X- _ O
reasoning -X- _ O
capability -X- _ O
. -X- _ O

Similar -X- _ O
challenges -X- _ O
have -X- _ O
been -X- _ O
observed -X- _ O
in -X- _ O
imagegrounded -X- _ O
tasks -X- _ O
in -X- _ O
which -X- _ O
deep -X- _ O
neural -X- _ O
networks -X- _ O
exhibit -X- _ O
shallow -X- _ O
understanding -X- _ O
capability -X- _ O
as -X- _ O
they -X- _ O
exploit -X- _ O
superficial -X- _ O
visual -X- _ O
cues -X- _ O
( -X- _ O
Agrawal -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2016 -X- _ O
; -X- _ O
Goyal -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017 -X- _ O
; -X- _ O
Feng -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2018 -X- _ O
; -X- _ O
Serrano -X- _ O
and -X- _ O
Smith -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
. -X- _ O
Andreas -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2016b -X- _ O
) -X- _ O
propose -X- _ O
neural -X- _ O
module -X- _ O
networks -X- _ O
( -X- _ O
NMNs -X- _ O
) -X- _ O
by -X- _ O
decomposing -X- _ O
a -X- _ O
question -X- _ O
into -X- _ O
sub -X- _ O
- -X- _ O
sequences -X- _ O
called -X- _ O
program -X- _ O
and -X- _ O
assembling -X- _ O
a -X- _ O
network -X- _ O
of -X- _ O
neural -X- _ O
operations -X- _ O
. -X- _ O
Motivated -X- _ O
by -X- _ O
this -X- _ O
line -X- _ O
of -X- _ O
research -X- _ O
, -X- _ O
we -X- _ O
propose -X- _ O
a -X- _ O
new -X- _ O
approach -X- _ O
, -X- _ O
VGNMN -X- _ B-MethodName
, -X- _ O
to -X- _ O
video -X- _ B-TaskName
- -X- _ I-TaskName
grounded -X- _ I-TaskName
language -X- _ I-TaskName
tasks -X- _ I-TaskName
. -X- _ O
Our -X- _ O
approach -X- _ O
benefits -X- _ O
from -X- _ O
integrating -X- _ O
neural -X- _ O
networks -X- _ O
with -X- _ O
a -X- _ O
compositional -X- _ O
reasoning -X- _ O
structure -X- _ O
to -X- _ O
exploit -X- _ O
low -X- _ O
- -X- _ O
level -X- _ O
information -X- _ O
signals -X- _ O
in -X- _ O
video -X- _ O
. -X- _ O
An -X- _ O
example -X- _ O
of -X- _ O
the -X- _ O
reasoning -X- _ O
structure -X- _ O
can -X- _ O
be -X- _ O
seen -X- _ O
on -X- _ O
the -X- _ O
right -X- _ O
side -X- _ O
of -X- _ O
Figure -X- _ O
1 -X- _ O
. -X- _ O

Video -X- _ B-MethodName
- -X- _ I-MethodName
grounded -X- _ I-MethodName
Neural -X- _ I-MethodName
Module -X- _ I-MethodName
Network -X- _ I-MethodName
( -X- _ O
VGNMN -X- _ B-MethodName
) -X- _ O
tackles -X- _ O
video -X- _ O
understanding -X- _ O
through -X- _ O
action -X- _ O
and -X- _ O
entity -X- _ O
- -X- _ O
paramterized -X- _ O
NMNs -X- _ O
to -X- _ O
retrieve -X- _ O
video -X- _ O
features -X- _ O
. -X- _ O
We -X- _ O
first -X- _ O
decompose -X- _ O
question -X- _ O
into -X- _ O
a -X- _ O
set -X- _ O
of -X- _ O
entities -X- _ O
and -X- _ O
extract -X- _ O
video -X- _ O
features -X- _ O
related -X- _ O
to -X- _ O
these -X- _ O
entities -X- _ O
. -X- _ O
VGNMN -X- _ B-MethodName
then -X- _ O
extracts -X- _ O
the -X- _ O
temporal -X- _ O
steps -X- _ O
by -X- _ O
focusing -X- _ O
on -X- _ O
relevant -X- _ O
actions -X- _ O
that -X- _ O
are -X- _ O
associated -X- _ O
with -X- _ O
these -X- _ O
entities -X- _ O
. -X- _ O
VGNMN -X- _ B-MethodName
is -X- _ O
analogous -X- _ O
to -X- _ O
how -X- _ O
human -X- _ O
processes -X- _ O
information -X- _ O
by -X- _ O
gradually -X- _ O
retrieving -X- _ O
signals -X- _ O
from -X- _ O
input -X- _ O
modalities -X- _ O
using -X- _ O
a -X- _ O
set -X- _ O
of -X- _ O
discrete -X- _ O
subjects -X- _ O
and -X- _ O
their -X- _ O
actions -X- _ O
. -X- _ O

To -X- _ O
tackle -X- _ O
dialogue -X- _ O
understanding -X- _ O
, -X- _ O
VGNMN -X- _ B-MethodName
is -X- _ O
trained -X- _ O
to -X- _ O
resolve -X- _ O
any -X- _ O
co -X- _ O
- -X- _ O
reference -X- _ O
in -X- _ O
language -X- _ O
inputs -X- _ O
, -X- _ O
e.g. -X- _ O
questions -X- _ O
in -X- _ O
a -X- _ O
dialogue -X- _ O
context -X- _ O
, -X- _ O
to -X- _ O
identify -X- _ O
the -X- _ O
unique -X- _ O
entities -X- _ O
in -X- _ O
each -X- _ O
dialogue -X- _ O
. -X- _ O
Previous -X- _ O
approaches -X- _ O
to -X- _ O
video -X- _ O
- -X- _ O
grounded -X- _ O
dialogues -X- _ O
often -X- _ O
obtain -X- _ O
question -X- _ O
global -X- _ O
representations -X- _ O
in -X- _ O
relation -X- _ O
to -X- _ O
dialogue -X- _ O
context -X- _ O
. -X- _ O
These -X- _ O
approaches -X- _ O
might -X- _ O
be -X- _ O
suitable -X- _ O
to -X- _ O
represent -X- _ O
general -X- _ O
semantics -X- _ O
in -X- _ O
open -X- _ O
- -X- _ O
domain -X- _ O
dialogues -X- _ O
( -X- _ O
Serban -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2016 -X- _ O
) -X- _ O
. -X- _ O
However -X- _ O
, -X- _ O
they -X- _ O
are -X- _ O
not -X- _ O
ideal -X- _ O
to -X- _ O
detect -X- _ O
fine -X- _ O
- -X- _ O
grained -X- _ O
information -X- _ O
in -X- _ O
a -X- _ O
video -X- _ O
- -X- _ O
grounded -X- _ O
dialogue -X- _ O
which -X- _ O
frequently -X- _ O
entails -X- _ O
dependencies -X- _ O
between -X- _ O
questions -X- _ O
and -X- _ O
past -X- _ O
dialogue -X- _ O
turns -X- _ O
in -X- _ O
the -X- _ O
form -X- _ O
of -X- _ O
entity -X- _ O
references -X- _ O
. -X- _ O

In -X- _ O
summary -X- _ O
, -X- _ O
our -X- _ O
contributions -X- _ O
include -X- _ O
: -X- _ O

â€¢ -X- _ O
VGNMN -X- _ B-MethodName
, -X- _ O
a -X- _ O
neural -X- _ O
module -X- _ O
network -X- _ O
- -X- _ O
based -X- _ O
approach -X- _ O
for -X- _ O
video -X- _ B-TaskName
- -X- _ I-TaskName
grounded -X- _ I-TaskName
dialogues -X- _ I-TaskName
. -X- _ O

â€¢ -X- _ O
The -X- _ O
approach -X- _ O
includes -X- _ O
a -X- _ O
modularized -X- _ O
system -X- _ O
that -X- _ O
creates -X- _ O
a -X- _ O
reasoning -X- _ O
pipeline -X- _ O
parameterized -X- _ O
by -X- _ O
entity -X- _ O
and -X- _ O
action -X- _ O
- -X- _ O
based -X- _ O
representations -X- _ O
from -X- _ O
both -X- _ O
dialogue -X- _ O
and -X- _ O
video -X- _ O
contexts -X- _ O
. -X- _ O

â€¢ -X- _ O
Our -X- _ O
experiments -X- _ O
are -X- _ O
conducted -X- _ O
on -X- _ O
the -X- _ O
challenging -X- _ O
benchmark -X- _ O
for -X- _ O
video -X- _ O
- -X- _ O
grounded -X- _ O
dialogues -X- _ O
, -X- _ O
Audio -X- _ B-DatasetName
- -X- _ I-DatasetName
visual -X- _ I-DatasetName
Scene -X- _ I-DatasetName
- -X- _ I-DatasetName
Aware -X- _ I-DatasetName
Dialogues -X- _ I-DatasetName
( -X- _ O
AVSD -X- _ B-DatasetName
) -X- _ O
as -X- _ O
well -X- _ O
as -X- _ O
TGIF -X- _ B-DatasetName
- -X- _ I-DatasetName
QA -X- _ I-DatasetName
( -X- _ O
Jang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
for -X- _ O
video -X- _ O
QA -X- _ O
task -X- _ O
. -X- _ O

â€¢ -X- _ O
Our -X- _ O
results -X- _ O
indicate -X- _ O
strong -X- _ O
performance -X- _ O
of -X- _ O
VGNMN -X- _ B-MethodName
as -X- _ O
well -X- _ O
as -X- _ O
improved -X- _ O
model -X- _ O
inter -X- _ O
- -X- _ O
pretability -X- _ O
and -X- _ O
robustness -X- _ O
to -X- _ O
difficult -X- _ O
scenarios -X- _ O
of -X- _ O
dialogues -X- _ O
, -X- _ O
videos -X- _ O
, -X- _ O
and -X- _ O
question -X- _ O
structures -X- _ O
. -X- _ O

2 -X- _ O
Related -X- _ O
Work -X- _ O

Video -X- _ O
- -X- _ O
Language -X- _ O
Understanding -X- _ O

The -X- _ O
research -X- _ O
of -X- _ O
video -X- _ O
- -X- _ O
language -X- _ O
understanding -X- _ O
aims -X- _ O
to -X- _ O
develop -X- _ O
a -X- _ O
model -X- _ O
's -X- _ O
joint -X- _ O
understanding -X- _ O
capability -X- _ O
of -X- _ O
language -X- _ O
, -X- _ O
video -X- _ O
, -X- _ O
and -X- _ O
their -X- _ O
interactions -X- _ O
. -X- _ O
Jang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2017 -X- _ O
) -X- _ O
; -X- _ O
Gao -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2018 -X- _ O
) -X- _ O
; -X- _ O
propose -X- _ O
to -X- _ O
learn -X- _ O
attention -X- _ O
guided -X- _ O
by -X- _ O
question -X- _ O
global -X- _ O
representation -X- _ O
to -X- _ O
retrieve -X- _ O
spatial -X- _ O
- -X- _ O
level -X- _ O
and -X- _ O
temporal -X- _ O
- -X- _ O
level -X- _ O
visual -X- _ O
features -X- _ O
. -X- _ O
Li -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2019 -X- _ O
) -X- _ O
; -X- _ O
Fan -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2019 -X- _ O
) -X- _ O
; -X- _ O
Jiang -X- _ O
and -X- _ O
Han -X- _ O
( -X- _ O
2020 -X- _ O
) -X- _ O
model -X- _ O
interaction -X- _ O
between -X- _ O
all -X- _ O
pairs -X- _ O
of -X- _ O
question -X- _ O
tokenlevel -X- _ O
representations -X- _ O
and -X- _ O
temporal -X- _ O
- -X- _ O
level -X- _ O
features -X- _ O
of -X- _ O
the -X- _ O
input -X- _ O
video -X- _ O
through -X- _ O
similarity -X- _ O
matrix -X- _ O
, -X- _ O
memory -X- _ O
networks -X- _ O
, -X- _ O
and -X- _ O
graph -X- _ O
networks -X- _ O
respectively -X- _ O
. -X- _ O
; -X- _ O
Le -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2019cLe -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
, -X- _ O
2020b -X- _ O
; -X- _ O
Lei -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2020 -X- _ O
) -X- _ O
; -X- _ O
Huang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2020 -X- _ O
) -X- _ O
extends -X- _ O
the -X- _ O
previous -X- _ O
approach -X- _ O
by -X- _ O
dividing -X- _ O
a -X- _ O
video -X- _ O
into -X- _ O
equal -X- _ O
segments -X- _ O
, -X- _ O
sub -X- _ O
- -X- _ O
sampling -X- _ O
video -X- _ O
frames -X- _ O
, -X- _ O
or -X- _ O
considering -X- _ O
objectlevel -X- _ O
representations -X- _ O
of -X- _ O
input -X- _ O
video -X- _ O
. -X- _ O
We -X- _ O
propose -X- _ O
to -X- _ O
replace -X- _ O
token -X- _ O
- -X- _ O
level -X- _ O
and -X- _ O
global -X- _ O
question -X- _ O
representations -X- _ O
with -X- _ O
question -X- _ O
representations -X- _ O
composed -X- _ O
of -X- _ O
specific -X- _ O
entities -X- _ O
and -X- _ O
actions -X- _ O
. -X- _ O
Recently -X- _ O
, -X- _ O
we -X- _ O
have -X- _ O
witnessed -X- _ O
emerging -X- _ O
techniques -X- _ O
in -X- _ O
video -X- _ O
- -X- _ O
language -X- _ O
systems -X- _ O
that -X- _ O
exploit -X- _ O
deep -X- _ O
transformer -X- _ O
- -X- _ O
based -X- _ O
architectures -X- _ O
such -X- _ O
as -X- _ O
BERT -X- _ O
( -X- _ O
Devlin -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
for -X- _ O
pretraining -X- _ O
multimodal -X- _ O
representations -X- _ O
( -X- _ O
Li -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020a -X- _ O
; -X- _ O
Yang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020 -X- _ O
; -X- _ O
Kim -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021 -X- _ O
; -X- _ O
Tang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021 -X- _ O
; -X- _ O
Zellers -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O
in -X- _ O
very -X- _ O
large -X- _ O
- -X- _ O
scale -X- _ O
videolanguage -X- _ O
datasets -X- _ O
. -X- _ O
While -X- _ O
these -X- _ O
systems -X- _ O
can -X- _ O
achieve -X- _ O
impressive -X- _ O
performance -X- _ O
, -X- _ O
they -X- _ O
are -X- _ O
not -X- _ O
straightforward -X- _ O
to -X- _ O
apply -X- _ O
in -X- _ O
domains -X- _ O
with -X- _ O
limited -X- _ O
data -X- _ O
such -X- _ O
as -X- _ O
video -X- _ O
- -X- _ O
grounded -X- _ O
dialogues -X- _ O
. -X- _ O
Moreover -X- _ O
, -X- _ O
as -X- _ O
we -X- _ O
shown -X- _ O
in -X- _ O
our -X- _ O
qualitative -X- _ O
examples -X- _ O
, -X- _ O
our -X- _ O
approach -X- _ O
facilitates -X- _ O
better -X- _ O
interpretability -X- _ O
through -X- _ O
the -X- _ O
output -X- _ O
of -X- _ O
decoded -X- _ O
functional -X- _ O
programs -X- _ O
. -X- _ O

Video -X- _ B-TaskName
- -X- _ I-TaskName
grounded -X- _ I-TaskName
Dialogues -X- _ I-TaskName

Extended -X- _ O
from -X- _ O
video -X- _ O
QA -X- _ O
, -X- _ O
video -X- _ B-TaskName
- -X- _ I-TaskName
grounded -X- _ I-TaskName
dialogue -X- _ I-TaskName
is -X- _ O
an -X- _ O
emerging -X- _ O
task -X- _ O
that -X- _ O
combines -X- _ O
dialogue -X- _ O
response -X- _ O
generation -X- _ O
and -X- _ O
video -X- _ O
- -X- _ O
language -X- _ O
understanding -X- _ O
research -X- _ O
. -X- _ O
This -X- _ O
task -X- _ O
entails -X- _ O
a -X- _ O
novel -X- _ O
requirement -X- _ O
for -X- _ O
models -X- _ O
to -X- _ O
learn -X- _ O
dialogue -X- _ O
semantics -X- _ O
and -X- _ O
decode -X- _ O
entity -X- _ O
co -X- _ O
- -X- _ O
references -X- _ O
in -X- _ O
questions -X- _ O
. -X- _ O
Nguyen -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2018 -X- _ O
) -X- _ O
; -X- _ O
; -X- _ O
; -X- _ O
Sanabria -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2019 -X- _ O
) -X- _ O
; -X- _ O
Le -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2019a -X- _ O
, -X- _ O
b -X- _ O
) -X- _ O
extend -X- _ O
traditional -X- _ O
QA -X- _ O
models -X- _ O
by -X- _ O
adding -X- _ O
dialogue -X- _ O
history -X- _ O
neural -X- _ O
encoders -X- _ O
. -X- _ O
Kumar -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2019 -X- _ O
) -X- _ O
en -X- _ O
- -X- _ O
hances -X- _ O
dialogue -X- _ O
features -X- _ O
with -X- _ O
topic -X- _ O
- -X- _ O
level -X- _ O
representations -X- _ O
to -X- _ O
express -X- _ O
the -X- _ O
general -X- _ O
topic -X- _ O
in -X- _ O
each -X- _ O
dialogue -X- _ O
. -X- _ O
Schwartz -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2019 -X- _ O
) -X- _ O
treats -X- _ O
each -X- _ O
dialogue -X- _ O
turn -X- _ O
as -X- _ O
an -X- _ O
independent -X- _ O
sequence -X- _ O
and -X- _ O
allows -X- _ O
interaction -X- _ O
between -X- _ O
questions -X- _ O
and -X- _ O
each -X- _ O
dialogue -X- _ O
turn -X- _ O
. -X- _ O
Le -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2019b -X- _ O
) -X- _ O
encodes -X- _ O
dialogue -X- _ O
history -X- _ O
as -X- _ O
a -X- _ O
sequence -X- _ O
with -X- _ O
embedding -X- _ O
and -X- _ O
positional -X- _ O
representations -X- _ O
. -X- _ O
Different -X- _ O
from -X- _ O
prior -X- _ O
work -X- _ O
, -X- _ O
we -X- _ O
dissect -X- _ O
the -X- _ O
question -X- _ O
sequence -X- _ O
and -X- _ O
explicitly -X- _ O
detect -X- _ O
and -X- _ O
decode -X- _ O
any -X- _ O
entities -X- _ O
and -X- _ O
their -X- _ O
references -X- _ O
. -X- _ O
Our -X- _ O
approach -X- _ O
also -X- _ O
enables -X- _ O
insights -X- _ O
on -X- _ O
how -X- _ O
models -X- _ O
extract -X- _ O
deductive -X- _ O
bias -X- _ O
from -X- _ O
dialogues -X- _ O
to -X- _ O
extract -X- _ O
video -X- _ O
information -X- _ O
. -X- _ O

Neural -X- _ O
Module -X- _ O
Network -X- _ O

Neural -X- _ O
Module -X- _ O
Network -X- _ O
( -X- _ O
NMN -X- _ O
) -X- _ O
( -X- _ O
Andreas -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2016b -X- _ O
, -X- _ O
a -X- _ O
) -X- _ O
is -X- _ O
introduced -X- _ O
to -X- _ O
address -X- _ O
visual -X- _ O
QA -X- _ O
by -X- _ O
decomposing -X- _ O
questions -X- _ O
into -X- _ O
linguistic -X- _ O
sub -X- _ O
- -X- _ O
structures -X- _ O
, -X- _ O
known -X- _ O
as -X- _ O
programs -X- _ O
, -X- _ O
to -X- _ O
instantiate -X- _ O
a -X- _ O
network -X- _ O
of -X- _ O
neural -X- _ O
modules -X- _ O
. -X- _ O
NMN -X- _ O
models -X- _ O
have -X- _ O
achieved -X- _ O
success -X- _ O
in -X- _ O
synthetic -X- _ O
image -X- _ O
domains -X- _ O
where -X- _ O
a -X- _ O
multi -X- _ O
- -X- _ O
step -X- _ O
reasoning -X- _ O
process -X- _ O
is -X- _ O
required -X- _ O
( -X- _ O
Johnson -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017b -X- _ O
; -X- _ O
Hu -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2018 -X- _ O
; -X- _ O
Han -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
. -X- _ O
Yi -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2018 -X- _ O
) -X- _ O
; -X- _ O
Han -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2019 -X- _ O
) -X- _ O
; -X- _ O
improve -X- _ O
NMN -X- _ O
models -X- _ O
by -X- _ O
decoupling -X- _ O
visual -X- _ O
- -X- _ O
language -X- _ O
understanding -X- _ O
and -X- _ O
visual -X- _ O
concept -X- _ O
learning -X- _ O
. -X- _ O
Our -X- _ O
work -X- _ O
is -X- _ O
related -X- _ O
to -X- _ O
the -X- _ O
recent -X- _ O
work -X- _ O
( -X- _ O
Kottur -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2018 -X- _ O
; -X- _ O
Jiang -X- _ O
and -X- _ O
Bansal -X- _ O
, -X- _ O
2019 -X- _ O
; -X- _ O
Gupta -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
that -X- _ O
extended -X- _ O
NMNs -X- _ O
to -X- _ O
image -X- _ O
reasoning -X- _ O
in -X- _ O
dialogues -X- _ O
and -X- _ O
reading -X- _ O
comprehension -X- _ O
reasoning -X- _ O
. -X- _ O
Our -X- _ O
approach -X- _ O
follows -X- _ O
the -X- _ O
previous -X- _ O
approaches -X- _ O
that -X- _ O
learn -X- _ O
to -X- _ O
generate -X- _ O
program -X- _ O
structure -X- _ O
and -X- _ O
require -X- _ O
no -X- _ O
parser -X- _ O
at -X- _ O
evaluation -X- _ O
time -X- _ O
. -X- _ O
Compared -X- _ O
to -X- _ O
prior -X- _ O
work -X- _ O
, -X- _ O
we -X- _ O
use -X- _ O
NMN -X- _ O
to -X- _ O
learn -X- _ O
dependencies -X- _ O
between -X- _ O
the -X- _ O
composition -X- _ O
in -X- _ O
language -X- _ O
inputs -X- _ O
and -X- _ O
the -X- _ O
spatio -X- _ O
- -X- _ O
temporal -X- _ O
dynamics -X- _ O
in -X- _ O
videos -X- _ O
. -X- _ O
Specifically -X- _ O
, -X- _ O
we -X- _ O
propose -X- _ O
to -X- _ O
construct -X- _ O
a -X- _ O
reasoning -X- _ O
structure -X- _ O
from -X- _ O
text -X- _ O
, -X- _ O
from -X- _ O
which -X- _ O
detected -X- _ O
entities -X- _ O
are -X- _ O
used -X- _ O
to -X- _ O
extract -X- _ O
visual -X- _ O
information -X- _ O
in -X- _ O
the -X- _ O
spatial -X- _ O
space -X- _ O
and -X- _ O
detected -X- _ O
actions -X- _ O
are -X- _ O
used -X- _ O
to -X- _ O
find -X- _ O
visual -X- _ O
information -X- _ O
in -X- _ O
the -X- _ O
temporal -X- _ O
space -X- _ O
. -X- _ O

Method -X- _ O

In -X- _ O
this -X- _ O
section -X- _ O
, -X- _ O
we -X- _ O
present -X- _ O
the -X- _ O
design -X- _ O
of -X- _ O
our -X- _ O
model -X- _ O
. -X- _ O
An -X- _ O
overview -X- _ O
of -X- _ O
the -X- _ O
model -X- _ O
can -X- _ O
be -X- _ O
seen -X- _ O
in -X- _ O
Figure -X- _ O
2 -X- _ O
. -X- _ O

Task -X- _ O
Definition -X- _ O

The -X- _ O
input -X- _ O
to -X- _ O
the -X- _ O
model -X- _ O
consists -X- _ O
of -X- _ O
a -X- _ O
dialogue -X- _ O
D -X- _ O
which -X- _ O
is -X- _ O
grounded -X- _ O
on -X- _ O
a -X- _ O
video -X- _ O
V. -X- _ O
The -X- _ O
input -X- _ O
components -X- _ O
include -X- _ O
the -X- _ O
question -X- _ O
of -X- _ O
current -X- _ O
dialogue -X- _ O
turn -X- _ O
Q -X- _ O
, -X- _ O
dialogue -X- _ O
history -X- _ O
H -X- _ O
, -X- _ O
and -X- _ O
the -X- _ O
features -X- _ O
of -X- _ O
the -X- _ O
input -X- _ O
video -X- _ O
, -X- _ O
including -X- _ O
visual -X- _ O
and -X- _ O
audio -X- _ O
input -X- _ O
. -X- _ O
The -X- _ O
output -X- _ O
is -X- _ O
a -X- _ O
dialogue -X- _ O
response -X- _ O
, -X- _ O
denoted -X- _ O
as -X- _ O
R. -X- _ O
Each -X- _ O
text -X- _ O
input -X- _ O
component -X- _ O
is -X- _ O
a -X- _ O
sequence -X- _ O
of -X- _ O
words -X- _ O
w -X- _ O
1 -X- _ O
, -X- _ O
... -X- _ O
, -X- _ O
w -X- _ O
m -X- _ O
âˆˆ -X- _ O
V -X- _ O
in -X- _ O
, -X- _ O
the -X- _ O
input -X- _ O
vocabulary -X- _ O
. -X- _ O
Similarly -X- _ O
, -X- _ O
the -X- _ O
output -X- _ O
response -X- _ O
R -X- _ O
is -X- _ O
a -X- _ O
sequence -X- _ O
of -X- _ O
tokens -X- _ O
w -X- _ O
1 -X- _ O
, -X- _ O
... -X- _ O
, -X- _ O
w -X- _ O
n -X- _ O
âˆˆ -X- _ O
V -X- _ O
out -X- _ O
, -X- _ O
the -X- _ O
output -X- _ O
vocabulary -X- _ O
. -X- _ O
The -X- _ O
objective -X- _ O
of -X- _ O
the -X- _ O
task -X- _ O
is -X- _ O
the -X- _ O
generation -X- _ O
objective -X- _ O
that -X- _ O
output -X- _ O
answers -X- _ O
of -X- _ O
the -X- _ O
current -X- _ O
dialogue -X- _ O
turn -X- _ O
t -X- _ O
: -X- _ O

R -X- _ O
t -X- _ O
= -X- _ O
arg -X- _ O
max -X- _ O
Rt -X- _ O
P -X- _ O
( -X- _ O
R -X- _ O
t -X- _ O
|V -X- _ O
, -X- _ O
H -X- _ O
t -X- _ O
, -X- _ O
Q -X- _ O
t -X- _ O
; -X- _ O
Î¸ -X- _ O
) -X- _ O
= -X- _ O
arg -X- _ O
max -X- _ O
Rt -X- _ O
L -X- _ O
R -X- _ O
n=1 -X- _ O
P -X- _ O
m -X- _ O
( -X- _ O
w -X- _ O
n -X- _ O
|R -X- _ O
t,1 -X- _ O
: -X- _ O
nâˆ’1 -X- _ O
, -X- _ O
V -X- _ O
, -X- _ O
H -X- _ O
t -X- _ O
, -X- _ O
Q -X- _ O
t -X- _ O
; -X- _ O
Î¸ -X- _ O
) -X- _ O

where -X- _ O
L -X- _ O
R -X- _ O
is -X- _ O
the -X- _ O
length -X- _ O
of -X- _ O
the -X- _ O
sequence -X- _ O
R. -X- _ O
In -X- _ O
a -X- _ O
Video -X- _ O
- -X- _ O
QA -X- _ O
task -X- _ O
, -X- _ O
the -X- _ O
dialogue -X- _ O
history -X- _ O
H -X- _ O
is -X- _ O
simply -X- _ O
absent -X- _ O
and -X- _ O
the -X- _ O
output -X- _ O
response -X- _ O
is -X- _ O
typically -X- _ O
collapsed -X- _ O
to -X- _ O
a -X- _ O
single -X- _ O
- -X- _ O
token -X- _ O
response -X- _ O
. -X- _ O

Encoders -X- _ O

Text -X- _ O
Encoder -X- _ O
. -X- _ O
A -X- _ O
text -X- _ O
encoder -X- _ O
is -X- _ O
shared -X- _ O
to -X- _ O
encode -X- _ O
text -X- _ O
inputs -X- _ O
, -X- _ O
including -X- _ O
dialogue -X- _ O
history -X- _ O
, -X- _ O
questions -X- _ O
, -X- _ O
and -X- _ O
captions -X- _ O
. -X- _ O
The -X- _ O
text -X- _ O
encoder -X- _ O
converts -X- _ O
each -X- _ O
text -X- _ O
sequence -X- _ O
X -X- _ O
= -X- _ O
w -X- _ O
1 -X- _ O
, -X- _ O
... -X- _ O
, -X- _ O
w -X- _ O
m -X- _ O
into -X- _ O
a -X- _ O
sequence -X- _ O
of -X- _ O
embeddings -X- _ O
X -X- _ O
âˆˆ -X- _ O
R -X- _ O
mÃ—d -X- _ O
. -X- _ O
We -X- _ O
use -X- _ O
a -X- _ O
trainable -X- _ O
embedding -X- _ O
matrix -X- _ O
to -X- _ O
map -X- _ O
token -X- _ O
indices -X- _ O
to -X- _ O
vector -X- _ O
representations -X- _ O
of -X- _ O
d -X- _ O
dimensions -X- _ O
through -X- _ O
a -X- _ O
mapping -X- _ O
function -X- _ O
Ï† -X- _ O
. -X- _ O
These -X- _ O
vectors -X- _ O
are -X- _ O
then -X- _ O
integrated -X- _ O
with -X- _ O
ordering -X- _ O
information -X- _ O
of -X- _ O
tokens -X- _ O
through -X- _ O
a -X- _ O
positional -X- _ O
encoding -X- _ O
function -X- _ O
with -X- _ O
layer -X- _ O
normalization -X- _ O
( -X- _ O
Ba -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2016 -X- _ O
; -X- _ O
Vaswani -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017 -X- _ O
1 -X- _ O
: -X- _ O
Description -X- _ O
of -X- _ O
the -X- _ O
modules -X- _ O
and -X- _ O
their -X- _ O
functionalities -X- _ O
. -X- _ O
We -X- _ O
denote -X- _ O
P -X- _ O
as -X- _ O
the -X- _ O
parameter -X- _ O
to -X- _ O
instantiate -X- _ O
each -X- _ O
module -X- _ O
, -X- _ O
H -X- _ O
as -X- _ O
the -X- _ O
dialogue -X- _ O
history -X- _ O
, -X- _ O
Q -X- _ O
as -X- _ O
the -X- _ O
question -X- _ O
of -X- _ O
the -X- _ O
current -X- _ O
dialogue -X- _ O
turn -X- _ O
, -X- _ O
and -X- _ O
V -X- _ O
as -X- _ O
video -X- _ O
input -X- _ O
. -X- _ O

corresponding -X- _ O
coordinates -X- _ O
projected -X- _ O
to -X- _ O
d -X- _ O
vis -X- _ O
dimensions -X- _ O
. -X- _ O
We -X- _ O
also -X- _ O
make -X- _ O
use -X- _ O
of -X- _ O
a -X- _ O
CNN -X- _ B-MethodName
- -X- _ O
based -X- _ O
pretrained -X- _ O
model -X- _ O
to -X- _ O
obtain -X- _ O
features -X- _ O
of -X- _ O
temporal -X- _ O
dimension -X- _ O
Z -X- _ O
cnn -X- _ O
âˆˆ -X- _ O
R -X- _ O
F -X- _ O
Ã—d -X- _ O
vis -X- _ O
. -X- _ O
The -X- _ O
audio -X- _ O
feature -X- _ O
is -X- _ O
obtained -X- _ O
through -X- _ O
a -X- _ O
pretrained -X- _ O
audio -X- _ O
model -X- _ O
, -X- _ O
Z -X- _ O
aud -X- _ O
âˆˆ -X- _ O
R -X- _ O
F -X- _ O
Ã—d -X- _ O
aud -X- _ O
. -X- _ O
We -X- _ O
passed -X- _ O
all -X- _ O
video -X- _ O
features -X- _ O
through -X- _ O
a -X- _ O
linear -X- _ O
transformation -X- _ O
layer -X- _ O
with -X- _ O
ReLU -X- _ O
activation -X- _ B-HyperparameterName
to -X- _ O
the -X- _ O
same -X- _ O
embedding -X- _ O
dimension -X- _ O
d -X- _ O
. -X- _ O

Neural -X- _ O
Modules -X- _ O

We -X- _ O
introduce -X- _ O
neural -X- _ O
modules -X- _ O
that -X- _ O
are -X- _ O
used -X- _ O
to -X- _ O
assemble -X- _ O
an -X- _ O
executable -X- _ O
program -X- _ O
constructed -X- _ O
by -X- _ O
the -X- _ O
generated -X- _ O
sequence -X- _ O
from -X- _ O
question -X- _ O
parsers -X- _ O
. -X- _ O
We -X- _ O
provide -X- _ O
an -X- _ O
overview -X- _ O
of -X- _ O
neural -X- _ O
modules -X- _ O
in -X- _ O
Table -X- _ O
1 -X- _ O
and -X- _ O
demonstrate -X- _ O
dialogue -X- _ O
understanding -X- _ O
and -X- _ O
video -X- _ O
understanding -X- _ O
modules -X- _ O
in -X- _ O
Figure -X- _ O
3 -X- _ O
and -X- _ O
4 -X- _ O
respectively -X- _ O
. -X- _ O
Each -X- _ O
module -X- _ O
parameter -X- _ O
, -X- _ O
e.g. -X- _ O
" -X- _ O
a -X- _ O
backpack -X- _ O
" -X- _ O
, -X- _ O
is -X- _ O
extracted -X- _ O
from -X- _ O
the -X- _ O
parsed -X- _ O
program -X- _ O
( -X- _ O
See -X- _ O
Section -X- _ O
3.4 -X- _ O
) -X- _ O
. -X- _ O

For -X- _ O
each -X- _ O
parameter -X- _ O
, -X- _ O
we -X- _ O
denote -X- _ O
P -X- _ O
âˆˆ -X- _ O
R -X- _ O
d -X- _ O
as -X- _ O
the -X- _ O
average -X- _ O
pooling -X- _ O
of -X- _ O
component -X- _ O
token -X- _ O
embeddings -X- _ O
. -X- _ O
find -X- _ O
( -X- _ O
P -X- _ O
, -X- _ O
H -X- _ O
) -X- _ O
â†’H -X- _ O
ent -X- _ O
. -X- _ O
This -X- _ O
module -X- _ O
handles -X- _ O
entity -X- _ O
tracing -X- _ O
by -X- _ O
obtaining -X- _ O
a -X- _ O
distribution -X- _ O
over -X- _ O
tokens -X- _ O
in -X- _ O
the -X- _ O
dialogue -X- _ O
history -X- _ O
. -X- _ O
We -X- _ O
use -X- _ O
an -X- _ O
entity -X- _ O
- -X- _ O
todialogue -X- _ O
- -X- _ O
history -X- _ O
attention -X- _ O
mechanism -X- _ O
applied -X- _ O
from -X- _ O
an -X- _ O
entity -X- _ O
P -X- _ O
i -X- _ O
to -X- _ O
all -X- _ O
tokens -X- _ O
in -X- _ O
the -X- _ O
dialogue -X- _ O
history -X- _ O
. -X- _ O
Any -X- _ O
neural -X- _ O
network -X- _ O
that -X- _ O
learn -X- _ O
to -X- _ O
generate -X- _ O
attention -X- _ O
between -X- _ O
two -X- _ O
tensors -X- _ O
is -X- _ O
applicable -X- _ O
.e.g -X- _ O
. -X- _ O
( -X- _ O
Bahdanau -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2015 -X- _ O
; -X- _ O
Vaswani -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
. -X- _ O
The -X- _ O
attention -X- _ O
matrix -X- _ O
normalized -X- _ O
by -X- _ O
softmax -X- _ O
, -X- _ O
A -X- _ O
find -X- _ O
, -X- _ O
i -X- _ O
âˆˆ -X- _ O
R -X- _ O
L -X- _ O
H -X- _ O
, -X- _ O
is -X- _ O
used -X- _ O
to -X- _ O
compute -X- _ O
the -X- _ O
weighted -X- _ O
sum -X- _ O
of -X- _ O
dialogue -X- _ O
history -X- _ O
token -X- _ O
representations -X- _ O
. -X- _ O
The -X- _ O
output -X- _ O
is -X- _ O
combined -X- _ O
with -X- _ O
entity -X- _ O
embedding -X- _ O
P -X- _ O
i -X- _ O
to -X- _ O
obtain -X- _ O
contextual -X- _ O
entity -X- _ O
representation -X- _ O
H -X- _ O
ent -X- _ O
, -X- _ O
i -X- _ O
âˆˆ -X- _ O
R -X- _ O
d -X- _ O
. -X- _ O

summarize -X- _ O
( -X- _ O
H -X- _ O
ent -X- _ O
, -X- _ O
Q -X- _ O
) -X- _ O
â†’Q -X- _ O
ctx -X- _ O
. -X- _ O
For -X- _ O
each -X- _ O
contextual -X- _ O
entity -X- _ O
representation -X- _ O
H -X- _ O
ent -X- _ O
, -X- _ O
i -X- _ O
, -X- _ O
i -X- _ O
= -X- _ O
1 -X- _ O
, -X- _ O
... -X- _ O
, -X- _ O
N -X- _ O
ent -X- _ O
, -X- _ O
it -X- _ O
is -X- _ O
projected -X- _ O
to -X- _ O
L -X- _ O
Q -X- _ O
dimensions -X- _ O
and -X- _ O
is -X- _ O
combined -X- _ O
with -X- _ O
question -X- _ O
token -X- _ O
embeddings -X- _ O
through -X- _ O
elementwise -X- _ O
summation -X- _ O
to -X- _ O
obtain -X- _ O
entity -X- _ O
- -X- _ O
aware -X- _ O
question -X- _ O
representation -X- _ O
Q -X- _ O
ent -X- _ O
, -X- _ O
i -X- _ O
âˆˆ -X- _ O
R -X- _ O
L -X- _ O
Q -X- _ O
Ã—d -X- _ O
. -X- _ O
It -X- _ O
is -X- _ O
fed -X- _ O
to -X- _ O
a -X- _ O
onedimensional -X- _ O
CNN -X- _ O
with -X- _ O
max -X- _ O
- -X- _ O
pooling -X- _ O
layer -X- _ O
( -X- _ O
Kim -X- _ O
, -X- _ O
2014 -X- _ O
) -X- _ O
to -X- _ O
obtain -X- _ O
a -X- _ O
contextual -X- _ O
entity -X- _ O
- -X- _ O
aware -X- _ O
question -X- _ O
representation -X- _ O
. -X- _ O
We -X- _ O
denote -X- _ O
the -X- _ O
final -X- _ O
output -X- _ O

as -X- _ O
Q -X- _ O
ctx -X- _ O
âˆˆ -X- _ O
R -X- _ O
NentÃ—d -X- _ O
. -X- _ O

While -X- _ O
previous -X- _ O
models -X- _ O
usually -X- _ O
focus -X- _ O
on -X- _ O
global -X- _ O
or -X- _ O
token -X- _ O
- -X- _ O
level -X- _ O
dependencies -X- _ O
Le -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019b -X- _ O
) -X- _ O
to -X- _ O
encode -X- _ O
question -X- _ O
features -X- _ O
, -X- _ O
our -X- _ O
modules -X- _ O
compress -X- _ O
fine -X- _ O
- -X- _ O
grained -X- _ O
question -X- _ O
representations -X- _ O
at -X- _ O
the -X- _ O
entity -X- _ O
level -X- _ O
. -X- _ O
Specifically -X- _ O
, -X- _ O
find -X- _ O
and -X- _ O
summarize -X- _ O
modules -X- _ O
can -X- _ O
generate -X- _ O
entitydependent -X- _ O
local -X- _ O
and -X- _ O
global -X- _ O
representations -X- _ O
of -X- _ O
question -X- _ O
semantics -X- _ O
. -X- _ O
We -X- _ O
show -X- _ O
that -X- _ O
our -X- _ O
modularized -X- _ O
approach -X- _ O
can -X- _ O
achieve -X- _ O
better -X- _ O
performance -X- _ O
and -X- _ O
transparency -X- _ O
than -X- _ O
traditional -X- _ O
approaches -X- _ O
to -X- _ O
encode -X- _ O
dialogue -X- _ O
context -X- _ O
( -X- _ O
Serban -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2016 -X- _ O
; -X- _ O
Vaswani -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
( -X- _ O
Section -X- _ O
4 -X- _ O
) -X- _ O
. -X- _ O

where -X- _ O
( -X- _ O
P -X- _ O
, -X- _ O
V -X- _ O
) -X- _ O
â†’V -X- _ O
ent -X- _ O
. -X- _ O
Similar -X- _ O
to -X- _ O
the -X- _ O
find -X- _ O
module -X- _ O
, -X- _ O
this -X- _ O
module -X- _ O
handles -X- _ O
entity -X- _ O
- -X- _ O
based -X- _ O
attention -X- _ O
to -X- _ O
the -X- _ O
video -X- _ O
input -X- _ O
. -X- _ O
However -X- _ O
, -X- _ O
the -X- _ O
entity -X- _ O
representation -X- _ O
P -X- _ O
, -X- _ O
in -X- _ O
this -X- _ O
case -X- _ O
, -X- _ O
is -X- _ O
parameterized -X- _ O
by -X- _ O
the -X- _ O
original -X- _ O
entity -X- _ O
in -X- _ O
dialogue -X- _ O
rather -X- _ O
than -X- _ O
in -X- _ O
question -X- _ O
( -X- _ O
See -X- _ O
Section -X- _ O
3.4 -X- _ O
for -X- _ O
more -X- _ O
description -X- _ O
) -X- _ O
. -X- _ O
Each -X- _ O
entity -X- _ O
P -X- _ O
i -X- _ O
is -X- _ O
stacked -X- _ O
to -X- _ O
match -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
sampled -X- _ O
video -X- _ O
frames -X- _ O
/ -X- _ O
clips -X- _ O
F -X- _ O
. -X- _ O
An -X- _ O
attention -X- _ B-HyperparameterName
network -X- _ I-HyperparameterName
is -X- _ O
used -X- _ O
to -X- _ O
obtain -X- _ O
entity -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
object -X- _ O
attention -X- _ O
matrix -X- _ O
A -X- _ O
where -X- _ O
, -X- _ O
i -X- _ O
âˆˆ -X- _ O
R -X- _ O
F -X- _ O
Ã—O -X- _ O
. -X- _ O
The -X- _ O
attended -X- _ O
feature -X- _ O
are -X- _ O
compressed -X- _ O
through -X- _ O
weighted -X- _ O
sum -X- _ O
pooling -X- _ O
along -X- _ O
the -X- _ O
spatial -X- _ O
dimension -X- _ O
, -X- _ O
resulting -X- _ O
in -X- _ O
V -X- _ O
ent -X- _ O
, -X- _ O
i -X- _ O
âˆˆ -X- _ O
R -X- _ O
F -X- _ O
Ã—d -X- _ O
, -X- _ O
i -X- _ O
= -X- _ O
1 -X- _ O
, -X- _ O
... -X- _ O
, -X- _ O
N -X- _ O
ent -X- _ O
. -X- _ O
when -X- _ O
( -X- _ O
P -X- _ O
, -X- _ O
V -X- _ O
ent -X- _ O
) -X- _ O
â†’V -X- _ O
ent+act -X- _ O
. -X- _ O
This -X- _ O
module -X- _ O
follows -X- _ O
a -X- _ O
similar -X- _ O
architecture -X- _ O
as -X- _ O
the -X- _ O
where -X- _ O
module -X- _ O
. -X- _ O
However -X- _ O
, -X- _ O
the -X- _ O
action -X- _ O
parameter -X- _ O
P -X- _ O
i -X- _ O
is -X- _ O
stacked -X- _ O
to -X- _ O
match -X- _ O
N -X- _ O
ent -X- _ O
dimensions -X- _ O
. -X- _ O
The -X- _ O
attention -X- _ O
matrix -X- _ O
A -X- _ O
when -X- _ O
, -X- _ O
i -X- _ O
âˆˆ -X- _ O
R -X- _ O
F -X- _ O
is -X- _ O
then -X- _ O
used -X- _ O
to -X- _ O
compute -X- _ O
the -X- _ O
visual -X- _ O
entity -X- _ O
- -X- _ O
action -X- _ O
representations -X- _ O
through -X- _ O
weighted -X- _ O
sum -X- _ O
along -X- _ O
the -X- _ O
temporal -X- _ O
dimension -X- _ O
. -X- _ O
We -X- _ O
denote -X- _ O
the -X- _ O
output -X- _ O
for -X- _ O
all -X- _ O
actions -X- _ O
P -X- _ O
i -X- _ O
as -X- _ O
V -X- _ O
ent+act -X- _ O
âˆˆ -X- _ O
R -X- _ O
NentÃ—NactÃ—d -X- _ O
describe -X- _ O
( -X- _ O
P -X- _ O
, -X- _ O
V -X- _ O
ent+act -X- _ O
) -X- _ O
â†’V -X- _ O
ctx -X- _ O
. -X- _ O
This -X- _ O
module -X- _ O
is -X- _ O
a -X- _ O
linear -X- _ O
transformation -X- _ O
to -X- _ O
compute -X- _ O
V -X- _ O
ctx -X- _ O
= -X- _ O
W -X- _ O
desc -X- _ O
T -X- _ O
[ -X- _ O
V -X- _ O
ent+act -X- _ O
; -X- _ O
P -X- _ O
stack -X- _ O
] -X- _ O
âˆˆ -X- _ O
R -X- _ O
NentÃ—NactÃ—d -X- _ O
where -X- _ O
W -X- _ O
desc -X- _ O
âˆˆ -X- _ O
R -X- _ O
2dÃ—d -X- _ O
, -X- _ O
P -X- _ O
stack -X- _ O
is -X- _ O
the -X- _ O
stacked -X- _ O
representations -X- _ O
of -X- _ O
parameter -X- _ O
embedding -X- _ O
P -X- _ O
to -X- _ O
N -X- _ O
ent -X- _ O
Ã— -X- _ O
N -X- _ O
act -X- _ O
dimensions -X- _ O
, -X- _ O
and -X- _ O
[ -X- _ O
; -X- _ O
] -X- _ O
is -X- _ O
the -X- _ O
concatenation -X- _ O
operation -X- _ O
. -X- _ O
Note -X- _ O
that -X- _ O
the -X- _ O
parameter -X- _ O
P -X- _ O
here -X- _ O
is -X- _ O
extracted -X- _ O
from -X- _ O
questions -X- _ O
, -X- _ O
often -X- _ O
as -X- _ O
the -X- _ O
type -X- _ O
of -X- _ O
questions -X- _ O
e. -X- _ O
and -X- _ O
" -X- _ O
how -X- _ O
" -X- _ O
. -X- _ O
This -X- _ O
eliminates -X- _ O
the -X- _ O
need -X- _ O
to -X- _ O
have -X- _ O
different -X- _ O
modules -X- _ O
for -X- _ O
different -X- _ O
question -X- _ O
types -X- _ O
. -X- _ O
However -X- _ O
, -X- _ O
we -X- _ O
noted -X- _ O
the -X- _ O
current -X- _ O
design -X- _ O
may -X- _ O
be -X- _ O
challenged -X- _ O
in -X- _ O
rare -X- _ O
cases -X- _ O
in -X- _ O
which -X- _ O
an -X- _ O
utterance -X- _ O
contain -X- _ O
numerous -X- _ O
questions -X- _ O
( -X- _ O
refer -X- _ O
to -X- _ O
Figure -X- _ O
5 -X- _ O
) -X- _ O
. -X- _ O

The -X- _ O
exist -X- _ O
module -X- _ O
is -X- _ O
used -X- _ O
when -X- _ O
the -X- _ O
questions -X- _ O
are -X- _ O
" -X- _ O
yes -X- _ O
/ -X- _ O
no -X- _ O
" -X- _ O
questions -X- _ O
. -X- _ O
This -X- _ O
module -X- _ O
is -X- _ O
a -X- _ O
special -X- _ O
case -X- _ O
of -X- _ O
describe -X- _ O
module -X- _ O
where -X- _ O
the -X- _ O
parameter -X- _ O
P -X- _ O
is -X- _ O
simply -X- _ O
the -X- _ O
average -X- _ O
pooled -X- _ O
question -X- _ O
embeddings -X- _ O
. -X- _ O
The -X- _ O
above -X- _ O
where -X- _ O
module -X- _ O
is -X- _ O
applied -X- _ O
to -X- _ O
object -X- _ O
- -X- _ O
level -X- _ O
features -X- _ O
. -X- _ O
For -X- _ O
temporal -X- _ O
- -X- _ O
based -X- _ O
features -X- _ O
such -X- _ O
as -X- _ O
CNN -X- _ O
- -X- _ O
based -X- _ O
and -X- _ O
audio -X- _ O
features -X- _ O
, -X- _ O
the -X- _ O
same -X- _ O
neural -X- _ O
operation -X- _ O
is -X- _ O
applied -X- _ O
along -X- _ O
the -X- _ O
temporal -X- _ O
dimension -X- _ O
. -X- _ O
Each -X- _ O
resulting -X- _ O
entity -X- _ O
- -X- _ O
aware -X- _ O
output -X- _ O
is -X- _ O
then -X- _ O
incorporated -X- _ O
to -X- _ O
frame -X- _ O
- -X- _ O
level -X- _ O
features -X- _ O
through -X- _ O
element -X- _ O
- -X- _ O
wise -X- _ O
summation -X- _ O
. -X- _ O

An -X- _ O
advantage -X- _ O
of -X- _ O
our -X- _ O
architecture -X- _ O
is -X- _ O
that -X- _ O
it -X- _ O
separates -X- _ O
dialogue -X- _ O
and -X- _ O
video -X- _ O
understanding -X- _ O
. -X- _ O
We -X- _ O
adopt -X- _ O
a -X- _ O
transparent -X- _ O
approach -X- _ O
to -X- _ O
solve -X- _ O
linguistic -X- _ O
entity -X- _ O
references -X- _ O
during -X- _ O
the -X- _ O
dialogue -X- _ O
understanding -X- _ O
phase -X- _ O
. -X- _ O
The -X- _ O
resolved -X- _ O
entities -X- _ O
are -X- _ O
fed -X- _ O
to -X- _ O
the -X- _ O
video -X- _ O
understanding -X- _ O
phase -X- _ O
to -X- _ O
learn -X- _ O
entity -X- _ O
- -X- _ O
action -X- _ O
dynamics -X- _ O
in -X- _ O
the -X- _ O
video -X- _ O
. -X- _ O
We -X- _ O
show -X- _ O
that -X- _ O
our -X- _ O
approach -X- _ O
is -X- _ O
robust -X- _ O
when -X- _ O
dialogue -X- _ O
evolves -X- _ O
to -X- _ O
many -X- _ O
turns -X- _ O
and -X- _ O
video -X- _ O
extends -X- _ O
over -X- _ O
time -X- _ O
( -X- _ O
Please -X- _ O
refer -X- _ O
to -X- _ O
Section -X- _ O
4 -X- _ O
) -X- _ O
. -X- _ O

Question -X- _ O
Parsers -X- _ O

To -X- _ O
learn -X- _ O
compositional -X- _ O
programs -X- _ O
, -X- _ O
we -X- _ O
follow -X- _ O
( -X- _ O
Johnson -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017a -X- _ O
; -X- _ O
Hu -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
and -X- _ O
consider -X- _ O
program -X- _ O
generation -X- _ O
as -X- _ O
a -X- _ O
sequence -X- _ O
- -X- _ O
tosequence -X- _ O
task -X- _ O
. -X- _ O
We -X- _ O
adopt -X- _ O
a -X- _ O
simple -X- _ O
template -X- _ O
" -X- _ O
param -X- _ O
1 -X- _ O
module -X- _ O
1 -X- _ O
param -X- _ O
2 -X- _ O
module -X- _ O
2 -X- _ O
... -X- _ O
" -X- _ O
as -X- _ O
the -X- _ O
target -X- _ O
sequence -X- _ O
. -X- _ O
The -X- _ O
resulting -X- _ O
target -X- _ O
sequences -X- _ O
for -X- _ O
dialogue -X- _ O
and -X- _ O
video -X- _ O
understanding -X- _ O
programs -X- _ O
are -X- _ O
sequences -X- _ O
P -X- _ O
dial -X- _ O
and -X- _ O
P -X- _ O
vid -X- _ O
respectively -X- _ O
. -X- _ O

The -X- _ O
parsers -X- _ O
decompose -X- _ O
questions -X- _ O
into -X- _ O
sub -X- _ O
- -X- _ O
sequences -X- _ O
to -X- _ O
construct -X- _ O
compositional -X- _ O
reasoning -X- _ O
programs -X- _ O
for -X- _ O
dialogue -X- _ O
and -X- _ O
video -X- _ O
understanding -X- _ O
. -X- _ O
Each -X- _ O
parser -X- _ O
is -X- _ O
a -X- _ O
vanilla -X- _ B-HyperparameterName
Transformer -X- _ I-HyperparameterName
decoder -X- _ I-HyperparameterName
, -X- _ O
including -X- _ O
multi -X- _ B-HyperparameterName
- -X- _ I-HyperparameterName
head -X- _ I-HyperparameterName
attention -X- _ I-HyperparameterName
layers -X- _ O
on -X- _ O
questions -X- _ O
and -X- _ O
past -X- _ O
dialogue -X- _ O
turns -X- _ O
( -X- _ O
Please -X- _ O
refer -X- _ O
to -X- _ O
Appendix -X- _ O
A.1 -X- _ O
for -X- _ O
more -X- _ O
technical -X- _ O
details -X- _ O
) -X- _ O
. -X- _ O

Response -X- _ O
Decoder -X- _ O

System -X- _ O
response -X- _ O
is -X- _ O
decoded -X- _ O
by -X- _ O
incorporating -X- _ O
the -X- _ O
dialogue -X- _ O
context -X- _ O
and -X- _ O
video -X- _ O
context -X- _ O
outputs -X- _ O
from -X- _ O
the -X- _ O
corresponding -X- _ O
reasoning -X- _ O
programs -X- _ O
to -X- _ O
target -X- _ O
token -X- _ O
representations -X- _ O
. -X- _ O
We -X- _ O
follows -X- _ O
a -X- _ O
vanilla -X- _ O
Transformer -X- _ O
decoder -X- _ O
architecture -X- _ O
( -X- _ O
Le -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019b -X- _ O
) -X- _ O
, -X- _ O
which -X- _ O
consists -X- _ O
of -X- _ O
3 -X- _ O
attention -X- _ O
layers -X- _ O
: -X- _ O
self -X- _ O
- -X- _ O
attention -X- _ O
to -X- _ O
attend -X- _ O
on -X- _ O
existing -X- _ O
tokens -X- _ O
, -X- _ O
attention -X- _ O
to -X- _ O
Q -X- _ O
ctx -X- _ O
from -X- _ O
dialogue -X- _ O
understanding -X- _ O
program -X- _ O
execution -X- _ O
, -X- _ O
and -X- _ O
attention -X- _ O
to -X- _ O
V -X- _ O
ctx -X- _ O
from -X- _ O
video -X- _ O
understanding -X- _ O
program -X- _ O
execution -X- _ O
. -X- _ O

A -X- _ O
( -X- _ O
1 -X- _ O
) -X- _ O
res -X- _ O
= -X- _ O
Attention -X- _ O
( -X- _ O
R| -X- _ O
jâˆ’1 -X- _ O
0 -X- _ O
, -X- _ O
R| -X- _ O
jâˆ’1 -X- _ O
0 -X- _ O
, -X- _ O
R| -X- _ O
jâˆ’1 -X- _ O
0 -X- _ O
) -X- _ O
âˆˆ -X- _ O
R -X- _ O
jÃ—d -X- _ O
A -X- _ O
( -X- _ O
2 -X- _ O
) -X- _ O
res -X- _ O
= -X- _ O
Attention -X- _ O
( -X- _ O
A -X- _ O
( -X- _ O
1 -X- _ O
) -X- _ O
res -X- _ O
, -X- _ O
Q -X- _ O
ctx -X- _ O
, -X- _ O
Q -X- _ O
ctx -X- _ O
) -X- _ O
âˆˆ -X- _ O
R -X- _ O
jÃ—d -X- _ O
A -X- _ O
( -X- _ O
3 -X- _ O
) -X- _ O
res -X- _ O
= -X- _ O
Attention -X- _ O
( -X- _ O
A -X- _ O
( -X- _ O
2 -X- _ O
) -X- _ O
res -X- _ O
, -X- _ O
V -X- _ O
ctx -X- _ O
, -X- _ O
V -X- _ O
ctx -X- _ O
) -X- _ O
âˆˆ -X- _ O
R -X- _ O
jÃ—d -X- _ O
Multimodal -X- _ O
Fusion -X- _ O
. -X- _ O

For -X- _ O
video -X- _ O
features -X- _ O
come -X- _ O
from -X- _ O
multiple -X- _ O
modalities -X- _ O
, -X- _ O
visual -X- _ O
and -X- _ O
audio -X- _ O
, -X- _ O
the -X- _ O
contextual -X- _ O
features -X- _ O
, -X- _ O
denoted -X- _ O
V -X- _ O
ctx -X- _ O
, -X- _ O
is -X- _ O
obtained -X- _ O
through -X- _ O
a -X- _ O
weighted -X- _ O
sum -X- _ O
of -X- _ O
component -X- _ O
modalities -X- _ O
, -X- _ O
e.g. -X- _ O
contextual -X- _ O
visual -X- _ O
features -X- _ O
V -X- _ O
vis -X- _ O
ctx -X- _ O
and -X- _ O
contextual -X- _ O
audio -X- _ O
features -X- _ O
V -X- _ O
aud -X- _ O
ctx -X- _ O
. -X- _ O
The -X- _ O
scores -X- _ O
S -X- _ O
fusion -X- _ O
to -X- _ O
compute -X- _ O
the -X- _ O
weighted -X- _ O
sum -X- _ O
is -X- _ O
defined -X- _ O
as -X- _ O
: -X- _ O

S -X- _ O
fusion -X- _ O
= -X- _ O
Softmax -X- _ O
( -X- _ O
W -X- _ O
T -X- _ O
fusion -X- _ O
[ -X- _ O
Q -X- _ O
stack -X- _ O
; -X- _ O
V -X- _ O
vis -X- _ O
ctx -X- _ O
; -X- _ O
V -X- _ O
aud -X- _ O
ctx -X- _ O
] -X- _ O

) -X- _ O
where -X- _ O
Q -X- _ O
stack -X- _ O
is -X- _ O
the -X- _ O
mean -X- _ O
pooling -X- _ O
output -X- _ O
of -X- _ O
question -X- _ O
embeddings -X- _ O
Q -X- _ O
which -X- _ O
is -X- _ O
then -X- _ O
stacked -X- _ O
to -X- _ O
N -X- _ O
ent -X- _ O
+ -X- _ O
N -X- _ O
act -X- _ O
dimensions -X- _ O
, -X- _ O
and -X- _ O
W -X- _ O
fusion -X- _ O
âˆˆ -X- _ O
R -X- _ O
3dÃ—2 -X- _ O
are -X- _ O
trainable -X- _ O
model -X- _ O
parameters -X- _ O
. -X- _ O
The -X- _ O
resulting -X- _ O
S -X- _ O
fusion -X- _ O
has -X- _ O
a -X- _ O
dimension -X- _ O
of -X- _ O
âˆˆ -X- _ O
R -X- _ O
( -X- _ O
Nent+Nact -X- _ O
) -X- _ O
Ã—2 -X- _ O
. -X- _ O

Response -X- _ O
Generation -X- _ O
. -X- _ O
To -X- _ O
generate -X- _ O
response -X- _ O
sequences -X- _ O
, -X- _ O
a -X- _ O
special -X- _ O
token -X- _ O
" -X- _ O
_ -X- _ O
sos -X- _ O
" -X- _ O
is -X- _ O
concatenated -X- _ O
as -X- _ O
the -X- _ O
first -X- _ O
token -X- _ O
w -X- _ O
0 -X- _ O
. -X- _ O
The -X- _ O
decoded -X- _ O
token -X- _ O
w -X- _ O
1 -X- _ O
is -X- _ O
then -X- _ O
appended -X- _ O
to -X- _ O
w -X- _ O
0 -X- _ O
as -X- _ O
input -X- _ O
to -X- _ O
decode -X- _ O
w -X- _ O
2 -X- _ O
and -X- _ O
so -X- _ O
on -X- _ O
. -X- _ O
Similarly -X- _ O
to -X- _ O
input -X- _ O
source -X- _ O
sequences -X- _ O
, -X- _ O
at -X- _ O
decoding -X- _ O
time -X- _ O
step -X- _ O
j -X- _ O
, -X- _ O
the -X- _ O
input -X- _ O
target -X- _ O
sequence -X- _ O
is -X- _ O
encoded -X- _ O
to -X- _ O
obtain -X- _ O
representations -X- _ O
of -X- _ O
system -X- _ O
response -X- _ O
R| -X- _ O
jâˆ’1 -X- _ O
0 -X- _ O
. -X- _ O
We -X- _ O
combine -X- _ O
vocabulary -X- _ O
of -X- _ O
input -X- _ O
and -X- _ O
output -X- _ O
sequences -X- _ O
and -X- _ O
share -X- _ O
the -X- _ O
embedding -X- _ O
matrix -X- _ O
E -X- _ O
âˆˆ -X- _ O
R -X- _ O
|V|Ã—d -X- _ O
where -X- _ O
V -X- _ O
= -X- _ O
V -X- _ O
in -X- _ O
âˆ© -X- _ O
V -X- _ O
out -X- _ O
. -X- _ O
During -X- _ O
training -X- _ O
time -X- _ O
, -X- _ O
we -X- _ O
directly -X- _ O
use -X- _ O
the -X- _ O
ground -X- _ O
- -X- _ O
truth -X- _ O
responses -X- _ O
as -X- _ O
input -X- _ O
to -X- _ O
the -X- _ O
decoder -X- _ O
and -X- _ O
optimize -X- _ O
VGNMN -X- _ B-MethodName
with -X- _ O
a -X- _ O
cross -X- _ O
- -X- _ O
entropy -X- _ O
loss -X- _ B-HyperparameterName
to -X- _ O
decode -X- _ O
the -X- _ O
next -X- _ O
ground -X- _ O
- -X- _ O
truth -X- _ O
tokens -X- _ O
. -X- _ O
During -X- _ O
test -X- _ O
time -X- _ O
, -X- _ O
responses -X- _ O
are -X- _ O
generated -X- _ O
auto -X- _ O
- -X- _ O
regressively -X- _ O
through -X- _ O
beam -X- _ O
search -X- _ O
with -X- _ O
beam -X- _ O
size -X- _ O
5 -X- _ O
. -X- _ O
Note -X- _ O
that -X- _ O
we -X- _ O
apply -X- _ O
the -X- _ O
same -X- _ O
procedure -X- _ O
to -X- _ O
generate -X- _ O
reasoning -X- _ O
programs -X- _ O
from -X- _ O
question -X- _ O
parsers -X- _ O
. -X- _ O

Experiments -X- _ O

Datasets -X- _ O
. -X- _ O
We -X- _ O
use -X- _ O
the -X- _ O
AVSD -X- _ B-DatasetName
benchmark -X- _ O
from -X- _ O
the -X- _ O
Dialogue -X- _ O
System -X- _ O
Technology -X- _ O
Challenge -X- _ O
7 -X- _ O
( -X- _ O
DSTC7 -X- _ O
) -X- _ O
. -X- _ O
The -X- _ O
benchmark -X- _ O
consists -X- _ O
of -X- _ O
dialogues -X- _ O
grounded -X- _ O
on -X- _ O
the -X- _ O
Charades -X- _ O
videos -X- _ O
( -X- _ O
Sigurdsson -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2016 -X- _ O
) -X- _ O
. -X- _ O
Each -X- _ O
dialogue -X- _ O
contains -X- _ O
up -X- _ O
to -X- _ O
10 -X- _ O
dialogue -X- _ O
turns -X- _ O
, -X- _ O
each -X- _ O
turn -X- _ O
consists -X- _ O
of -X- _ O
a -X- _ O
question -X- _ O
and -X- _ O
expected -X- _ O
response -X- _ O
about -X- _ O
a -X- _ O
given -X- _ O
video -X- _ O
. -X- _ O
For -X- _ O
visual -X- _ O
features -X- _ O
, -X- _ O
we -X- _ O
use -X- _ O
the -X- _ O
3D -X- _ O
CNN -X- _ O
- -X- _ O
based -X- _ O
features -X- _ O
from -X- _ O
a -X- _ O
pretrained -X- _ O
I3D -X- _ O
model -X- _ O
( -X- _ O
Carreira -X- _ O
and -X- _ O
Zisserman -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
and -X- _ O
object -X- _ O
- -X- _ O
level -X- _ O
features -X- _ O
from -X- _ O
a -X- _ O
pretrained -X- _ O
FasterRNN -X- _ O
model -X- _ O
( -X- _ O
Ren -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2015b -X- _ O
) -X- _ O
. -X- _ O
The -X- _ O
audio -X- _ O
features -X- _ O
are -X- _ O
obtained -X- _ O
from -X- _ O
a -X- _ O
pretrained -X- _ O
VGGish -X- _ O
model -X- _ O
( -X- _ O
Hershey -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
. -X- _ O
In -X- _ O
the -X- _ O
experiments -X- _ O
with -X- _ O
AVSD -X- _ B-DatasetName
, -X- _ O
we -X- _ O
consider -X- _ O
two -X- _ O
settings -X- _ O
: -X- _ O
one -X- _ O
with -X- _ O
video -X- _ O
summary -X- _ O
and -X- _ O
one -X- _ O
without -X- _ O
video -X- _ O
summary -X- _ O
as -X- _ O
input -X- _ O
. -X- _ O
In -X- _ O
the -X- _ O
setting -X- _ O
with -X- _ O
video -X- _ O
summary -X- _ O
, -X- _ O
the -X- _ O
summary -X- _ O
is -X- _ O
concatenated -X- _ O
to -X- _ O
the -X- _ O
dialogue -X- _ O
history -X- _ O
before -X- _ O
the -X- _ O
first -X- _ O
dialogue -X- _ O
turn -X- _ O
. -X- _ O
We -X- _ O
also -X- _ O
adapt -X- _ O
VGNMN -X- _ B-MethodName
to -X- _ O
the -X- _ O
video -X- _ O
QA -X- _ O
benchmark -X- _ O
TGIF -X- _ B-DatasetName
- -X- _ I-DatasetName
QA -X- _ I-DatasetName
( -X- _ O
Jang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017 -X- _ O
For -X- _ O
the -X- _ O
TGIF -X- _ B-DatasetName
- -X- _ I-DatasetName
QA -X- _ I-DatasetName
benchmark -X- _ O
, -X- _ O
we -X- _ O
use -X- _ O
the -X- _ O
extracted -X- _ O
features -X- _ O
from -X- _ O
a -X- _ O
pretrained -X- _ O
ResNet -X- _ O
model -X- _ O
( -X- _ O
He -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2016 -X- _ O
) -X- _ O
. -X- _ O
Table -X- _ O
2 -X- _ O
shows -X- _ O
a -X- _ O
summary -X- _ O
of -X- _ O
the -X- _ O
AVSD -X- _ B-DatasetName
and -X- _ O
TGIF -X- _ B-DatasetName
- -X- _ I-DatasetName
QA -X- _ I-DatasetName
benchmarks -X- _ I-DatasetName
. -X- _ O

Training -X- _ O
Details -X- _ O
. -X- _ O
We -X- _ O
follow -X- _ O
prior -X- _ O
approaches -X- _ O
( -X- _ O
Hu -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017 -X- _ O
( -X- _ O
Hu -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
, -X- _ O
2018Kottur -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2018 -X- _ O
) -X- _ O
by -X- _ O
obtaining -X- _ O
the -X- _ O
annotations -X- _ O
of -X- _ O
the -X- _ O
programs -X- _ O
through -X- _ O
a -X- _ O
language -X- _ O
parser -X- _ O
( -X- _ O
Hu -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2016 -X- _ O
) -X- _ O
and -X- _ O
a -X- _ O
reference -X- _ O
resolution -X- _ O
model -X- _ O
( -X- _ O
Clark -X- _ O
and -X- _ O
Manning -X- _ O
, -X- _ O
2016 -X- _ O
) -X- _ O
. -X- _ O

During -X- _ O
training -X- _ O
, -X- _ O
we -X- _ O
directly -X- _ O
use -X- _ O
these -X- _ O
as -X- _ O
groundtruth -X- _ O
labels -X- _ O
of -X- _ O
programs -X- _ O
to -X- _ O
train -X- _ O
our -X- _ O
models -X- _ O
. -X- _ O
The -X- _ O
ground -X- _ O
- -X- _ O
truth -X- _ O
responses -X- _ O
are -X- _ O
augmented -X- _ O
with -X- _ O
label -X- _ O
smoothing -X- _ O
technique -X- _ O
( -X- _ O
Szegedy -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2016 -X- _ O
) -X- _ O
. -X- _ O
During -X- _ O
inference -X- _ O
time -X- _ O
, -X- _ O
we -X- _ O
generate -X- _ O
all -X- _ O
programs -X- _ O
and -X- _ O
responses -X- _ O
from -X- _ O
given -X- _ O
dialogues -X- _ O
and -X- _ O
videos -X- _ O
. -X- _ O
We -X- _ O
run -X- _ O
beam -X- _ O
search -X- _ O
to -X- _ O
enumerate -X- _ O
programs -X- _ O
for -X- _ O
dialogue -X- _ O
and -X- _ O
video -X- _ O
understanding -X- _ O
and -X- _ O
dialogue -X- _ O
responses -X- _ O
. -X- _ O
We -X- _ O
use -X- _ O
a -X- _ O
training -X- _ O
batch -X- _ B-HyperparameterName
size -X- _ I-HyperparameterName
of -X- _ O
32 -X- _ B-HyperparameterValue
and -X- _ O
embedding -X- _ B-HyperparameterName
dimension -X- _ I-HyperparameterName
d -X- _ B-HyperparameterName
= -X- _ O
128 -X- _ B-HyperparameterValue
in -X- _ O
all -X- _ O
experiments -X- _ O
. -X- _ O
Where -X- _ O
Transformer -X- _ O
attention -X- _ O
is -X- _ O
used -X- _ O
, -X- _ O
we -X- _ O
fix -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
attention -X- _ B-HyperparameterName
heads -X- _ I-HyperparameterName
to -X- _ O
8 -X- _ B-HyperparameterValue
in -X- _ O
all -X- _ O
attention -X- _ O
layers -X- _ O
. -X- _ O
In -X- _ O
neural -X- _ O
modules -X- _ O
with -X- _ O
MLP -X- _ O
layers -X- _ O
, -X- _ O
the -X- _ O
MLP -X- _ O
network -X- _ O
is -X- _ O
fixed -X- _ O
to -X- _ O
2 -X- _ B-HyperparameterValue
linear -X- _ B-HyperparameterName
layers -X- _ I-HyperparameterName
with -X- _ O
a -X- _ O
ReLU -X- _ O
activation -X- _ B-HyperparameterName
in -X- _ O
between -X- _ O
. -X- _ O
In -X- _ O
neural -X- _ O
modules -X- _ O
with -X- _ O
CNN -X- _ O
, -X- _ O
we -X- _ O
adopt -X- _ O
a -X- _ O
vanilla -X- _ O
CNN -X- _ B-MethodName
architecture -X- _ O
for -X- _ O
text -X- _ O
classification -X- _ O
( -X- _ O
without -X- _ O
the -X- _ O
last -X- _ O
MLP -X- _ O
layer -X- _ O
) -X- _ O
where -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
input -X- _ O
channels -X- _ O
is -X- _ O
1 -X- _ O
, -X- _ O
the -X- _ O
kernel -X- _ O
sizes -X- _ O
are -X- _ O
{ -X- _ O
3 -X- _ O
, -X- _ O
4 -X- _ O
, -X- _ O
5 -X- _ O
} -X- _ O
, -X- _ O
and -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
output -X- _ O
channels -X- _ O
is -X- _ O
d. -X- _ O
We -X- _ O
initialize -X- _ O
models -X- _ O
with -X- _ O
uniform -X- _ O
distribution -X- _ O
( -X- _ O
Glorot -X- _ O
and -X- _ O
Bengio -X- _ O
, -X- _ O
2010 -X- _ O
) -X- _ O
. -X- _ O
During -X- _ O
training -X- _ O
, -X- _ O
we -X- _ O
adopt -X- _ O
the -X- _ O
Adam -X- _ O
optimizer -X- _ O
( -X- _ O
Kingma -X- _ O
and -X- _ O
Ba -X- _ O
, -X- _ O
2015 -X- _ O
) -X- _ O
and -X- _ O
a -X- _ O
decaying -X- _ O
learning -X- _ O
rate -X- _ O
( -X- _ O
Vaswani -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
where -X- _ O
we -X- _ O
fix -X- _ O
the -X- _ O
warm -X- _ O
- -X- _ O
up -X- _ O
steps -X- _ O
to -X- _ O
15 -X- _ O
K -X- _ O
training -X- _ O
steps -X- _ O
. -X- _ O
We -X- _ O
employ -X- _ O
dropout -X- _ O
( -X- _ O
Srivastava -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2014 -X- _ O
) -X- _ O
of -X- _ O
0.2 -X- _ O
at -X- _ O
all -X- _ O
networks -X- _ O
except -X- _ O
the -X- _ O
last -X- _ O
linear -X- _ O
layers -X- _ O
of -X- _ O
question -X- _ O
parsers -X- _ O
and -X- _ O
response -X- _ O
decoder -X- _ O
. -X- _ O
We -X- _ O
train -X- _ O
models -X- _ O
up -X- _ O
to -X- _ O
50 -X- _ B-HyperparameterValue
epochs -X- _ B-HyperparameterName
and -X- _ O
select -X- _ O
the -X- _ O
best -X- _ O
models -X- _ O
based -X- _ O
on -X- _ O
the -X- _ O
average -X- _ O
loss -X- _ B-HyperparameterName
per -X- _ O
epoch -X- _ B-HyperparameterName
in -X- _ O
the -X- _ O
validation -X- _ O
set -X- _ O
. -X- _ O

All -X- _ O
models -X- _ O
are -X- _ O
trained -X- _ O
in -X- _ O
a -X- _ O
V100 -X- _ O
GPU -X- _ O
with -X- _ O
a -X- _ O
capacity -X- _ O
of -X- _ O
16 -X- _ O
GB -X- _ O
. -X- _ O
We -X- _ O
approximated -X- _ O
each -X- _ O
training -X- _ O
epoch -X- _ B-HyperparameterName
took -X- _ O
about -X- _ O
20 -X- _ O
minutes -X- _ O
to -X- _ O
run -X- _ O
. -X- _ O
For -X- _ O
each -X- _ O
model -X- _ O
experiment -X- _ O
with -X- _ O
VGNMN -X- _ B-MethodName
, -X- _ O
we -X- _ O
obtained -X- _ O
at -X- _ O
least -X- _ O
2 -X- _ O
runs -X- _ O
and -X- _ O
reported -X- _ O
the -X- _ O
average -X- _ O
results -X- _ O
. -X- _ O
We -X- _ O
implemented -X- _ O
models -X- _ O
in -X- _ O
Pytorch -X- _ O
and -X- _ O
released -X- _ O
the -X- _ O
code -X- _ O
and -X- _ O
model -X- _ O
checkpoints -X- _ O
1 -X- _ O
. -X- _ O

Optimization -X- _ B-HyperparameterName
. -X- _ O
We -X- _ O
optimize -X- _ O
models -X- _ O
by -X- _ O
joint -X- _ O
training -X- _ O
to -X- _ O
minimize -X- _ O
the -X- _ O
cross -X- _ O
- -X- _ O
entropy -X- _ O
losses -X- _ B-HyperparameterName
to -X- _ O
generate -X- _ O
responses -X- _ O
and -X- _ O
functional -X- _ O
programs -X- _ O
. -X- _ O

L -X- _ O
= -X- _ O
Î±L -X- _ O
dial -X- _ O
+ -X- _ O
Î²L -X- _ O
vid -X- _ O
+ -X- _ O
L -X- _ O
res -X- _ O
= -X- _ O
Î± -X- _ O
j -X- _ O
âˆ’ -X- _ O
log -X- _ O
( -X- _ O
P -X- _ O
dial -X- _ O
( -X- _ O
P -X- _ O
dial -X- _ O
, -X- _ O
j -X- _ O
) -X- _ O
) -X- _ O
+ -X- _ O
Î² -X- _ O
l -X- _ O
âˆ’ -X- _ O
log -X- _ O
( -X- _ O
P -X- _ O
video -X- _ O
( -X- _ O
P -X- _ O
video -X- _ O
, -X- _ O
l -X- _ O
) -X- _ O
) -X- _ O
+ -X- _ O
n -X- _ O
âˆ’ -X- _ O
log -X- _ O
( -X- _ O
P -X- _ O
res -X- _ O
( -X- _ O
R -X- _ O
n -X- _ O
) -X- _ O
) -X- _ O

where -X- _ O
P -X- _ O
is -X- _ O
the -X- _ O
probability -X- _ O
distribution -X- _ O
of -X- _ O
an -X- _ O
output -X- _ O
token -X- _ O
. -X- _ O
The -X- _ O
probability -X- _ O
is -X- _ O
computed -X- _ O
by -X- _ O
passing -X- _ O
output -X- _ O
representations -X- _ O
from -X- _ O
the -X- _ O
parsers -X- _ O
and -X- _ O
decoder -X- _ O
to -X- _ O
a -X- _ O
linear -X- _ O
layer -X- _ O
W -X- _ O
âˆˆ -X- _ O
R -X- _ O
dÃ—V -X- _ O
with -X- _ O
softmax -X- _ O
activation -X- _ O
. -X- _ O
We -X- _ O
share -X- _ O
the -X- _ O
parameters -X- _ O
between -X- _ O
W -X- _ O
and -X- _ O
embedding -X- _ O
matrix -X- _ O
E. -X- _ O
AVSD -X- _ B-MethodName
Results -X- _ O
. -X- _ O
We -X- _ O
evaluate -X- _ O
model -X- _ O
performance -X- _ O
by -X- _ O
the -X- _ O
objective -X- _ O
metrics -X- _ O
, -X- _ O
including -X- _ O
BLEU -X- _ B-MetricName
( -X- _ O
Papineni -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2002 -X- _ O
) -X- _ O
, -X- _ O
METEOR -X- _ O
( -X- _ O
Banerjee -X- _ O
and -X- _ O
Lavie -X- _ O
, -X- _ O
2005 -X- _ O
) -X- _ O
, -X- _ O
ROUGE -X- _ B-MetricName
- -X- _ O
L -X- _ O
( -X- _ O
Lin -X- _ O
, -X- _ O
2004 -X- _ O
) -X- _ O
, -X- _ O
and -X- _ O
CIDEr -X- _ B-MetricName
( -X- _ O
Vedantam -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2015 -X- _ O
) -X- _ O
, -X- _ O
between -X- _ O
each -X- _ O
generated -X- _ O
response -X- _ O
and -X- _ O
6 -X- _ O
reference -X- _ O
gold -X- _ O
responses -X- _ O
. -X- _ O
As -X- _ O
seen -X- _ O
in -X- _ O
Table -X- _ O
3 -X- _ O
, -X- _ O
our -X- _ O
models -X- _ O
outperform -X- _ O
most -X- _ O
of -X- _ O
existing -X- _ O
approaches -X- _ O
. -X- _ O
We -X- _ O
observed -X- _ O
that -X- _ O
our -X- _ O
approach -X- _ O
did -X- _ O
not -X- _ O
outperform -X- _ O
the -X- _ O
GPT -X- _ O
- -X- _ O
based -X- _ O
baselines -X- _ O
( -X- _ O
Li -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020b -X- _ O
; -X- _ O
Le -X- _ O
and -X- _ O
Hoi -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
in -X- _ O
the -X- _ O
setting -X- _ O
that -X- _ O
allows -X- _ O
video -X- _ O
summary -X- _ O
/ -X- _ O
caption -X- _ O
input -X- _ O
. -X- _ O
However -X- _ O
, -X- _ O
the -X- _ O
performance -X- _ O
of -X- _ O
our -X- _ O
model -X- _ O
in -X- _ O
the -X- _ O
setting -X- _ O
without -X- _ O
video -X- _ O
summary -X- _ O
/ -X- _ O
caption -X- _ O
input -X- _ O
is -X- _ O
on -X- _ O
par -X- _ O
with -X- _ O
the -X- _ O
GPT -X- _ O
- -X- _ O
based -X- _ O
baseline -X- _ O
( -X- _ O
Li -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020b -X- _ O
) -X- _ O
, -X- _ O
even -X- _ O
though -X- _ O
our -X- _ O
model -X- _ O
did -X- _ O
not -X- _ O
rely -X- _ O
on -X- _ O
deep -X- _ O
pretrained -X- _ O
representations -X- _ O
on -X- _ O
large -X- _ O
- -X- _ O
scale -X- _ O
text -X- _ O
data -X- _ O
. -X- _ O
These -X- _ O
observations -X- _ O
imply -X- _ O
that -X- _ O
GPT -X- _ O
- -X- _ O
based -X- _ O
models -X- _ O
can -X- _ O
better -X- _ O
capture -X- _ O
video -X- _ O
context -X- _ O
from -X- _ O
video -X- _ O
caption -X- _ O
/ -X- _ O
summary -X- _ O
through -X- _ O
rich -X- _ O
pretrained -X- _ O
representations -X- _ O
. -X- _ O
However -X- _ O
, -X- _ O
without -X- _ O
access -X- _ O
to -X- _ O
video -X- _ O
caption -X- _ O
/ -X- _ O
summary -X- _ O
, -X- _ O
these -X- _ O
models -X- _ O
may -X- _ O
fail -X- _ O
to -X- _ O
understand -X- _ O
video -X- _ O
from -X- _ O
visual -X- _ O
- -X- _ O
only -X- _ O
representations -X- _ O
. -X- _ O
In -X- _ O
this -X- _ O
setting -X- _ O
, -X- _ O
GPT -X- _ O
- -X- _ O
based -X- _ O
models -X- _ O
may -X- _ O
be -X- _ O
inferior -X- _ O
to -X- _ O
VGNMN -X- _ B-MethodName
, -X- _ O
which -X- _ O
explicitly -X- _ O
exploits -X- _ O
the -X- _ O
compositional -X- _ O
structures -X- _ O
from -X- _ O
textual -X- _ O
inputs -X- _ O
to -X- _ O
integrate -X- _ O
visual -X- _ O
features -X- _ O
. -X- _ O
We -X- _ O
also -X- _ O
found -X- _ O
that -X- _ O
VGNMN -X- _ B-MethodName
applied -X- _ O
to -X- _ O
object -X- _ O
- -X- _ O
level -X- _ O
features -X- _ O
is -X- _ O
competitive -X- _ O
to -X- _ O
the -X- _ O
model -X- _ O
applied -X- _ O
to -X- _ O
CNN -X- _ B-MethodName
- -X- _ O
based -X- _ O
features -X- _ O
. -X- _ O
The -X- _ O
as -X- _ O
well -X- _ O
as -X- _ O
model -X- _ O
( -X- _ O
1 -X- _ O
) -X- _ O
. -X- _ O
We -X- _ O
observed -X- _ O
that -X- _ O
related -X- _ O
factors -X- _ O
might -X- _ O
affect -X- _ O
the -X- _ O
discrepancy -X- _ O
, -X- _ O
such -X- _ O
as -X- _ O
the -X- _ O
complexity -X- _ O
of -X- _ O
the -X- _ O
questions -X- _ O
for -X- _ O
these -X- _ O
short -X- _ O
and -X- _ O
long -X- _ O
- -X- _ O
range -X- _ O
videos -X- _ O
. -X- _ O
Potentially -X- _ O
, -X- _ O
our -X- _ O
question -X- _ O
parser -X- _ O
for -X- _ O
the -X- _ O
video -X- _ O
understanding -X- _ O
program -X- _ O
needs -X- _ O
to -X- _ O
be -X- _ O
improved -X- _ O
( -X- _ O
e.g. -X- _ O
for -X- _ O
tree -X- _ O
- -X- _ O
based -X- _ O
programs -X- _ O
) -X- _ O
to -X- _ O
retrieve -X- _ O
information -X- _ O
in -X- _ O
these -X- _ O
ranges -X- _ O
. -X- _ O
Robustness -X- _ O
to -X- _ O
dialogue -X- _ O
turn -X- _ O
: -X- _ O
In -X- _ O
Table -X- _ O
4b -X- _ O
, -X- _ O
we -X- _ O
observed -X- _ O
that -X- _ O
model -X- _ O
( -X- _ O
1 -X- _ O
) -X- _ O
performs -X- _ O
better -X- _ O
than -X- _ O
model -X- _ O
( -X- _ O
2 -X- _ O
) -X- _ O
overall -X- _ O
, -X- _ O
especially -X- _ O
in -X- _ O
higher -X- _ O
turn -X- _ O
positions -X- _ O
, -X- _ O
i.e. -X- _ O
from -X- _ O
the -X- _ O
4 -X- _ O
th -X- _ O
turn -X- _ O
to -X- _ O
8 -X- _ O
th -X- _ O
turn -X- _ O
. -X- _ O
Interestingly -X- _ O
, -X- _ O
we -X- _ O
noted -X- _ O
some -X- _ O
mixed -X- _ O
results -X- _ O
in -X- _ O
very -X- _ O
low -X- _ O
turn -X- _ O
position -X- _ O
, -X- _ O
i.e. -X- _ O
the -X- _ O
2 -X- _ O
nd -X- _ O
and -X- _ O
3 -X- _ O
rd -X- _ O
turn -X- _ O
, -X- _ O
and -X- _ O
very -X- _ O
high -X- _ O
turn -X- _ O
position -X- _ O
, -X- _ O
i.e. -X- _ O
the -X- _ O
10 -X- _ O
th -X- _ O
turn -X- _ O
. -X- _ O
Potentially -X- _ O
, -X- _ O
with -X- _ O
a -X- _ O
large -X- _ O
dialogue -X- _ O
turn -X- _ O
position -X- _ O
, -X- _ O
the -X- _ O
neural -X- _ O
- -X- _ O
based -X- _ O
approach -X- _ O
such -X- _ O
as -X- _ O
hierarchical -X- _ B-MethodName
RNN -X- _ I-MethodName
can -X- _ O
better -X- _ O
capture -X- _ O
the -X- _ O
global -X- _ O
dependencies -X- _ O
within -X- _ O
dialogue -X- _ O
context -X- _ O
than -X- _ O
the -X- _ O
entity -X- _ O
- -X- _ O
based -X- _ O
compositional -X- _ O
NMN -X- _ O
method -X- _ O
. -X- _ O

Robustness -X- _ O
to -X- _ O
question -X- _ O
structure -X- _ O
: -X- _ O
Finally -X- _ O
, -X- _ O
we -X- _ O
compared -X- _ O
performance -X- _ O
of -X- _ O
VGNMN -X- _ B-MethodName
with -X- _ O
the -X- _ O
no -X- _ O
- -X- _ O
NMN -X- _ O
variant -X- _ O
( -X- _ O
1 -X- _ O
) -X- _ O
in -X- _ O
different -X- _ O
cases -X- _ O
of -X- _ O
question -X- _ O
structures -X- _ O
: -X- _ O
single -X- _ O
- -X- _ O
question -X- _ O
vs. -X- _ O
multiple -X- _ O
- -X- _ O
part -X- _ O
structure -X- _ O
. -X- _ O
In -X- _ O
single -X- _ O
- -X- _ O
question -X- _ O
structures -X- _ O
, -X- _ O
we -X- _ O
examined -X- _ O
by -X- _ O
the -X- _ O
question -X- _ O
types -X- _ O
( -X- _ O
e.g. -X- _ O
yes -X- _ O
/ -X- _ O
no -X- _ O
, -X- _ O
wh -X- _ O
- -X- _ O
questions -X- _ O
) -X- _ O
. -X- _ O
In -X- _ O
multi -X- _ O
- -X- _ O
part -X- _ O
structures -X- _ O
, -X- _ O
we -X- _ O
further -X- _ O
classified -X- _ O
whether -X- _ O
there -X- _ O
are -X- _ O
sentences -X- _ O
preceding -X- _ O
the -X- _ O
question -X- _ O
( -X- _ O
e.g. -X- _ O
" -X- _ O
1Sent+Que -X- _ O
" -X- _ O
) -X- _ O
or -X- _ O
there -X- _ O
are -X- _ O
smaller -X- _ O
( -X- _ O
sub- -X- _ O
) -X- _ O
questions -X- _ O
( -X- _ O
e.g. -X- _ O
" -X- _ O
2SubQue -X- _ O
" -X- _ O
) -X- _ O
within -X- _ O
the -X- _ O
question -X- _ O
. -X- _ O
In -X- _ O
Table -X- _ O
4c -X- _ O
, -X- _ O
we -X- _ O
observed -X- _ O
that -X- _ O
VGNMN -X- _ B-MethodName
has -X- _ O
clearer -X- _ O
performance -X- _ O
gains -X- _ O
in -X- _ O
multi -X- _ O
- -X- _ O
part -X- _ O
structures -X- _ O
than -X- _ O
singlequestion -X- _ O
structures -X- _ O
. -X- _ O
In -X- _ O
multi -X- _ O
- -X- _ O
part -X- _ O
structures -X- _ O
, -X- _ O
we -X- _ O
observed -X- _ O
higher -X- _ O
gaps -X- _ O
between -X- _ O
VGNMN -X- _ B-MethodName
and -X- _ O
model -X- _ O
( -X- _ O
1 -X- _ O
) -X- _ O
in -X- _ O
highly -X- _ O
complex -X- _ O
cases -X- _ O
e.g. -X- _ O
" -X- _ O
2Sent+Que -X- _ O
" -X- _ O
vs. -X- _ O
" -X- _ O
1Sent+Que -X- _ O
" -X- _ O
. -X- _ O
These -X- _ O
observations -X- _ O
indicate -X- _ O
the -X- _ O
robustness -X- _ O
of -X- _ O
VGNMN -X- _ B-MethodName
and -X- _ O
the -X- _ O
underlying -X- _ O
compositionality -X- _ O
principle -X- _ O
to -X- _ O
deal -X- _ O
with -X- _ O
complex -X- _ O
question -X- _ O
structures -X- _ O
. -X- _ O
We -X- _ O
also -X- _ O
noted -X- _ O
that -X- _ O
VGNMN -X- _ B-MethodName
is -X- _ O
still -X- _ O
susceptible -X- _ O
to -X- _ O
extremely -X- _ O
long -X- _ O
questions -X- _ O
( -X- _ O
" -X- _ O
> -X- _ O
2Sent+Que -X- _ O
" -X- _ O
) -X- _ O
and -X- _ O
future -X- _ O
work -X- _ O
is -X- _ O
needed -X- _ O
to -X- _ O
ad -X- _ O
- -X- _ O
dress -X- _ O
these -X- _ O
scenarios -X- _ O
. -X- _ O
Interpretability -X- _ O
. -X- _ O
In -X- _ O
Figure -X- _ O
5 -X- _ O
, -X- _ O
we -X- _ O
show -X- _ O
both -X- _ O
success -X- _ O
and -X- _ O
failure -X- _ O
cases -X- _ O
of -X- _ O
generated -X- _ O
responses -X- _ O
and -X- _ O
corresponding -X- _ O
generated -X- _ O
functional -X- _ O
programs -X- _ O
. -X- _ O
In -X- _ O
each -X- _ O
example -X- _ O
, -X- _ O
we -X- _ O
marked -X- _ O
predicted -X- _ O
outputs -X- _ O
as -X- _ O
incorrect -X- _ O
if -X- _ O
they -X- _ O
do -X- _ O
not -X- _ O
match -X- _ O
the -X- _ O
ground -X- _ O
- -X- _ O
truth -X- _ O
completely -X- _ O
( -X- _ O
even -X- _ O
though -X- _ O
the -X- _ O
outputs -X- _ O
might -X- _ O
be -X- _ O
partially -X- _ O
correct -X- _ O
) -X- _ O
. -X- _ O
From -X- _ O
Figure -X- _ O
5 -X- _ O
, -X- _ O
we -X- _ O
observe -X- _ O
that -X- _ O
in -X- _ O
cases -X- _ O
where -X- _ O
generated -X- _ O
dialogue -X- _ O
programs -X- _ O
and -X- _ O
video -X- _ O
programs -X- _ O
match -X- _ O
or -X- _ O
are -X- _ O
close -X- _ O
to -X- _ O
the -X- _ O
gold -X- _ O
labels -X- _ O
, -X- _ O
the -X- _ O
model -X- _ O
can -X- _ O
generate -X- _ O
generally -X- _ O
correct -X- _ O
responses -X- _ O
. -X- _ O
For -X- _ O
cases -X- _ O
where -X- _ O
some -X- _ O
module -X- _ O
parameters -X- _ O
do -X- _ O
not -X- _ O
exactly -X- _ O
match -X- _ O
but -X- _ O
are -X- _ O
closed -X- _ O
to -X- _ O
the -X- _ O
gold -X- _ O
labels -X- _ O
, -X- _ O
the -X- _ O
model -X- _ O
can -X- _ O
still -X- _ O
generate -X- _ O
responses -X- _ O
with -X- _ O
the -X- _ O
correct -X- _ O
visual -X- _ O
information -X- _ O
( -X- _ O
e.g. -X- _ O
the -X- _ O
4 -X- _ O
th -X- _ O
turn -X- _ O
in -X- _ O
example -X- _ O
B -X- _ O
) -X- _ O
. -X- _ O
In -X- _ O
cases -X- _ O
of -X- _ O
wrong -X- _ O
predicted -X- _ O
responses -X- _ O
, -X- _ O
we -X- _ O
can -X- _ O
further -X- _ O
look -X- _ O
at -X- _ O
how -X- _ O
the -X- _ O
model -X- _ O
understands -X- _ O
the -X- _ O
questions -X- _ O
based -X- _ O
on -X- _ O
predicted -X- _ O
programs -X- _ O
. -X- _ O
In -X- _ O
the -X- _ O
3 -X- _ O
rd -X- _ O
turn -X- _ O
of -X- _ O
example -X- _ O
A -X- _ O
, -X- _ O
the -X- _ O
output -X- _ O
response -X- _ O
is -X- _ O
missing -X- _ O
a -X- _ O
minor -X- _ O
detail -X- _ O
as -X- _ O
compared -X- _ O
to -X- _ O
the -X- _ O
label -X- _ O
response -X- _ O
because -X- _ O
the -X- _ O
video -X- _ O
program -X- _ O
fails -X- _ O
to -X- _ O
capture -X- _ O
" -X- _ O
rooftop -X- _ O
" -X- _ O
as -X- _ O
a -X- _ O
where -X- _ O
parameter -X- _ O
. -X- _ O
These -X- _ O
subtle -X- _ O
yet -X- _ O
important -X- _ O
details -X- _ O
can -X- _ O
determine -X- _ O
whether -X- _ O
output -X- _ O
responses -X- _ O
can -X- _ O
fully -X- _ O
address -X- _ O
user -X- _ O
queries -X- _ O
. -X- _ O
In -X- _ O
the -X- _ O
3 -X- _ O
rd -X- _ O
turn -X- _ O
of -X- _ O
example -X- _ O
B -X- _ O
, -X- _ O
the -X- _ O
model -X- _ O
wrongly -X- _ O
identifies -X- _ O
" -X- _ O
what -X- _ O
room -X- _ O
" -X- _ O
as -X- _ O
a -X- _ O
where -X- _ O
parameter -X- _ O
and -X- _ O
subsequently -X- _ O
generates -X- _ O
a -X- _ O
wrong -X- _ O
response -X- _ O
that -X- _ O
it -X- _ O
is -X- _ O
" -X- _ O
a -X- _ O
living -X- _ O
room -X- _ O
" -X- _ O
. -X- _ O

TGIF -X- _ B-DatasetName
- -X- _ I-DatasetName
QA -X- _ I-DatasetName
Results -X- _ O
. -X- _ O
We -X- _ O
report -X- _ O
the -X- _ O
result -X- _ O
using -X- _ O
the -X- _ O
L2 -X- _ O
loss -X- _ B-HyperparameterName
in -X- _ O
Count -X- _ O
task -X- _ O
and -X- _ O
accuracy -X- _ B-MetricName
in -X- _ O
other -X- _ O
tasks -X- _ O
. -X- _ O
From -X- _ O
Table -X- _ O
5 -X- _ O
, -X- _ O
VGNMN -X- _ B-MethodName
outperforms -X- _ O
the -X- _ O
majority -X- _ O
of -X- _ O
the -X- _ O
baseline -X- _ O
models -X- _ O
in -X- _ O
all -X- _ O
tasks -X- _ O
by -X- _ O
a -X- _ O
large -X- _ O
margin -X- _ O
. -X- _ O
Compared -X- _ O
to -X- _ O
AVSD -X- _ B-DatasetName
experiments -X- _ O
, -X- _ O
the -X- _ O
TGIF -X- _ B-DatasetName
- -X- _ I-DatasetName
QA -X- _ I-DatasetName
experiments -X- _ O
emphasize -X- _ O
the -X- _ O
video -X- _ O
understanding -X- _ O
ability -X- _ O
of -X- _ O
the -X- _ O
models -X- _ O
, -X- _ O
removing -X- _ O
the -X- _ O
requirement -X- _ O
for -X- _ O
dialogue -X- _ O
understanding -X- _ O
and -X- _ O
natural -X- _ O
language -X- _ O
generation -X- _ O
. -X- _ O
Since -X- _ O
TGIF -X- _ B-DatasetName
- -X- _ I-DatasetName
QA -X- _ I-DatasetName
questions -X- _ O
follow -X- _ O
a -X- _ O
very -X- _ O
specific -X- _ O
question -X- _ O
type -X- _ O
distribution -X- _ O
( -X- _ O
count -X- _ O
, -X- _ O
action -X- _ O
, -X- _ O
transition -X- _ O
, -X- _ O
and -X- _ O
frameQA -X- _ O
) -X- _ O
, -X- _ O
the -X- _ O
question -X- _ O
structures -X- _ O
are -X- _ O
simpler -X- _ O
and -X- _ O
easier -X- _ O
to -X- _ O
learn -X- _ O
than -X- _ O
AVSD -X- _ B-MetricName
. -X- _ O
Using -X- _ O
exact -X- _ B-MetricName
- -X- _ I-MetricName
match -X- _ I-MetricName
accuracy -X- _ O
of -X- _ O
parsed -X- _ O
programs -X- _ O
vs. -X- _ O
label -X- _ O
programs -X- _ O
as -X- _ O
a -X- _ O
metric -X- _ O
, -X- _ O
our -X- _ O
question -X- _ O
parser -X- _ O
can -X- _ O
achieve -X- _ O
a -X- _ O
performance -X- _ O
81 -X- _ B-MetricValue
% -X- _ I-MetricValue
to -X- _ O
94 -X- _ B-MetricValue
% -X- _ I-MetricValue
accuracy -X- _ O
in -X- _ O
TGIF -X- _ B-DatasetName
- -X- _ I-DatasetName
QA -X- _ I-DatasetName
vs. -X- _ O
41 -X- _ B-MetricValue
- -X- _ I-MetricValue
45 -X- _ I-MetricValue
% -X- _ I-MetricValue
in -X- _ O
AVSD -X- _ B-DatasetName
. -X- _ O
The -X- _ O
higher -X- _ O
accuracy -X- _ B-MetricName
in -X- _ O
decoding -X- _ O
a -X- _ O
reasoning -X- _ O
structure -X- _ O
translates -X- _ O
to -X- _ O
better -X- _ O
adaptation -X- _ O
between -X- _ O
training -X- _ O
and -X- _ O
test -X- _ O
time -X- _ O
, -X- _ O
resulting -X- _ O
in -X- _ O
higher -X- _ O
performance -X- _ O
gains -X- _ O
. -X- _ O
Cascading -X- _ O
Errors -X- _ O
. -X- _ O
Compared -X- _ O
to -X- _ O
prior -X- _ O
approaches -X- _ O
, -X- _ O
we -X- _ O
noted -X- _ O
that -X- _ O
VGNMN -X- _ O
is -X- _ O
a -X- _ O
modularized -X- _ O
system -X- _ O
which -X- _ O
may -X- _ O
result -X- _ O
in -X- _ O
cascading -X- _ O
errors -X- _ O
to -X- _ O
downstream -X- _ O
modules -X- _ O
. -X- _ O
One -X- _ O
major -X- _ O
error -X- _ O
is -X- _ O
the -X- _ O
error -X- _ O
of -X- _ O
generated -X- _ O
programs -X- _ O
which -X- _ O
is -X- _ O
used -X- _ O
as -X- _ O
parameters -X- _ O
in -X- _ O
neural -X- _ O
modules -X- _ O
. -X- _ O
To -X- _ O
gauge -X- _ O
this -X- _ O
error -X- _ O
, -X- _ O
we -X- _ O
compare -X- _ O
the -X- _ O
performance -X- _ O
of -X- _ O
VGNMN -X- _ B-MethodName
between -X- _ O
2 -X- _ O
cases -X- _ O
: -X- _ O
with -X- _ O
generated -X- _ O
programs -X- _ O
and -X- _ O
with -X- _ O
groundtruth -X- _ O
programs -X- _ O
. -X- _ O
From -X- _ O
Table -X- _ O
6 -X- _ O
, -X- _ O
we -X- _ O
noticed -X- _ O
some -X- _ O
performance -X- _ O
gaps -X- _ O
between -X- _ O
these -X- _ O
cases -X- _ O
. -X- _ O
These -X- _ O
observations -X- _ O
imply -X- _ O
that -X- _ O
: -X- _ O
( -X- _ O
1 -X- _ O
For -X- _ O
additional -X- _ O
experiment -X- _ O
results -X- _ O
, -X- _ O
qualitative -X- _ O
samples -X- _ O
, -X- _ O
and -X- _ O
analysis -X- _ O
between -X- _ O
model -X- _ O
variants -X- _ O
, -X- _ O
refer -X- _ O
to -X- _ O
Appendix -X- _ O
B -X- _ O
and -X- _ O
C -X- _ O
. -X- _ O

Conclusion -X- _ O

In -X- _ O
this -X- _ O
work -X- _ O
, -X- _ O
we -X- _ O
introduce -X- _ O
Video -X- _ B-MethodName
- -X- _ I-MethodName
grounded -X- _ I-MethodName
Neural -X- _ I-MethodName
Module -X- _ I-MethodName
Network -X- _ I-MethodName
( -X- _ O
VGNMN -X- _ B-MethodName
) -X- _ O
. -X- _ O
VGNMN -X- _ B-MethodName
consists -X- _ O
of -X- _ O
dialogue -X- _ O
and -X- _ O
video -X- _ O
understanding -X- _ O
neural -X- _ O
modules -X- _ O
, -X- _ O
each -X- _ O
of -X- _ O
which -X- _ O
performs -X- _ O
entity -X- _ O
and -X- _ O
action -X- _ O
- -X- _ O
level -X- _ O
operations -X- _ O
on -X- _ O
language -X- _ O
and -X- _ O
video -X- _ O
components -X- _ O
. -X- _ O
Our -X- _ O
comprehensive -X- _ O
experiments -X- _ O
on -X- _ O
AVSD -X- _ B-DatasetName
and -X- _ O
TGIF -X- _ B-DatasetName
- -X- _ I-DatasetName
QA -X- _ I-DatasetName
benchmarks -X- _ O
show -X- _ O
that -X- _ O
our -X- _ O
models -X- _ O
can -X- _ O
achieve -X- _ O
competitive -X- _ O
performance -X- _ O
while -X- _ O
promoting -X- _ O
a -X- _ O
compositional -X- _ O
and -X- _ O
interpretable -X- _ O
learning -X- _ O
approach -X- _ O
. -X- _ O

Broader -X- _ O
Impacts -X- _ O

During -X- _ O
the -X- _ O
duration -X- _ O
of -X- _ O
this -X- _ O
work -X- _ O
, -X- _ O
there -X- _ O
have -X- _ O
been -X- _ O
no -X- _ O
ethical -X- _ O
concerns -X- _ O
regarding -X- _ O
the -X- _ O
model -X- _ O
implementation -X- _ O
, -X- _ O
training -X- _ O
, -X- _ O
and -X- _ O
testing -X- _ O
. -X- _ O
The -X- _ O
data -X- _ O
used -X- _ O
in -X- _ O
this -X- _ O
work -X- _ O
has -X- _ O
been -X- _ O
carefully -X- _ O
reviewed -X- _ O
and -X- _ O
accordingly -X- _ O
to -X- _ O
the -X- _ O
description -X- _ O
from -X- _ O
the -X- _ O
original -X- _ O
authors -X- _ O
, -X- _ O
we -X- _ O
did -X- _ O
not -X- _ O
find -X- _ O
any -X- _ O
concerns -X- _ O
on -X- _ O
any -X- _ O
significant -X- _ O
biases -X- _ O
. -X- _ O
For -X- _ O
any -X- _ O
potential -X- _ O
application -X- _ O
or -X- _ O
extension -X- _ O
of -X- _ O
this -X- _ O
work -X- _ O
, -X- _ O
we -X- _ O
would -X- _ O
like -X- _ O
to -X- _ O
highlight -X- _ O
some -X- _ O
specific -X- _ O
concerns -X- _ O
. -X- _ O
First -X- _ O
, -X- _ O
as -X- _ O
the -X- _ O
work -X- _ O
is -X- _ O
developed -X- _ O
to -X- _ O
build -X- _ O
an -X- _ O
intelligent -X- _ O
dialogue -X- _ O
agents -X- _ O
, -X- _ O
models -X- _ O
should -X- _ O
not -X- _ O
be -X- _ O
used -X- _ O
with -X- _ O
the -X- _ O
intention -X- _ O
to -X- _ O
create -X- _ O
fake -X- _ O
human -X- _ O
profiles -X- _ O
for -X- _ O
any -X- _ O
harmful -X- _ O
purposes -X- _ O
( -X- _ O
e.g. -X- _ O
fishing -X- _ O
or -X- _ O
spreading -X- _ O
fake -X- _ O
news -X- _ O
) -X- _ O
. -X- _ O
For -X- _ O
wider -X- _ O
use -X- _ O
of -X- _ O
dialogue -X- _ O
systems -X- _ O
, -X- _ O
the -X- _ O
application -X- _ O
of -X- _ O
work -X- _ O
might -X- _ O
result -X- _ O
in -X- _ O
certain -X- _ O
impacts -X- _ O
to -X- _ O
some -X- _ O
stakeholders -X- _ O
whose -X- _ O
jobs -X- _ O
may -X- _ O
be -X- _ O
affected -X- _ O
by -X- _ O
this -X- _ O
application -X- _ O
( -X- _ O
e.g. -X- _ O
customer -X- _ O
service -X- _ O
call -X- _ O
agents -X- _ O
) -X- _ O
. -X- _ O
We -X- _ O
hope -X- _ O
any -X- _ O
application -X- _ O
should -X- _ O
be -X- _ O
carefully -X- _ O
considered -X- _ O
against -X- _ O
these -X- _ O
potential -X- _ O
risks -X- _ O
. -X- _ O

Each -X- _ O
attention -X- _ O
is -X- _ O
followed -X- _ O
by -X- _ O
a -X- _ O
feed -X- _ O
- -X- _ O
forward -X- _ O
network -X- _ O
applied -X- _ O
to -X- _ O
each -X- _ O
position -X- _ O
identically -X- _ O
. -X- _ O
We -X- _ O
exploit -X- _ O
the -X- _ O
multi -X- _ O
- -X- _ O
head -X- _ O
and -X- _ O
feed -X- _ O
- -X- _ O
forward -X- _ O
architecture -X- _ O
, -X- _ O
which -X- _ O
show -X- _ O
good -X- _ O
performance -X- _ O
in -X- _ O
NLP -X- _ O
tasks -X- _ O
such -X- _ O
as -X- _ O
NMT -X- _ O
and -X- _ O
QA -X- _ O
( -X- _ O
Vaswani -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017 -X- _ O
; -X- _ O
Dehghani -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
, -X- _ O
to -X- _ O
efficiently -X- _ O
incorporate -X- _ O
contextual -X- _ O
cues -X- _ O
from -X- _ O
dialogue -X- _ O
components -X- _ O
to -X- _ O
parse -X- _ O
question -X- _ O
into -X- _ O
reasoning -X- _ O
programs -X- _ O
. -X- _ O
At -X- _ O
decoding -X- _ O
step -X- _ O
0 -X- _ O
, -X- _ O
we -X- _ O
simply -X- _ O
use -X- _ O
a -X- _ O
special -X- _ O
token -X- _ O
_ -X- _ O
sos -X- _ O
as -X- _ O
the -X- _ O
input -X- _ O
to -X- _ O
the -X- _ O
parser -X- _ O
. -X- _ O
In -X- _ O
each -X- _ O
subsequent -X- _ O
decoding -X- _ O
step -X- _ O
, -X- _ O
we -X- _ O
concatenate -X- _ O
the -X- _ O
prior -X- _ O
input -X- _ O
sequence -X- _ O
with -X- _ O
the -X- _ O
generated -X- _ O
token -X- _ O
to -X- _ O
decode -X- _ O
in -X- _ O
an -X- _ O
auto -X- _ O
- -X- _ O
regressive -X- _ O
manner -X- _ O
. -X- _ O
We -X- _ O
share -X- _ O
the -X- _ O
vocabulary -X- _ O
sets -X- _ O
of -X- _ O
input -X- _ O
and -X- _ O
output -X- _ O
components -X- _ O
and -X- _ O
thus -X- _ O
, -X- _ O
use -X- _ O
the -X- _ O
same -X- _ O
embedding -X- _ O
matrix -X- _ O
. -X- _ O
Given -X- _ O
the -X- _ O
encoded -X- _ O
question -X- _ O
Q -X- _ O
, -X- _ O
to -X- _ O
decode -X- _ O
the -X- _ O
program -X- _ O
for -X- _ O
dialogue -X- _ O
understanding -X- _ O
, -X- _ O
the -X- _ O
contextual -X- _ O
signals -X- _ O
are -X- _ O
integrated -X- _ O
through -X- _ O
2 -X- _ O
attention -X- _ O
layers -X- _ O
: -X- _ O
one -X- _ O
attention -X- _ O
on -X- _ O
previously -X- _ O
generated -X- _ O
tokens -X- _ O
, -X- _ O
and -X- _ O
the -X- _ O
other -X- _ O
on -X- _ O
question -X- _ O
tokens -X- _ O
. -X- _ O
At -X- _ O
time -X- _ O
step -X- _ O
j -X- _ O
, -X- _ O
we -X- _ O
denote -X- _ O
the -X- _ O
output -X- _ O
from -X- _ O
an -X- _ O
attention -X- _ O
layer -X- _ O
as -X- _ O
A -X- _ O
dial -X- _ O
, -X- _ O
j -X- _ O
. -X- _ O

A -X- _ O

( -X- _ O
1 -X- _ O
) -X- _ O
dial -X- _ O
= -X- _ O
Attention -X- _ O
( -X- _ O
P -X- _ O
dial -X- _ O
| -X- _ O
jâˆ’1 -X- _ O
0 -X- _ O
, -X- _ O
P -X- _ O
dial -X- _ O
| -X- _ O
jâˆ’1 -X- _ O
0 -X- _ O
, -X- _ O
P -X- _ O
dial -X- _ O
| -X- _ O
jâˆ’1 -X- _ O
0 -X- _ O
) -X- _ O
A -X- _ O

( -X- _ O
2 -X- _ O
) -X- _ O
dial -X- _ O
= -X- _ O
Attention -X- _ O
( -X- _ O
A -X- _ O
( -X- _ O
1 -X- _ O
) -X- _ O
dial -X- _ O
, -X- _ O
Q -X- _ O
, -X- _ O
Q -X- _ O
) -X- _ O
âˆˆ -X- _ O
R -X- _ O
jÃ—d -X- _ O
To -X- _ O
generate -X- _ O
programs -X- _ O
for -X- _ O
video -X- _ O
understanding -X- _ O
, -X- _ O
the -X- _ O
contextual -X- _ O
signals -X- _ O
are -X- _ O
learned -X- _ O
and -X- _ O
incorporated -X- _ O
in -X- _ O
a -X- _ O
similar -X- _ O
manner -X- _ O
. -X- _ O
However -X- _ O
, -X- _ O
to -X- _ O
exploit -X- _ O
dialogue -X- _ O
contextual -X- _ O
cues -X- _ O
, -X- _ O
the -X- _ O
execution -X- _ O
output -X- _ O
of -X- _ O
dialogue -X- _ O
understanding -X- _ O
neural -X- _ O
modules -X- _ O
Q -X- _ O
ctx -X- _ O
is -X- _ O
incorporated -X- _ O
to -X- _ O
each -X- _ O
vector -X- _ O
in -X- _ O
P -X- _ O
dial -X- _ O
through -X- _ O
an -X- _ O
additional -X- _ O
attention -X- _ O
layer -X- _ O
. -X- _ O
This -X- _ O
layer -X- _ O
integrates -X- _ O
the -X- _ O
resolved -X- _ O
entity -X- _ O
information -X- _ O
to -X- _ O
decode -X- _ O
the -X- _ O
original -X- _ O
entities -X- _ O
for -X- _ O
video -X- _ O
understanding -X- _ O
. -X- _ O
It -X- _ O
is -X- _ O
equivalent -X- _ O
to -X- _ O
a -X- _ O
reasoning -X- _ O
process -X- _ O
that -X- _ O
converts -X- _ O
the -X- _ O
question -X- _ O
from -X- _ O
its -X- _ O
original -X- _ O
multi -X- _ O
- -X- _ O
turn -X- _ O
semantics -X- _ O
to -X- _ O
single -X- _ O
- -X- _ O
turn -X- _ O
semantics -X- _ O
. -X- _ O

A -X- _ O

( -X- _ O
1 -X- _ O
) -X- _ O
vid -X- _ O
= -X- _ O
Attention -X- _ O
( -X- _ O
P -X- _ O
vid -X- _ O
| -X- _ O
jâˆ’1 -X- _ O
0 -X- _ O
, -X- _ O
P -X- _ O
vid -X- _ O
| -X- _ O
jâˆ’1 -X- _ O
0 -X- _ O
, -X- _ O
P -X- _ O
vid -X- _ O
| -X- _ O
jâˆ’1 -X- _ O
0 -X- _ O
) -X- _ O
A -X- _ O
Noted -X- _ O
that -X- _ O
in -X- _ O
the -X- _ O
neural -X- _ O
modules -X- _ O
described -X- _ O
in -X- _ O
Section -X- _ O
3.3 -X- _ O
, -X- _ O
during -X- _ O
training -X- _ O
, -X- _ O
we -X- _ O
simply -X- _ O
feed -X- _ O
the -X- _ O
ground -X- _ O
- -X- _ O
truth -X- _ O
programs -X- _ O
to -X- _ O
optimize -X- _ O
these -X- _ O
modules -X- _ O
. -X- _ O
For -X- _ O
instance -X- _ O
, -X- _ O
the -X- _ O
neural -X- _ O
module -X- _ O
where -X- _ O
received -X- _ O
the -X- _ O
ground -X- _ O
truth -X- _ O
entities -X- _ O
P -X- _ O
which -X- _ O
is -X- _ O
then -X- _ O
used -X- _ O
to -X- _ O
instantiate -X- _ O
the -X- _ O
neural -X- _ O
network -X- _ O
and -X- _ O
retrieve -X- _ O
from -X- _ O
video -X- _ O
V -X- _ O
. -X- _ O
During -X- _ O
test -X- _ O
time -X- _ O
, -X- _ O
we -X- _ O
decode -X- _ O
the -X- _ O
programs -X- _ O
token -X- _ O
by -X- _ O
token -X- _ O
through -X- _ O
the -X- _ O
question -X- _ O
parsers -X- _ O
, -X- _ O
and -X- _ O
feed -X- _ O
the -X- _ O
predicted -X- _ O
entitiesP -X- _ O
to -X- _ O
neural -X- _ O
modules -X- _ O
. -X- _ O
Note -X- _ O
that -X- _ O
we -X- _ O
do -X- _ O
not -X- _ O
assume -X- _ O
, -X- _ O
and -X- _ O
hence -X- _ O
not -X- _ O
train -X- _ O
model -X- _ O
to -X- _ O
retrieve -X- _ O
ground -X- _ O
- -X- _ O
truth -X- _ O
locations -X- _ O
of -X- _ O
visual -X- _ O
entities -X- _ O
in -X- _ O
videos -X- _ O
. -X- _ O
This -X- _ O
strategy -X- _ O
enables -X- _ O
the -X- _ O
applicability -X- _ O
of -X- _ O
VGNMN -X- _ O
as -X- _ O
we -X- _ O
consider -X- _ O
these -X- _ O
entity -X- _ O
annotations -X- _ O
mostly -X- _ O
unavailable -X- _ O
in -X- _ O
real -X- _ O
- -X- _ O
world -X- _ O
systems -X- _ O
. -X- _ O

B -X- _ O
Additional -X- _ O
Experimental -X- _ O
Results -X- _ O

B.1 -X- _ O
Non -X- _ O
- -X- _ O
NMN -X- _ O
Models -X- _ O

We -X- _ O
experiment -X- _ O
with -X- _ O
several -X- _ O
Non -X- _ B-MethodName
- -X- _ I-MethodName
NMN -X- _ I-MethodName
based -X- _ O
variants -X- _ O
of -X- _ O
our -X- _ O
models -X- _ O
. -X- _ O
As -X- _ O
can -X- _ O
be -X- _ O
seen -X- _ O
in -X- _ O
Table -X- _ O
7 -X- _ O
, -X- _ O
our -X- _ O
approach -X- _ O
to -X- _ O
video -X- _ O
and -X- _ O
dialogue -X- _ O
understanding -X- _ O
through -X- _ O
compositional -X- _ O
reasoning -X- _ O
programs -X- _ O
exhibits -X- _ O
better -X- _ O
performance -X- _ O
than -X- _ O
non -X- _ O
- -X- _ O
compositional -X- _ O
approaches -X- _ O
. -X- _ O
Compared -X- _ O
to -X- _ O
the -X- _ O
approaches -X- _ O
that -X- _ O
directly -X- _ O
process -X- _ O
frame -X- _ O
- -X- _ O
level -X- _ O
features -X- _ O
in -X- _ O
videos -X- _ O
( -X- _ O
Row -X- _ O
B -X- _ O
) -X- _ O
or -X- _ O
token -X- _ O
- -X- _ O
level -X- _ O
features -X- _ O
in -X- _ O
dialogues -X- _ O
( -X- _ O
Row -X- _ O
C -X- _ O
, -X- _ O
D -X- _ O
) -X- _ O
, -X- _ O
our -X- _ O
full -X- _ O
VGNMN -X- _ B-MethodName
( -X- _ O
Row -X- _ O
A -X- _ O
) -X- _ O
considers -X- _ O
entitylevel -X- _ O
and -X- _ O
action -X- _ O
- -X- _ O
level -X- _ O
information -X- _ O
extraction -X- _ O
and -X- _ O
thus -X- _ O
, -X- _ O
avoids -X- _ O
unnecessary -X- _ O
and -X- _ O
possibly -X- _ O
noisy -X- _ O
extraction -X- _ O
. -X- _ O
Compared -X- _ O
to -X- _ O
the -X- _ O
approaches -X- _ O
that -X- _ O
obtain -X- _ O
dialogue -X- _ O
contextual -X- _ O
cues -X- _ O
through -X- _ O
a -X- _ O
hierarchical -X- _ O
encoding -X- _ O
architecture -X- _ O
( -X- _ O
Row -X- _ O
E -X- _ O
, -X- _ O
F -X- _ O
) -X- _ O
such -X- _ O
as -X- _ O
( -X- _ O
Serban -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2016 -X- _ O
; -X- _ O
, -X- _ O
VGNMN -X- _ B-MethodName
directly -X- _ O
addresses -X- _ O
the -X- _ O
challenge -X- _ O
of -X- _ O
entity -X- _ O
references -X- _ O
in -X- _ O
dialogues -X- _ O
. -X- _ O
As -X- _ O
mentioned -X- _ O
, -X- _ O
we -X- _ O
hypothesize -X- _ O
that -X- _ O
the -X- _ O
hierarchical -X- _ O
encoding -X- _ O
architecture -X- _ O
is -X- _ O
more -X- _ O
appropriate -X- _ O
for -X- _ O
less -X- _ O
entity -X- _ O
- -X- _ O
sensitive -X- _ O
dialogues -X- _ O
such -X- _ O
as -X- _ O
chit -X- _ O
- -X- _ O
chat -X- _ O
and -X- _ O
open -X- _ O
- -X- _ O
domain -X- _ O
dialogues -X- _ O
. -X- _ O

B.2 -X- _ O
Dialogue -X- _ O
context -X- _ O
integration -X- _ O

Experimenting -X- _ O
with -X- _ O
different -X- _ O
ways -X- _ O
to -X- _ O
integrate -X- _ O
dialogue -X- _ O
context -X- _ O
representations -X- _ O
, -X- _ O
we -X- _ O
observe -X- _ O
that -X- _ O
adding -X- _ O
an -X- _ O
attention -X- _ O
layer -X- _ O
attending -X- _ O
to -X- _ O
question -X- _ O
during -X- _ O
response -X- _ O
decoding -X- _ O
( -X- _ O
Row -X- _ O
G -X- _ O
) -X- _ O
is -X- _ O
not -X- _ O
necessary -X- _ O
. -X- _ O
This -X- _ O
can -X- _ O
be -X- _ O
explained -X- _ O
as -X- _ O
the -X- _ O
representation -X- _ O
Q -X- _ O
ctx -X- _ O
obtained -X- _ O
from -X- _ O
dialogue -X- _ O
understanding -X- _ O
program -X- _ O
already -X- _ O
contains -X- _ O
contextual -X- _ O
information -X- _ O
of -X- _ O
both -X- _ O
dialogue -X- _ O
history -X- _ O
and -X- _ O
question -X- _ O
and -X- _ O
question -X- _ O
input -X- _ O
is -X- _ O
no -X- _ O
longer -X- _ O
needed -X- _ O
in -X- _ O
the -X- _ O
decoding -X- _ O
phase -X- _ O
. -X- _ O
Furthermore -X- _ O
, -X- _ O
we -X- _ O
investigate -X- _ O
the -X- _ O
model -X- _ O
sensitivity -X- _ O
to -X- _ O
natural -X- _ O
language -X- _ O
generation -X- _ O
through -X- _ O
its -X- _ O
ability -X- _ O
to -X- _ O
construct -X- _ O
linguistically -X- _ O
correct -X- _ O
programs -X- _ O
and -X- _ O
responses -X- _ O
. -X- _ O
To -X- _ O
generate -X- _ O
responses -X- _ O
that -X- _ O
are -X- _ O
linguistically -X- _ O
appropriate -X- _ O
, -X- _ O
VGNMN -X- _ B-MethodName
needs -X- _ O
dialogue -X- _ O
context -X- _ O
representation -X- _ O
Q -X- _ O
ctx -X- _ O
as -X- _ O
input -X- _ O
to -X- _ O
the -X- _ O
response -X- _ O
decoder -X- _ O
( -X- _ O
Row -X- _ O
H -X- _ O
) -X- _ O
. -X- _ O
The -X- _ O
model -X- _ O
also -X- _ O
needs -X- _ O
encoded -X- _ O
question -X- _ O
Q -X- _ O
as -X- _ O
input -X- _ O
to -X- _ O
the -X- _ O
video -X- _ O
understanding -X- _ O
program -X- _ O
parser -X- _ O
to -X- _ O
be -X- _ O
able -X- _ O
to -X- _ O
decompose -X- _ O
this -X- _ O
sequence -X- _ O
to -X- _ O
entity -X- _ O
and -X- _ O
action -X- _ O
module -X- _ O
parameters -X- _ O
( -X- _ O
Row -X- _ O
I -X- _ O
) -X- _ O
. -X- _ O

A -X- _ O
Additional -X- _ O
Model -X- _ O
Details -X- _ O

A.1 -X- _ O
Question -X- _ O
Parsers -X- _ O

To -X- _ O
learn -X- _ O
compositional -X- _ O
programs -X- _ O
, -X- _ O
we -X- _ O
follow -X- _ O
( -X- _ O
Johnson -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017a -X- _ O
; -X- _ O
Hu -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
and -X- _ O
consider -X- _ O
program -X- _ O
generation -X- _ O
as -X- _ O
a -X- _ O
sequence -X- _ O
- -X- _ O
tosequence -X- _ O
task -X- _ O
. -X- _ O
We -X- _ O
adopt -X- _ O
a -X- _ O
simple -X- _ O
template -X- _ O
" -X- _ O
param -X- _ O
1 -X- _ O
module -X- _ O
1 -X- _ O
param -X- _ O
2 -X- _ O
module -X- _ O
2 -X- _ O
... -X- _ O
" -X- _ O
as -X- _ O
the -X- _ O
target -X- _ O
sequence -X- _ O
. -X- _ O
The -X- _ O
resulting -X- _ O
target -X- _ O
sequences -X- _ O
for -X- _ O
dialogue -X- _ O
and -X- _ O
video -X- _ O
understanding -X- _ O
programs -X- _ O
are -X- _ O
sequences -X- _ O
P -X- _ O
dial -X- _ O
and -X- _ O
P -X- _ O
vid -X- _ O
respectively -X- _ O
. -X- _ O

The -X- _ O
parsers -X- _ O
decompose -X- _ O
questions -X- _ O
into -X- _ O
subsequences -X- _ O
to -X- _ O
construct -X- _ O
compositional -X- _ O
reasoning -X- _ O
programs -X- _ O
for -X- _ O
dialogue -X- _ O
and -X- _ O
video -X- _ O
understanding -X- _ O
. -X- _ O
Each -X- _ O
parser -X- _ O
is -X- _ O
an -X- _ O
attention -X- _ O
- -X- _ O
based -X- _ O
Transformer -X- _ O
decoder -X- _ O
. -X- _ O
The -X- _ O
Transformer -X- _ O
attention -X- _ O
is -X- _ O
a -X- _ O
multi -X- _ O
- -X- _ O
head -X- _ O
attention -X- _ O
on -X- _ O
query -X- _ O
q -X- _ O
, -X- _ O
key -X- _ O
k -X- _ O
, -X- _ O
and -X- _ O
value -X- _ O
v -X- _ O
tensors -X- _ O
, -X- _ O
denoted -X- _ O
as -X- _ O
Attention -X- _ O
( -X- _ O
q -X- _ O
, -X- _ O
k -X- _ O
, -X- _ O
v -X- _ O
) -X- _ O
. -X- _ O
For -X- _ O
each -X- _ O
token -X- _ O
in -X- _ O
the -X- _ O
q -X- _ O
sequence -X- _ O
, -X- _ O
the -X- _ O
distribution -X- _ O
over -X- _ O
tokens -X- _ O
in -X- _ O
the -X- _ O
k -X- _ O
sequence -X- _ O
is -X- _ O
used -X- _ O
to -X- _ O
obtain -X- _ O
the -X- _ O
weighted -X- _ O
sum -X- _ O
of -X- _ O
the -X- _ O
corresponding -X- _ O
representations -X- _ O
in -X- _ O
the -X- _ O
v -X- _ O
sequence -X- _ O
. -X- _ O

C -X- _ O
Interpretability -X- _ O

We -X- _ O
extract -X- _ O
the -X- _ O
predicted -X- _ O
programs -X- _ O
and -X- _ O
responses -X- _ O
for -X- _ O
some -X- _ O
example -X- _ O
dialogues -X- _ O
in -X- _ O
Figure -X- _ O
6 -X- _ O
, -X- _ O
7 -X- _ O
, -X- _ O
8 -X- _ O
, -X- _ O
and -X- _ O
9 -X- _ O
and -X- _ O
report -X- _ O
our -X- _ O
observations -X- _ O
: -X- _ O

â€¢ -X- _ O
We -X- _ O
observe -X- _ O
that -X- _ O
when -X- _ O
the -X- _ O
predicted -X- _ O
programs -X- _ O
are -X- _ O
correct -X- _ O
, -X- _ O
the -X- _ O
output -X- _ O
responses -X- _ O
generally -X- _ O
match -X- _ O
the -X- _ O
ground -X- _ O
- -X- _ O
truth -X- _ O
( -X- _ O
See -X- _ O
the -X- _ O
1 -X- _ O
st -X- _ O
and -X- _ O
2 -X- _ O
nd -X- _ O
turn -X- _ O
in -X- _ O
Figure -X- _ O
6 -X- _ O
, -X- _ O
and -X- _ O
the -X- _ O
1 -X- _ O
st -X- _ O
and -X- _ O
4 -X- _ O
th -X- _ O
turn -X- _ O
in -X- _ O
Figure -X- _ O
8 -X- _ O
) -X- _ O
or -X- _ O
close -X- _ O
to -X- _ O
the -X- _ O
ground -X- _ O
- -X- _ O
truth -X- _ O
responses -X- _ O
( -X- _ O
1 -X- _ O
st -X- _ O
turn -X- _ O
in -X- _ O
Figure -X- _ O
7 -X- _ O
) -X- _ O
. -X- _ O

â€¢ -X- _ O
When -X- _ O
the -X- _ O
output -X- _ O
responses -X- _ O
do -X- _ O
not -X- _ O
match -X- _ O
the -X- _ O
ground -X- _ O
truth -X- _ O
, -X- _ O
we -X- _ O
can -X- _ O
understand -X- _ O
the -X- _ O
model -X- _ O
mistakes -X- _ O
by -X- _ O
interpreting -X- _ O
the -X- _ O
predicted -X- _ O
programs -X- _ O
. -X- _ O
For -X- _ O
example -X- _ O
, -X- _ O
in -X- _ O
the -X- _ O
3 -X- _ O
rd -X- _ O
turn -X- _ O
in -X- _ O
Figure -X- _ O
6 -X- _ O
, -X- _ O
the -X- _ O
output -X- _ O
response -X- _ O
describes -X- _ O
a -X- _ O
room -X- _ O
because -X- _ O
the -X- _ O
predicted -X- _ O
video -X- _ O
program -X- _ O
focuses -X- _ O
on -X- _ O
the -X- _ O
entity -X- _ O
" -X- _ O
what -X- _ O
room -X- _ O
" -X- _ O
instead -X- _ O
of -X- _ O
the -X- _ O
entity -X- _ O
" -X- _ O
an -X- _ O
object -X- _ O
" -X- _ O
in -X- _ O
the -X- _ O
question -X- _ O
. -X- _ O
Another -X- _ O
example -X- _ O
is -X- _ O
the -X- _ O
3 -X- _ O
rd -X- _ O
turn -X- _ O
in -X- _ O
Figure -X- _ O
8 -X- _ O
where -X- _ O
the -X- _ O
entity -X- _ O
" -X- _ O
rooftop -X- _ O
" -X- _ O
is -X- _ O
missing -X- _ O
in -X- _ O
the -X- _ O
video -X- _ O
program -X- _ O
. -X- _ O
These -X- _ O
mismatches -X- _ O
can -X- _ O
deviate -X- _ O
the -X- _ O
information -X- _ O
retrieved -X- _ O
from -X- _ O
the -X- _ O
video -X- _ O
during -X- _ O
video -X- _ O
program -X- _ O
execution -X- _ O
, -X- _ O
leading -X- _ O
to -X- _ O
wrong -X- _ O
output -X- _ O
responses -X- _ O
with -X- _ O
wrong -X- _ O
visual -X- _ O
contents -X- _ O
. -X- _ O

â€¢ -X- _ O
We -X- _ O
also -X- _ O
note -X- _ O
that -X- _ O
in -X- _ O
some -X- _ O
cases -X- _ O
, -X- _ O
one -X- _ O
or -X- _ O
both -X- _ O
of -X- _ O
the -X- _ O
predicted -X- _ O
programs -X- _ O
are -X- _ O
incorrect -X- _ O
, -X- _ O
but -X- _ O
the -X- _ O
predicted -X- _ O
responses -X- _ O
still -X- _ O
match -X- _ O
the -X- _ O
groundtruth -X- _ O
responses -X- _ O
. -X- _ O
This -X- _ O
might -X- _ O
be -X- _ O
explained -X- _ O
as -X- _ O
the -X- _ O
predicted -X- _ O
module -X- _ O
parameters -X- _ O
are -X- _ O
still -X- _ O
close -X- _ O
enough -X- _ O
to -X- _ O
the -X- _ O
" -X- _ O
gold -X- _ O
" -X- _ O
labels -X- _ O
( -X- _ O
e.g. -X- _ O
4 -X- _ O
th -X- _ O
turn -X- _ O
in -X- _ O
Figure -X- _ O
6 -X- _ O
) -X- _ O
. -X- _ O
Sometimes -X- _ O
, -X- _ O
our -X- _ O
model -X- _ O
predicted -X- _ O
programs -X- _ O
that -X- _ O
are -X- _ O
more -X- _ O
appropriate -X- _ O
than -X- _ O
the -X- _ O
ground -X- _ O
truth -X- _ O
. -X- _ O
For -X- _ O
example -X- _ O
, -X- _ O
in -X- _ O
the -X- _ O
2 -X- _ O
nd -X- _ O
turn -X- _ O
in -X- _ O
Figure -X- _ O
7 -X- _ O
, -X- _ O
the -X- _ O
program -X- _ O
is -X- _ O
added -X- _ O
with -X- _ O
a -X- _ O
where -X- _ O
module -X- _ O
parameterized -X- _ O
by -X- _ O
the -X- _ O
entity -X- _ O
" -X- _ O
the -X- _ O
shopping -X- _ O
bag -X- _ O
" -X- _ O
which -X- _ O
was -X- _ O
solved -X- _ O
from -X- _ O
the -X- _ O
reference -X- _ O
" -X- _ O
them -X- _ O
" -X- _ O
mentioned -X- _ O
in -X- _ O
the -X- _ O
question -X- _ O
. -X- _ O

â€¢ -X- _ O
We -X- _ O
observe -X- _ O
that -X- _ O
for -X- _ O
complex -X- _ O
questions -X- _ O
that -X- _ O
involve -X- _ O
more -X- _ O
than -X- _ O
one -X- _ O
queries -X- _ O
( -X- _ O
e.g. -X- _ O
the -X- _ O
3 -X- _ O
rd -X- _ O
turn -X- _ O
in -X- _ O
Figure -X- _ O
8 -X- _ O
) -X- _ O
, -X- _ O
it -X- _ O
becomes -X- _ O
more -X- _ O
challenging -X- _ O
to -X- _ O
decode -X- _ O
an -X- _ O
appropriate -X- _ O
video -X- _ O
understanding -X- _ O
program -X- _ O
and -X- _ O
generate -X- _ O
responses -X- _ O
that -X- _ O
can -X- _ O
address -X- _ O
all -X- _ O
queries -X- _ O
. -X- _ O

â€¢ -X- _ O
In -X- _ O
Figure -X- _ O
9 -X- _ O
, -X- _ O
we -X- _ O
demonstrate -X- _ O
some -X- _ O
output -X- _ O
examples -X- _ O
of -X- _ O
VGNMN -X- _ B-MethodName
and -X- _ O
compare -X- _ O
with -X- _ O
two -X- _ O
baselines -X- _ O
: -X- _ O
Baseline -X- _ B-MethodName
and -X- _ O
MTN -X- _ B-MethodName
( -X- _ O
Le -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019b -X- _ O
) -X- _ O
. -X- _ O
We -X- _ O
noted -X- _ O
that -X- _ O
VGNMN -X- _ B-MethodName
can -X- _ O
include -X- _ O
important -X- _ O
entities -X- _ O
relevant -X- _ O
to -X- _ O
the -X- _ O
current -X- _ O
dialogue -X- _ O
turn -X- _ O
to -X- _ O
construct -X- _ O
output -X- _ O
responses -X- _ O
while -X- _ O
other -X- _ O
models -X- _ O
might -X- _ O
miss -X- _ O
some -X- _ O
entity -X- _ O
details -X- _ O
, -X- _ O
e.g. -X- _ O
" -X- _ O
them -X- _ O
/ -X- _ O
dishes -X- _ O
" -X- _ O
in -X- _ O
example -X- _ O
A -X- _ O
and -X- _ O
" -X- _ O
the -X- _ O
magazine -X- _ O
" -X- _ O
in -X- _ O
example -X- _ O
B. -X- _ O
These -X- _ O
small -X- _ O
yet -X- _ O
important -X- _ O
details -X- _ O
can -X- _ O
determine -X- _ O
the -X- _ O
correctness -X- _ O
of -X- _ O
dialogue -X- _ O
responses -X- _ O
. -X- _ O
Figure -X- _ O
9 -X- _ O
: -X- _ O
Interpretability -X- _ O
of -X- _ O
example -X- _ O
outputs -X- _ O
from -X- _ O
VGNMN -X- _ B-MethodName
and -X- _ O
baselines -X- _ O
models -X- _ O
Le -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019b -X- _ O
) -X- _ O

