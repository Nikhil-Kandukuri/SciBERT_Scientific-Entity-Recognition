-DOCSTART- -X- O
Modeling -X- _ O
Bilingual -X- _ O
Conversational -X- _ O
Characteristics -X- _ O
for -X- _ O
Neural -X- _ B-TaskName
Chat -X- _ I-TaskName
Translation -X- _ I-TaskName

Neural -X- _ B-TaskName
chat -X- _ I-TaskName
translation -X- _ I-TaskName
aims -X- _ O
to -X- _ O
translate -X- _ O
bilingual -X- _ O
conversational -X- _ O
text -X- _ O
, -X- _ O
which -X- _ O
has -X- _ O
a -X- _ O
broad -X- _ O
application -X- _ O
in -X- _ O
international -X- _ O
exchanges -X- _ O
and -X- _ O
cooperation -X- _ O
. -X- _ O
Despite -X- _ O
the -X- _ O
impressive -X- _ O
performance -X- _ O
of -X- _ O
sentence -X- _ O
- -X- _ O
level -X- _ O
and -X- _ O
context -X- _ O
- -X- _ O
aware -X- _ O
Neural -X- _ B-TaskName
Machine -X- _ I-TaskName
Translation -X- _ I-TaskName
( -X- _ O
NMT -X- _ B-TaskName
) -X- _ O
, -X- _ O
there -X- _ O
still -X- _ O
remain -X- _ O
challenges -X- _ O
to -X- _ O
translate -X- _ O
bilingual -X- _ O
conversational -X- _ O
text -X- _ O
due -X- _ O
to -X- _ O
its -X- _ O
inherent -X- _ O
characteristics -X- _ O
such -X- _ O
as -X- _ O
role -X- _ O
preference -X- _ O
, -X- _ O
dialogue -X- _ O
coherence -X- _ O
, -X- _ O
and -X- _ O
translation -X- _ O
consistency -X- _ O
. -X- _ O
In -X- _ O
this -X- _ O
paper -X- _ O
, -X- _ O
we -X- _ O
aim -X- _ O
to -X- _ O
promote -X- _ O
the -X- _ O
translation -X- _ O
quality -X- _ O
of -X- _ O
conversational -X- _ O
text -X- _ O
by -X- _ O
modeling -X- _ O
the -X- _ O
above -X- _ O
properties -X- _ O
. -X- _ O
Specifically -X- _ O
, -X- _ O
we -X- _ O
design -X- _ O
three -X- _ O
latent -X- _ O
variational -X- _ O
modules -X- _ O
to -X- _ O
learn -X- _ O
the -X- _ O
distributions -X- _ O
of -X- _ O
bilingual -X- _ O
conversational -X- _ O
characteristics -X- _ O
. -X- _ O
Through -X- _ O
sampling -X- _ O
from -X- _ O
these -X- _ O
learned -X- _ O
distributions -X- _ O
, -X- _ O
the -X- _ O
latent -X- _ O
variables -X- _ O
, -X- _ O
tailored -X- _ O
for -X- _ O
role -X- _ O
preference -X- _ O
, -X- _ O
dialogue -X- _ O
coherence -X- _ O
, -X- _ O
and -X- _ O
translation -X- _ O
consistency -X- _ O
, -X- _ O
are -X- _ O
incorporated -X- _ O
into -X- _ O
the -X- _ O
NMT -X- _ B-TaskName
model -X- _ O
for -X- _ O
better -X- _ O
translation -X- _ O
. -X- _ O
We -X- _ O
evaluate -X- _ O
our -X- _ O
approach -X- _ O
on -X- _ O
the -X- _ O
benchmark -X- _ O
dataset -X- _ O
BConTrasT -X- _ B-DatasetName
( -X- _ O
English⇔German -X- _ O
) -X- _ O
and -X- _ O
a -X- _ O
self -X- _ O
- -X- _ O
collected -X- _ O
bilingual -X- _ O
dialogue -X- _ O
corpus -X- _ O
, -X- _ O
named -X- _ O
BMELD -X- _ B-DatasetName
( -X- _ O
English⇔Chinese -X- _ O
) -X- _ O
. -X- _ O
Extensive -X- _ O
experiments -X- _ O
show -X- _ O
that -X- _ O
our -X- _ O
approach -X- _ O
notably -X- _ O
boosts -X- _ O
the -X- _ O
performance -X- _ O
over -X- _ O
strong -X- _ O
baselines -X- _ O
by -X- _ O
a -X- _ O
large -X- _ O
margin -X- _ O
and -X- _ O
significantly -X- _ O
surpasses -X- _ O
some -X- _ O
state -X- _ O
- -X- _ O
of -X- _ O
- -X- _ O
the -X- _ O
- -X- _ O
art -X- _ O
context -X- _ O
- -X- _ O
aware -X- _ O
NMT -X- _ B-TaskName
models -X- _ O
in -X- _ O
terms -X- _ O
of -X- _ O
BLEU -X- _ B-MetricName
and -X- _ O
TER -X- _ B-MetricName
. -X- _ O
Additionally -X- _ O
, -X- _ O
we -X- _ O
make -X- _ O
the -X- _ O
BMELD -X- _ B-DatasetName
dataset -X- _ O
publicly -X- _ O
available -X- _ O
for -X- _ O
the -X- _ O
research -X- _ O
community -X- _ O
. -X- _ O
1 -X- _ O

Introduction -X- _ O

A -X- _ O
conversation -X- _ O
may -X- _ O
involve -X- _ O
participants -X- _ O
that -X- _ O
speak -X- _ O
in -X- _ O
different -X- _ O
languages -X- _ O
( -X- _ O
e.g. -X- _ O
, -X- _ O
one -X- _ O
speaking -X- _ O
in -X- _ O
English -X- _ O
and -X- _ O
another -X- _ O
in -X- _ O
Chinese -X- _ O
) -X- _ O
. -X- _ O
Fig -X- _ O
. -X- _ O
1 -X- _ O
shows -X- _ O
an -X- _ O
example -X- _ O
, -X- _ O
where -X- _ O
the -X- _ O
English -X- _ O
role -X- _ O
R -X- _ O
1 -X- _ O
and -X- _ O
the -X- _ O
Chinese -X- _ O
role -X- _ O
R -X- _ O
2 -X- _ O
are -X- _ O
talking -X- _ O
about -X- _ O
the -X- _ O
" -X- _ O
boat -X- _ O
" -X- _ O
. -X- _ O
The -X- _ O
, -X- _ O
where -X- _ O
the -X- _ O
Chinese -X- _ O
utterances -X- _ O
are -X- _ O
presented -X- _ O
in -X- _ O
pinyin -X- _ O
style -X- _ O
. -X- _ O
R -X- _ O
i -X- _ O
: -X- _ O
Role -X- _ O
i. -X- _ O
The -X- _ O
dashed -X- _ O
arrows -X- _ O
mark -X- _ O
the -X- _ O
translation -X- _ O
direction -X- _ O
. -X- _ O
The -X- _ O
green -X- _ O
and -X- _ O
red -X- _ O
arrows -X- _ O
represent -X- _ O
the -X- _ O
monolingual -X- _ O
and -X- _ O
bilingual -X- _ O
conversation -X- _ O
flow -X- _ O
, -X- _ O
respectively -X- _ O
. -X- _ O
Although -X- _ O
the -X- _ O
translation -X- _ O
of -X- _ O
Y -X- _ O
5 -X- _ O
produced -X- _ O
by -X- _ O
the -X- _ O
" -X- _ O
S -X- _ B-TaskName
- -X- _ I-TaskName
NMT -X- _ I-TaskName
" -X- _ O
( -X- _ O
a -X- _ O
context -X- _ O
- -X- _ O
free -X- _ O
sentencelevel -X- _ O
NMT -X- _ O
system -X- _ O
) -X- _ O
is -X- _ O
reasonable -X- _ O
at -X- _ O
the -X- _ O
sentence -X- _ O
level -X- _ O
, -X- _ O
the -X- _ O
coherence -X- _ O
of -X- _ O
the -X- _ O
entire -X- _ O
dialogue -X- _ O
translation -X- _ O
is -X- _ O
poor -X- _ O
. -X- _ O

goal -X- _ O
of -X- _ O
chat -X- _ O
translation -X- _ O
is -X- _ O
to -X- _ O
translate -X- _ O
bilingual -X- _ O
conversational -X- _ O
text -X- _ O
, -X- _ O
i.e. -X- _ O
, -X- _ O
converting -X- _ O
one -X- _ O
participant -X- _ O
's -X- _ O
language -X- _ O
( -X- _ O
e.g. -X- _ O
, -X- _ O
English -X- _ O
) -X- _ O
to -X- _ O
another -X- _ O
's -X- _ O
( -X- _ O
e.g. -X- _ O
, -X- _ O
Chinese -X- _ O
) -X- _ O
and -X- _ O
vice -X- _ O
versa -X- _ O
( -X- _ O
Farajian -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
. -X- _ O
It -X- _ O
enables -X- _ O
multiple -X- _ O
speakers -X- _ O
to -X- _ O
communicate -X- _ O
with -X- _ O
each -X- _ O
other -X- _ O
in -X- _ O
their -X- _ O
native -X- _ O
languages -X- _ O
, -X- _ O
which -X- _ O
has -X- _ O
a -X- _ O
wide -X- _ O
application -X- _ O
in -X- _ O
industry -X- _ O
- -X- _ O
level -X- _ O
services -X- _ O
. -X- _ O

Although -X- _ O
sentence -X- _ B-TaskName
- -X- _ I-TaskName
level -X- _ I-TaskName
Neural -X- _ I-TaskName
Machine -X- _ I-TaskName
Translation -X- _ I-TaskName
( -X- _ O
NMT -X- _ B-TaskName
) -X- _ O
( -X- _ O
Sutskever -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2014 -X- _ O
; -X- _ O
Vaswani -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017 -X- _ O
; -X- _ O
Hassan -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2018 -X- _ O
; -X- _ O
Yan -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020 -X- _ O
; -X- _ O
has -X- _ O
achieved -X- _ O
promising -X- _ O
progress -X- _ O
, -X- _ O
it -X- _ O
still -X- _ O
faces -X- _ O
challenges -X- _ O
in -X- _ O
accurately -X- _ O
translating -X- _ O
conversational -X- _ O
text -X- _ O
due -X- _ O
to -X- _ O
abandoning -X- _ O
the -X- _ O
dialogue -X- _ O
history -X- _ O
, -X- _ O
which -X- _ O
leads -X- _ O
to -X- _ O
role -X- _ O
- -X- _ O
irrelevant -X- _ O
, -X- _ O
incoherent -X- _ O
and -X- _ O
inconsistent -X- _ O
translations -X- _ O
( -X- _ O
Mirkin -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2015 -X- _ O
; -X- _ O
Wang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017a -X- _ O
; -X- _ O
Läubli -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2018 -X- _ O
; -X- _ O
Toral -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2018 -X- _ O
) -X- _ O
. -X- _ O
Further -X- _ O
, -X- _ O
context -X- _ O
- -X- _ O
aware -X- _ O
NMT -X- _ O
( -X- _ O
Tiedemann -X- _ O
and -X- _ O
Scherrer -X- _ O
, -X- _ O
2017 -X- _ O
; -X- _ O
Voita -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2018Voita -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
, -X- _ O
2019aMaruf -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
; -X- _ O
Ma -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
can -X- _ O
be -X- _ O
directly -X- _ O
applied -X- _ O
to -X- _ O
chat -X- _ O
translation -X- _ O
through -X- _ O
incorporating -X- _ O
the -X- _ O
dialogue -X- _ O
history -X- _ O
but -X- _ O
can -X- _ O
not -X- _ O
obtain -X- _ O
satisfactory -X- _ O
results -X- _ O
in -X- _ O
this -X- _ O
sce -X- _ O
- -X- _ O
nario -X- _ O
( -X- _ O
Moghe -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
. -X- _ O
One -X- _ O
important -X- _ O
reason -X- _ O
is -X- _ O
the -X- _ O
lack -X- _ O
of -X- _ O
explicitly -X- _ O
modeling -X- _ O
the -X- _ O
inherent -X- _ O
bilingual -X- _ O
conversational -X- _ O
characteristics -X- _ O
, -X- _ O
e.g. -X- _ O
, -X- _ O
role -X- _ O
preference -X- _ O
, -X- _ O
dialogue -X- _ O
coherence -X- _ O
, -X- _ O
and -X- _ O
translation -X- _ O
consistency -X- _ O
, -X- _ O
as -X- _ O
pointed -X- _ O
out -X- _ O
by -X- _ O
Farajian -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2020 -X- _ O
) -X- _ O
. -X- _ O

For -X- _ O
a -X- _ O
conversation -X- _ O
, -X- _ O
its -X- _ O
dialogue -X- _ O
history -X- _ O
contains -X- _ O
rich -X- _ O
role -X- _ O
preference -X- _ O
information -X- _ O
such -X- _ O
as -X- _ O
emotion -X- _ O
, -X- _ O
style -X- _ O
, -X- _ O
and -X- _ O
humor -X- _ O
, -X- _ O
which -X- _ O
is -X- _ O
beneficial -X- _ O
to -X- _ O
rolerelevant -X- _ O
utterance -X- _ O
generation -X- _ O
( -X- _ O
Wu -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
. -X- _ O
As -X- _ O
shown -X- _ O
in -X- _ O
Fig -X- _ O
. -X- _ O
1 -X- _ O
, -X- _ O
the -X- _ O
utterances -X- _ O
X -X- _ O
1 -X- _ O
, -X- _ O
X -X- _ O
3 -X- _ O
and -X- _ O
X -X- _ O
5 -X- _ O
from -X- _ O
role -X- _ O
R -X- _ O
1 -X- _ O
always -X- _ O
have -X- _ O
strong -X- _ O
emotions -X- _ O
( -X- _ O
i.e. -X- _ O
, -X- _ O
joy -X- _ O
) -X- _ O
because -X- _ O
of -X- _ O
his -X- _ O
/ -X- _ O
her -X- _ O
preference -X- _ O
, -X- _ O
and -X- _ O
preserving -X- _ O
the -X- _ O
same -X- _ O
preference -X- _ O
information -X- _ O
across -X- _ O
languages -X- _ O
can -X- _ O
help -X- _ O
raise -X- _ O
emotional -X- _ O
resonance -X- _ O
and -X- _ O
mutual -X- _ O
understanding -X- _ O
( -X- _ O
Moghe -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
. -X- _ O
Meanwhile -X- _ O
, -X- _ O
there -X- _ O
exists -X- _ O
semantic -X- _ O
coherence -X- _ O
in -X- _ O
the -X- _ O
conversation -X- _ O
, -X- _ O
as -X- _ O
the -X- _ O
solid -X- _ O
green -X- _ O
arrow -X- _ O
in -X- _ O
Fig -X- _ O
. -X- _ O
1 -X- _ O
, -X- _ O
where -X- _ O
the -X- _ O
utterance -X- _ O
X -X- _ O
5 -X- _ O
naturally -X- _ O
and -X- _ O
semantically -X- _ O
connects -X- _ O
with -X- _ O
the -X- _ O
dialogue -X- _ O
history -X- _ O
( -X- _ O
X -X- _ O
1∼4 -X- _ O
) -X- _ O
on -X- _ O
the -X- _ O
topic -X- _ O
" -X- _ O
boat -X- _ O
" -X- _ O
. -X- _ O
In -X- _ O
addition -X- _ O
, -X- _ O
the -X- _ O
bilingual -X- _ O
conversation -X- _ O
exhibits -X- _ O
translation -X- _ O
consistency -X- _ O
, -X- _ O
where -X- _ O
the -X- _ O
correct -X- _ O
lexical -X- _ O
choice -X- _ O
to -X- _ O
translate -X- _ O
the -X- _ O
current -X- _ O
utterance -X- _ O
might -X- _ O
have -X- _ O
appeared -X- _ O
in -X- _ O
preceding -X- _ O
turns -X- _ O
. -X- _ O
For -X- _ O
instance -X- _ O
, -X- _ O
the -X- _ O
word -X- _ O
" -X- _ O
sail -X- _ O
" -X- _ O
in -X- _ O
X -X- _ O
1 -X- _ O
is -X- _ O
translated -X- _ O
into -X- _ O
" -X- _ O
jiàchuán -X- _ O
" -X- _ O
, -X- _ O
and -X- _ O
thus -X- _ O
the -X- _ O
word -X- _ O
" -X- _ O
sailing -X- _ O
" -X- _ O
in -X- _ O
X -X- _ O
3 -X- _ O
should -X- _ O
be -X- _ O
mapped -X- _ O
into -X- _ O
" -X- _ O
jiàchuán -X- _ O
" -X- _ O
rather -X- _ O
than -X- _ O
other -X- _ O
words -X- _ O
( -X- _ O
e.g. -X- _ O
, -X- _ O
" -X- _ O
hángxíng -X- _ O
" -X- _ O
2 -X- _ O
) -X- _ O
to -X- _ O
maintain -X- _ O
translation -X- _ O
consistency -X- _ O
. -X- _ O
On -X- _ O
the -X- _ O
contrary -X- _ O
, -X- _ O
if -X- _ O
we -X- _ O
ignore -X- _ O
these -X- _ O
characteristics -X- _ O
, -X- _ O
translations -X- _ O
might -X- _ O
be -X- _ O
role -X- _ O
- -X- _ O
irrelevant -X- _ O
, -X- _ O
incoherent -X- _ O
, -X- _ O
inconsistent -X- _ O
, -X- _ O
and -X- _ O
detrimental -X- _ O
to -X- _ O
further -X- _ O
communication -X- _ O
like -X- _ O
the -X- _ O
translation -X- _ O
produced -X- _ O
by -X- _ O
the -X- _ O
" -X- _ O
S -X- _ B-TaskName
- -X- _ I-TaskName
NMT -X- _ I-TaskName
" -X- _ O
in -X- _ O
Fig -X- _ O
. -X- _ O
1 -X- _ O
. -X- _ O
Although -X- _ O
the -X- _ O
translation -X- _ O
is -X- _ O
acceptable -X- _ O
at -X- _ O
the -X- _ O
sentence -X- _ O
level -X- _ O
, -X- _ O
it -X- _ O
is -X- _ O
abrupt -X- _ O
at -X- _ O
the -X- _ O
bilingual -X- _ O
conversation -X- _ O
level -X- _ O
. -X- _ O

Apparently -X- _ O
, -X- _ O
how -X- _ O
to -X- _ O
effectively -X- _ O
exploit -X- _ O
these -X- _ O
bilingual -X- _ O
conversational -X- _ O
characteristics -X- _ O
is -X- _ O
one -X- _ O
of -X- _ O
the -X- _ O
core -X- _ O
issues -X- _ O
in -X- _ O
chat -X- _ O
translation -X- _ O
. -X- _ O
And -X- _ O
it -X- _ O
is -X- _ O
challenging -X- _ O
to -X- _ O
implicitly -X- _ O
capture -X- _ O
these -X- _ O
properties -X- _ O
by -X- _ O
just -X- _ O
incorporating -X- _ O
the -X- _ O
complex -X- _ O
dialogue -X- _ O
history -X- _ O
into -X- _ O
encoders -X- _ O
due -X- _ O
to -X- _ O
lacking -X- _ O
the -X- _ O
relevant -X- _ O
information -X- _ O
guidance -X- _ O
( -X- _ O
Farajian -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
. -X- _ O
On -X- _ O
the -X- _ O
other -X- _ O
hand -X- _ O
, -X- _ O
the -X- _ O
Conditional -X- _ O
Variational -X- _ O
Auto -X- _ O
- -X- _ O
Encoder -X- _ O
( -X- _ O
CVAE -X- _ O
) -X- _ O
( -X- _ O
Sohn -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2015 -X- _ O
) -X- _ O
has -X- _ O
shown -X- _ O
its -X- _ O
superiority -X- _ O
in -X- _ O
learning -X- _ O
distributions -X- _ O
of -X- _ O
data -X- _ O
properties -X- _ O
, -X- _ O
which -X- _ O
is -X- _ O
often -X- _ O
utilized -X- _ O
to -X- _ O
model -X- _ O
the -X- _ O
diversity -X- _ O
( -X- _ O
Zhao -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
, -X- _ O
coherence -X- _ O
( -X- _ O
Wang -X- _ O
and -X- _ O
Wan -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
and -X- _ O
users -X- _ O
' -X- _ O
personalities -X- _ O
( -X- _ O
Bak -X- _ O
and -X- _ O
Oh -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
, -X- _ O
etc -X- _ O
. -X- _ O
In -X- _ O
spite -X- _ O
of -X- _ O
its -X- _ O
success -X- _ O
, -X- _ O
adapting -X- _ O
it -X- _ O
to -X- _ O
chat -X- _ O
translation -X- _ O
is -X- _ O
non -X- _ O
- -X- _ O
trivial -X- _ O
, -X- _ O
especially -X- _ O
involving -X- _ O
multiple -X- _ O
tailored -X- _ O
latent -X- _ O
variables -X- _ O
. -X- _ O
Therefore -X- _ O
, -X- _ O
in -X- _ O
this -X- _ O
paper -X- _ O
, -X- _ O
we -X- _ O
propose -X- _ O
a -X- _ O
model -X- _ O
, -X- _ O
named -X- _ O
CPCC -X- _ B-MethodName
, -X- _ O
to -X- _ O
capture -X- _ O
role -X- _ O
preference -X- _ O
, -X- _ O
dialogue -X- _ O
coherence -X- _ O
, -X- _ O
and -X- _ O
translation -X- _ O
consistency -X- _ O
with -X- _ O
latent -X- _ O
variables -X- _ O
learned -X- _ O
by -X- _ O
the -X- _ O
CVAE -X- _ O
for -X- _ O
neural -X- _ O
chat -X- _ O
translation -X- _ O
. -X- _ O
CPCC -X- _ B-MethodName
contains -X- _ O
three -X- _ O
specific -X- _ O
latent -X- _ O
variational -X- _ O
modules -X- _ O
to -X- _ O
learn -X- _ O
the -X- _ O
distributions -X- _ O
of -X- _ O
role -X- _ O
preference -X- _ O
, -X- _ O
dialogue -X- _ O
coherence -X- _ O
, -X- _ O
and -X- _ O
translation -X- _ O
consistency -X- _ O
, -X- _ O
respectively -X- _ O
. -X- _ O
Specifically -X- _ O
, -X- _ O
we -X- _ O
firstly -X- _ O
use -X- _ O
one -X- _ O
role -X- _ O
- -X- _ O
tailored -X- _ O
latent -X- _ O
variable -X- _ O
, -X- _ O
sampled -X- _ O
from -X- _ O
the -X- _ O
learned -X- _ O
distribution -X- _ O
conditioned -X- _ O
only -X- _ O
on -X- _ O
the -X- _ O
utterances -X- _ O
from -X- _ O
this -X- _ O
role -X- _ O
, -X- _ O
to -X- _ O
preserve -X- _ O
preference -X- _ O
. -X- _ O
Then -X- _ O
, -X- _ O
we -X- _ O
utilize -X- _ O
another -X- _ O
latent -X- _ O
variable -X- _ O
, -X- _ O
generated -X- _ O
by -X- _ O
the -X- _ O
distribution -X- _ O
conditioned -X- _ O
on -X- _ O
source -X- _ O
- -X- _ O
language -X- _ O
dialogue -X- _ O
history -X- _ O
, -X- _ O
to -X- _ O
maintain -X- _ O
coherence -X- _ O
. -X- _ O
Finally -X- _ O
, -X- _ O
we -X- _ O
leverage -X- _ O
the -X- _ O
last -X- _ O
latent -X- _ O
variable -X- _ O
, -X- _ O
generated -X- _ O
by -X- _ O
the -X- _ O
distribution -X- _ O
conditioned -X- _ O
on -X- _ O
paired -X- _ O
bilingual -X- _ O
conversational -X- _ O
utterances -X- _ O
, -X- _ O
to -X- _ O
keep -X- _ O
translation -X- _ O
consistency -X- _ O
. -X- _ O
As -X- _ O
a -X- _ O
result -X- _ O
, -X- _ O
these -X- _ O
tailored -X- _ O
latent -X- _ O
variables -X- _ O
allow -X- _ O
our -X- _ O
CPCC -X- _ B-MethodName
to -X- _ O
produce -X- _ O
role -X- _ O
- -X- _ O
specific -X- _ O
, -X- _ O
coherent -X- _ O
, -X- _ O
and -X- _ O
consistent -X- _ O
translations -X- _ O
, -X- _ O
and -X- _ O
hence -X- _ O
make -X- _ O
the -X- _ O
bilingual -X- _ O
conversation -X- _ O
go -X- _ O
fluently -X- _ O
. -X- _ O

We -X- _ O
conduct -X- _ O
experiments -X- _ O
on -X- _ O
WMT20 -X- _ O
Chat -X- _ O
Translation -X- _ O
dataset -X- _ O
: -X- _ O
BConTrasT -X- _ B-DatasetName
( -X- _ O
En⇔De -X- _ O
3 -X- _ O
) -X- _ O
( -X- _ O
Farajian -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
and -X- _ O
a -X- _ O
self -X- _ O
- -X- _ O
collected -X- _ O
dialogue -X- _ O
corpus -X- _ O
: -X- _ O
BMELD -X- _ B-DatasetName
( -X- _ O
En⇔Ch -X- _ O
) -X- _ O
. -X- _ O
Results -X- _ O
demonstrate -X- _ O
that -X- _ O
our -X- _ O
model -X- _ O
achieves -X- _ O
consistent -X- _ O
improvements -X- _ O
in -X- _ O
four -X- _ O
directions -X- _ O
in -X- _ O
terms -X- _ O
of -X- _ O
BLEU -X- _ B-MetricValue
( -X- _ O
Papineni -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2002 -X- _ O
) -X- _ O
and -X- _ O
TER -X- _ B-MetricValue
( -X- _ O
Snover -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2006 -X- _ O
) -X- _ O
, -X- _ O
showing -X- _ O
its -X- _ O
effectiveness -X- _ O
and -X- _ O
generalizability -X- _ O
. -X- _ O
Human -X- _ O
evaluation -X- _ O
further -X- _ O
suggests -X- _ O
that -X- _ O
our -X- _ O
model -X- _ O
effectively -X- _ O
alleviates -X- _ O
the -X- _ O
issue -X- _ O
of -X- _ O
role -X- _ O
- -X- _ O
irrelevant -X- _ O
, -X- _ O
incoherent -X- _ O
and -X- _ O
inconsistent -X- _ O
translations -X- _ O
compared -X- _ O
to -X- _ O
other -X- _ O
methods -X- _ O
. -X- _ O
Our -X- _ O
contributions -X- _ O
are -X- _ O
summarized -X- _ O
as -X- _ O
follows -X- _ O
: -X- _ O

• -X- _ O
To -X- _ O
the -X- _ O
best -X- _ O
of -X- _ O
our -X- _ O
knowledge -X- _ O
, -X- _ O
we -X- _ O
are -X- _ O
the -X- _ O
first -X- _ O
to -X- _ O
incorporate -X- _ O
the -X- _ O
role -X- _ O
preference -X- _ O
, -X- _ O
dialogue -X- _ O
coherence -X- _ O
, -X- _ O
and -X- _ O
translation -X- _ O
consistency -X- _ O
into -X- _ O
neural -X- _ O
chat -X- _ O
translation -X- _ O
. -X- _ O
• -X- _ O
We -X- _ O
are -X- _ O
the -X- _ O
first -X- _ O
to -X- _ O
build -X- _ O
a -X- _ O
bridge -X- _ O
between -X- _ O
the -X- _ O
dialogue -X- _ O
and -X- _ O
machine -X- _ O
translation -X- _ O
via -X- _ O
conditional -X- _ O
variational -X- _ O
auto -X- _ O
- -X- _ O
encoder -X- _ O
, -X- _ O
which -X- _ O
effectively -X- _ O
models -X- _ O
three -X- _ O
inherent -X- _ O
characteristics -X- _ O
in -X- _ O
bilingual -X- _ O
conversation -X- _ O
for -X- _ O
neural -X- _ B-TaskName
chat -X- _ I-TaskName
translation -X- _ I-TaskName
. -X- _ O
• -X- _ O
Our -X- _ O
approach -X- _ O
gains -X- _ O
consistent -X- _ O
and -X- _ O
significant -X- _ O
performance -X- _ O
over -X- _ O
the -X- _ O
standard -X- _ O
context -X- _ O
- -X- _ O
aware -X- _ O
baseline -X- _ O
and -X- _ O
remarkably -X- _ O
outperforms -X- _ O
some -X- _ O
state -X- _ O
- -X- _ O
of -X- _ O
- -X- _ O
the -X- _ O
- -X- _ O
art -X- _ O
context -X- _ O
- -X- _ O
aware -X- _ O
NMT -X- _ B-TaskName
models -X- _ O
. -X- _ O
• -X- _ O
We -X- _ O
contribute -X- _ O
a -X- _ O
new -X- _ O
bilingual -X- _ O
dialogue -X- _ O
corpus -X- _ O
( -X- _ O
BMELD -X- _ B-DatasetName
, -X- _ O
En⇔Ch -X- _ O
) -X- _ O
with -X- _ O
manual -X- _ O
translations -X- _ O
and -X- _ O
our -X- _ O
codes -X- _ O
to -X- _ O
the -X- _ O
research -X- _ O
community -X- _ O
. -X- _ O

We -X- _ O
aim -X- _ O
to -X- _ O
learn -X- _ O
a -X- _ O
model -X- _ O
that -X- _ O
can -X- _ O
capture -X- _ O
inherent -X- _ O
characteristics -X- _ O
in -X- _ O
the -X- _ O
bilingual -X- _ O
dialogue -X- _ O
history -X- _ O
for -X- _ O
producing -X- _ O
high -X- _ O
- -X- _ O
quality -X- _ O
translations -X- _ O
, -X- _ O
i.e. -X- _ O
, -X- _ O
using -X- _ O
the -X- _ O
context -X- _ O
for -X- _ O
better -X- _ O
translations -X- _ O
( -X- _ O
Farajian -X- _ O

|T -X- _ O
|−1 -X- _ O
2 -X- _ O
] -X- _ O

and -X- _ O
T -X- _ O
is -X- _ O
the -X- _ O
total -X- _ O
number -X- _ O
of -X- _ O
turns -X- _ O
( -X- _ O
assumed -X- _ O
to -X- _ O
be -X- _ O
odd -X- _ O
here -X- _ O
) -X- _ O
. -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
. -X- _ O
Following -X- _ O
, -X- _ O
we -X- _ O
define -X- _ O
paired -X- _ O
bilingual -X- _ O
utterances -X- _ O
( -X- _ O
X -X- _ O
i -X- _ O
, -X- _ O
Y -X- _ O
i -X- _ O
) -X- _ O
as -X- _ O
a -X- _ O
turn -X- _ O
in -X- _ O
Fig -X- _ O
. -X- _ O
2 -X- _ O
, -X- _ O
where -X- _ O
we -X- _ O
will -X- _ O
translate -X- _ O
the -X- _ O
current -X- _ O
utterance -X- _ O
X -X- _ O
2k+1 -X- _ O
at -X- _ O
the -X- _ O
( -X- _ O
2k -X- _ O
+ -X- _ O
1 -X- _ O
) -X- _ O
-th -X- _ O
turn -X- _ O
. -X- _ O
Here -X- _ O
, -X- _ O
we -X- _ O
denote -X- _ O
the -X- _ O
utterance -X- _ O
X -X- _ O
2k+1 -X- _ O
as -X- _ O
X -X- _ O
u -X- _ O
and -X- _ O
its -X- _ O
translation -X- _ O
Y -X- _ O
2k+1 -X- _ O
as -X- _ O
Y -X- _ O
u -X- _ O
for -X- _ O
simplicity -X- _ O
, -X- _ O
where -X- _ O
X -X- _ O
u -X- _ O
= -X- _ O
{ -X- _ O
x -X- _ O
i -X- _ O
} -X- _ O
m -X- _ O
i=1 -X- _ O
with -X- _ O
m -X- _ O
tokens -X- _ O
and -X- _ O
Y -X- _ O
u -X- _ O
= -X- _ O
{ -X- _ O
y -X- _ O
i -X- _ O
} -X- _ O
n -X- _ O
i=1 -X- _ O
with -X- _ O
n -X- _ O
tokens -X- _ O
. -X- _ O
Formally -X- _ O
, -X- _ O
the -X- _ O
conditional -X- _ O
distribution -X- _ O
for -X- _ O
the -X- _ O
current -X- _ O
utterance -X- _ O
is -X- _ O

p -X- _ O
θ -X- _ O
( -X- _ O
Y -X- _ O
u -X- _ O
|X -X- _ O
u -X- _ O
, -X- _ O
C -X- _ O
) -X- _ O
= -X- _ O
n -X- _ O
t=1 -X- _ O
p -X- _ O
θ -X- _ O
( -X- _ O
y -X- _ O
t -X- _ O
|X -X- _ O
u -X- _ O
, -X- _ O
y -X- _ O
1 -X- _ O
: -X- _ O
t−1 -X- _ O
, -X- _ O
C -X- _ O
) -X- _ O
, -X- _ O

where -X- _ O
C -X- _ O
is -X- _ O
the -X- _ O
bilingual -X- _ O
dialogue -X- _ O
history -X- _ O
. -X- _ O

Before -X- _ O
we -X- _ O
dig -X- _ O
into -X- _ O
the -X- _ O
details -X- _ O
of -X- _ O
how -X- _ O
to -X- _ O
utilize -X- _ O
C -X- _ O
, -X- _ O
we -X- _ O
define -X- _ O
three -X- _ O
types -X- _ O
of -X- _ O
context -X- _ O
in -X- _ O
C -X- _ O
( -X- _ O
as -X- _ O
shown -X- _ O
in -X- _ O
Fig -X- _ O
. -X- _ O
2 -X- _ O
) -X- _ O
: -X- _ O
( -X- _ O
1 -X- _ O
) -X- _ O
the -X- _ O
set -X- _ O
of -X- _ O
previous -X- _ O
role -X- _ O
- -X- _ O
specific -X- _ O
source -X- _ O
- -X- _ O
language -X- _ O
turns -X- _ O
, -X- _ O
denoted -X- _ O
as -X- _ O
C -X- _ O
role -X- _ O
X -X- _ O
= -X- _ O
{ -X- _ O
X -X- _ O
1 -X- _ O
, -X- _ O
X -X- _ O
3 -X- _ O
, -X- _ O
X -X- _ O
5 -X- _ O
, -X- _ O
... -X- _ O
, -X- _ O
X -X- _ O
2k+1 -X- _ O
} -X- _ O
4 -X- _ O
where -X- _ O
k -X- _ O
∈ -X- _ O
[ -X- _ O
0 -X- _ O
, -X- _ O
|T -X- _ O
|−3 -X- _ O
2 -X- _ O
] -X- _ O
and -X- _ O
T -X- _ O
is -X- _ O
the -X- _ O
total -X- _ O
number -X- _ O
of -X- _ O
turns -X- _ O
; -X- _ O
( -X- _ O
2 -X- _ O
) -X- _ O
the -X- _ O
set -X- _ O
of -X- _ O
previous -X- _ O
source -X- _ O
- -X- _ O
language -X- _ O
turns -X- _ O
, -X- _ O
denoted -X- _ O
as -X- _ O
C -X- _ O
X -X- _ O
= -X- _ O
{ -X- _ O
X -X- _ O
1 -X- _ O
, -X- _ O
X -X- _ O
2 -X- _ O
, -X- _ O
X -X- _ O
3 -X- _ O
, -X- _ O
... -X- _ O
, -X- _ O
X -X- _ O
2k -X- _ O
} -X- _ O
; -X- _ O
and -X- _ O
( -X- _ O
3 -X- _ O
) -X- _ O
the -X- _ O
set -X- _ O
of -X- _ O
previous -X- _ O
target -X- _ O
- -X- _ O
language -X- _ O
turns -X- _ O
, -X- _ O
denoted -X- _ O
as -X- _ O

C -X- _ O
Y -X- _ O
= -X- _ O
{ -X- _ O
Y -X- _ O
1 -X- _ O
, -X- _ O
Y -X- _ O
2 -X- _ O
, -X- _ O
Y -X- _ O
3 -X- _ O
, -X- _ O
... -X- _ O
, -X- _ O
Y -X- _ O
2k -X- _ O
} -X- _ O
. -X- _ O

4 -X- _ O
Our -X- _ O
Methodology -X- _ O
Fig -X- _ O
. -X- _ O
3 -X- _ O
demonstrates -X- _ O
an -X- _ O
overview -X- _ O
of -X- _ O
our -X- _ O
model -X- _ O
, -X- _ O
consisting -X- _ O
of -X- _ O
five -X- _ O
components -X- _ O
: -X- _ O
input -X- _ O
representation -X- _ O
, -X- _ O
encoder -X- _ O
, -X- _ O
latent -X- _ O
variational -X- _ O
modules -X- _ O
, -X- _ O
decoder -X- _ O
, -X- _ O
and -X- _ O
training -X- _ O
objectives -X- _ O
. -X- _ O
Specifically -X- _ O
, -X- _ O
we -X- _ O
aim -X- _ O
to -X- _ O
model -X- _ O
both -X- _ O
dialogue -X- _ O
and -X- _ O
translation -X- _ O
simultaneously -X- _ O
. -X- _ O
Therefore -X- _ O
, -X- _ O
for -X- _ O
the -X- _ O
input -X- _ O
representation -X- _ O
( -X- _ O
§ -X- _ O
4.1 -X- _ O
) -X- _ O
, -X- _ O
we -X- _ O
incorporate -X- _ O
dialogue -X- _ O
- -X- _ O
level -X- _ O
embeddings -X- _ O
, -X- _ O
i.e. -X- _ O
, -X- _ O
role -X- _ O
and -X- _ O
dialogue -X- _ O
turn -X- _ O
embeddings -X- _ O
, -X- _ O
into -X- _ O
the -X- _ O
encoder -X- _ O
( -X- _ O
§ -X- _ O
4.2 -X- _ O
) -X- _ O
. -X- _ O
Then -X- _ O
, -X- _ O
we -X- _ O
introduce -X- _ O
three -X- _ O
specific -X- _ O
latent -X- _ O
variational -X- _ O
modules -X- _ O
( -X- _ O
§ -X- _ O
4.3 -X- _ O
) -X- _ O
to -X- _ O
learn -X- _ O
the -X- _ O
distributions -X- _ O
for -X- _ O
varied -X- _ O
inherent -X- _ O
bilingual -X- _ O
characteristics -X- _ O
. -X- _ O
Finally -X- _ O
, -X- _ O
we -X- _ O
elaborate -X- _ O
on -X- _ O
how -X- _ O
to -X- _ O
incorporate -X- _ O
the -X- _ O
three -X- _ O
tailored -X- _ O
latent -X- _ O
variables -X- _ O
sampled -X- _ O
from -X- _ O
4 -X- _ O
C -X- _ O
role -X- _ O
Y -X- _ O
= -X- _ O
{ -X- _ O
Y2 -X- _ O
, -X- _ O
Y4 -X- _ O
, -X- _ O
Y6 -X- _ O
, -X- _ O
... -X- _ O
, -X- _ O
Y -X- _ O
2k -X- _ O
} -X- _ O
is -X- _ O
also -X- _ O
role -X- _ O
- -X- _ O
specific -X- _ O
utterances -X- _ O
of -X- _ O
the -X- _ O
interlocutor -X- _ O
, -X- _ O
which -X- _ O
is -X- _ O
used -X- _ O
to -X- _ O
model -X- _ O
the -X- _ O
interlocutor -X- _ O
's -X- _ O
consistency -X- _ O
in -X- _ O
the -X- _ O
reverse -X- _ O
translation -X- _ O
direction -X- _ O
. -X- _ O
Here -X- _ O
, -X- _ O
we -X- _ O
take -X- _ O
one -X- _ O
translation -X- _ O
direction -X- _ O
( -X- _ O
i.e. -X- _ O
, -X- _ O
En⇒Ch -X- _ O
) -X- _ O
as -X- _ O
an -X- _ O
example -X- _ O
. -X- _ O
the -X- _ O
distributions -X- _ O
into -X- _ O
the -X- _ O
decoder -X- _ O
( -X- _ O
§ -X- _ O
4.4 -X- _ O
) -X- _ O
and -X- _ O
our -X- _ O
two -X- _ O
- -X- _ O
stage -X- _ O
training -X- _ O
objectives -X- _ O
( -X- _ O
§ -X- _ O
4.5 -X- _ O
) -X- _ O
. -X- _ O

Input -X- _ O
Representation -X- _ O

The -X- _ O
CPCC -X- _ B-MethodName
contains -X- _ O
three -X- _ O
types -X- _ O
of -X- _ O
inputs -X- _ O
: -X- _ O
source -X- _ O
input -X- _ O
X -X- _ O
u -X- _ O
, -X- _ O
target -X- _ O
input -X- _ O
Y -X- _ O
u -X- _ O
, -X- _ O
and -X- _ O
context -X- _ O
inputs -X- _ O
{ -X- _ O
C -X- _ O
role -X- _ O
X -X- _ O
, -X- _ O
C -X- _ O
X -X- _ O
, -X- _ O
C -X- _ O
Y -X- _ O
} -X- _ O
. -X- _ O
Apart -X- _ O
from -X- _ O
the -X- _ O
conventional -X- _ O
word -X- _ O
embeddings -X- _ O
WE -X- _ O
and -X- _ O
position -X- _ O
embeddings -X- _ O
PE -X- _ O
( -X- _ O
Vaswani -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
, -X- _ O
we -X- _ O
also -X- _ O
introduce -X- _ O
role -X- _ O
embeddings -X- _ O
RE -X- _ O
and -X- _ O
dialogue -X- _ O
turn -X- _ O
embeddings -X- _ O
TE -X- _ O
to -X- _ O
identify -X- _ O
different -X- _ O
utterances -X- _ O
. -X- _ O
Specifically -X- _ O
, -X- _ O
for -X- _ O
X -X- _ O
u -X- _ O
, -X- _ O
we -X- _ O
firstly -X- _ O
project -X- _ O
it -X- _ O
into -X- _ O
these -X- _ O
embeddings -X- _ O
. -X- _ O
Then -X- _ O
, -X- _ O
we -X- _ O
perform -X- _ O
a -X- _ O
sum -X- _ O
operation -X- _ O
to -X- _ O
unify -X- _ O
them -X- _ O
into -X- _ O
a -X- _ O
single -X- _ O
input -X- _ O
for -X- _ O
each -X- _ O
token -X- _ O
x -X- _ O
i -X- _ O
: -X- _ O

h -X- _ O
0 -X- _ O
i -X- _ O
= -X- _ O
WE -X- _ O
( -X- _ O
x -X- _ O
i -X- _ O
) -X- _ O
+ -X- _ O
PE -X- _ O
( -X- _ O
x -X- _ O
i -X- _ O
) -X- _ O
+ -X- _ O
RE -X- _ O
( -X- _ O
x -X- _ O
i -X- _ O
) -X- _ O
+ -X- _ O
TE -X- _ O
( -X- _ O
x -X- _ O
i -X- _ O
) -X- _ O
, -X- _ O

( -X- _ O
2 -X- _ O
) -X- _ O
where -X- _ O
1 -X- _ O
≤ -X- _ O
i -X- _ O
≤ -X- _ O
m -X- _ O
and -X- _ O
WE -X- _ O
∈ -X- _ O
R -X- _ O
|V -X- _ O
|×d -X- _ O
, -X- _ O
RE -X- _ O
∈ -X- _ O
R -X- _ O
|R|×d -X- _ O
and -X- _ O
SE -X- _ O
∈ -X- _ O
R -X- _ O
|T -X- _ O
|×d -X- _ O
. -X- _ O
|V -X- _ O
| -X- _ O
, -X- _ O
|R| -X- _ O
, -X- _ O
|T -X- _ O
| -X- _ O
, -X- _ O
and -X- _ O
d -X- _ O
denote -X- _ O
the -X- _ O
size -X- _ O
of -X- _ O
shared -X- _ O
vocabulary -X- _ O
, -X- _ O
number -X- _ O
of -X- _ O
roles -X- _ O
, -X- _ O
max -X- _ O
turns -X- _ O
of -X- _ O
dialogue -X- _ O
, -X- _ O
and -X- _ O
hidden -X- _ O
size -X- _ O
, -X- _ O
respectively -X- _ O
. -X- _ O
h -X- _ O
0 -X- _ O
∈ -X- _ O
R -X- _ O
m×d -X- _ O
, -X- _ O
similarly -X- _ O
for -X- _ O
Y -X- _ O
u -X- _ O
. -X- _ O
For -X- _ O
each -X- _ O
of -X- _ O
{ -X- _ O
C -X- _ O
role -X- _ O
X -X- _ O
, -X- _ O
C -X- _ O
X -X- _ O
, -X- _ O
C -X- _ O
Y -X- _ O
} -X- _ O
, -X- _ O
we -X- _ O
add -X- _ O
' -X- _ O
[ -X- _ O
cls -X- _ O
] -X- _ O
' -X- _ O
tag -X- _ O
at -X- _ O
the -X- _ O
head -X- _ O
of -X- _ O
it -X- _ O
and -X- _ O
use -X- _ O
' -X- _ O
[ -X- _ O
sep -X- _ O
] -X- _ O
' -X- _ O
tag -X- _ O
to -X- _ O
separate -X- _ O
its -X- _ O
utterances -X- _ O
( -X- _ O
Devlin -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
, -X- _ O
and -X- _ O
then -X- _ O
get -X- _ O
its -X- _ O
embeddings -X- _ O
via -X- _ O
Eq -X- _ O
. -X- _ O
2 -X- _ O
. -X- _ O

Encoder -X- _ O

The -X- _ O
Transformer -X- _ O
encoder -X- _ O
consists -X- _ O
of -X- _ O
N -X- _ B-HyperparameterName
e -X- _ I-HyperparameterName
stacked -X- _ O
layers -X- _ O
and -X- _ O
each -X- _ O
layer -X- _ O
includes -X- _ O
two -X- _ O
sub -X- _ O
- -X- _ O
layers -X- _ O
: -X- _ O
5 -X- _ O
a -X- _ O
multi -X- _ O
- -X- _ O
head -X- _ O
self -X- _ O
- -X- _ O
attention -X- _ O
( -X- _ O
SelfAtt -X- _ O
) -X- _ O
sub -X- _ O
- -X- _ O
layer -X- _ O
and -X- _ O
a -X- _ O
position -X- _ O
- -X- _ O
wise -X- _ O
feed -X- _ O
- -X- _ O
forward -X- _ O
network -X- _ O
( -X- _ O
FFN -X- _ O
) -X- _ O
sublayer -X- _ O
( -X- _ O
Vaswani -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
: -X- _ O

s -X- _ O
e -X- _ O
= -X- _ O
SelfAtt -X- _ O
( -X- _ O
h -X- _ O
−1 -X- _ O
e -X- _ O
) -X- _ O
+ -X- _ O
h -X- _ O
−1 -X- _ O
e -X- _ O
, -X- _ O
h -X- _ O
−1 -X- _ O
e -X- _ O
∈ -X- _ O
R -X- _ O
m×d -X- _ O
, -X- _ O

h -X- _ O
e -X- _ O
= -X- _ O
FFN -X- _ O
( -X- _ O
s -X- _ O
e -X- _ O
) -X- _ O
+ -X- _ O
s -X- _ O
e -X- _ O
, -X- _ O
{ -X- _ O
h -X- _ O
e -X- _ O
, -X- _ O
s -X- _ O
e -X- _ O
} -X- _ O
∈ -X- _ O
R -X- _ O
m×d -X- _ O
, -X- _ O
5 -X- _ O
We -X- _ O
omit -X- _ O
the -X- _ O
layer -X- _ O
normalization -X- _ O
for -X- _ O
simplicity -X- _ O
, -X- _ O
and -X- _ O
you -X- _ O
may -X- _ O
refer -X- _ O
to -X- _ O
( -X- _ O
Vaswani -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
for -X- _ O
more -X- _ O
details -X- _ O
. -X- _ O

where -X- _ O
h -X- _ O
e -X- _ O
denotes -X- _ O
the -X- _ O
state -X- _ O
of -X- _ O
the -X- _ O
-th -X- _ O
encoder -X- _ O
layer -X- _ O
and -X- _ O
h -X- _ O
0 -X- _ O
e -X- _ O
denotes -X- _ O
the -X- _ O
initialized -X- _ O
feature -X- _ O
h -X- _ O
0 -X- _ O
. -X- _ O
We -X- _ O
prepare -X- _ O
the -X- _ O
representations -X- _ O
of -X- _ O
X -X- _ O
u -X- _ O
and -X- _ O
{ -X- _ O
C -X- _ O
role -X- _ O
X -X- _ O
, -X- _ O
C -X- _ O
X -X- _ O
, -X- _ O
C -X- _ O
Y -X- _ O
} -X- _ O
for -X- _ O
training -X- _ O
prior -X- _ O
and -X- _ O
recognition -X- _ O
networks -X- _ O
. -X- _ O
For -X- _ O
X -X- _ O
u -X- _ O
, -X- _ O
we -X- _ O
apply -X- _ O
mean -X- _ O
- -X- _ O
pooling -X- _ O
with -X- _ O
mask -X- _ O
operation -X- _ O
over -X- _ O
the -X- _ O
output -X- _ O
h -X- _ O
Ne -X- _ B-HyperparameterName
, -X- _ O
X -X- _ O
e -X- _ O
of -X- _ O
the -X- _ O
N -X- _ B-HyperparameterName
eth -X- _ I-HyperparameterName
encoder -X- _ O
layer -X- _ O
, -X- _ O
i.e. -X- _ O
, -X- _ O
h -X- _ O

X -X- _ O
= -X- _ O
1 -X- _ O
m -X- _ O
m -X- _ O
i=1 -X- _ O
( -X- _ O
M -X- _ O
X -X- _ O
i -X- _ O
h -X- _ O
Ne -X- _ B-HyperparameterName
, -X- _ O
X -X- _ O
e -X- _ O
, -X- _ O
i -X- _ O

) -X- _ O
, -X- _ O
h -X- _ O
X -X- _ O
∈ -X- _ O
R -X- _ O
d -X- _ O
, -X- _ O
where -X- _ O
M -X- _ O
X -X- _ O
∈ -X- _ O
R -X- _ O
m -X- _ O
denotes -X- _ O
the -X- _ O
mask -X- _ O
matrix -X- _ O
, -X- _ O
whose -X- _ O
value -X- _ O
is -X- _ O
either -X- _ O
1 -X- _ O
or -X- _ O
0 -X- _ O
indicating -X- _ O
whether -X- _ O
the -X- _ O
token -X- _ O
is -X- _ O
padded -X- _ O
. -X- _ O
For -X- _ O
C -X- _ O
role -X- _ O
X -X- _ O
, -X- _ O
as -X- _ O
shown -X- _ O
in -X- _ O
Fig -X- _ O
. -X- _ O
3 -X- _ O
, -X- _ O
we -X- _ O
follow -X- _ O
( -X- _ O
Ma -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
and -X- _ O
share -X- _ O
the -X- _ O
first -X- _ O
encoder -X- _ O
layer -X- _ O
to -X- _ O
obtain -X- _ O
the -X- _ O
context -X- _ O
representation -X- _ O
. -X- _ O
Here -X- _ O
, -X- _ O
we -X- _ O
take -X- _ O
the -X- _ O
hidden -X- _ O
state -X- _ O
of -X- _ O
' -X- _ O
[ -X- _ O
cls -X- _ O
] -X- _ O
' -X- _ O
as -X- _ O
its -X- _ O
representation -X- _ O
, -X- _ O
denoted -X- _ O
as -X- _ O
h -X- _ O
ctx -X- _ O
role -X- _ O
∈ -X- _ O
R -X- _ O
d -X- _ O
. -X- _ O
Similarly -X- _ O
, -X- _ O
we -X- _ O
obtain -X- _ O
representations -X- _ O
of -X- _ O
C -X- _ O
X -X- _ O
and -X- _ O
C -X- _ O
Y -X- _ O
, -X- _ O
denoted -X- _ O
as -X- _ O
h -X- _ O
ctx -X- _ O
X -X- _ O
∈ -X- _ O
R -X- _ O
d -X- _ O
and -X- _ O
h -X- _ O
ctx -X- _ O
Y -X- _ O
∈ -X- _ O
R -X- _ O
d -X- _ O
, -X- _ O
respectively -X- _ O
. -X- _ O
For -X- _ O
training -X- _ O
recognition -X- _ O
networks -X- _ O
, -X- _ O
we -X- _ O
obtain -X- _ O
the -X- _ O
representation -X- _ O
of -X- _ O

Y -X- _ O
u -X- _ O
as -X- _ O
h -X- _ O
Y -X- _ O
= -X- _ O
1 -X- _ O
n -X- _ O
n -X- _ O
i=1 -X- _ O
( -X- _ O
M -X- _ O
Y -X- _ O
i -X- _ O
h -X- _ O
Ne -X- _ B-HyperparameterName
, -X- _ O
Y -X- _ O
e -X- _ O
, -X- _ O
i -X- _ O
) -X- _ O
, -X- _ O
h -X- _ O
Y -X- _ O
∈ -X- _ O
R -X- _ O
d -X- _ O
, -X- _ O
where -X- _ O
M -X- _ O
Y -X- _ O
∈ -X- _ O
R -X- _ O
n -X- _ O
, -X- _ O
similar -X- _ O
to -X- _ O
M -X- _ O
X -X- _ O
. -X- _ O

Latent -X- _ O
Variational -X- _ O
Modules -X- _ O

We -X- _ O
design -X- _ O
three -X- _ O
tailored -X- _ O
latent -X- _ O
variational -X- _ O
modules -X- _ O
to -X- _ O
learn -X- _ O
the -X- _ O
distributions -X- _ O
of -X- _ O
inherent -X- _ O
bilingual -X- _ O
conversational -X- _ O
characteristics -X- _ O
, -X- _ O
i.e. -X- _ O
, -X- _ O
role -X- _ O
preference -X- _ O
, -X- _ O
dialogue -X- _ O
coherence -X- _ O
, -X- _ O
and -X- _ O
translation -X- _ O
consistency -X- _ O
. -X- _ O

Role -X- _ O
Preference -X- _ O
. -X- _ O
To -X- _ O
preserve -X- _ O
the -X- _ O
role -X- _ O
preference -X- _ O
when -X- _ O
translating -X- _ O
the -X- _ O
role -X- _ O
's -X- _ O
current -X- _ O
utterance -X- _ O
, -X- _ O
we -X- _ O
only -X- _ O
encode -X- _ O
the -X- _ O
previous -X- _ O
utterances -X- _ O
of -X- _ O
this -X- _ O
role -X- _ O
and -X- _ O
produce -X- _ O
a -X- _ O
role -X- _ O
- -X- _ O
tailored -X- _ O
latent -X- _ O
variable -X- _ O
z -X- _ O
role -X- _ O
∈ -X- _ O
R -X- _ O
dz -X- _ O
, -X- _ O
where -X- _ O
d -X- _ O
z -X- _ O
is -X- _ O
the -X- _ O
latent -X- _ O
size -X- _ O
. -X- _ O
Inspired -X- _ O
by -X- _ O
( -X- _ O
Wang -X- _ O
and -X- _ O
Wan -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
, -X- _ O
we -X- _ O
use -X- _ O
isotropic -X- _ O
Gaussian -X- _ O
distribution -X- _ O
as -X- _ O
the -X- _ O
prior -X- _ O
distribution -X- _ O
of -X- _ O
z -X- _ O
role -X- _ O
: -X- _ O
p -X- _ O
θ -X- _ O
( -X- _ O
z -X- _ O
role -X- _ O
|X -X- _ O
u -X- _ O
, -X- _ O
C -X- _ O
role -X- _ O
X -X- _ O
) -X- _ O
∼ -X- _ O
N -X- _ O
( -X- _ O
µ -X- _ O
role -X- _ O
, -X- _ O
σ -X- _ O
2 -X- _ O
role -X- _ O
I -X- _ O
) -X- _ O
, -X- _ O
where -X- _ O
I -X- _ O
denotes -X- _ O
the -X- _ O
identity -X- _ O
matrix -X- _ O
and -X- _ O
we -X- _ O
have -X- _ O
µ -X- _ O
role -X- _ O
= -X- _ O
MLP -X- _ O
role -X- _ O
θ -X- _ O
( -X- _ O
h -X- _ O
X -X- _ O
; -X- _ O
h -X- _ O
ctx -X- _ O
role -X- _ O
) -X- _ O
, -X- _ O
σ -X- _ O
role -X- _ O
= -X- _ O
Softplus -X- _ O
( -X- _ O
MLP -X- _ O
role -X- _ O
θ -X- _ O
( -X- _ O
h -X- _ O
X -X- _ O
; -X- _ O
h -X- _ O
ctx -X- _ O
role -X- _ O
) -X- _ O
) -X- _ O
, -X- _ O
where -X- _ O
MLP -X- _ O
( -X- _ O
• -X- _ O
) -X- _ O
and -X- _ O
Softplus -X- _ O
( -X- _ O
• -X- _ O
) -X- _ O
are -X- _ O
multi -X- _ O
- -X- _ O
layer -X- _ O
perceptron -X- _ O
and -X- _ O
approximation -X- _ O
of -X- _ O
ReLU -X- _ O
function -X- _ O
, -X- _ O
respectively -X- _ O
. -X- _ O
( -X- _ O
• -X- _ O
; -X- _ O
• -X- _ O
) -X- _ O
indicates -X- _ O
concatenation -X- _ O
operation -X- _ O
. -X- _ O

At -X- _ O
training -X- _ O
, -X- _ O
the -X- _ O
posterior -X- _ O
distribution -X- _ O
conditions -X- _ O
on -X- _ O
both -X- _ O
role -X- _ O
- -X- _ O
specific -X- _ O
utterances -X- _ O
and -X- _ O
the -X- _ O
current -X- _ O
translation -X- _ O
, -X- _ O
which -X- _ O
contain -X- _ O
rich -X- _ O
role -X- _ O
preference -X- _ O
information -X- _ O
. -X- _ O
Therefore -X- _ O
, -X- _ O
the -X- _ O
prior -X- _ O
network -X- _ O
can -X- _ O
learn -X- _ O
a -X- _ O
role -X- _ O
- -X- _ O
tailored -X- _ O
distribution -X- _ O
by -X- _ O
approaching -X- _ O
the -X- _ O
posterior -X- _ O
network -X- _ O
via -X- _ O
KL -X- _ B-MetricName
divergence -X- _ I-MetricName
( -X- _ O
Sohn -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2015 -X- _ O
) -X- _ O
: -X- _ O

q -X- _ O
φ -X- _ O
( -X- _ O
z -X- _ O
role -X- _ O
|X -X- _ O
u -X- _ O
, -X- _ O
C -X- _ O
role -X- _ O
X -X- _ O
, -X- _ O
Y -X- _ O
u -X- _ O
) -X- _ O
∼ -X- _ O
N -X- _ O
( -X- _ O
µ -X- _ O
role -X- _ O
, -X- _ O
σ -X- _ O
2 -X- _ O
role -X- _ O
I -X- _ O

) -X- _ O
and -X- _ O
{ -X- _ O
µ -X- _ O
role -X- _ O
, -X- _ O
σ -X- _ O
role -X- _ O
} -X- _ O
are -X- _ O
calculated -X- _ O
as -X- _ O
: -X- _ O

µ -X- _ O
role -X- _ O
= -X- _ O
MLP -X- _ O
role -X- _ O
φ -X- _ O
( -X- _ O
h -X- _ O
X -X- _ O
; -X- _ O
h -X- _ O
ctx -X- _ O
role -X- _ O
; -X- _ O
h -X- _ O
Y -X- _ O
) -X- _ O
, -X- _ O
σ -X- _ O
role -X- _ O
= -X- _ O
Softplus -X- _ O
( -X- _ O
MLP -X- _ O
role -X- _ O
φ -X- _ O
( -X- _ O
h -X- _ O
X -X- _ O
; -X- _ O
h -X- _ O
ctx -X- _ O
role -X- _ O
; -X- _ O
h -X- _ O
Y -X- _ O
) -X- _ O
) -X- _ O
. -X- _ O

Dialogue -X- _ O
Coherence -X- _ O
. -X- _ O
To -X- _ O
maintain -X- _ O
the -X- _ O
coherence -X- _ O
in -X- _ O
chat -X- _ O
translation -X- _ O
, -X- _ O
we -X- _ O
encode -X- _ O
the -X- _ O
entire -X- _ O
sourcelanguage -X- _ O
utterances -X- _ O
and -X- _ O
then -X- _ O
generate -X- _ O
a -X- _ O
latent -X- _ O
variable -X- _ O
z -X- _ O
dia -X- _ O
∈ -X- _ O
R -X- _ O
dz -X- _ O
. -X- _ O
Similar -X- _ O
to -X- _ O
z -X- _ O
role -X- _ O
, -X- _ O
we -X- _ O
define -X- _ O
its -X- _ O
prior -X- _ O
distribution -X- _ O
as -X- _ O
: -X- _ O

p -X- _ O
θ -X- _ O
( -X- _ O
z -X- _ O
dia -X- _ O
|X -X- _ O
u -X- _ O
, -X- _ O
C -X- _ O
X -X- _ O
) -X- _ O
∼ -X- _ O
N -X- _ O
( -X- _ O
µ -X- _ O
dia -X- _ O
, -X- _ O
σ -X- _ O
2 -X- _ O
dia -X- _ O
I -X- _ O
) -X- _ O
and -X- _ O
{ -X- _ O
µ -X- _ O
dia -X- _ O
, -X- _ O
σ -X- _ O
dia -X- _ O
} -X- _ O
are -X- _ O
calculated -X- _ O
as -X- _ O
: -X- _ O
µ -X- _ O
dia -X- _ O
= -X- _ O
MLP -X- _ O
dia -X- _ O
θ -X- _ O
( -X- _ O
h -X- _ O
X -X- _ O
; -X- _ O
h -X- _ O
ctx -X- _ O
X -X- _ O
) -X- _ O
, -X- _ O
σ -X- _ O
dia -X- _ O
= -X- _ O
Softplus -X- _ O
( -X- _ O
MLP -X- _ O
dia -X- _ O
θ -X- _ O
( -X- _ O
h -X- _ O
X -X- _ O
; -X- _ O
h -X- _ O
ctx -X- _ O
X -X- _ O
) -X- _ O
) -X- _ O
. -X- _ O

At -X- _ O
training -X- _ O
, -X- _ O
the -X- _ O
posterior -X- _ O
distribution -X- _ O
conditions -X- _ O
on -X- _ O
both -X- _ O
the -X- _ O
entire -X- _ O
source -X- _ O
- -X- _ O
language -X- _ O
utterances -X- _ O
and -X- _ O
the -X- _ O
translation -X- _ O
that -X- _ O
provide -X- _ O
a -X- _ O
dialoguelevel -X- _ O
coherence -X- _ O
clue -X- _ O
, -X- _ O
and -X- _ O
is -X- _ O
responsible -X- _ O
for -X- _ O
guiding -X- _ O
the -X- _ O
learning -X- _ O
of -X- _ O
the -X- _ O
prior -X- _ O
distribution -X- _ O
. -X- _ O
Specifically -X- _ O
, -X- _ O
we -X- _ O
define -X- _ O
the -X- _ O
posterior -X- _ O
distribution -X- _ O
as -X- _ O
: -X- _ O

q -X- _ O
φ -X- _ O
( -X- _ O
z -X- _ O
dia -X- _ O
|X -X- _ O
u -X- _ O
, -X- _ O
C -X- _ O
X -X- _ O
, -X- _ O
Y -X- _ O
u -X- _ O
) -X- _ O
∼ -X- _ O
N -X- _ O
( -X- _ O
µ -X- _ O
dia -X- _ O
, -X- _ O
σ -X- _ O
2 -X- _ O
dia -X- _ O
I -X- _ O
) -X- _ O

, -X- _ O
where -X- _ O
µ -X- _ O
dia -X- _ O
and -X- _ O
σ -X- _ O
dia -X- _ O
are -X- _ O
calculated -X- _ O
as -X- _ O
: -X- _ O

µ -X- _ O
dia -X- _ O
= -X- _ O
MLP -X- _ O
dia -X- _ O
φ -X- _ O
( -X- _ O
h -X- _ O
X -X- _ O
; -X- _ O
h -X- _ O
ctx -X- _ O
X -X- _ O
; -X- _ O
h -X- _ O
Y -X- _ O
) -X- _ O
, -X- _ O
σ -X- _ O
dia -X- _ O
= -X- _ O
Softplus -X- _ O
( -X- _ O
MLP -X- _ O
dia -X- _ O
φ -X- _ O
( -X- _ O
h -X- _ O
X -X- _ O
; -X- _ O
h -X- _ O
ctx -X- _ O
X -X- _ O
; -X- _ O
h -X- _ O
Y -X- _ O
) -X- _ O
) -X- _ O
. -X- _ O

Translation -X- _ O
Consistency -X- _ O
. -X- _ O
To -X- _ O
keep -X- _ O
the -X- _ O
lexical -X- _ O
choice -X- _ O
of -X- _ O
translation -X- _ O
consistent -X- _ O
with -X- _ O
those -X- _ O
of -X- _ O
previous -X- _ O
utterances -X- _ O
, -X- _ O
we -X- _ O
encode -X- _ O
the -X- _ O
paired -X- _ O
sourcetarget -X- _ O
utterances -X- _ O
and -X- _ O
then -X- _ O
sample -X- _ O
a -X- _ O
latent -X- _ O
variable -X- _ O
z -X- _ O
tra -X- _ O
∈ -X- _ O
R -X- _ O
dz -X- _ O
. -X- _ O
We -X- _ O
define -X- _ O
its -X- _ O
prior -X- _ O
distribution -X- _ O
as -X- _ O
: -X- _ O
p -X- _ O
θ -X- _ O
( -X- _ O
z -X- _ O
tra -X- _ O
|X -X- _ O
u -X- _ O
, -X- _ O
C -X- _ O
X -X- _ O
, -X- _ O
C -X- _ O
Y -X- _ O
) -X- _ O
∼ -X- _ O
N -X- _ O
( -X- _ O
µ -X- _ O
tra -X- _ O
, -X- _ O
σ -X- _ O
2 -X- _ O
tra -X- _ O
I -X- _ O
) -X- _ O
and -X- _ O
{ -X- _ O
µ -X- _ O
tra -X- _ O
, -X- _ O
σ -X- _ O
tra -X- _ O
} -X- _ O
are -X- _ O
calculated -X- _ O
as -X- _ O
: -X- _ O

µ -X- _ O
tra -X- _ O
= -X- _ O
MLP -X- _ O
tra -X- _ O
θ -X- _ O
( -X- _ O
h -X- _ O
X -X- _ O
; -X- _ O
h -X- _ O
ctx -X- _ O
X -X- _ O
; -X- _ O
h -X- _ O
ctx -X- _ O
Y -X- _ O
) -X- _ O
, -X- _ O
σ -X- _ O
tra -X- _ O
= -X- _ O
Softplus -X- _ O
( -X- _ O
MLP -X- _ O
tra -X- _ O
θ -X- _ O
( -X- _ O
h -X- _ O
X -X- _ O
; -X- _ O
h -X- _ O
ctx -X- _ O
X -X- _ O
; -X- _ O
h -X- _ O
ctx -X- _ O
Y -X- _ O
) -X- _ O
) -X- _ O
. -X- _ O

At -X- _ O
training -X- _ O
, -X- _ O
the -X- _ O
posterior -X- _ O
distribution -X- _ O
conditions -X- _ O
on -X- _ O
all -X- _ O
paired -X- _ O
bilingual -X- _ O
dialogue -X- _ O
utterances -X- _ O
that -X- _ O
contain -X- _ O
implicit -X- _ O
and -X- _ O
aligned -X- _ O
information -X- _ O
, -X- _ O
and -X- _ O
serves -X- _ O
as -X- _ O
learning -X- _ O
of -X- _ O
the -X- _ O
prior -X- _ O
distribution -X- _ O
. -X- _ O
Specifically -X- _ O
, -X- _ O
we -X- _ O
define -X- _ O
the -X- _ O
posterior -X- _ O
distribution -X- _ O
as -X- _ O
: -X- _ O
q -X- _ O
φ -X- _ O
( -X- _ O
z -X- _ O
tra -X- _ O
|X -X- _ O
u -X- _ O
, -X- _ O
C -X- _ O
X -X- _ O
, -X- _ O
C -X- _ O
Y -X- _ O
, -X- _ O
Y -X- _ O
u -X- _ O
) -X- _ O
∼ -X- _ O
N -X- _ O
( -X- _ O
µ -X- _ O
tra -X- _ O
, -X- _ O
σ -X- _ O
2 -X- _ O
tra -X- _ O
I -X- _ O
) -X- _ O
, -X- _ O
where -X- _ O
µ -X- _ O
tra -X- _ O
and -X- _ O
σ -X- _ O
tra -X- _ O
are -X- _ O
calculated -X- _ O
as -X- _ O
: -X- _ O

µ -X- _ O
tra -X- _ O
= -X- _ O
MLP -X- _ O
tra -X- _ O
φ -X- _ O
( -X- _ O
h -X- _ O
X -X- _ O
; -X- _ O
h -X- _ O
ctx -X- _ O
X -X- _ O
; -X- _ O
h -X- _ O
ctx -X- _ O
Y -X- _ O
; -X- _ O
h -X- _ O
Y -X- _ O
) -X- _ O
, -X- _ O
σ -X- _ O
tra -X- _ O
= -X- _ O
Softplus -X- _ O
( -X- _ O
MLP -X- _ O
tra -X- _ O
φ -X- _ O
( -X- _ O
h -X- _ O
X -X- _ O
; -X- _ O
h -X- _ O
ctx -X- _ O
X -X- _ O
; -X- _ O
h -X- _ O
ctx -X- _ O
Y -X- _ O
; -X- _ O
h -X- _ O
Y -X- _ O
) -X- _ O
) -X- _ O
. -X- _ O

Decoder -X- _ O

The -X- _ O
decoder -X- _ O
adopts -X- _ O
a -X- _ O
similar -X- _ O
structure -X- _ O
to -X- _ O
the -X- _ O
encoder -X- _ O
, -X- _ O
and -X- _ O
each -X- _ O
of -X- _ O
N -X- _ B-HyperparameterValue
d -X- _ I-HyperparameterValue
decoder -X- _ O
layers -X- _ O
contains -X- _ O
an -X- _ O
additional -X- _ O
cross -X- _ O
- -X- _ O
attention -X- _ O
sub -X- _ O
- -X- _ O
layer -X- _ O
( -X- _ O
CrossAtt -X- _ O
) -X- _ O
: -X- _ O

s -X- _ O
d -X- _ O
= -X- _ O
SelfAtt -X- _ O
( -X- _ O
h -X- _ O
−1 -X- _ O
d -X- _ O
) -X- _ O
+ -X- _ O
h -X- _ O
−1 -X- _ O
d -X- _ O
, -X- _ O
h -X- _ O
−1 -X- _ O
d -X- _ O
∈ -X- _ O
R -X- _ O
n×d -X- _ O
, -X- _ O
c -X- _ O
d -X- _ O
= -X- _ O
CrossAtt -X- _ O
( -X- _ O
s -X- _ O
d -X- _ O
, -X- _ O
h -X- _ O
Ne -X- _ B-HyperparameterName
e -X- _ O
) -X- _ O
+ -X- _ O
s -X- _ O
d -X- _ O
, -X- _ O
s -X- _ O
d -X- _ O
∈ -X- _ O
R -X- _ O
n×d -X- _ O
, -X- _ O
h -X- _ O
d -X- _ O
= -X- _ O
FFN -X- _ O
( -X- _ O
c -X- _ O
d -X- _ O
) -X- _ O
+ -X- _ O
c -X- _ O
d -X- _ O
, -X- _ O
{ -X- _ O
c -X- _ O
d -X- _ O
, -X- _ O
h -X- _ O
d -X- _ O
} -X- _ O
∈ -X- _ O
R -X- _ O
n×d -X- _ O
, -X- _ O

where -X- _ O
h -X- _ O
d -X- _ O
denotes -X- _ O
the -X- _ O
state -X- _ O
of -X- _ O
the -X- _ O
-th -X- _ O
decoder -X- _ O
layer -X- _ O
. -X- _ O

As -X- _ O
shown -X- _ O
in -X- _ O
Fig -X- _ O
. -X- _ O
3 -X- _ O
, -X- _ O
we -X- _ O
obtain -X- _ O
the -X- _ O
latent -X- _ O
variables -X- _ O
{ -X- _ O
z -X- _ O
role -X- _ O
, -X- _ O
z -X- _ O
dia -X- _ O
, -X- _ O
z -X- _ O
tra -X- _ O
} -X- _ O
either -X- _ O
from -X- _ O
the -X- _ O
posterior -X- _ O
distribution -X- _ O
predicted -X- _ O
by -X- _ O
recognition -X- _ O
networks -X- _ O
( -X- _ O
training -X- _ O
process -X- _ O
as -X- _ O
the -X- _ O
solid -X- _ O
grey -X- _ O
lines -X- _ O
) -X- _ O
or -X- _ O
from -X- _ O
prior -X- _ O
distribution -X- _ O
predicted -X- _ O
by -X- _ O
prior -X- _ O
networks -X- _ O
( -X- _ O
inference -X- _ O
process -X- _ O
as -X- _ O
the -X- _ O
dashed -X- _ O
red -X- _ O
lines -X- _ O
) -X- _ O
. -X- _ O
Finally -X- _ O
, -X- _ O
we -X- _ O
incorporate -X- _ O
{ -X- _ O
z -X- _ O
role -X- _ O
, -X- _ O
z -X- _ O
dia -X- _ O
, -X- _ O
z -X- _ O
tra -X- _ O
} -X- _ O
into -X- _ O
the -X- _ O
state -X- _ O
of -X- _ O
the -X- _ O
top -X- _ O
layer -X- _ O
of -X- _ O
the -X- _ O
decoder -X- _ O
with -X- _ O
a -X- _ O
projection -X- _ O
layer -X- _ O
: -X- _ O

o -X- _ O
t -X- _ O
= -X- _ O
Tanh -X- _ O
( -X- _ O
W -X- _ O
p -X- _ O
[ -X- _ O
h -X- _ O
N -X- _ O
d -X- _ O
d -X- _ O
, -X- _ O
t -X- _ O
; -X- _ O
z -X- _ O
role -X- _ O
; -X- _ O
z -X- _ O
dia -X- _ O
; -X- _ O
z -X- _ O
tra -X- _ O
] -X- _ O
+ -X- _ O
b -X- _ O
p -X- _ O
) -X- _ O
, -X- _ O
o -X- _ O
t -X- _ O
∈ -X- _ O
R -X- _ O
d -X- _ O
, -X- _ O

where -X- _ O
W -X- _ O
p -X- _ O
∈ -X- _ O
R -X- _ O
d× -X- _ O
( -X- _ O
d+3dz -X- _ O
) -X- _ O
and -X- _ O
b -X- _ O
p -X- _ O
∈ -X- _ O
R -X- _ O
d -X- _ O
are -X- _ O
training -X- _ O
parameters -X- _ O
, -X- _ O
h -X- _ O
N -X- _ B-HyperparameterName
d -X- _ I-HyperparameterName
d -X- _ O
, -X- _ O
t -X- _ O
is -X- _ O
the -X- _ O
hidden -X- _ O
state -X- _ O
at -X- _ O
time -X- _ O
- -X- _ O
step -X- _ O
t -X- _ O
of -X- _ O
the -X- _ O
N -X- _ B-HyperparameterName
d -X- _ I-HyperparameterName
-th -X- _ O
decoder -X- _ O
layer -X- _ O
. -X- _ O
Then -X- _ O
, -X- _ O
o -X- _ O
t -X- _ O
is -X- _ O
fed -X- _ O
to -X- _ O
a -X- _ O
linear -X- _ O
transformation -X- _ O
and -X- _ O
softmax -X- _ O
layer -X- _ O
to -X- _ O
predict -X- _ O
the -X- _ O
probability -X- _ O
distribution -X- _ O
of -X- _ O
the -X- _ O
next -X- _ O
target -X- _ O
token -X- _ O
: -X- _ O

p -X- _ O
t -X- _ O
= -X- _ O
Softmax -X- _ O
( -X- _ O
W -X- _ O
o -X- _ O
o -X- _ O
t -X- _ O
+ -X- _ O
b -X- _ O
o -X- _ O
) -X- _ O
, -X- _ O
p -X- _ O
t -X- _ O
∈ -X- _ O
R -X- _ O
|V -X- _ O
| -X- _ O
, -X- _ O

where -X- _ O
W -X- _ O
o -X- _ O
∈ -X- _ O
R -X- _ O
|V -X- _ O
|×d -X- _ O
and -X- _ O
b -X- _ O
o -X- _ O
∈ -X- _ O
R -X- _ O
|V -X- _ O
| -X- _ O
are -X- _ O
training -X- _ O
parameters -X- _ O
. -X- _ O

Training -X- _ O
Objectives -X- _ O

We -X- _ O
apply -X- _ O
a -X- _ O
two -X- _ O
- -X- _ O
stage -X- _ O
training -X- _ O
strategy -X- _ O
Ma -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
. -X- _ O
Firstly -X- _ O
, -X- _ O
we -X- _ O
train -X- _ O
our -X- _ O
model -X- _ O
on -X- _ O
large -X- _ O
- -X- _ O
scale -X- _ O
sentence -X- _ O
- -X- _ O
level -X- _ O
NMT -X- _ B-TaskName
data -X- _ O
to -X- _ O
minimize -X- _ O
the -X- _ O
cross -X- _ B-MetricName
- -X- _ I-MetricName
entropy -X- _ I-MetricName
objective -X- _ O
: -X- _ O

L -X- _ O
( -X- _ O
θ -X- _ O
; -X- _ O
X -X- _ O
, -X- _ O
Y -X- _ O
) -X- _ O
= -X- _ O
− -X- _ O
N -X- _ O
t=1 -X- _ O
logp -X- _ O
θ -X- _ O
( -X- _ O
y -X- _ O
t -X- _ O
|X -X- _ O
, -X- _ O
y -X- _ O
1 -X- _ O
: -X- _ O
t−1 -X- _ O
) -X- _ O
. -X- _ O

Secondly -X- _ O
, -X- _ O
we -X- _ O
fine -X- _ O
- -X- _ O
tune -X- _ O
it -X- _ O
on -X- _ O
the -X- _ O
chat -X- _ O
translation -X- _ O
data -X- _ O
to -X- _ O
maximize -X- _ O
the -X- _ O
following -X- _ O
objective -X- _ O
: -X- _ O

J -X- _ O
( -X- _ O
θ -X- _ O
, -X- _ O
φ -X- _ O
; -X- _ O
X -X- _ O
u -X- _ O
, -X- _ O
C -X- _ O
role -X- _ O
X -X- _ O
, -X- _ O
C -X- _ O
X -X- _ O
, -X- _ O
C -X- _ O
Y -X- _ O
, -X- _ O
Y -X- _ O
u -X- _ O
) -X- _ O
= -X- _ O
− -X- _ O
KL -X- _ O
( -X- _ O
q -X- _ O
φ -X- _ O
( -X- _ O
z -X- _ O
role -X- _ O
|X -X- _ O
u -X- _ O
, -X- _ O
C -X- _ O
role -X- _ O
X -X- _ O
, -X- _ O
Y -X- _ O
u -X- _ O
) -X- _ O
p -X- _ O
θ -X- _ O
( -X- _ O
z -X- _ O
role -X- _ O
|X -X- _ O
u -X- _ O
, -X- _ O
C -X- _ O
role -X- _ O
X -X- _ O
) -X- _ O
) -X- _ O
− -X- _ O
KL -X- _ O
( -X- _ O
q -X- _ O
φ -X- _ O
( -X- _ O
z -X- _ O
dia -X- _ O
|X -X- _ O
u -X- _ O
, -X- _ O
C -X- _ O
X -X- _ O
, -X- _ O
Y -X- _ O
u -X- _ O
) -X- _ O
p -X- _ O
θ -X- _ O
( -X- _ O
z -X- _ O
dia -X- _ O
|X -X- _ O
u -X- _ O
, -X- _ O
C -X- _ O
X -X- _ O
) -X- _ O
) -X- _ O
− -X- _ O
KL -X- _ O
( -X- _ O
q -X- _ O
φ -X- _ O
( -X- _ O
z -X- _ O
tra -X- _ O
|X -X- _ O
u -X- _ O
, -X- _ O
C -X- _ O
X -X- _ O
, -X- _ O
C -X- _ O
Y -X- _ O
, -X- _ O
Y -X- _ O
u -X- _ O
) -X- _ O
p -X- _ O
θ -X- _ O
( -X- _ O
z -X- _ O
tra -X- _ O
|X -X- _ O
u -X- _ O
, -X- _ O
C -X- _ O
X -X- _ O
, -X- _ O
C -X- _ O
Y -X- _ O
) -X- _ O
) -X- _ O
+ -X- _ O
E -X- _ O
q -X- _ O
φ -X- _ O
[ -X- _ O
logp -X- _ O
θ -X- _ O
( -X- _ O
Y -X- _ O
u -X- _ O
|X -X- _ O
u -X- _ O
, -X- _ O
z -X- _ O
role -X- _ O
, -X- _ O
z -X- _ O
dia -X- _ O
, -X- _ O
z -X- _ O
tra -X- _ O
) -X- _ O
] -X- _ O
. -X- _ O

We -X- _ O
use -X- _ O
the -X- _ O
reparameterization -X- _ O
trick -X- _ O
( -X- _ O
Kingma -X- _ O
and -X- _ O
Welling -X- _ O
, -X- _ O
2013 -X- _ O
) -X- _ O
to -X- _ O
estimate -X- _ O
the -X- _ O
gradients -X- _ O
of -X- _ O
the -X- _ O
prior -X- _ O
and -X- _ O
recognition -X- _ O
networks -X- _ O
( -X- _ O
Zhao -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
. -X- _ O

Experiments -X- _ O

Datasets -X- _ O
and -X- _ O
Metrics -X- _ O

Datasets -X- _ O
. -X- _ O
We -X- _ O
apply -X- _ O
a -X- _ O
two -X- _ O
- -X- _ O
stage -X- _ O
training -X- _ O
strategy -X- _ O
, -X- _ O
i.e. -X- _ O
, -X- _ O
firstly -X- _ O
training -X- _ O
on -X- _ O
a -X- _ O
large -X- _ O
- -X- _ O
scale -X- _ O
sentence -X- _ O
- -X- _ O
level -X- _ O
NMT -X- _ B-TaskName
corpus -X- _ O
( -X- _ O
WMT20 -X- _ O
6 -X- _ O
) -X- _ O
and -X- _ O
then -X- _ O
fine -X- _ O
- -X- _ O
tuning -X- _ O
on -X- _ O
chat -X- _ O
translation -X- _ O
corpus -X- _ O
( -X- _ O
BConTrasT -X- _ B-DatasetName
( -X- _ O
Farajian -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
7 -X- _ O
and -X- _ O
BMELD -X- _ B-DatasetName
) -X- _ O
. -X- _ O
The -X- _ O
details -X- _ O
( -X- _ O
WMT20 -X- _ O
data -X- _ O
and -X- _ O
results -X- _ O
of -X- _ O
the -X- _ O
first -X- _ O
stage -X- _ O
) -X- _ O
are -X- _ O
shown -X- _ O
in -X- _ O
Appendix -X- _ O
A -X- _ O
. -X- _ O

BConTrasT. -X- _ B-DatasetName
The -X- _ O
dataset -X- _ O
8 -X- _ O
is -X- _ O
first -X- _ O
provided -X- _ O
by -X- _ O
WMT -X- _ O
2020 -X- _ O
Chat -X- _ B-TaskName
Translation -X- _ I-TaskName
Task -X- _ O
( -X- _ O
Farajian -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
, -X- _ O
which -X- _ O
is -X- _ O
translated -X- _ O
from -X- _ O
English -X- _ O
into -X- _ O
German -X- _ O
and -X- _ O
is -X- _ O
based -X- _ O
on -X- _ O
the -X- _ O
monolingual -X- _ O
Taskmaster-1 -X- _ O
corpus -X- _ O
( -X- _ O
Byrne -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
. -X- _ O
The -X- _ O
conversations -X- _ O
( -X- _ O
originally -X- _ O
in -X- _ O
English -X- _ O
) -X- _ O
were -X- _ O
first -X- _ O
automatically -X- _ O
translated -X- _ O
into -X- _ O
German -X- _ O
and -X- _ O
then -X- _ O
manually -X- _ O
postedited -X- _ O
by -X- _ O
Unbabel -X- _ O
editors -X- _ O
, -X- _ O
9 -X- _ O
who -X- _ O
are -X- _ O
native -X- _ O
German -X- _ O
speakers -X- _ O
. -X- _ O
Having -X- _ O
the -X- _ O
conversations -X- _ O
in -X- _ O
both -X- _ O
languages -X- _ O
allows -X- _ O
us -X- _ O
to -X- _ O
simulate -X- _ O
bilingual -X- _ O
conversations -X- _ O
in -X- _ O
which -X- _ O
one -X- _ O
speaker -X- _ O
, -X- _ O
the -X- _ O
customer -X- _ O
, -X- _ O
speaks -X- _ O
in -X- _ O
German -X- _ O
and -X- _ O
the -X- _ O
other -X- _ O
speaker -X- _ O
, -X- _ O
the -X- _ O
agent -X- _ O
, -X- _ O
answers -X- _ O
in -X- _ O
English -X- _ O
. -X- _ O

BMELD -X- _ B-DatasetName
. -X- _ O
Similarly -X- _ O
, -X- _ O
based -X- _ O
on -X- _ O
the -X- _ O
dialogue -X- _ O
dataset -X- _ O
in -X- _ O
the -X- _ O
MELD -X- _ B-DatasetName
( -X- _ O
originally -X- _ O
in -X- _ O
English -X- _ O
) -X- _ O
( -X- _ O
Poria -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
, -X- _ O
10 -X- _ O
we -X- _ O
firstly -X- _ O
crawled -X- _ O
the -X- _ O
corresponding -X- _ O
Chinese -X- _ O
translations -X- _ O
from -X- _ O
this -X- _ O
11 -X- _ O
and -X- _ O
then -X- _ O
manually -X- _ O
post -X- _ O
- -X- _ O
edited -X- _ O
them -X- _ O
according -X- _ O
to -X- _ O
the -X- _ O
dialogue -X- _ O
history -X- _ O
by -X- _ O
native -X- _ O
Chinese -X- _ O
speakers -X- _ O
, -X- _ O
who -X- _ O
are -X- _ O
postgraduate -X- _ O
students -X- _ O
majoring -X- _ O
in -X- _ O
English -X- _ O
. -X- _ O
Finally -X- _ O
, -X- _ O
following -X- _ O
( -X- _ O
Farajian -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
, -X- _ O
we -X- _ O
assume -X- _ O
50 -X- _ O
% -X- _ O
speakers -X- _ O
as -X- _ O
Chinese -X- _ O
speakers -X- _ O
to -X- _ O
keep -X- _ O
data -X- _ O
balance -X- _ O
for -X- _ O
Ch⇒En -X- _ O
translations -X- _ O
and -X- _ O
build -X- _ O
the -X- _ O
bilingual -X- _ B-DatasetName
MELD -X- _ I-DatasetName
( -X- _ O
BMELD -X- _ B-DatasetName
) -X- _ O
. -X- _ O
For -X- _ O
the -X- _ O
Chinese -X- _ O
, -X- _ O
we -X- _ O
segment -X- _ O
the -X- _ O
sentence -X- _ O
using -X- _ O
Stanford -X- _ O
CoreNLP -X- _ O
toolkit -X- _ O
12 -X- _ O
. -X- _ O

Metrics -X- _ O
. -X- _ O
For -X- _ O
fair -X- _ O
comparison -X- _ O
, -X- _ O
we -X- _ O
use -X- _ O
the -X- _ O
Sacre -X- _ B-MetricName
- -X- _ I-MetricName
BLEU -X- _ I-MetricName
13 -X- _ O
( -X- _ O
Post -X- _ O
, -X- _ O
2018 -X- _ O
) -X- _ O
and -X- _ O
v0.7.25 -X- _ O
for -X- _ O
TER -X- _ B-MetricName
( -X- _ O
Snover -X- _ O
6 -X- _ O
http -X- _ O
: -X- _ O
/ -X- _ O
/ -X- _ O
www.statmt.org -X- _ O
/ -X- _ O
wmt20 -X- _ O
/ -X- _ O
translation-task.html -X- _ O
7 -X- _ O
http -X- _ O
: -X- _ O
/ -X- _ O
/ -X- _ O
www.statmt.org -X- _ O
/ -X- _ O
wmt20 -X- _ O
/ -X- _ O
chat-task.html -X- _ O
8 -X- _ O
https -X- _ O
: -X- _ O
/ -X- _ O
/ -X- _ O
github.com -X- _ O
/ -X- _ O
Unbabel -X- _ O
/ -X- _ O
BConTrasT -X- _ B-DatasetName
9 -X- _ O
www.unbabel.com -X- _ O
10 -X- _ O
The -X- _ O
MELD -X- _ B-DatasetName
is -X- _ O
a -X- _ O
multimodal -X- _ O
emotionLines -X- _ O
dialogue -X- _ O
dataset -X- _ O
, -X- _ O
each -X- _ O
utterance -X- _ O
of -X- _ O
which -X- _ O
corresponds -X- _ O
to -X- _ O
a -X- _ O
video -X- _ O
, -X- _ O
voice -X- _ O
, -X- _ O
and -X- _ O
text -X- _ O
, -X- _ O
and -X- _ O
is -X- _ O
annotated -X- _ O
with -X- _ O
detailed -X- _ O
emotion -X- _ O
and -X- _ O
sentiment -X- _ O
. -X- _ O
11 -X- _ O
( -X- _ O
Koehn -X- _ O
, -X- _ O
2004 -X- _ O
) -X- _ O
. -X- _ O
For -X- _ O
En⇔De -X- _ O
, -X- _ O
we -X- _ O
report -X- _ O
case -X- _ O
- -X- _ O
sensitive -X- _ O
score -X- _ O
following -X- _ O
the -X- _ O
WMT20 -X- _ O
chat -X- _ O
task -X- _ O
( -X- _ O
Farajian -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
. -X- _ O

For -X- _ O
Ch⇒En -X- _ O
, -X- _ O
we -X- _ O
report -X- _ O
case -X- _ O
- -X- _ O
insensitive -X- _ O
score -X- _ O
. -X- _ O
For -X- _ O
En⇒Ch -X- _ O
, -X- _ O
we -X- _ O
report -X- _ O
the -X- _ O
character -X- _ O
- -X- _ O
level -X- _ O
BLEU -X- _ B-MetricName
score -X- _ I-MetricName
. -X- _ O

Implementation -X- _ O
Details -X- _ O

For -X- _ O
all -X- _ O
experiments -X- _ O
, -X- _ O
we -X- _ O
follow -X- _ O
the -X- _ O
Transformer -X- _ B-MethodName
- -X- _ I-MethodName
Base -X- _ I-MethodName
and -X- _ O
Transformer -X- _ B-MethodName
- -X- _ I-MethodName
Big -X- _ I-MethodName
settings -X- _ O
illustrated -X- _ O
in -X- _ O
( -X- _ O
Vaswani -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
. -X- _ O
In -X- _ O
Transformer -X- _ B-MethodName
- -X- _ I-MethodName
Base -X- _ I-MethodName
, -X- _ O
we -X- _ O
use -X- _ O
512 -X- _ B-HyperparameterValue
as -X- _ O
hidden -X- _ B-HyperparameterName
size -X- _ I-HyperparameterName
( -X- _ O
i.e. -X- _ O
, -X- _ O
d -X- _ B-HyperparameterName
) -X- _ O
, -X- _ O
2048 -X- _ B-HyperparameterValue
as -X- _ O
filter -X- _ B-HyperparameterName
size -X- _ I-HyperparameterName
and -X- _ O
8 -X- _ B-HyperparameterValue
heads -X- _ B-HyperparameterName
in -X- _ O
multi -X- _ O
- -X- _ O
head -X- _ O
attention -X- _ O
. -X- _ O
In -X- _ O
Transformer -X- _ B-MethodName
- -X- _ I-MethodName
Big -X- _ I-MethodName
, -X- _ O
we -X- _ O
use -X- _ O
1024 -X- _ B-HyperparameterValue
as -X- _ O
hidden -X- _ B-HyperparameterName
size -X- _ I-HyperparameterName
, -X- _ O
4096 -X- _ B-HyperparameterValue
as -X- _ O
filter -X- _ B-HyperparameterName
size -X- _ I-HyperparameterName
, -X- _ O
and -X- _ O
16 -X- _ B-HyperparameterValue
heads -X- _ B-HyperparameterName
in -X- _ O
multi -X- _ O
- -X- _ O
head -X- _ O
attention -X- _ O
. -X- _ O

All -X- _ O
our -X- _ O
Transformer -X- _ O
models -X- _ O
contain -X- _ O
N -X- _ O
e -X- _ O
= -X- _ O
6 -X- _ O
encoder -X- _ O
layers -X- _ O
and -X- _ O
N -X- _ O
d -X- _ O
= -X- _ O
6 -X- _ O
decoder -X- _ O
layers -X- _ O
and -X- _ O
all -X- _ O
models -X- _ O
are -X- _ O
trained -X- _ O
using -X- _ O
THUMT -X- _ O
( -X- _ O
Tan -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
framework -X- _ O
. -X- _ O
We -X- _ O
conduct -X- _ O
experiments -X- _ O
on -X- _ O
the -X- _ O
validation -X- _ O
set -X- _ O
of -X- _ O
En⇒De -X- _ O
to -X- _ O
select -X- _ O
the -X- _ O
hyperparameters -X- _ O
of -X- _ O
context -X- _ O
length -X- _ O
and -X- _ O
latent -X- _ O
dimension -X- _ O
, -X- _ O
which -X- _ O
are -X- _ O
then -X- _ O
shared -X- _ O
for -X- _ O
all -X- _ O
tasks -X- _ O
. -X- _ O
For -X- _ O
the -X- _ O
results -X- _ O
and -X- _ O
more -X- _ O
details -X- _ O
( -X- _ O
other -X- _ O
hyperparameters -X- _ O
setting -X- _ O
and -X- _ O
average -X- _ O
running -X- _ O
time -X- _ O
) -X- _ O
, -X- _ O
please -X- _ O
refer -X- _ O
to -X- _ O
Appendix -X- _ O
B -X- _ O
, -X- _ O
C -X- _ O
, -X- _ O
and -X- _ O
D -X- _ O
. -X- _ O

Comparison -X- _ O
Models -X- _ O

Baseline -X- _ O
NMT -X- _ B-TaskName
Models -X- _ O
. -X- _ O
Transformer -X- _ B-MethodName
( -X- _ O
Vaswani -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
: -X- _ O
the -X- _ O
de -X- _ O
- -X- _ O
facto -X- _ O
NMT -X- _ B-TaskName
model -X- _ O
that -X- _ O
does -X- _ O
not -X- _ O
fine -X- _ O
- -X- _ O
tune -X- _ O
on -X- _ O
chat -X- _ O
translation -X- _ O
data -X- _ O
. -X- _ O
Transformer+FT -X- _ B-MethodName
: -X- _ O
fine -X- _ O
- -X- _ O
tuning -X- _ O
on -X- _ O
the -X- _ O
chat -X- _ O
translation -X- _ O
data -X- _ O
after -X- _ O
being -X- _ O
pre -X- _ O
- -X- _ O
trained -X- _ O
on -X- _ O
sentence -X- _ O
- -X- _ O
level -X- _ O
NMT -X- _ B-TaskName
corpus -X- _ O
. -X- _ O

Main -X- _ O
Results -X- _ O

Overall -X- _ O
, -X- _ O
we -X- _ O
separate -X- _ O
the -X- _ O
models -X- _ O
into -X- _ O
two -X- _ O
parts -X- _ O
in -X- _ O
Tab -X- _ O
. -X- _ O
2 -X- _ O
: -X- _ O
the -X- _ O
Base -X- _ O
setting -X- _ O
and -X- _ O
the -X- _ O
Big -X- _ O
setting -X- _ O
. -X- _ O
In -X- _ O
each -X- _ O
part -X- _ O
, -X- _ O
we -X- _ O
show -X- _ O
the -X- _ O
results -X- _ O
of -X- _ O
our -X- _ O
re -X- _ O
- -X- _ O
implemented -X- _ O
Transformer -X- _ B-MethodName
baselines -X- _ O
, -X- _ O
the -X- _ O
context -X- _ O
- -X- _ O
aware -X- _ O
NMT -X- _ B-TaskName
systems -X- _ O
, -X- _ O
and -X- _ O
our -X- _ O
approach -X- _ O
on -X- _ O
En⇔De -X- _ O
and -X- _ O
En⇔Ch -X- _ O
. -X- _ O

Results -X- _ O
on -X- _ O
En⇔De -X- _ O
. -X- _ O
Under -X- _ O
the -X- _ O
Base -X- _ O
setting -X- _ O
, -X- _ O
CPCC -X- _ B-MethodName
substantially -X- _ O
outperforms -X- _ O
the -X- _ O
baselines -X- _ O
( -X- _ O
e.g. -X- _ O
, -X- _ O
" -X- _ O
Transformer+FT -X- _ B-MethodName
" -X- _ O
) -X- _ O
by -X- _ O
a -X- _ O
large -X- _ O
margin -X- _ O
with -X- _ O
1.70↑ -X- _ B-MetricValue
and -X- _ O
1.48↑ -X- _ B-MetricValue
BLEU -X- _ B-MetricName
scores -X- _ I-MetricName
on -X- _ O
En⇒De -X- _ O
and -X- _ O
De⇒En -X- _ O
, -X- _ O
respectively -X- _ O
. -X- _ O
On -X- _ O
the -X- _ O
TER -X- _ B-MetricName
, -X- _ O
our -X- _ O
CPCC -X- _ B-MethodName
achieves -X- _ O
a -X- _ O
significant -X- _ O
improvement -X- _ O
of -X- _ O
1.3 -X- _ B-MetricValue
points -X- _ I-MetricValue
in -X- _ O
both -X- _ O
language -X- _ O
pairs -X- _ O
. -X- _ O
Under -X- _ O
the -X- _ O
Big -X- _ O
setting -X- _ O
, -X- _ O
our -X- _ O
CPCC -X- _ B-MethodName
also -X- _ O
consistently -X- _ O
boosts -X- _ O
the -X- _ O
performance -X- _ O
in -X- _ O
both -X- _ O
direc -X- _ O
- -X- _ O
tions -X- _ O
( -X- _ O
i.e. -X- _ O
, -X- _ O
1.22↑ -X- _ B-MetricValue
and -X- _ O
1.47↑ -X- _ B-MetricValue
BLEU -X- _ B-MetricName
scores -X- _ I-MetricName
, -X- _ O
0.4↓ -X- _ B-MetricValue
and -X- _ O
1.1↓ -X- _ B-MetricValue
TER -X- _ B-MetricName
scores -X- _ I-MetricName
) -X- _ O
, -X- _ O
showing -X- _ O
its -X- _ O
effectiveness -X- _ O
. -X- _ O

Compared -X- _ O
against -X- _ O
the -X- _ O
strong -X- _ O
context -X- _ O
- -X- _ O
aware -X- _ O
NMT -X- _ B-TaskName
systems -X- _ O
( -X- _ O
underlined -X- _ O
results -X- _ O
) -X- _ O
, -X- _ O
our -X- _ O
CPCC -X- _ B-MethodName
significantly -X- _ O
surpasses -X- _ O
them -X- _ O
( -X- _ O
about -X- _ O
1.39∼1.59↑ -X- _ B-MetricValue
BLEU -X- _ B-MetricName
scores -X- _ I-MetricName
and -X- _ O
0.6∼0.9↓ -X- _ B-MetricValue
TER -X- _ B-MetricName
scores -X- _ O
) -X- _ O
in -X- _ O
both -X- _ O
language -X- _ O
directions -X- _ O
under -X- _ O
both -X- _ O
Base -X- _ O
and -X- _ O
Big -X- _ O
settings -X- _ O
, -X- _ O
demonstrating -X- _ O
the -X- _ O
superiority -X- _ O
of -X- _ O
our -X- _ O
model -X- _ O
. -X- _ O

Results -X- _ O
on -X- _ O
En⇔Ch -X- _ O
. -X- _ O

We -X- _ O
also -X- _ O
conduct -X- _ O
experiments -X- _ O
on -X- _ O
our -X- _ O
self -X- _ O
- -X- _ O
collected -X- _ O
data -X- _ O
to -X- _ O
validate -X- _ O
the -X- _ O
generalizability -X- _ O
across -X- _ O
languages -X- _ O
in -X- _ O
Tab -X- _ O
. -X- _ O
2 -X- _ O
. -X- _ O

Our -X- _ O
CPCC -X- _ B-MethodName
presents -X- _ O
remarkable -X- _ O
BLEU -X- _ B-MetricName
improvements -X- _ O
over -X- _ O
the -X- _ O
" -X- _ O
Transformer+FT -X- _ B-MethodName
" -X- _ O
by -X- _ O
a -X- _ O
large -X- _ O
margin -X- _ O
in -X- _ O
two -X- _ O
directions -X- _ O
by -X- _ O
2.33↑ -X- _ B-MetricValue
and -X- _ O
0.91↑ -X- _ B-MetricValue
BLEU -X- _ B-MethodName
gains -X- _ O
under -X- _ O
the -X- _ O
Base -X- _ O
setting -X- _ O
, -X- _ O
respectively -X- _ O
, -X- _ O
and -X- _ O
by -X- _ O
2.03↑ -X- _ B-MetricValue
and -X- _ O
0.83↑ -X- _ B-MetricValue
BLEU -X- _ B-MetricValue
gains -X- _ O
in -X- _ O
both -X- _ O
directions -X- _ O
under -X- _ O
the -X- _ O
Big -X- _ O
setting -X- _ O
. -X- _ O
These -X- _ O
results -X- _ O
suggest -X- _ O
that -X- _ O
CPCC -X- _ B-MethodName
consistently -X- _ O
performs -X- _ O
well -X- _ O
across -X- _ O
languages -X- _ O
. -X- _ O

Compared -X- _ O
with -X- _ O
strong -X- _ O
context -X- _ O
- -X- _ O
aware -X- _ O
NMT -X- _ B-TaskName
systems -X- _ O
( -X- _ O
e.g. -X- _ O
, -X- _ O
" -X- _ O
V -X- _ O
- -X- _ O
Transformer+FT -X- _ B-MethodName
" -X- _ O
) -X- _ O
, -X- _ O
our -X- _ O
approach -X- _ O
notably -X- _ O
surpasses -X- _ O
them -X- _ O
in -X- _ O
both -X- _ O
language -X- _ O
directions -X- _ O
under -X- _ O
both -X- _ O
Base -X- _ O
and -X- _ O
Big -X- _ O
settings -X- _ O
, -X- _ O
which -X- _ O
shows -X- _ O
the -X- _ O
generalizability -X- _ O
and -X- _ O
superiority -X- _ O
of -X- _ O
our -X- _ O
model -X- _ O
. -X- _ O

Analysis -X- _ O

Ablation -X- _ O
Study -X- _ O

We -X- _ O
conduct -X- _ O
ablation -X- _ O
studies -X- _ O
to -X- _ O
investigate -X- _ O
how -X- _ O
well -X- _ O
each -X- _ O
tailored -X- _ O
latent -X- _ O
variable -X- _ O
of -X- _ O
our -X- _ O
model -X- _ O
works -X- _ O
. -X- _ O
When -X- _ O
removing -X- _ O
latent -X- _ O
variables -X- _ O
listed -X- _ O
in -X- _ O
Tab -X- _ O
. -X- _ O
3 -X- _ O
, -X- _ O
we -X- _ O
have -X- _ O
the -X- _ O
following -X- _ O
findings -X- _ O
. -X- _ O

( -X- _ O
1 -X- _ O
) -X- _ O
All -X- _ O
latent -X- _ O
variables -X- _ O
make -X- _ O
substantial -X- _ O
contributions -X- _ O
to -X- _ O
performance -X- _ O
, -X- _ O
proving -X- _ O
the -X- _ O
importance -X- _ O
of -X- _ O
modeling -X- _ O
role -X- _ O
preference -X- _ O
, -X- _ O
dialogue -X- _ O
coherence -X- _ O
, -X- _ O
and -X- _ O
translation -X- _ O
consistency -X- _ O
, -X- _ O
which -X- _ O
is -X- _ O
consistent -X- _ O
with -X- _ O
our -X- _ O
intuition -X- _ O
that -X- _ O
the -X- _ O
properties -X- _ O
should -X- _ O
be -X- _ O
beneficial -X- _ O
to -X- _ O
better -X- _ O
translations -X- _ O
( -X- _ O
rows -X- _ O
1∼3 -X- _ O
vs. -X- _ O
row -X- _ O
0 -X- _ O
) -X- _ O
. -X- _ O

( -X- _ O
2 -X- _ O
) -X- _ O
Results -X- _ O
of -X- _ O
rows -X- _ O
4∼7 -X- _ O
show -X- _ O
the -X- _ O
combination -X- _ O
effect -X- _ O
of -X- _ O
three -X- _ O
latent -X- _ O
variables -X- _ O
, -X- _ O
suggesting -X- _ O
that -X- _ O
the -X- _ O
combination -X- _ O
among -X- _ O
three -X- _ O
latent -X- _ O
variables -X- _ O
has -X- _ O
a -X- _ O
cumulative -X- _ O
effect -X- _ O
( -X- _ O
rows -X- _ O
4∼7 -X- _ O
vs. -X- _ O
rows -X- _ O
0∼3 -X- _ O
) -X- _ O
. -X- _ O

( -X- _ O
3 -X- _ O
) -X- _ O
Row -X- _ O
7 -X- _ O
vs. -X- _ O
row -X- _ O
0 -X- _ O
shows -X- _ O
that -X- _ O
explicitly -X- _ O
modeling -X- _ O
the -X- _ O
bilingual -X- _ O
conversational -X- _ O
characteristics -X- _ O
significantly -X- _ O
outperforms -X- _ O
implicit -X- _ O
modeling -X- _ O
( -X- _ O
i.e. -X- _ O
, -X- _ O
just -X- _ O
incorporating -X- _ O
the -X- _ O
dialogue -X- _ O
history -X- _ O
into -X- _ O
encoders -X- _ O
) -X- _ O
, -X- _ O
which -X- _ O
lacks -X- _ O
the -X- _ O
relevant -X- _ O
information -X- _ O
guidance -X- _ O
. -X- _ O

Dialogue -X- _ O
Coherence -X- _ O

Following -X- _ O
( -X- _ O
Lapata -X- _ O
and -X- _ O
Barzilay -X- _ O
, -X- _ O
2005 -X- _ O
; -X- _ O
Xiong -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
, -X- _ O
we -X- _ O
measure -X- _ O
dialogue -X- _ O
coherence -X- _ O
as -X- _ O
sentence -X- _ O
similarity -X- _ O
. -X- _ O
Specifically -X- _ O
, -X- _ O
the -X- _ O
representation -X- _ O
of -X- _ O
each -X- _ O
sentence -X- _ O
is -X- _ O
the -X- _ O
mean -X- _ O
of -X- _ O
the -X- _ O
distributed -X- _ O
vectors -X- _ O
of -X- _ O
its -X- _ O
words -X- _ O
, -X- _ O
and -X- _ O
the -X- _ O
dialogue -X- _ O
coherence -X- _ O
between -X- _ O
two -X- _ O
sentences -X- _ O
s -X- _ O
1 -X- _ O
and -X- _ O
s -X- _ O
2 -X- _ O
is -X- _ O
determined -X- _ O
by -X- _ O
the -X- _ O
cosine -X- _ O
similarity -X- _ O
: -X- _ O

sim -X- _ O
( -X- _ O
s -X- _ O
1 -X- _ O
, -X- _ O
s -X- _ O
2 -X- _ O
) -X- _ O
= -X- _ O
cos -X- _ O
( -X- _ O
f -X- _ O
( -X- _ O
s -X- _ O
1 -X- _ O
) -X- _ O
, -X- _ O
f -X- _ O
( -X- _ O
s -X- _ O
2 -X- _ O
) -X- _ O
) -X- _ O
, -X- _ O
f -X- _ O
( -X- _ O
s -X- _ O
i -X- _ O
) -X- _ O
= -X- _ O
1 -X- _ O
|s -X- _ O
i -X- _ O
| -X- _ O
w∈s -X- _ O
i -X- _ O
( -X- _ O
w -X- _ O
) -X- _ O
, -X- _ O

where -X- _ O
w -X- _ O
is -X- _ O
the -X- _ O
vector -X- _ O
for -X- _ O
word -X- _ O
w -X- _ O
. -X- _ O

We -X- _ O
use -X- _ O
Word2Vec -X- _ O
14 -X- _ O
( -X- _ O
Mikolov -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2013 -X- _ O
) -X- _ O
to -X- _ O
learn -X- _ O
the -X- _ O
distributed -X- _ O
vectors -X- _ O
of -X- _ O
words -X- _ O
by -X- _ O
training -X- _ O
on -X- _ O
the -X- _ O
monolingual -X- _ O
dialogue -X- _ O
dataset -X- _ O
: -X- _ O
Taskmaster-1 -X- _ O
( -X- _ O
Byrne -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
. -X- _ O
And -X- _ O
we -X- _ O
set -X- _ O
the -X- _ O
dimensionality -X- _ O
of -X- _ O
word -X- _ O
embeddings -X- _ O
to -X- _ O
100 -X- _ B-HyperparameterValue
. -X- _ O

Tab -X- _ O
. -X- _ O
4 -X- _ O
shows -X- _ O
the -X- _ O
cosine -X- _ B-MetricName
similarity -X- _ I-MetricName
on -X- _ O
the -X- _ O
test -X- _ O
set -X- _ O
of -X- _ O
De⇒En -X- _ O
. -X- _ O
It -X- _ O
reveals -X- _ O
that -X- _ O
our -X- _ O
model -X- _ O
encouraged -X- _ O
by -X- _ O
tailor -X- _ O
- -X- _ O
made -X- _ O
latent -X- _ O
variables -X- _ O
produces -X- _ O
better -X- _ O
coherence -X- _ O
in -X- _ O
chat -X- _ O
translation -X- _ O
than -X- _ O
contrast -X- _ O
systems -X- _ O
. -X- _ O

Human -X- _ O
Evaluation -X- _ O

Inspired -X- _ O
by -X- _ O
( -X- _ O
Bao -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020 -X- _ O
; -X- _ O
Farajian -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
, -X- _ O
we -X- _ O
use -X- _ O
four -X- _ O
criteria -X- _ O
for -X- _ O
human -X- _ O
evaluation -X- _ O
: -X- _ O
( -X- _ O
1 -X- _ O
) -X- _ O
Preference -X- _ B-MetricName
measures -X- _ O
whether -X- _ O
the -X- _ O
translation -X- _ O
preserves -X- _ O
the -X- _ O
role -X- _ O
preference -X- _ O
information -X- _ O
; -X- _ O
( -X- _ O
2 -X- _ O
) -X- _ O
Coherence -X- _ B-MetricName
denotes -X- _ O
whether -X- _ O
the -X- _ O
translation -X- _ O
is -X- _ O
semantically -X- _ O
coherent -X- _ O
with -X- _ O
the -X- _ O
dialogue -X- _ O
history -X- _ O
; -X- _ O
( -X- _ O
3 -X- _ O
) -X- _ O
Consistency -X- _ B-MetricName
measures -X- _ O
whether -X- _ O
the -X- _ O
lexical -X- _ O
choice -X- _ O
of -X- _ O
translation -X- _ O
is -X- _ O
consistent -X- _ O
with -X- _ O
the -X- _ O
preceding -X- _ O
utterances -X- _ O
; -X- _ O
( -X- _ O
4 -X- _ O
) -X- _ O
Fluency -X- _ B-MetricName
measures -X- _ O
whether -X- _ O
the -X- _ O
translation -X- _ O
is -X- _ O
logically -X- _ O
reasonable -X- _ O
and -X- _ O
grammatically -X- _ O
correct -X- _ O
. -X- _ O
Table -X- _ O
4 -X- _ O
: -X- _ O
Results -X- _ O
of -X- _ O
dialogue -X- _ O
coherence -X- _ O
in -X- _ O
terms -X- _ O
of -X- _ O
sentence -X- _ O
similarity -X- _ O
( -X- _ O
De⇒En -X- _ O
, -X- _ O
Base -X- _ O
) -X- _ O
. -X- _ O
The -X- _ O
" -X- _ O
# -X- _ O
-th -X- _ O
Pr -X- _ O
. -X- _ O
" -X- _ O
denotes -X- _ O
the -X- _ O
# -X- _ O
-th -X- _ O
preceding -X- _ O
utterance -X- _ O
to -X- _ O
the -X- _ O
current -X- _ O
one -X- _ O
. -X- _ O
" -X- _ O
† -X- _ O
† -X- _ O
" -X- _ O
indicates -X- _ O
that -X- _ O
statistically -X- _ O
significant -X- _ O
better -X- _ O
than -X- _ O
the -X- _ O
best -X- _ O
result -X- _ O
of -X- _ O
all -X- _ O
contrast -X- _ O
NMT -X- _ B-TaskName
models -X- _ O
( -X- _ O
p -X- _ O
< -X- _ O
0.01 -X- _ O
) -X- _ O
. -X- _ O
We -X- _ O
firstly -X- _ O
randomly -X- _ O
sample -X- _ O
200 -X- _ O
examples -X- _ O
from -X- _ O
the -X- _ O
test -X- _ O
set -X- _ O
of -X- _ O
Ch⇒En -X- _ O
. -X- _ O
Then -X- _ O
, -X- _ O
we -X- _ O
assign -X- _ O
each -X- _ O
bilingual -X- _ O
dialogue -X- _ O
history -X- _ O
and -X- _ O
corresponding -X- _ O
6 -X- _ O
generated -X- _ O
translations -X- _ O
to -X- _ O
three -X- _ O
human -X- _ O
annotators -X- _ O
without -X- _ O
order -X- _ O
, -X- _ O
and -X- _ O
ask -X- _ O
them -X- _ O
to -X- _ O
evaluate -X- _ O
whether -X- _ O
each -X- _ O
translation -X- _ O
meets -X- _ O
the -X- _ O
criteria -X- _ O
defined -X- _ O
above -X- _ O
. -X- _ O
All -X- _ O
annotators -X- _ O
are -X- _ O
postgraduate -X- _ O
students -X- _ O
and -X- _ O
not -X- _ O
involved -X- _ O
in -X- _ O
other -X- _ O
parts -X- _ O
of -X- _ O
our -X- _ O
experiments -X- _ O
. -X- _ O

Models -X- _ O

Tab -X- _ O
. -X- _ O
5 -X- _ O
shows -X- _ O
that -X- _ O
our -X- _ O
CPCC -X- _ B-MethodName
effectively -X- _ O
alleviates -X- _ O
the -X- _ O
problem -X- _ O
of -X- _ O
role -X- _ O
- -X- _ O
irrelevant -X- _ O
, -X- _ O
incoherent -X- _ O
and -X- _ O
inconsistent -X- _ O
translations -X- _ O
compared -X- _ O
with -X- _ O
other -X- _ O
models -X- _ O
( -X- _ O
significance -X- _ O
test -X- _ O
( -X- _ O
Koehn -X- _ O
, -X- _ O
2004 -X- _ O
) -X- _ O
, -X- _ O
p -X- _ O
< -X- _ O
0.05 -X- _ O
) -X- _ O
, -X- _ O
indicating -X- _ O
the -X- _ O
superiority -X- _ O
of -X- _ O
our -X- _ O
model -X- _ O
. -X- _ O
The -X- _ O
interannotator -X- _ B-MetricName
agreement -X- _ I-MetricName
is -X- _ O
0.527 -X- _ B-MetricValue
, -X- _ O
0.491 -X- _ B-MetricValue
, -X- _ O
0.556 -X- _ B-MetricValue
and -X- _ O
0.485 -X- _ B-MetricValue
calculated -X- _ O
by -X- _ O
the -X- _ O
Fleiss -X- _ B-MetricName
' -X- _ I-MetricName
kappa -X- _ I-MetricName
( -X- _ O
Fleiss -X- _ O
and -X- _ O
Cohen -X- _ O
, -X- _ O
1973 -X- _ O
) -X- _ O
, -X- _ O
for -X- _ O
preference -X- _ B-MetricName
, -X- _ O
coherence -X- _ B-MetricName
, -X- _ O
consistency -X- _ B-MetricName
and -X- _ O
fluency -X- _ B-MetricName
, -X- _ O
respectively -X- _ O
, -X- _ O
indicating -X- _ O
" -X- _ O
Moderate -X- _ O
Agreement -X- _ O
" -X- _ O
for -X- _ O
all -X- _ O
four -X- _ O
criteria -X- _ O
. -X- _ O
We -X- _ O
also -X- _ O
present -X- _ O
some -X- _ O
case -X- _ O
studies -X- _ O
in -X- _ O
Appendix -X- _ O
H -X- _ O
. -X- _ O

Related -X- _ O
Work -X- _ O

Chat -X- _ O
NMT -X- _ B-TaskName
. -X- _ O
It -X- _ O
only -X- _ O
involves -X- _ O
several -X- _ O
researches -X- _ O
due -X- _ O
to -X- _ O
the -X- _ O
lack -X- _ O
of -X- _ O
human -X- _ O
- -X- _ O
annotated -X- _ O
publicly -X- _ O
available -X- _ O
data -X- _ O
( -X- _ O
Farajian -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
. -X- _ O
Therefore -X- _ O
, -X- _ O
some -X- _ O
existing -X- _ O
work -X- _ O
( -X- _ O
Wang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2016 -X- _ O
; -X- _ O
Zhang -X- _ O
and -X- _ O
Zhou -X- _ O
, -X- _ O
2019 -X- _ O
; -X- _ O
Rikters -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
mainly -X- _ O
pays -X- _ O
attention -X- _ O
to -X- _ O
designing -X- _ O
methods -X- _ O
to -X- _ O
automatically -X- _ O
construct -X- _ O
the -X- _ O
subtitles -X- _ O
corpus -X- _ O
, -X- _ O
which -X- _ O
may -X- _ O
contain -X- _ O
noisy -X- _ O
bilingual -X- _ O
utterances -X- _ O
. -X- _ O
Recently -X- _ O
, -X- _ O
Farajian -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2020 -X- _ O
) -X- _ O
organize -X- _ O
the -X- _ O
WMT20 -X- _ O
chat -X- _ O
translation -X- _ O
task -X- _ O
and -X- _ O
first -X- _ O
provide -X- _ O
a -X- _ O
human -X- _ O
postedited -X- _ O
corpus -X- _ O
, -X- _ O
where -X- _ O
some -X- _ O
teams -X- _ O
investigate -X- _ O
the -X- _ O
effect -X- _ O
of -X- _ O
dialogue -X- _ O
history -X- _ O
and -X- _ O
finally -X- _ O
ensemble -X- _ O
their -X- _ O
models -X- _ O
for -X- _ O
higher -X- _ O
ranks -X- _ O
( -X- _ O
Berard -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020 -X- _ O
; -X- _ O
Mohammed -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020 -X- _ O
; -X- _ O
Bao -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020 -X- _ O
; -X- _ O
Moghe -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
. -X- _ O
As -X- _ O
a -X- _ O
synchronizing -X- _ O
study -X- _ O
, -X- _ O
Wang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2021 -X- _ O
) -X- _ O
use -X- _ O
multitask -X- _ O
learning -X- _ O
to -X- _ O
auto -X- _ O
- -X- _ O
correct -X- _ O
the -X- _ O
translation -X- _ O
error -X- _ O
, -X- _ O
such -X- _ O
as -X- _ O
pronoun -X- _ O
dropping -X- _ O
, -X- _ O
punctuation -X- _ O
dropping -X- _ O
, -X- _ O
and -X- _ O
typos -X- _ O
. -X- _ O
Unlike -X- _ O
them -X- _ O
, -X- _ O
we -X- _ O
focus -X- _ O
on -X- _ O
explicitly -X- _ O
modeling -X- _ O
role -X- _ O
preference -X- _ O
, -X- _ O
dialogue -X- _ O
coherence -X- _ O
, -X- _ O
and -X- _ O
translation -X- _ O
consistency -X- _ O
with -X- _ O
tailored -X- _ O
latent -X- _ O
variables -X- _ O
to -X- _ O
promote -X- _ O
the -X- _ O
translation -X- _ O
quality -X- _ O
. -X- _ O

Context -X- _ O
- -X- _ O
Aware -X- _ O
NMT -X- _ B-TaskName
. -X- _ O

Chat -X- _ O
NMT -X- _ B-TaskName
can -X- _ O
be -X- _ O
viewed -X- _ O
as -X- _ O
a -X- _ O
special -X- _ O
case -X- _ O
of -X- _ O
context -X- _ O
- -X- _ O
aware -X- _ O
NMT -X- _ B-TaskName
, -X- _ O
which -X- _ O
has -X- _ O
attracted -X- _ O
many -X- _ O
researchers -X- _ O
( -X- _ O
Gong -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2011 -X- _ O
; -X- _ O
Jean -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017 -X- _ O
; -X- _ O
Wang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017b -X- _ O
; -X- _ O
Bawden -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2018 -X- _ O
; -X- _ O
Miculicich -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2018 -X- _ O
; -X- _ O
Kuang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2018 -X- _ O
; -X- _ O
Tu -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2018 -X- _ O
; -X- _ O
Kang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020 -X- _ O
; -X- _ O
Ma -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
to -X- _ O
extend -X- _ O
the -X- _ O
encoder -X- _ O
or -X- _ O
decoder -X- _ O
for -X- _ O
exploring -X- _ O
the -X- _ O
context -X- _ O
impact -X- _ O
on -X- _ O
translation -X- _ O
quality -X- _ O
. -X- _ O
Although -X- _ O
these -X- _ O
models -X- _ O
can -X- _ O
be -X- _ O
directly -X- _ O
applied -X- _ O
to -X- _ O
chat -X- _ O
translation -X- _ O
, -X- _ O
they -X- _ O
can -X- _ O
not -X- _ O
explicitly -X- _ O
capture -X- _ O
the -X- _ O
bilingual -X- _ O
conversational -X- _ O
characteristics -X- _ O
and -X- _ O
thus -X- _ O
lead -X- _ O
to -X- _ O
unsatisfactory -X- _ O
translations -X- _ O
( -X- _ O
Moghe -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
. -X- _ O
Different -X- _ O
from -X- _ O
these -X- _ O
studies -X- _ O
, -X- _ O
we -X- _ O
focus -X- _ O
on -X- _ O
explicitly -X- _ O
modeling -X- _ O
these -X- _ O
bilingual -X- _ O
conversational -X- _ O
characteristics -X- _ O
via -X- _ O
CVAE -X- _ O
for -X- _ O
better -X- _ O
translations -X- _ O
. -X- _ O

Conditional -X- _ O

Variational -X- _ O
Auto -X- _ O
- -X- _ O
Encoder -X- _ O
. -X- _ O
CVAE -X- _ O
has -X- _ O
verified -X- _ O
its -X- _ O
superiority -X- _ O
in -X- _ O
many -X- _ O
fields -X- _ O
( -X- _ O
Sohn -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2015 -X- _ O
) -X- _ O
. -X- _ O
In -X- _ O
NMT -X- _ O
, -X- _ O
and -X- _ O
Su -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2018 -X- _ O
) -X- _ O
extend -X- _ O
CVAE -X- _ O
to -X- _ O
capture -X- _ O
the -X- _ O
global -X- _ O
/ -X- _ O
local -X- _ O
information -X- _ O
of -X- _ O
source -X- _ O
sentence -X- _ O
for -X- _ O
better -X- _ O
results -X- _ O
. -X- _ O
McCarthy -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2020 -X- _ O
) -X- _ O
focus -X- _ O
on -X- _ O
addressing -X- _ O
the -X- _ O
posterior -X- _ O
collapse -X- _ O
with -X- _ O
mutual -X- _ O
information -X- _ O
. -X- _ O
Besides -X- _ O
, -X- _ O
some -X- _ O
studies -X- _ O
use -X- _ O
CVAE -X- _ O
to -X- _ O
model -X- _ O
the -X- _ O
correlations -X- _ O
between -X- _ O
image -X- _ O
and -X- _ O
text -X- _ O
for -X- _ O
multimodal -X- _ O
NMT -X- _ B-TaskName
( -X- _ O
Toyama -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2016 -X- _ O
; -X- _ O
Calixto -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
. -X- _ O
Although -X- _ O
the -X- _ O
CVAE -X- _ O
has -X- _ O
been -X- _ O
widely -X- _ O
used -X- _ O
in -X- _ O
NLP -X- _ O
tasks -X- _ O
, -X- _ O
its -X- _ O
adaption -X- _ O
and -X- _ O
utilization -X- _ O
to -X- _ O
chat -X- _ O
translation -X- _ O
for -X- _ O
modeling -X- _ O
inherent -X- _ O
bilingual -X- _ O
conversational -X- _ O
characteristics -X- _ O
are -X- _ O
non -X- _ O
- -X- _ O
trivial -X- _ O
, -X- _ O
and -X- _ O
to -X- _ O
the -X- _ O
best -X- _ O
of -X- _ O
our -X- _ O
knowledge -X- _ O
, -X- _ O
has -X- _ O
never -X- _ O
been -X- _ O
investigated -X- _ O
before -X- _ O
. -X- _ O

Conclusion -X- _ O
and -X- _ O
Future -X- _ O
Work -X- _ O

We -X- _ O
propose -X- _ O
to -X- _ O
model -X- _ O
bilingual -X- _ O
conversational -X- _ O
characteristics -X- _ O
through -X- _ O
tailored -X- _ O
latent -X- _ O
variables -X- _ O
for -X- _ O
neural -X- _ B-TaskName
chat -X- _ I-TaskName
translation -X- _ I-TaskName
. -X- _ O
Experiments -X- _ O
on -X- _ O
En⇔De -X- _ O
and -X- _ O
En⇔Ch -X- _ O
directions -X- _ O
show -X- _ O
that -X- _ O
our -X- _ O
model -X- _ O
notably -X- _ O
improves -X- _ O
translation -X- _ O
quality -X- _ O
on -X- _ O
both -X- _ O
BLEU -X- _ B-MetricName
and -X- _ O
TER -X- _ B-MetricName
metrics -X- _ O
, -X- _ O
showing -X- _ O
its -X- _ O
superiority -X- _ O
and -X- _ O
generalizability -X- _ O
. -X- _ O
Human -X- _ O
evaluation -X- _ O
further -X- _ O
verifies -X- _ O
that -X- _ O
our -X- _ O
model -X- _ O
yields -X- _ O
role -X- _ O
- -X- _ O
specific -X- _ O
, -X- _ O
coherent -X- _ O
, -X- _ O
and -X- _ O
consistent -X- _ O
translations -X- _ O
by -X- _ O
incorporating -X- _ O
tailored -X- _ O
latent -X- _ O
variables -X- _ O
into -X- _ O
NMT -X- _ B-TaskName
. -X- _ O
Moreover -X- _ O
, -X- _ O
we -X- _ O
contribute -X- _ O
a -X- _ O
new -X- _ O
bilingual -X- _ O
dialogue -X- _ O
data -X- _ O
( -X- _ O
BMELD -X- _ B-DatasetName
, -X- _ O
En⇔Ch -X- _ O
) -X- _ O
with -X- _ O
manual -X- _ O
translations -X- _ O
to -X- _ O
the -X- _ O
research -X- _ O
community -X- _ O
. -X- _ O
In -X- _ O
the -X- _ O
future -X- _ O
, -X- _ O
we -X- _ O
would -X- _ O
like -X- _ O
to -X- _ O
explore -X- _ O
the -X- _ O
effect -X- _ O
of -X- _ O
multimodality -X- _ O
and -X- _ O
emotion -X- _ O
on -X- _ O
chat -X- _ O
translation -X- _ O
, -X- _ O
which -X- _ O
has -X- _ O
been -X- _ O
well -X- _ O
studied -X- _ O
in -X- _ O
dialogue -X- _ O
field -X- _ O
. -X- _ O
Corpus -X- _ O
, -X- _ O
and -X- _ O
WikiMatrix -X- _ O
for -X- _ O
the -X- _ O
En⇔Ch -X- _ O
. -X- _ O
We -X- _ O
firstly -X- _ O
filter -X- _ O
noisy -X- _ O
sentence -X- _ O
pairs -X- _ O
according -X- _ O
to -X- _ O
their -X- _ O
characteristics -X- _ O
in -X- _ O
terms -X- _ O
of -X- _ O
duplication -X- _ O
and -X- _ O
length -X- _ O
( -X- _ O
whose -X- _ O
length -X- _ O
exceeds -X- _ O
80 -X- _ O
) -X- _ O
. -X- _ O
To -X- _ O
pre -X- _ O
- -X- _ O
process -X- _ O
the -X- _ O
raw -X- _ O
data -X- _ O
, -X- _ O
we -X- _ O
employ -X- _ O
a -X- _ O
series -X- _ O
of -X- _ O
open -X- _ O
- -X- _ O
source -X- _ O
/ -X- _ O
in -X- _ O
- -X- _ O
house -X- _ O
scripts -X- _ O
, -X- _ O
including -X- _ O
full- -X- _ O
/ -X- _ O
half -X- _ O
- -X- _ O
width -X- _ O
conversion -X- _ O
, -X- _ O
unicode -X- _ O
conversation -X- _ O
, -X- _ O
punctuation -X- _ O
normalization -X- _ O
, -X- _ O
and -X- _ O
tokenization -X- _ O
. -X- _ O
After -X- _ O
filtering -X- _ O
steps -X- _ O
, -X- _ O
we -X- _ O
generate -X- _ O
subwords -X- _ O
via -X- _ O
joint -X- _ O
BPE -X- _ O
( -X- _ O
Sennrich -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2016 -X- _ O
) -X- _ O
with -X- _ O
32 -X- _ O
K -X- _ O
merge -X- _ O
operations -X- _ O
. -X- _ O
Finally -X- _ O
, -X- _ O
we -X- _ O
obtain -X- _ O
45,541,367 -X- _ O
sentence -X- _ O
pairs -X- _ O
for -X- _ O
En⇔De -X- _ O
and -X- _ O
22,244,006 -X- _ O
sentence -X- _ O
pairs -X- _ O
for -X- _ O
En⇔Ch -X- _ O
, -X- _ O
respectively -X- _ O
. -X- _ O

We -X- _ O
test -X- _ O
the -X- _ O
model -X- _ O
performance -X- _ O
of -X- _ O
the -X- _ O
first -X- _ O
stage -X- _ O
on -X- _ O
newstest2019 -X- _ B-DatasetName
. -X- _ O
The -X- _ O
results -X- _ O
are -X- _ O
shown -X- _ O
in -X- _ O
Tab -X- _ O
. -X- _ O
6 -X- _ O
. -X- _ O

B -X- _ O
Implementation -X- _ O
Details -X- _ O

For -X- _ O
all -X- _ O
experiments -X- _ O
, -X- _ O
we -X- _ O
follow -X- _ O
two -X- _ O
model -X- _ O
settings -X- _ O
illustrated -X- _ O
in -X- _ O
( -X- _ O
Vaswani -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
, -X- _ O
namely -X- _ O
Transformer -X- _ B-MethodName
- -X- _ I-MethodName
Base -X- _ I-MethodName
and -X- _ O
Transformer -X- _ B-MethodName
- -X- _ I-MethodName
Big -X- _ I-MethodName
. -X- _ O
The -X- _ O
training -X- _ B-HyperparameterName
step -X- _ I-HyperparameterName
is -X- _ O
set -X- _ O
to -X- _ O
200,000 -X- _ B-HyperparameterValue
and -X- _ O
2,000 -X- _ B-HyperparameterValue
for -X- _ O
the -X- _ O
first -X- _ O
stage -X- _ O
and -X- _ O
the -X- _ O
fine -X- _ O
- -X- _ O
tuning -X- _ O
stage -X- _ O
, -X- _ O
respectively -X- _ O
. -X- _ O
The -X- _ O
batch -X- _ B-HyperparameterName
size -X- _ I-HyperparameterName
for -X- _ O
each -X- _ O
GPU -X- _ O
is -X- _ O
set -X- _ O
to -X- _ O
4096 -X- _ B-HyperparameterValue
tokens -X- _ O
. -X- _ O
The -X- _ O
beam -X- _ B-HyperparameterName
size -X- _ I-HyperparameterName
is -X- _ O
set -X- _ O
to -X- _ O
4 -X- _ B-HyperparameterValue
, -X- _ O
and -X- _ O
the -X- _ O
length -X- _ B-HyperparameterName
penalty -X- _ I-HyperparameterName
is -X- _ O
0.6 -X- _ B-HyperparameterValue
among -X- _ O
all -X- _ O
experiments -X- _ O
. -X- _ O
All -X- _ O
experiments -X- _ O
in -X- _ O
the -X- _ O
first -X- _ O
stage -X- _ O
are -X- _ O
conducted -X- _ O
utilizing -X- _ O
8 -X- _ O
NVIDIA -X- _ O
Tesla -X- _ O
V100 -X- _ O
GPUs -X- _ O
, -X- _ O
while -X- _ O
we -X- _ O
use -X- _ O
2 -X- _ O
GPUs -X- _ O
for -X- _ O
the -X- _ O
second -X- _ O
stage -X- _ O
, -X- _ O
i.e. -X- _ O
, -X- _ O
fine -X- _ O
- -X- _ O
tuning -X- _ O
. -X- _ O
That -X- _ O
gives -X- _ O
us -X- _ O
about -X- _ O
8 -X- _ O
* -X- _ O
4096 -X- _ O
and -X- _ O
2 -X- _ O
* -X- _ O
4096 -X- _ O
tokens -X- _ O
per -X- _ O
update -X- _ O
for -X- _ O
all -X- _ O
experiments -X- _ O
in -X- _ O
the -X- _ O
first -X- _ O
- -X- _ O
stage -X- _ O
and -X- _ O
second -X- _ O
- -X- _ O
stage -X- _ O
, -X- _ O
respectively -X- _ O
. -X- _ O
All -X- _ O
models -X- _ O
are -X- _ O
optimized -X- _ O
using -X- _ O
Adam -X- _ O
( -X- _ O
Kingma -X- _ O
and -X- _ O
Ba -X- _ O
, -X- _ O
2015 -X- _ O
) -X- _ O
with -X- _ O
β -X- _ B-HyperparameterName
1 -X- _ I-HyperparameterName
= -X- _ O
0.9 -X- _ B-HyperparameterValue
and -X- _ O
β -X- _ B-HyperparameterName
2 -X- _ I-HyperparameterName
= -X- _ O
0.998 -X- _ B-HyperparameterValue
, -X- _ O
and -X- _ O
learning -X- _ B-HyperparameterName
rate -X- _ I-HyperparameterName
is -X- _ O
set -X- _ O
to -X- _ O
1.0 -X- _ B-HyperparameterValue
for -X- _ O
all -X- _ O
experiments -X- _ O
. -X- _ O
Label -X- _ B-HyperparameterName
smoothing -X- _ I-HyperparameterName
is -X- _ O
set -X- _ O
to -X- _ O
0.1 -X- _ B-HyperparameterValue
. -X- _ O
We -X- _ O
use -X- _ O
dropout -X- _ B-HyperparameterName
of -X- _ O
0.1 -X- _ B-HyperparameterValue
/ -X- _ O
0.3 -X- _ B-HyperparameterValue
for -X- _ O
Base -X- _ O
and -X- _ O
Big -X- _ O
setting -X- _ O
, -X- _ O
respectively -X- _ O
. -X- _ O
To -X- _ O
alleviate -X- _ O
the -X- _ O
degeneration -X- _ O
problem -X- _ O
of -X- _ O
the -X- _ O
variational -X- _ O
framework -X- _ O
, -X- _ O
we -X- _ O
apply -X- _ O
KL -X- _ O
annealing -X- _ O
. -X- _ O
The -X- _ O
KL -X- _ B-HyperparameterName
multiplier -X- _ I-HyperparameterName
λ -X- _ B-HyperparameterName
gradually -X- _ O
increases -X- _ O
from -X- _ O
0 -X- _ B-HyperparameterValue
to -X- _ O
1 -X- _ B-HyperparameterValue
over -X- _ O
10 -X- _ O
, -X- _ O
000 -X- _ O
steps -X- _ O
. -X- _ O
|R| -X- _ B-HyperparameterName
is -X- _ O
set -X- _ O
to -X- _ O
2 -X- _ B-HyperparameterValue
for -X- _ O
En⇔De -X- _ O
and -X- _ O
7 -X- _ B-HyperparameterValue
for -X- _ O
En⇔Ch -X- _ O
, -X- _ O
respectively -X- _ O
. -X- _ O
|T -X- _ B-HyperparameterName
| -X- _ I-HyperparameterName
is -X- _ O
set -X- _ O
to -X- _ O
10 -X- _ B-HyperparameterValue
. -X- _ O
The -X- _ O
criterion -X- _ O
for -X- _ O
selecting -X- _ O
hyperparameters -X- _ O
is -X- _ O
the -X- _ O
BLEU -X- _ B-MetricName
score -X- _ O
on -X- _ O
validation -X- _ O
sets -X- _ O
for -X- _ O
both -X- _ O
tasks -X- _ O
. -X- _ O
The -X- _ O
average -X- _ O
running -X- _ O
time -X- _ O
is -X- _ O
shown -X- _ O
in -X- _ O
Tab -X- _ O
. -X- _ O
7 -X- _ O
. -X- _ O

Stages -X- _ O

En⇒De -X- _ O
De⇒En -X- _ O
En⇒Ch -X- _ O
Ch⇒En -X- _ O
In -X- _ O
the -X- _ O
case -X- _ O
of -X- _ O
blind -X- _ O
testing -X- _ O
or -X- _ O
online -X- _ O
use -X- _ O
( -X- _ O
assumed -X- _ O
dealing -X- _ O
with -X- _ O
En⇒De -X- _ O
) -X- _ O
, -X- _ O
since -X- _ O
translations -X- _ O
of -X- _ O
target -X- _ O
utterances -X- _ O
( -X- _ O
i.e. -X- _ O
, -X- _ O
English -X- _ O
) -X- _ O
will -X- _ O
not -X- _ O
be -X- _ O
given -X- _ O
, -X- _ O
an -X- _ O
inverse -X- _ O
De⇒En -X- _ O
model -X- _ O
is -X- _ O
simultaneously -X- _ O
trained -X- _ O
and -X- _ O
used -X- _ O
to -X- _ O
back -X- _ O
- -X- _ O
translate -X- _ O
target -X- _ O
utterances -X- _ O
( -X- _ O
Bao -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
, -X- _ O
similar -X- _ O
to -X- _ O
all -X- _ O
tasks -X- _ O
. -X- _ O

C -X- _ O
Effect -X- _ O
of -X- _ O
Context -X- _ B-HyperparameterName
Length -X- _ I-HyperparameterName

We -X- _ O
firstly -X- _ O
investigate -X- _ O
the -X- _ O
effect -X- _ O
of -X- _ O
context -X- _ B-HyperparameterName
length -X- _ I-HyperparameterName
( -X- _ O
i.e. -X- _ O
, -X- _ O
the -X- _ O
number -X- _ B-HyperparameterName
of -X- _ I-HyperparameterName
preceding -X- _ I-HyperparameterName
utterances -X- _ I-HyperparameterName
) -X- _ O
on -X- _ O
our -X- _ O
approach -X- _ O
under -X- _ O
the -X- _ O
Transformer -X- _ B-MethodName
Base -X- _ O
setting -X- _ O
. -X- _ O
As -X- _ O
shown -X- _ O
in -X- _ O
the -X- _ O
left -X- _ O
of -X- _ O
Fig -X- _ O
. -X- _ O
4 -X- _ O
, -X- _ O
using -X- _ O
three -X- _ O
preceding -X- _ O
source -X- _ O
sentences -X- _ O
as -X- _ O
dialogue -X- _ O
history -X- _ O
achieves -X- _ O
the -X- _ O
best -X- _ O
translation -X- _ O
performance -X- _ O
on -X- _ O
the -X- _ O
validation -X- _ O
set -X- _ O
( -X- _ O
En⇒De -X- _ O
) -X- _ O
. -X- _ O
Using -X- _ O
more -X- _ O
preceding -X- _ O
sentences -X- _ O
does -X- _ O
not -X- _ O
bring -X- _ O
any -X- _ O
improvement -X- _ O
and -X- _ O
increases -X- _ O
the -X- _ O
computational -X- _ O
cost -X- _ O
. -X- _ O
This -X- _ O
confirms -X- _ O
the -X- _ O
finding -X- _ O
of -X- _ O
Tu -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2018 -X- _ O
) -X- _ O
and -X- _ O
that -X- _ O
longdistance -X- _ O
context -X- _ O
only -X- _ O
has -X- _ O
limited -X- _ O
influence -X- _ O
. -X- _ O
Therefore -X- _ O
, -X- _ O
we -X- _ O
set -X- _ O
the -X- _ O
number -X- _ B-HyperparameterName
of -X- _ I-HyperparameterName
preceding -X- _ I-HyperparameterName
sentences -X- _ I-HyperparameterName
to -X- _ O
3 -X- _ B-HyperparameterValue
in -X- _ O
all -X- _ O
experiments -X- _ O
. -X- _ O

D -X- _ O
Effect -X- _ O
of -X- _ O
Latent -X- _ B-HyperparameterName
Dimension -X- _ I-HyperparameterName

The -X- _ O
right -X- _ O
of -X- _ O
Fig -X- _ O
. -X- _ O
4 -X- _ O
shows -X- _ O
the -X- _ O
effect -X- _ O
of -X- _ O
the -X- _ O
latent -X- _ B-HyperparameterName
dimension -X- _ I-HyperparameterName
on -X- _ O
translation -X- _ O
quality -X- _ O
under -X- _ O
the -X- _ O
Transformer -X- _ B-MethodName
Base -X- _ O
setting -X- _ O
. -X- _ O
Obviously -X- _ O
, -X- _ O
using -X- _ O
latent -X- _ B-HyperparameterName
dimension -X- _ I-HyperparameterName
32 -X- _ B-HyperparameterValue
suffices -X- _ O
to -X- _ O
achieve -X- _ O
superior -X- _ O
performance -X- _ O
. -X- _ O
Increasing -X- _ O
the -X- _ O
dimension -X- _ O
does -X- _ O
not -X- _ O
lead -X- _ O
to -X- _ O
any -X- _ O
improvements -X- _ O
. -X- _ O
Therefore -X- _ O
, -X- _ O
we -X- _ O
set -X- _ O
the -X- _ O
latent -X- _ B-HyperparameterName
dimension -X- _ I-HyperparameterName
to -X- _ O
32 -X- _ B-HyperparameterValue
in -X- _ O
all -X- _ O
experiments -X- _ O
. -X- _ O

E -X- _ O
KL -X- _ B-MetricName
Divergence -X- _ I-MetricName

Generally -X- _ O
, -X- _ O
KL -X- _ B-MetricName
divergence -X- _ I-MetricName
measures -X- _ O
the -X- _ O
amount -X- _ O
of -X- _ O
information -X- _ O
encoded -X- _ O
in -X- _ O
a -X- _ O
latent -X- _ O
variable -X- _ O
. -X- _ O
In -X- _ O
the -X- _ O
extreme -X- _ O
case -X- _ O
where -X- _ O
the -X- _ O
KL -X- _ B-MetricName
divergence -X- _ I-MetricName
of -X- _ O
latent -X- _ O
variable -X- _ O
z -X- _ O
equals -X- _ O
to -X- _ O
zero -X- _ O
, -X- _ O
the -X- _ O
model -X- _ O
completely -X- _ O
ignores -X- _ O
z -X- _ O
, -X- _ O
i.e. -X- _ O
, -X- _ O
it -X- _ O
degenerates -X- _ O
. -X- _ O
Fig -X- _ O
. -X- _ O
5 -X- _ O
shows -X- _ O
that -X- _ O
the -X- _ O
total -X- _ O
KL -X- _ O
divergence -X- _ O
of -X- _ O
our -X- _ O
model -X- _ O
maintains -X- _ O
around -X- _ O
0.2∼0.5 -X- _ B-MetricValue
indicating -X- _ O
that -X- _ O
the -X- _ O
degeneration -X- _ O
problem -X- _ O
does -X- _ O
not -X- _ O
exist -X- _ O
in -X- _ O
our -X- _ O
model -X- _ O
and -X- _ O
latent -X- _ O
variables -X- _ O
can -X- _ O
play -X- _ O
their -X- _ O
corresponding -X- _ O
roles -X- _ O
. -X- _ O

F -X- _ O
Case -X- _ O
Study -X- _ O

In -X- _ O
this -X- _ O
section -X- _ O
, -X- _ O
we -X- _ O
show -X- _ O
some -X- _ O
cases -X- _ O
in -X- _ O
Fig -X- _ O
. -X- _ O
6 -X- _ O
and -X- _ O
Fig -X- _ O
. -X- _ O
7 -X- _ O
to -X- _ O
investigate -X- _ O
the -X- _ O
effect -X- _ O
of -X- _ O
different -X- _ O
models -X- _ O
. -X- _ O

Role -X- _ O
Preference -X- _ O
and -X- _ O
Dialogue -X- _ O
Coherence -X- _ O
. -X- _ O
As -X- _ O
shown -X- _ O
in -X- _ O
Fig -X- _ O
. -X- _ O
6 -X- _ O
, -X- _ O
we -X- _ O
observe -X- _ O
that -X- _ O
the -X- _ O
baseline -X- _ O
models -X- _ O
and -X- _ O
the -X- _ O
context -X- _ O
- -X- _ O
aware -X- _ O
models -X- _ O
except -X- _ O
" -X- _ O
V -X- _ O
- -X- _ O
Transformer+FT -X- _ B-MethodName
" -X- _ O
can -X- _ O
not -X- _ O
preserve -X- _ O
the -X- _ O
role -X- _ O
preference -X- _ O
information -X- _ O
, -X- _ O
e.g. -X- _ O
, -X- _ O
joy -X- _ O
emotion -X- _ O
, -X- _ O
even -X- _ O
these -X- _ O
" -X- _ O
* -X- _ O
-Transformer+FT -X- _ B-MethodName
" -X- _ O
models -X- _ O
incorporate -X- _ O
the -X- _ O
bilingual -X- _ O
conversational -X- _ O
history -X- _ O
into -X- _ O
the -X- _ O
encoder -X- _ O
. -X- _ O
The -X- _ O
" -X- _ O
V -X- _ O
- -X- _ O
Transformer+FT -X- _ B-MethodName
" -X- _ O
model -X- _ O
produces -X- _ O
very -X- _ O
slightly -X- _ O
emotional -X- _ O
elements -X- _ O
( -X- _ O
e.g. -X- _ O
, -X- _ O
" -X- _ O
zěnme -X- _ O
? -X- _ O
" -X- _ O
) -X- _ O
due -X- _ O
to -X- _ O
the -X- _ O
latent -X- _ O
variable -X- _ O
over -X- _ O
the -X- _ O
source -X- _ O
sentence -X- _ O
capturing -X- _ O
relevant -X- _ O
preference -X- _ O
information -X- _ O
. -X- _ O
Meanwhile -X- _ O
, -X- _ O
we -X- _ O
find -X- _ O
that -X- _ O
all -X- _ O
comparison -X- _ O
models -X- _ O
can -X- _ O
not -X- _ O
generate -X- _ O
a -X- _ O
coherent -X- _ O
translation -X- _ O
. -X- _ O
The -X- _ O
reason -X- _ O
may -X- _ O
be -X- _ O
that -X- _ O
they -X- _ O
fail -X- _ O
to -X- _ O
capture -X- _ O
the -X- _ O
conversation -X- _ O
- -X- _ O
level -X- _ O
coherence -X- _ O
clue -X- _ O
, -X- _ O
i.e. -X- _ O
, -X- _ O
" -X- _ O
boat -X- _ O
" -X- _ O
. -X- _ O
By -X- _ O
contrast -X- _ O
, -X- _ O
we -X- _ O
explicitly -X- _ O
model -X- _ O
the -X- _ O
two -X- _ O
characteristics -X- _ O
through -X- _ O
tailored -X- _ O
latent -X- _ O
variables -X- _ O
and -X- _ O
thus -X- _ O
obtain -X- _ O
satisfactory -X- _ O
results -X- _ O
. -X- _ O

Translation -X- _ O
Consistency -X- _ O
. -X- _ O
As -X- _ O
shown -X- _ O
in -X- _ O
Fig -X- _ O
. -X- _ O
7 -X- _ O
, -X- _ O
we -X- _ O
observe -X- _ O
that -X- _ O
all -X- _ O
comparison -X- _ O
models -X- _ O
can -X- _ O
not -X- _ O
maintain -X- _ O
the -X- _ O
translation -X- _ O
consistency -X- _ O
due -X- _ O
to -X- _ O
the -X- _ O
lack -X- _ O
of -X- _ O
explicitly -X- _ O
modeling -X- _ O
this -X- _ O
characteristic -X- _ O
. -X- _ O
Our -X- _ O
model -X- _ O
has -X- _ O
the -X- _ O
ability -X- _ O
to -X- _ O
overcome -X- _ O
the -X- _ O
issue -X- _ O
and -X- _ O
can -X- _ O
keep -X- _ O
the -X- _ O
correct -X- _ O
lexical -X- _ O
choice -X- _ O
to -X- _ O
translate -X- _ O
the -X- _ O
current -X- _ O
utterance -X- _ O
that -X- _ O
might -X- _ O
have -X- _ O
appeared -X- _ O
in -X- _ O
preceding -X- _ O
turns -X- _ O
, -X- _ O
i.e. -X- _ O
, -X- _ O
" -X- _ O
jiàchuàn -X- _ O
" -X- _ O
. -X- _ O
To -X- _ O
sum -X- _ O
up -X- _ O
, -X- _ O
both -X- _ O
cases -X- _ O
show -X- _ O
that -X- _ O
our -X- _ O
model -X- _ O
yields -X- _ O
role -X- _ O
- -X- _ O
specific -X- _ O
, -X- _ O
coherent -X- _ O
, -X- _ O
and -X- _ O
consistent -X- _ O
translations -X- _ O
by -X- _ O
incorporating -X- _ O
tailored -X- _ O
latent -X- _ O
variables -X- _ O
into -X- _ O
translators -X- _ O
, -X- _ O
demonstrating -X- _ O
its -X- _ O
effectiveness -X- _ O
and -X- _ O
superiority -X- _ O
. -X- _ O

Acknowledgments -X- _ O

The -X- _ O
research -X- _ O
work -X- _ O
descried -X- _ O
in -X- _ O
this -X- _ O
paper -X- _ O
has -X- _ O
been -X- _ O
supported -X- _ O
by -X- _ O
the -X- _ O
National -X- _ O
Key -X- _ O
R -X- _ O
& -X- _ O
D -X- _ O
Program -X- _ O
of -X- _ O
China -X- _ O
( -X- _ O
2020AAA0108001 -X- _ O
) -X- _ O
and -X- _ O
the -X- _ O
National -X- _ O
Nature -X- _ O
Science -X- _ O
Foundation -X- _ O
of -X- _ O
China -X- _ O
( -X- _ O
No -X- _ O
. -X- _ O
61976015 -X- _ O
, -X- _ O
61976016 -X- _ O
, -X- _ O
61876198 -X- _ O
and -X- _ O
61370130 -X- _ O
) -X- _ O
. -X- _ O
The -X- _ O
authors -X- _ O
would -X- _ O
like -X- _ O
to -X- _ O
thank -X- _ O
the -X- _ O
anonymous -X- _ O
reviewers -X- _ O
for -X- _ O
their -X- _ O
valuable -X- _ O
comments -X- _ O
and -X- _ O
suggestions -X- _ O
to -X- _ O
improve -X- _ O
this -X- _ O
paper -X- _ O
. -X- _ O

Appendix -X- _ O

A -X- _ O
Datasets -X- _ O
WMT20 -X- _ O
. -X- _ O
For -X- _ O
the -X- _ O
En⇔De -X- _ O
, -X- _ O
we -X- _ O
combine -X- _ O
six -X- _ O
corpora -X- _ O
including -X- _ O
Euporal -X- _ O
, -X- _ O
ParaCrawl -X- _ O
, -X- _ O
Common -X- _ O
- -X- _ O
Crawl -X- _ O
, -X- _ O
TildeRapid -X- _ O
, -X- _ O
NewsCommentary -X- _ O
, -X- _ O
and -X- _ O
Wiki -X- _ O
- -X- _ O
Matrix -X- _ O
, -X- _ O
and -X- _ O
we -X- _ O
combine -X- _ O
News -X- _ O
Commentary -X- _ O
v15 -X- _ O
, -X- _ O
Wiki -X- _ O
Titles -X- _ O
v2 -X- _ O
, -X- _ O
UN -X- _ O
Parallel -X- _ O
Corpus -X- _ O
V1.0 -X- _ O
, -X- _ O
CCMT -X- _ O