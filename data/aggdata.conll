SCIENCEWORLD B-DatasetName
: O
Is O
your O
Agent O
Smarter O
than O
a O
5 O
th O
Grader O
? O

We O
present O
SCIENCEWORLD B-DatasetName
, O
a O
benchmark O
to O
test O
agents O
' O
scientific O
reasoning O
abilities O
in O
a O
new O
interactive O
text O
environment O
at O
the O
level O
of O
a O
standard O
elementary O
school O
science O
curriculum O
. O
Despite O
the O
transformer O
- O
based O
progress O
seen O
in O
question B-TaskName
- I-TaskName
answering I-TaskName
and O
scientific O
text O
processing O
, O
we O
find O
that O
current O
models O
can O
not O
reason O
about O
or O
explain O
learned O
science O
concepts O
in O
novel O
contexts O
. O
For O
instance O
, O
models O
can O
easily O
answer O
what O
the O
conductivity O
of O
a O
known O
material O
is O
but O
struggle O
when O
asked O
how O
they O
would O
conduct O
an O
experiment O
in O
a O
grounded O
environment O
to O
find O
the O
conductivity O
of O
an O
unknown O
material O
. O
This O
begs O
the O
question O
of O
whether O
current O
models O
are O
simply O
retrieving O
answers O
by O
way O
of O
seeing O
a O
large O
number O
of O
similar O
examples O
or O
if O
they O
have O
learned O
to O
reason O
about O
concepts O
in O
a O
reusable O
manner O
. O
We O
hypothesize O
that O
agents O
need O
to O
be O
grounded O
in O
interactive O
environments O
to O
achieve O
such O
reasoning O
capabilities O
. O
Our O
experiments O
provide O
empirical O
evidence O
supporting O
this O
hypothesisshowing O
that O
a O
1.5 O
million O
parameter O
agent O
trained O
interactively O
for O
100k O
steps O
outperforms O
a O
11 O
billion O
parameter O
model O
statically O
trained O
for O
scientific B-TaskName
question I-TaskName
- I-TaskName
answering I-TaskName
and O
reasoning O
from O
millions O
of O
expert O
demonstrations O
. O

Introduction O

Question B-TaskName
answering I-TaskName
( O
QA B-TaskName
) O
has O
seen O
rapid O
progress O
recently O
. O
Standardized O
elementary O
and O
middle O
school O
science O
exams O
have O
served O
as O
a O
challenge O
task O
for O
QA B-TaskName
( O
Clark O
et O
al O
. O
, O
2018 O
) O
, O
as O
these O
questions O
require O
combining O
science O
- O
domain O
knowledge O
with O
world O
knowledge O
in O
complex O
reasoning O
procedures O
to O
solve O
. O
As O
large O
language O
models O
have O
toppled O
these O
benchmarks O
Khashabi O
et O
al O
. O
, O
2020 O
; O
Xu O
et O
al O
. O
, O
2021a O
) O
, O
the O
focus O
has O
shifted O
away O
from O
simply O
answering O
questions O
toward O
producing O
human O
- O
readable O
explanations O
for O
a O
model O
's O
1 O
Website O
: O
https O
: O
/ O
/ O
sciworld.apps.allenai.org O
2 O
Code O
: O
https O
: O
/ O
/ O
github.com O
/ O
allenai O
/ O
ScienceWorld O
SCIENCEWORLD O
Task O
3 O
( O
Test O
Electrical O
Conductivity O
Subtask O
) O

Task O
Description O

Your O
task O
is O
to O
determine O
if O
a O
metal O
fork O
is O
electrically O
conductive O
. O
The O
metal O
fork O
is O
located O
around O
the O
kitchen O
. O
First O
, O
focus O
on O
the O
metal O
fork O
. O
If O
conductive O
, O
place O
it O
in O
the O
blue O
box O
. O
If O
nonconductive O
, O
place O
it O
in O
the O
green O
box O
. O

Agent O
starts O
in O
a O
random O
location O
, O
navigates O
to O
kitchen O
Kitchen O
This O
room O
is O
called O
the O
kitchen O
. O
In O
it O
, O
you O
see O
: O
a O
fridge O
. O
The O
freezer O
door O
is O
closed O
. O
a O
cupboard O
. O
The O
cupboard O
door O
is O
closed O
. O
a O
table O
. O
On O
the O
table O
is O
: O
a O
glass O
cup O
( O
containing O
nothing O
) O
. O
a O
stove O
, O
which O
is O
turned O
o O
. O
On O
the O
stove O
is O
: O
nothing O
. O
a O
sink O
, O
which O
is O
turned O
o O
. O
In O
the O
sink O
is O
: O
nothing O
. O
a O
thermometer O
, O
currently O
reading O
a O
temperature O
of O
10 O
degrees O
celsius O
a O
metal O
fork O
a O
counter O
. O
On O
the O
counter O
is O
: O
a O
drawer O
, O
a O
bowl O
( O
containing O
an O
orange O
, O
a O
banana O
, O
a O
potato O
, O
a O
red O
apple O
) O
. O
You O
also O
see O
: O
A O
door O
to O
the O
outside O
( O
that O
is O
open O
) O
A O
door O
to O
the O
bathroom O
( O
that O
is O
open O
) O
A O
door O
to O
the O
hallway O
( O
that O
is O
open O
) O
Action O
: O
focus O
on O
metal O
fork O
You O
focus O
on O
the O
metal O
fork O
. O

Action O
: O
pick O
up O
metal O
fork O

You O
move O
the O
metal O
fork O
to O
the O
inventory O
. O

Agent O
navigates O
to O
room O
with O
electrical O
components O

Workshop O

This O
room O
is O
called O
the O
workshop O
. O
In O
it O
, O
you O
see O
: O
a O
electric O
buzzer O
, O
which O
is O
o O
a O
electric O
motor O
, O
which O
is O
o O
a O
solar O
panel O
, O
which O
is O
o O
a O
table O
. O
On O
the O
table O
is O
: O
a O
blue O
wire O
, O
a O
battery O
, O
a O
red O
light O
bulb O
, O
which O
is O
o O
, O
a O
black O
wire O
, O
a O
switch O
, O
which O
is O
o O
, O
a O
orange O
wire O
. O
a O
blue O
answer O
box O
a O
green O
answer O
box O
You O
also O
see O
: O
A O
door O
to O
the O
hallway O
( O
that O
is O
open O
) O
Action O
: O
connect O
battery O
cathode O
to O
orange O
wire O
terminal O
1 O
cathode O
on O
battery O
is O
now O
connected O
to O
terminal O
1 O
on O
orange O
wire O
. O

Action O
: O
connect O
metal O
fork O
terminal O
1 O
to O
cathode O
in O
red O
light O
bulb O
terminal O
1 O
on O
metal O
fork O
is O
now O
connected O
to O
cathode O
on O
red O
light O
bulb O
. O

Agent O
continues O
wiring O
a O
circuit O
containing O
the O
metal O
fork O
Action O
: O
examine O
red O
light O
bulb O
a O
red O
light O
bulb O
, O
which O
is O
on O
. O

Action O
: O
move O
metal O
fork O
to O
blue O
answer O
box O
( O
disconnecting O
metal O
fork O
) O
You O
move O
the O
metal O
fork O
to O
the O
blue O
box O
. O
The O
agent O
finds O
the O
metal O
fork O
, O
use O
it O
to O
build O
an O
electrical O
circuit O
with O
a O
red O
light O
bulb O
, O
and O
observe O
if O
the O
light O
turns O
on O
. O
Note O
the O
transcript O
has O
been O
simplified O
to O
fit O
in O
available O
space O
, O
and O
large O
sections O
have O
been O
omitted O
( O
greyed O
sections O
) O
. O

answers O
( O
Jansen O
et O
al O
. O
, O
2018 O
; O
Yang O
et O
al O
. O
, O
2018 O
; O
Xie O
et O
al O
. O
, O
2020 O
; O
Valentino O
et O
al O
. O
, O
2021 O
; O
Xu O
et O
al O
. O
, O
2021b O
; O
Lamm O
et O
al O
. O
, O
2021 O
; O
Aggarwal O
et O
al O
. O
, O
2021 O
) O
. O
While O
language O
models O
are O
able O
to O
produce O
compelling O
answers O
( O
Zoph O
et O
al O
. O
, O
2022 O
) O
or O
explanations O
to O
science O
questions O
, O
are O
they O
simply O
retrieving O
( O
or O
shallowly O
assembling O
) O
these O
answers O
, O
or O
can O
they O
understand O
and O
use O
the O
knowledge O
they O
output O
in O
a O
meaningful O
way O
? O
Also O
, O
how O
can O
we O
evaluate O
if O
a O
model O
's O
explanation O
is O
correct O
? O

In O
this O
work O
we O
explore O
these O
two O
questions O
by O
reframing O
science O
exam O
question O
answering O
into O
an O
interactive O
task O
where O
agents O
must O
complete O
elementary O
science O
experiments O
in O
a O
simulated O
textbased O
environment O
called O
SCIENCEWORLD B-DatasetName
( O
see O
Table O
1 O
) O
. O
Instead O
of O
simply O
answering O
a O
question O
( O
e.g. O
, O
Q O
: O
" O
What O
will O
happen O
to O
an O
ice O
cube O
when O
placed O
on O
a O
stove O
? O
" O
, O
A O
: O
" O
it O
will O
melt O
" O
) O
, O
the O
agent O
must O
demonstrate O
its O
capacity O
to O
combine O
declarative O
scientific O
knowledge O
with O
the O
procedural O
knowledge O
required O
to O
correctly O
complete O
the O
experiment O
in O
the O
virtual O
environment O
. O
Similarly O
, O
the O
sequence O
of O
virtual O
actions O
an O
agent O
performs O
can O
serve O
as O
a O
form O
of O
procedural O
( O
" O
how O
" O
) O
explanation O
to O
the O
question O
, O
that O
can O
be O
directly O
evaluated O
in O
the O
virtual O
environment O
for O
correctness O
( O
e.g. O
, O
whether O
the O
actions O
led O
to O
the O
ice O
cube O
melting O
) O
. O

The O
contributions O
of O
this O
work O
are O
: O

1 O
. O
We O
construct O
SCIENCEWORLD B-DatasetName
, O
a O
complex O
interactive O
text O
environment O
, O
with O
simulation O
engines O
for O
thermodynamics O
, O
electrical O
circuits O
, O
chemistry O
reactions O
, O
and O
biological O
processes O
. O

2 O
. O
We O
implement O
30 O
benchmark O
tasks O
across O
10 O
topics O
spanning O
the O
elementary O
science O
curriculum O
, O
including O
changes O
of O
state O
of O
matter O
and O
the O
role O
of O
pollinators O
when O
growing O
fruit O
. O

3 O
. O
We O
evaluate O
5 O
state O
- O
of O
- O
the O
- O
art O
reinforcement O
learning O
and O
language O
model O
agents O
on O
this O
benchmark O
, O
empirically O
showing O
that O
they O
perform O
poorly O
on O
tasks O
( O
e.g. O
, O
melting O
ice O
) O
that O
5 O
th O
grade O
students O
can O
perform O
with O
ease O
. O

Related O
Work O

Science O
- O
domain O
Inference O
: O
Standardized O
science O
exams O
are O
a O
challenging O
task O
for O
question B-TaskName
answering I-TaskName
due O
to O
their O
diverse O
knowledge O
and O
inference O
requirements O
( O
Clark O
et O
al O
. O
, O
2013 O
; O
Jansen O
et O
al O
. O
, O
2016 O
; O
Boratko O
et O
al O
. O
, O
2018 O
; O
Clark O
et O
al O
. O
, O
2018 O
) O
. O
Top O
performing O
models O
can O
answer O
more O
than O
90 O
% O
of O
multiple O
- O
choice O
questions O
correctly O
, O
typically O
through O
the O
use O
of O
large O
language O
models O
( O
Khashabi O
et O
al O
. O
, O
2020 O
; O
Zoph O
et O
al O
. O
, O
2022 O
) O
. O

A O
number O
of O
corpora O
of O
structured O
and O
semistructured O
science O
exam O
explanations O
exist O
for O
training O
the O
explanation O
- O
generation O
task O
( O
Jansen O
et O
al O
. O
, O
2018 O
; O
Xie O
et O
al O
. O
, O
2020 O
; O
Dalvi O
et O
al O
. O
, O
2021 O
) O
. O
Evaluating O
explanations O
is O
challenging O
, O
and O
typically O
done O
by O
comparing O
generated O
explanations O
to O
a O
single O
gold O
explanation O
. O
This O
has O
been O
shown O
to O
substantially O
underestimate O
explanation O
generation O
performance O
by O
up O
to O
40 O
% O
. O
While O
others O
have O
worked O
to O
mitigate O
this O
by O
generating O
multiple O
alternate O
explanations O
for O
each O
question O
, O
this O
is O
expensive O
, O
and O
has O
only O
been O
demonstrated O
for O
adding O
one O
or O
two O
additional O
explanations O
per O
question O
( O
Inoue O
et O
al O
. O
, O
2020 O
; O
Jhamtani O
and O
Clark O
, O
2020 O
) O
. O
Here O
, O
we O
propose O
a O
partial O
solution O
to O
this O
evaluation O
problem O
by O
treating O
the O
action O
sequences O
of O
agents O
as O
structured O
manner O
explanations O
for O
how O
to O
solve O
a O
task O
. O
These O
action O
sequences O
can O
be O
directly O
run O
in O
the O
SCIENCEWORLD B-DatasetName
simulator O
to O
automatically O
determine O
their O
correctness O
( O
i.e. O
, O
whether O
they O
accomplish O
the O
task O
) O
, O
independent O
of O
any O
variations O
in O
their O
solution O
methods O
. O
For O
example O
, O
whether O
an O
agent O
melts O
ice O
by O
using O
a O
stove O
, O
building O
a O
campfire O
, O
or O
leaving O
the O
ice O
out O
on O
a O
kitchen O
counter O
, O
the O
end O
result O
is O
the O
same O
and O
directly O
measurable O
through O
the O
formal O
semantics O
of O
the O
simulator O
. O

Environments O
: O
Interactive O
text O
environments O
are O
becoming O
a O
vehicle O
for O
research O
in O
natural O
language O
processing O
( O
see O
for O
a O
review O
) O
, O
primarily O
because O
of O
their O
reduced O
development O
costs O
relative O
to O
3D O
environments O
, O
combined O
with O
their O
ability O
to O
easily O
implement O
high O
- O
level O
tasks O
with O
large O
action O
spaces O
. O
In O
spite O
of O
these O
benefits O
, O
implementation O
costs O
can O
still O
be O
substantial O
for O
large O
complex O
environments O
, O
and O
text O
agents O
are O
frequently O
evaluated O
on O
a O
small O
set O
of O
exist O
- O
ing O
interactive O
fiction O
games O
such O
as O
Zork O
( O
Lebling O
et O
al O
. O
, O
1979 O
) O
using O
an O
unified O
interface O
like O
Jericho O
. O
A O
few O
purpose O
- O
built O
environments O
provide O
simple O
tasks O
for O
studying O
text O
agents O
, O
typically O
on O
procedurally O
generated O
pick O
- O
and O
- O
place O
or O
object O
- O
combination O
tasks O
( O
e.g. O
, O
cooking O
, O
Yin O
and O
May O
, O
2019 O
) O
. O
Kitchen B-DatasetName
Cleanup I-DatasetName
( O
Murugesan O
et O
al O
. O
, O
2020b O
) O
and O
TextWorld B-DatasetName
Common I-DatasetName
Sense I-DatasetName
( O
Murugesan O
et O
al O
. O
, O
2020a O
) O
require O
agents O
to O
tidy O
up O
one O
or O
more O
rooms O
in O
a O
house O
environment O
by O
putting O
objects O
in O
their O
typical O
locations O
( O
e.g. O
, O
a O
hat O
should O
be O
placed O
on O
the O
hat O
rack O
) O
, O
testing O
an O
agent O
's O
declarative O
knowledge O
of O
common O
object O
locations O
with O
the O
procedural O
knowledge O
required O
for O
this O
pick O
- O
and O
- O
place O
task O
. O
The O
closest O
existing O
interactive O
text O
environment O
to O
SCIENCEWORLD B-DatasetName
is O
TextLabs B-DatasetName
( O
Tamari O
et O
al O
. O
, O
2021 O
) O
which O
simulates O
chemistry O
wet O
- O
lab O
protocols O
with O
actions O
such O
as O
pipetting O
and O
centrifuging O
. O
Compared O
to O
these O
existing O
environments O
, O
SCIENCEWORLD B-DatasetName
is O
generally O
a O
larger O
and O
more O
dynamic O
environment O
, O
populated O
with O
more O
complex O
objects O
with O
greater O
depth O
of O
physical O
simulation O
. O
This O
simulation O
depth O
enables O
more O
complex O
tasks O
associated O
with O
elementary O
science O
( O
e.g. O
, O
thermodynamics O
, O
electrical O
circuits O
, O
etc O
. O
) O
to O
be O
tested O
, O
and O
a O
greater O
variety O
of O
solutions O
. O

Simulators O
: O
Nearly O
all O
text O
- O
based O
world O
simulations O
are O
currently O
implemented O
as O
Z O
- O
machine O
games O
( O
Infocom O
, O
1989 O
; O
Nelson O
, O
2014 O
) O
, O
frequently O
through O
higher O
- O
level O
application O
- O
specific O
languages O
( O
such O
as O
Inform7 O
, O
Nelson O
, O
2006 O
) O
that O
compile O
to O
Z O
- O
machine O
code O
. O
TextWorld B-DatasetName
( O
Côté O
et O
al O
. O
, O
2018 O
) O
creates O
environments O
using O
linear O
- O
logic O
statements O
that O
specify O
action O
preconditions O
and O
postconditions O
( O
Martens O
, O
2015 O
) O
and O
generate O
Inform7 O
code O
. O
Existing O
tooling O
is O
designed O
for O
simpler O
simulations O
than O
SCIENCEWORLD B-DatasetName
, O
with O
primarily O
agent O
- O
centered O
state O
changes O
that O
make O
modeling O
autonomous O
physical O
processes O
( O
e.g. O
, O
thermodynamics O
) O
difficult O
. O
As O
such O
, O
in O
this O
work O
we O
build O
a O
novel O
simulator O
to O
model O
physical O
processes O
in O
text O
environments O
. O

Agents O
: O
A O
variety O
of O
agent O
models O
have O
been O
proposed O
for O
reasoning O
in O
interactive O
text O
environments O
. O
Most O
approaches O
frame O
reasoning O
as O
a O
partially O
- O
observable O
Markov O
decision O
process O
( O
POMDP O
) O
, O
and O
model O
inference O
using O
reinforcement O
learning O
( O
RL O
) O
. O
This O
includes O
RL O
- O
based O
models O
that O
learn O
a O
policies O
to O
pick O
relevant O
actions O
from O
lists O
of O
candidate O
actions O
( O
He O
et O
al O
. O
, O
2016 O
) O
, O
or O
models O
that O
mix O
RL O
with O
knowledge O
graphs O
or O
language O
models O
( O
Yao O
et O
al O
. O
, O
2020 O
) O
to O
aid O
in O
next O
- O
action O
selection O
. O
Action O
selection O
has O
also O
been O
modelled O
using O
case O
- O
based O
reasoning O
( O
Atzeni O
et O
al O
. O
, O
2021 O
) O
, O
or O
directly O
as O
a O
sequence O
- O
prediction O
imitation O
learning O
task O
, O
using O
language O
models O
trained O
on O
gold O
action O
sequences O
to O
predict O
the O
next O
action O
given O
the O
current O
state O
( O
Torabi O
et O
al O
. O
, O
2018 O
; O
Ammanabrolu O
et O
al O
. O
, O
, O
2022 O
. O
In O
general O
, O
agent O
performance O
on O
solving O
interactive O
fictions O
is O
still O
modest O
, O
with O
only O
easier O
games O
close O
to O
completion O
. O

In O
this O
work O
, O
we O
benchmark O
state O
- O
of O
- O
the O
- O
art O
agents O
on O
SCIENCEWORLD B-DatasetName
as O
well O
as O
introduce O
novel O
agents O
. O
We O
empirically O
show O
that O
these O
elementary O
science O
tasks O
are O
difficult O
for O
current O
agents O
, O
and O
also O
that O
smaller O
and O
simpler O
agents O
can O
outperform O
billion O
- O
scale O
parameter O
language O
models O
trained O
on O
gold O
sequences O
, O
highlighting O
the O
difficulty O
of O
this O
task O
for O
transformer O
- O
based O
models O
. O

SCIENCEWORLD B-DatasetName

SCIENCEWORLD B-DatasetName
is O
a O
simulation O
of O
the O
world O
abstracted O
through O
a O
complex O
interactive O
text O
environment O
in O
English O
with O
many O
objects O
, O
actions O
, O
and O
simulation O
engines O
. O
The O
framework O
consists O
of O
40k O
lines O
of O
SCALA O
( O
speed O
) O
with O
a O
PYTHON O
interface O
. O

The O
SCIENCEWORLD B-DatasetName
environment O
contains O
10 O
interconnected O
locations O
( O
see O
Figure O
1 O
) O
, O
populated O
with O
up O
to O
200 O
types O
of O
objects O
, O
including O
devices O
, O
instruments O
, O
plants O
/ O
animals O
, O
electrical O
components O
, O
substances O
, O
containers O
, O
and O
common O
environment O
objects O
such O
as O
furniture O
, O
books O
, O
and O
paintings O
. O
The O
SCIENCEWORLD B-DatasetName
action O
space O
contains O
25 O
high O
- O
level O
actions O
, O
including O
both O
science O
- O
domain O
actions O
( O
e.g. O
, O
using O
thermometer O
) O
and O
common O
actions O
( O
e.g. O
, O
moving O
, O
opening O
containers O
, O
picking O
up O
items O
) O
, O
with O
approximately O
200k O
possible O
action O
- O
object O
combinations O
per O
step O
( O
though O
only O
a O
limited O
subset O
of O
these O
will O
be O
meaningful O
) O
. O
See O
Appendix O
A O
for O
details O
about O
SCIENCEWORLD B-DatasetName
, O
including O
the O
object O
model O
, O
actions O
, O
and O
input O
parser O
. O

Simulation O
Engines O

SCIENCEWORLD B-DatasetName
supports O
actions O
commonly O
found O
in O
interactive O
text O
environments O
-for O
example O
, O
objects O
can O
be O
moved O
or O
examined O
, O
foods O
can O
be O
eaten O
, O
and O
books O
can O
be O
read O
. O
In O
addition O
, O
the O
environment O
contains O
a O
number O
of O
elementary O
science O
- O
domain O
specific O
processes O
that O
either O
occur O
automatically O
( O
e.g. O
, O
thermodynamics O
) O
or O
are O
coupled O
to O
actions O
( O
e.g. O
, O
devices O
, O
mixing O
chemicals O
) O
. O

Those O
simulation O
engines O
3 O
are O
: O

Thermodynamics O
: O
All O
objects O
have O
temperatures O
and O
other O
thermal O
properties O
based O
on O
their O
materials O
. O
All O
objects O
within O
a O
container O
are O
considered O
in O
thermal O
contact O
with O
each O
other O
, O
and O
transfer O
heat O
energy O
using O
a O
simplified O
conductive O
heat O
model O
. O
The O
proportion O
of O
heat O
transferred O
between O
objects O
at O
each O
step O
is O
mediated O
by O
the O
object O
's O
thermal O
conduction O
coefficient O
, O
allowing O
thermal O
conductors O
( O
like O
metal O
pots O
) O
and O
insulators O
( O
like O
ceramics O
) O
to O
be O
modelled O
. O
Every O
material O
has O
phase O
transition O
points O
( O
i.e. O
, O
melting O
point O
, O
boiling O
point O
) O
and O
combustion O
points O
populated O
based O
on O
the O
best O
- O
known O
or O
approximate O
physical O
values O
for O
those O
materials O
. O

Objects O
that O
move O
past O
these O
thresholds O
will O
change O
state O
of O
matter O
( O
e.g. O
, O
from O
a O
solid O
to O
a O
liquid O
) O
, O
or O
begin O
a O
combustion O
process O
that O
ultimately O
ends O
in O
the O
object O
turning O
to O
ash O
unless O
its O
fire O
is O
put O
out O
. O
Convective O
heat O
transfer O
is O
also O
modelled O
in O
the O
form O
of O
heat O
sources O
( O
e.g. O
, O
oven O
, O
stove O
) O
and O
heat O
sinks O
( O
e.g. O
, O
fridge O
, O
freezer O
) O
that O
transfer O
heat O
energy O
to O
or O
from O
objects O
. O
Rooms O
also O
transfer O
ambient O
heat O
energy O
to O
/ O
from O
the O
objects O
they O
contain O
. O

Electricity O
: O
The O
simulator O
models O
simple O
series O
electrical O
circuits O
, O
where O
electrically O
- O
powered O
devices O
( O
e.g. O
, O
light O
bulb O
, O
motor O
) O
can O
be O
powered O
by O
being O
connected O
to O
electrical O
energy O
sources O
( O
e.g. O
, O
battery O
, O
solar O
panel O
) O
through O
electrical O
conductors O
( O
nominally O
, O
wires O
) O
. O
Polarized O
and O
unpolarized O
components O
are O
modelled O
, O
with O
each O
object O
having O
exactly O
two O
terminals O
( O
anode O
and O
cathode O
for O
polarized O
; O
terminals O
1 O
and O
2 O
for O
unpolarized O
) O
. O
Connection O
happens O
through O
explicit O
terminal O
- O
to O
- O
terminal O
connection O
actions O
( O
e.g. O
, O
connect O
battery O
anode O
to O
blue O
wire O
terminal O
1 O
) O
. O
Every O
non O
- O
electrical O
object O
in O
SCIENCEWORLD B-DatasetName
has O
virtual O
unpolarized O
terminals O
, O
allowing O
circuits O
to O
be O
build O
with O
valid O
electrical O
conductors O
( O
e.g. O
, O
using O
a O
metal O
fork O
in O
place O
of O
a O
wire O
) O
, O
and O
for O
the O
agent O
to O
build O
circuits O
that O
test O
conductivity O
by O
( O
for O
example O
) O
observing O
if O
a O
light O
bulb O
illuminates O
when O
a O
plastic O
versus O
metal O
fork O
is O
used O
in O
the O
circuit O
. O

Devices O
: O
Many O
objects O
are O
also O
considered O
devices O
, O
that O
can O
be O
activated O
or O
deactivated O
by O
the O
agent O
( O
e.g. O
, O
stove O
, O
electrical O
switch O
) O
, O
or O
may O
have O
environment O
- O
specific O
conditions O
to O
being O
activated O
( O
e.g. O
, O
a O
light O
bulb O
will O
only O
activate O
if O
it O
is O
properly O
electrically O
connected O
; O
a O
solar O
panel O
will O
only O
produce O
power O
if O
it O
is O
outside O
) O
. O
Objects O
can O
also O
be O
used O
with O
other O
objects O
in O
specific O
contexts O
( O
e.g. O
, O
a O
thermometer O
, O
to O
measure O
an O
object O
's O
temperature O
; O
a O
shovel O
, O
to O
dig O
soil O
from O
the O
ground O
) O
. O

Chemistry O
: O
A O
subset O
of O
specific O
chemical O
reactions O
are O
modelled O
, O
where O
mixing O
a O
set O
of O
substances O
together O
in O
a O
container O
will O
produce O
a O
resultant O
substance O
( O
e.g. O
, O
salt O
and O
water O
mix O
to O
produce O
salt O
water O
) O
. O
Friction O
( O
Inclined O
Plane O
) O
: O
Forces O
are O
a O
significant O
part O
of O
an O
elementary O
science O
curriculum O
, O
but O
difficult O
to O
incorporate O
without O
2D O
or O
3D O
simulation O
. O
SCIENCEWORLD B-DatasetName
models O
the O
forces O
of O
gravity O
and O
friction O
in O
the O
specific O
context O
of O
1 O
- O
dimensional O
inclined O
plane O
experiments O
. O
Objects O
placed O
at O
the O
top O
of O
an O
inclined O
plane O
will O
slide O
down O
the O
plane O
at O
a O
speed O
proportional O
to O
the O
plane O
's O
angle O
, O
and O
the O
friction O
coefficient O
of O
its O
surface O
material O
. O
The O
position O
is O
described O
to O
the O
agent O
( O
e.g. O
, O
" O
an O
inclined O
plant O
with O
a O
block O
60 O
% O
of O
the O
way O
down O
the O
plane O
" O
) O
, O
allowing O
experiments O
to O
determine O
either O
the O
relative O
angle O
or O
friction O
coefficients O
of O
different O
inclined O
planes O
based O
on O
the O
speed O
the O
object O
moves O
down O
a O
given O
plane O
. O

Containers O
: O
Containers O
can O
be O
always O
open O
( O
e.g. O
, O
a O
metal O
pot O
) O
or O
closeable O
( O
e.g. O
, O
a O
cupboard O
) O
. O
Objects O
contained O
inside O
containers O
are O
not O
visible O
until O
the O
container O
is O
open O
. O
Some O
effects O
spread O
beyond O
a O
container O
-for O
example O
, O
a O
wooden O
cupboard O
with O
a O
hot O
object O
inside O
may O
combust O
, O
causing O
other O
objects O
in O
the O
kitchen O
to O
also O
increase O
temperature O
. O

Experiments O

To O
understand O
how O
contemporary O
approaches O
to O
text O
agents O
perform O
at O
SCIENCEWORLD B-DatasetName
tasks O
, O
we O
benchmark O
a O
selection O
of O
recent O
architectures O
. O

Tasks O
. O
To O
support O
our O
goal O
of O
generating O
a O
diverse O
set O
of O
tasks O
, O
we O
identified O
a O
candidate O
set O
of O
10 O
broad O
science O
exam O
topics O
from O
the O
list O
of O
400 O
fine O
- O
grained O
science O
curriculum O
topics O
of O
Xu O
et O
al O
. O
( O
2020 O
) O
. O
Topics O
were O
chosen O
that O
would O
be O
amenable O
to O
text O
- O
based O
simulation O
, O
and O
that O
did O
not O
have O
critical O
fine O
- O
grained O
spatial O
reasoning O
requirements O
, O
and O
include O
: O
changes O
of O
state O
, O
temperature O
measurement O
, O
electrical O
circuits O
, O
friction O
, O
object O
classification O
, O
chemical O
mixtures O
, O
plants O
and O
pollinators O
, O
life O
spans O
, O
life O
stages O
, O
and O
Mendelian O
genetics O
. O
Each O
topic O
was O
further O
divided O
into O
between O
2 O
and O
4 O
specific O
tasks O
for O
agents O
to O
perform O
, O
producing O
a O
total O
of O
30 O
science O
- O
domain O
tasks O
. O
These O
topics O
and O
tasks O
are O
described O
in O
Appendix O
B.2 O
. O

To O
prevent O
overfitting O
and O
encourage O
generalization O
, O
each O
subtask O
contains O
between O
10 O
and O
1400 O
parametric O
variations O
( O
with O
7200 O
total O
variations O
across O
all O
30 O
subtasks O
) O
. O
Variations O
change O
critical O
task O
objects O
( O
e.g. O
, O
the O
specific O
substance O
to O
be O
melted O
) O
, O
the O
agent O
's O
starting O
location O
in O
the O
environment O
, O
as O
well O
as O
randomly O
vary O
the O
contents O
of O
the O
environment O
itself O
( O
e.g. O
, O
whether O
the O
living O
room O
contains O
a O
bookshelf O
, O
or O
a O
painting O
, O
or O
both O
) O
. O

Train O
, O
Development O
, O
Test O
sets O
: O
For O
a O
given O
subtask O
, O
variations O
are O
split B-HyperparameterName
into O
50 B-HyperparameterValue
% I-HyperparameterValue
training O
, O
25 B-HyperparameterValue
% I-HyperparameterValue
development O
, O
and O
25 B-HyperparameterValue
% I-HyperparameterValue
test O
sets O
. O
Variations O
are O
sorted O
such O
that O
critical O
unseen O
variations O
( O
e.g. O
, O
substances O
, O
animals O
, O
or O
plants O
unseen O
during O
training O
) O
are O
found O
in O
development O
and O
test O
sets O
. O

Goals O
and O
Rewards B-MetricName
. O
To O
reduce O
reward O
sparsity O
, O
each O
task O
includes O
between O
2 O
and O
15 O
optional O
subgoals O
( O
such O
as O
turning O
on O
the O
stove O
, O
or O
the O
substance O
increasing O
in O
temperature O
by O
10C O
) O
that O
help O
nudge O
agents O
in O
the O
direction O
of O
canonical O
solutions O
, O
if O
desired O
. O
Meeting O
required O
and O
optional O
subgoals O
increases O
the O
agent O
's O
score O
on O
a O
given O
subtask O
. O
Scores O
for O
all O
tasks O
are O
normalized O
to O
between O
0 B-MetricValue
and O
1 B-MetricValue
. O

Oracle O
Agents O
To O
support O
imitation O
learning O
, O
we O
provide O
gold O
trajectories O
from O
30 O
hand O
- O
coded O
oracles O
on O
all O
subtasks O
and O
variations O
. O
For O
tractability O
these O
solutions O
represent O
canonical O
solution O
methods O
( O
e.g. O
, O
using O
a O
stove O
to O
boil O
water O
) O
, O
rather O
than O
all O
possible O
solution O
methods O
that O
lead O
to O
the O
goal O
state O
( O
e.g. O
, O
building O
a O
campfire O
to O
boil O
water O
) O
. O
Learning O
Agents O
. O
An O
interactive O
text O
environment O
can O
be O
cast O
as O
a O
partially O
observable O
Markov O
decision O
process O
( O
POMDP O
) O
defined O
by O
⟨S O
, O
T O
, O
A O
, O
R O
, O
O O
, O
Ω O
, O
γ⟩ O
representing O
the O
set O
of O
possible O
states O
( O
S O
) O
, O
conditional O
transition O
probabilities O
between O
states O
( O
T O
) O
, O
available O
text O
commands O
( O
A O
) O
, O
reward O
function O
( O
R O
∶ O
S O
× O
A O
→ O
R O
) O
, O
set O
of O
possible O
text O
observations O
( O
O O
) O
, O
observation O
conditional O
probabilities O
( O
Ω O
∶ O
S O
→ O
O O
) O
, O
and O
discount O
factor O
( O
γ O
∈ O
[ O
0 O
, O
1 O
] O
) O
. O
The O
goal O
for O
a O
learning O
agent O
is O
then O
to O
learn O
a O
policy O
π O
θ O
( O
o O
) O
→ O
a O
that O
chooses O
or O
generates O
a O
text O
command O
a O
∈ O
A O
given O
the O
text O
observation O
o O
∈ O
Ω O
( O
s O
) O
of O
state O
s O
∈ O
S O
that O
maximizes O
the O
expected O
discounted O
sum O
of O
rewards O
E O
[ O
∑ O
t O
γ O
t O
R O
( O
s O
t O
, O
a O
t O
) O
] O
. O
In O
SCIENCEWORLD B-DatasetName
, O
the O
agent O
is O
also O
provided O
with O
a O
task O
description O
d O
. O

To O
provide O
a O
fair O
comparison O
, O
all O
models O
were O
run O
using O
identical O
experiment O
configurations O
when O
possible O
. O
Reinforcement O
learning O
models O
were O
run O
with O
identical O
training O
regiments O
( O
8 O
environment O
threads O
at O
100k O
steps O
per O
thread O
) O
. O
Training O
episodes O
reset O
after O
meeting O
an O
end O
state O
( O
success O
or O
failure O
) O
, O
or O
after O
reaching O
a O
maximum B-HyperparameterName
number I-HyperparameterName
of I-HyperparameterName
steps I-HyperparameterName
( O
we O
used O
100 B-HyperparameterValue
in O
all O
experiments O
) O
. O
Additional O
model O
details O
are O
provided O
in O
Appendix O
C. O
Random O
Baseline O
: O
At O
each O
time O
step O
t O
, O
this O
baseline O
randomly O
chooses O
an O
action O
a O
from O
the O
set O
of O
valid O
actions O
A O
t O
obtained O
from O
the O
simulator O
. O
DRRN B-MethodName
( O
He O
et O
al O
. O
, O
2016 O
) O
: O
The O
Deep B-MethodName
Reinforcement I-MethodName
Relevance I-MethodName
Network I-MethodName
( O
DRRN B-MethodName
) O
learns O
separate O
representations O
of O
the O
observation O
space O
and O
action O
space O
of O
an O
environment O
, O
then O
trains O
a O
policy O
that O
selects O
from O
A O
t O
the O
action O
that O
is O
the O
most O
relevant O
given O
the O
current O
text O
observation O
o O
t O
( O
which O
also O
includes O
the O
description O
of O
the O
current O
room O
o O
look O
t O
and O
the O
current O
agent O
inventory O
o O
inv O
t O
) O
and O
that O
would O
lead O
to O
an O
increased O
reward O
. O
The O
DRRN B-MethodName
is O
a O
strong O
baseline O
with O
near O
state O
- O
of O
- O
the O
- O
art O
performance O
on O
many O
medium O
- O
to O
- O
hard O
interactive O
text O
environments O
. O
KG O
- O
A2C O
: O
This O
model O
represents O
the O
state O
space O
with O
a O
knowledge O
graph O
built O
dynamically O
from O
the O
text O
observations O
o O
t O
using O
OpenIE O
triples O
( O
Angeli O
et O
al O
. O
, O
2015 O
) O
such O
as O
( O
glass O
bottle O
, O
contains O
, O
water O
) O
, O
while O
the O
action O
space O
is O
represented O
using O
action O
templates O
with O
placeholders O
( O
e.g. O
, O
open O
OBJ O
) O
obtained O
from O
SCIENCEWORLD B-DatasetName
. O
The O
model O
learns O
a O
policy O
that O
selects O
relevant O
action O
templates O
then O
populates O
them O
with O
objects O
from O
the O
knowledge O
graph O
. O
CALM B-MethodName
( O
GPT2 B-MethodName
) O
( O
Yao O
et O
al O
. O
, O
2020 O
) O
: O
We O
collect O
transcripts O
of O
expert O
demonstrations O
for O
the O
train O
variations O
of O
the O
tasks O
using O
the O
oracle O
agents O
, O
then O
use O
them O
to O
fine O
- O
tune O
a O
pre O
- O
trained O
language O
model O
( O
GPT-2 O
, O
Radford O
et O
al O
. O
( O
2019 O
) O
) O
. O
At O
runtime O
, O
the O
language O
model O
is O
provided O
with O
the O
current O
observation O
o O
t O
, O
last O
observation O
o O
t−1 O
, O
and O
last O
action O
a O
t−1 O
, O
then O
generates O
a O
shortlist O
of O
30 O
possible O
actions O
to O
take O
. O
This O
shortlist O
serves O
as O
input O
to O
an O
RL O
model O
similar O
to O
the O
DRRN B-MethodName
, O
which O
re O
- O
ranks O
the O
actions O
and O
chooses O
the O
next O
action O
to O
perform O
. O

Behavior B-MethodName
Cloning I-MethodName
( O
Torabi O
et O
al O
. O
, O
2018 O
) O
: O
We O
follow O
the O
methodology O
of O
in O
adapting O
the O
popular O
imitation O
learning O
method O
of O
behavior O
cloning O
from O
observations O
to O
text O
agents O
. O
We O
used O
the O
same O
transcripts O
of O
demonstrations O
as O
the O
CALM B-MethodName
( O
GPT2 B-MethodName
) O
agent O
to O
extract O
211,092 O
training O
examples O
with O
( O
d O
, O
o O
t−1 O
, O
a O
t−1 O
, O
o O
t O
) O
as O
inputs O
and O
a O
t O
as O
targets O
. O
We O
fine O
- O
tune O
a O
transformer O
- O
based O
text O
- O
to O
- O
text O
model O
with O
a O
T5 B-MethodName
architecture O
( O
Raffel O
et O
al O
. O
, O
2020 O
) O
initialized O
with O
the O
weights O
of O
a O
Macaw B-MethodName
model O
designed O
to O
answer O
science O
questions O
. O

At O
test O
time O
, O
the O
agent O
performs O
zero O
- O
shot O
inference O
online O
in O
the O
simulator O
by O
generating O
a O
fixed O
number O
of O
actions O
with O
beam O
search O
on O
the O
unseen O
test O
variations O
for O
each O
task O
. O
Despite O
training O
on O
a O
large O
number O
of O
demonstrations O
, O
the O
generated O
actions O
are O
often O
invalid O
or O
not O
useful O
- O
resulting O
in O
zero O
scores O
. O
Thus O
, O
we O
treat O
the O
beam O
search O
's O
output O
as O
a O
ranked O
- O
list O
and O
run O
the O
highest O
- O
ranked O
action O
appearing O
in O
A O
t O
, O
similar O
to O
the O
language O
- O
modelto O
- O
valid O
- O
action O
aligner O
of O
Huang O
et O
al O
. O
( O
2022 O
) O
. O

Text B-MethodName
Decision I-MethodName
Transformer I-MethodName
: O
Inspired O
by O
the O
Decision B-MethodName
Transformer I-MethodName
( O
Chen O
et O
al O
. O
, O
2021 O
) O
, O
we O
create O
a O
novel O
text O
game O
agent O
that O
models O
the O
entire O
POMDP O
trajectory O
as O
a O
sequence O
and O
has O
the O
ability O
to O
potentially O
predict O
actions O
that O
maximize O
future O
long O
term O
expected O
reward O
. O
We O
again O
used O
the O
same O
transcripts O
of O
demonstrations O
as O
the O
two O
previous O
agents O
to O
extract O
224,902 O
training O
examples O
with O
( O
d O
, O
o O
t−1 O
, O
R O
t−1 O
, O
a O
t−1 O
, O
o O
t O
, O
R O
t O
) O
as O
inputs O
and O
a O
t O
as O
targets O
. O
HereR O
is O
the O
returns O
- O
to O
- O
go O
( O
i.e. O
, O
sum O
of O
future O
rewards O
) O
R O
= O
∑ O
T O
t O
′ O
= O
t O
r O
t O
′ O
where O
r O
t O
′ O
is O
the O
future O
reward O
obtained O
by O
the O
expert O
at O
step O
t O
′enabling O
models O
to O
predict O
actions O
that O
maximize O
future O
expected O
rewards O
. O
The O
architecture O
, O
pretraining O
, O
parameter O
sizes O
, O
and O
test O
inference O
are O
otherwise O
similar O
to O
the O
behavior O
cloning O
agent O
. O
Both O
the O
Behavior B-MethodName
Cloning I-MethodName
and O
Text B-MethodName
Decision I-MethodName
Transformer I-MethodName
agents O
learn O
to O
perform O
SCIENCE B-DatasetName
- I-DatasetName
WORLD I-DatasetName
tasks O
offline O
from O
demonstrations O
once O
pre O
- O
trained O
for O
scientific O
QA B-TaskName
. O
They O
use O
the O
prevailing O
paradigm O
for O
achieving O
state O
- O
of O
- O
the O
- O
art O
on O
many O
language O
benchmarks O
( O
e.g. O
, O
QA B-TaskName
( O
Khashabi O
et O
al O
. O
, O
2020 O
; O
, O
language O
understanding O
( O
Raffel O
et O
al O
. O
, O
2020 O
; O
Brown O
et O
al O
. O
, O
2020 O
) O
) O
. O

Results O
. O
Performance O
for O
all O
agents O
across O
each O
SCIENCEWORLD B-DatasetName
task O
is O
shown O
in O
Table O
2 O
. O
Overall O
, O
these O
tasks O
are O
challenging O
for O
current O
models O
, O
with O
the O
best O
model O
( O
DRRN B-MethodName
) O
achieving O
an O
average O
score O
of O
0.17 B-MetricValue
across O
all O
30 O
subtasks O
. O
Models O
that O
rely O
on O
the O
valid O
action O
detection O
aid O
generally O
perform O
better O
than O
those O
that O
learn O
to O
generate O
valid O
actions O
in O
addition O
to O
learning O
what O
actions O
to O
pick O
to O
increase O
task O
performance O
. O
All O
models O
relying O
on O
large O
language O
model O
for O
action O
selection O
( O
CALM B-MethodName
, O
BC B-MethodName
- I-MethodName
T5 I-MethodName
, O
TDT B-MethodName
- I-MethodName
T5 I-MethodName
) O
generally O
achieve O
low O
performance O
as O
they O
tend O
to O
generate O
few O
valid O
actions O
in O
their O
candidate O
action O
lists O
. O

Figure O
2 O
shows O
episode O
reward O
curves O
for O
four O
selected O
tasks O
( O
with O
reward O
curves O
for O
all O
tasks O
included O
in O
Appendix O
C O
) O
. O
These O
four O
tasks O
include O
the O
current O
best O
- O
performing O
task O
( O
Task O
4 O
- O
2 O
: O
Find O
a O
non O
- O
living O
thing O
) O
, O
which O
requires O
an O
agent O
to O
focus O
on O
any O
non O
- O
living O
thing O
in O
the O
environment O
, O
pick O
it O
up O
, O
and O
place O
it O
in O
a O
specific O
container O
( O
typically O
in O
a O
different O
location O
than O
the O
agent O
's O
starting O
location O
) O
. O
Most O
RL O
models O
quickly O
solve O
the O
majority O
of O
this O
task O
, O
but O
struggle O
with O
picking O
up O
and O
moving O
the O
object O
to O
the O
final O
container O
. O
In O
contrast O
, O
other O
more O
open O
- O
ended O
tasks O
( O
such O
as O
Task O
1 O
- O
4 O
, O
which O
requires O
agents O
to O
perform O
any O
state O
- O
of O
- O
matter O
change O
to O
a O
specific O
substance O
) O
are O
performed O
poorly O
by O
all O
models O
. O
Finally O
, O
SCIENCEWORLD B-DatasetName
includes O
pairs O
of O
identical O
tasks O
where O
one O
can O
be O
solved O
by O
retrieving O
some O
critical O
component O
of O
the O
answer O
, O
while O
the O
other O
requires O
conducting O
the O
experimental O
procedure O
successfully O
. O
For O
example O
, O
in O
Task O
3 O
- O
3 O
, O
an O
agent O
could O
look O
up O
that O
a O
metal O
fork O
is O
an O
electrical O
conductor O
and O
solve O
the O
task O
with O
comparatively O
fewer O
steps O
then O
in O
its O
paired O
Task O
3 O
- O
4 O
, O
where O
the O
substance O
name O
is O
randomly O
generated O
( O
e.g. O
, O
unknown O
substance O
B O
) O
and O
the O
experiment O
must O
be O
completed O
to O
get O
the O
answer O
. O
We O
do O
not O
yet O
* O
signifies O
that O
the O
value O
of O
131 O
M O
parameters O
includes O
the O
number O
of O
the O
parameters O
of O
the O
pre O
- O
trained O
GPT-2 B-MethodName
action O
generator O
model O
. O
Only O
6.9 O
million O
policy O
parameters O
are O
updated O
in O
RL O
training O
. O
Performance O
on O
two O
variations O
of O
a O
task O
where O
the O
agent O
must O
determine O
whether O
a O
specific O
substance O
is O
electrically O
conductive O
or O
not O
. O
In O
one O
variation O
( O
center O
- O
right O
) O
, O
the O
substance O
is O
named O
( O
e.g. O
metal O
fork O
) O
, O
while O
in O
the O
other O
variation O
( O
right O
) O
the O
substance O
is O
randomly O
generated O
( O
e.g. O
unknown O
substance O
B O
) O
. O

observe O
this O
behavior O
with O
the O
agents O
under O
examination O
. O
They O
generally O
struggle O
with O
commonsense O
level O
tasks O
( O
e.g. O
, O
navigation O
) O
and O
are O
unable O
to O
reach O
a O
point O
where O
the O
language O
models O
( O
either O
GPT-2 B-MethodName
in O
CALM B-MethodName
, O
or O
T5 B-MethodName
initialized O
with O
Macaw B-MethodName
in O
BC B-MethodName
and O
TDT B-MethodName
) O
are O
able O
to O
leverage O
their O
internal O
knowledge O
to O
solve O
these O
tasks O
through O
retrieval O
. O

Discussion O

Elementary O
science O
tasks O
are O
challenging O
for O
text O
agents O
. O
With O
top O
- O
performing O
agents O
reaching O
normalized O
average O
scores O
of O
0.17 B-MetricValue
across O
tasks O
, O
performance O
on O
SCIENCEWORLD B-DatasetName
is O
comparable O
to O
the O
current O
best O
- O
performing O
agents O
on O
mediumdifficulty O
interactive O
fiction O
games O
such O
as O
Zork O
Yao O
et O
al O
. O
, O
2021 O
) O
. O
Much O
as O
in O
interactive O
fiction O
games O
, O
examining O
agent O
trajectories O
reveals O
that O
while O
agents O
appear O
to O
struggle O
with O
science O
- O
domain O
inference O
procedures O
such O
as O
how O
to O
heat O
a O
substance O
or O
how O
to O
grow O
a O
seed O
, O
they O
also O
currently O
lack O
a O
fluency O
with O
commonsense O
skills O
such O
as O
navigating O
the O
environment O
or O
storing O
liquids O
in O
containers O
. O
This O
underscores O
the O
need O
for O
models O
that O
can O
incorporate O
commonsense O
and O
science O
- O
domain O
knowledge O
, O
and O
integrate O
that O
declarative O
knowledge O
into O
actionable O
procedures O
to O
progress O
towards O
goals O
in O
the O
environment O
. O

Larger O
models O
are O
not O
necessarily O
better O
. O
While O
larger O
models O
generally O
perform O
better O
in O
question B-TaskName
answering I-TaskName
tasks O
( O
e.g. O
, O
Raffel O
et O
al O
. O
, O
2020 O
) O
, O
here O
we O
observe O
that O
larger O
models O
do O
not O
always O
increase O
performance O
. O
Our O
best O
- O
performing O
model O
, O
the O
DRRN B-MethodName
, O
has O
only O
1.5 O
million O
parameters O
-four O
orders O
of O
magnitude O
less O
than O
the O
T5 B-MethodName
models O
. O
Both O
models O
also O
receive O
the O
same O
number O
of O
gradient O
updates O
( O
10 O
6 O
) O
with O
respect O
to O
SCIENCEWORLD B-DatasetName
training O
tasks O
- O
though O
the O
T5 B-MethodName
models O
have O
the O
added O
benefit O
of O
pre O
- O
training O
both O
from O
science O
exam O
QA B-TaskName
and O
a O
large O
number O
of O
expert O
demonstrations O
. O
4 O
This O
underscores O
that O
how O
a O
model O
approaches O
modeling O
state O
spaces O
and O
action O
sequences O
may O
be O
more O
important O
than O
the O
scope O
of O
its O
pre O
- O
training O
. O
Online O
, O
interactive O
training O
enables O
models O
such O
as O
the O
DRRN B-MethodName
and O
KG O
- O
A2C O
to O
perform O
tasks O
requiring O
long O
action O
sequences O
more O
efficiently O
in O
terms O
of O
both O
samples O
and O
parameters O
. O

Limitations O
of O
agents O
and O
environments O
. O
While O
agents O
still O
find O
text O
environments O
challenging O
, O
it O
is O
important O
to O
recognize O
that O
even O
this O
modest O
performance O
is O
achieved O
through O
a O
number O
of O
simplifying O
properties O
. O
For O
example O
, O
because O
agents O
frequently O
generate O
plausible O
but O
invalid O
actions O
, O
all O
but O
two O
agents O
benchmarked O
here O
depend O
on O
SCI B-DatasetName
- I-DatasetName
ENCEWORLD I-DatasetName
's O
valid O
action O
detection O
aid O
at O
test O
time O
, O
substantially O
simplifying O
their O
search O
problem O
in O
the O
action O
space O
. O
Similarly O
, O
while O
SCIENCE B-DatasetName
- I-DatasetName
WORLD I-DatasetName
achieves O
a O
high O
environment O
fidelity O
for O
a O
text O
simulation O
, O
this O
is O
still O
tempered O
by O
pragmatic O
concerns O
, O
such O
as O
generating O
comparatively O
short O
descriptions O
of O
environments O
that O
can O
fit O
into O
the O
sequence O
lengths O
of O
most O
transformer O
models O
. O

As O
such O
, O
even O
environments O
with O
complex O
physical O
, O
chemical O
, O
and O
biological O
processes O
underlying O
their O
simulations O
( O
such O
as O
SCIENCEWORLD B-DatasetName
) O
still O
ultimately O
must O
limit O
the O
vividness O
of O
their O
descriptions O
, O
until O
these O
technical O
limitations O
in O
modelling O
can O
be O
surpassed O
. O
Hybrid O
environments O
( O
e.g. O
, O
Shridhar O
et O
al O
. O
, O
2020 O
) O
that O
concurrently O
model O
the O
same O
environment O
as O
both O
a O
high O
- O
fidelity O
3D O
world O
and O
comparatively O
low O
- O
fidelity O
text O
- O
based O
simulation O
have O
shown O
that O
text O
environments O
can O
be O
used O
to O
provide O
useful O
task O
pre O
- O
training O
that O
can O
transfer O
back O
to O
the O
3D O
environment O
with O
relatively O
low O
simulation O
compute O
cost O
. O

Explanations O
as O
action O
sequences O
. O
Explanations O
take O
on O
a O
variety O
of O
roles O
( O
Lombrozo O
, O
2006 O
; O
Gilpin O
et O
al O
. O
, O
2018 O
) O
, O
from O
detailed O
human O
- O
readable O
descriptions O
of O
classification O
processes O
that O
describe O
how O
a O
decision O
was O
made O
( O
e.g. O
, O
Ribeiro O
et O
al O
. O
, O
2016 O
) O
, O
to O
higher O
- O
level O
appeals O
to O
scientific O
processes O
to O
explain O
why O
an O
answer O
is O
correct O
( O
e.g. O
, O
Jansen O
et O
al O
. O
, O
2018 O
; O
Dalvi O
et O
al O
. O
, O
2021 O
) O
. O
Here O
, O
the O
action O
procedures O
generated O
by O
agents O
act O
as O
manner O
explanations O
for O
how O
to O
solve O
a O
particular O
task O
-but O
while O
they O
describe O
how O
to O
accomplish O
something O
, O
they O
do O
n't O
explain O
at O
a O
high O
- O
level O
why O
those O
actions O
accomplish O
the O
task O
. O
For O
example O
, O
action O
sequences O
lack O
high O
- O
level O
goal O
information O
such O
as O
" O
melting O
a O
substance O
requires O
heating O
it O
, O
so O
first O
the O
agent O
needs O
to O
heat O
the O
substance O
with O
a O
heating O
device O
, O
like O
a O
stove O
. O
" O
. O
Similar O
to O
how O
cuing O
agents O
to O
answer O
contextual O
questions O
can O
help O
improve O
their O
task O
performance O
( O
Peng O
et O
al O
. O
, O
2021 O
) O
, O
cuing O
agents O
to O
generate O
these O
explanatory O
scaffolds O
may O
help O
future O
agents O
increase O
task O
performance O
, O
while O
structuring O
their O
action O
sequence O
explanations O
for O
better O
human O
interpretability O
. O

Conclusion O

Despite O
recent O
progress O
in O
both O
interactive O
text O
agents O
and O
scientific O
text O
processing O
via O
transformers O
, O
current O
models O
are O
unable O
to O
reason O
about O
fundamental O
science O
concepts O
in O
a O
grounded O
and O
reusable O
manner O
- O
calling O
into O
question O
how O
much O
they O
are O
actually O
understanding O
the O
tasks O
at O
hand O
. O

To O
better O
measure O
such O
scientific O
reasoning O
abilities O
, O
we O
introduce O
SCIENCEWORLD B-DatasetName
, O
an O
interactive O
text O
environment O
derived O
from O
an O
elementary O
school O
science O
curriculum O
- O
with O
tasks O
ranging O
from O
electrical O
conductivity O
to O
Mendelian O
genetics O
. O
We O
evaluate O
three O
state O
- O
of O
- O
the O
- O
art O
reinforcement O
learning O
text O
game O
agents O
: O
DRRN B-MethodName
, O
KG B-MethodName
- I-MethodName
A2C I-MethodName
, O
and O
CALM B-MethodName
; O
and O
further O
introduce O
two O
large O
- O
scale O
transformer O
- O
based O
agents O
inspired O
by O
recent O
ad O
- O
vances O
such O
as O
Behavior B-MethodName
Cloning I-MethodName
and O
the O
Decision B-MethodName
Transformer I-MethodName
and O
trained O
for O
scientific O
reasoning O
in O
SCIENCEWORLD B-DatasetName
. O
While O
we O
find O
that O
overall O
performance O
on O
unseen O
tasks O
that O
require O
using O
science O
- O
domain O
knowledge O
is O
low O
across O
all O
agents O
, O
our O
results O
also O
suggest O
that O
agents O
that O
learn O
interactively O
in O
a O
grounded O
environment O
are O
more O
sample O
and O
parameter O
efficient O
than O
large O
language O
models O
that O
learn O
offline O
by O
reading O
text O
from O
static O
sources O
. O
The O
best O
agent O
performance O
is O
still O
modest O
-and O
on O
- O
par O
with O
medium O
- O
difficulty O
interactive O
fiction O
environments O
such O
as O
Zork O
-highlighting O
the O
need O
for O
agents O
that O
can O
integrate O
declarative O
scientific O
and O
world O
knowledge O
with O
procedural O
action O
sequences O
in O
virtual O
environments O
. O

Broader O
Impacts O

Interactive O
text O
environments O
can O
provide O
a O
faster O
and O
cheaper O
alternative O
to O
3D O
environments O
to O
teach O
agents O
how O
to O
plan O
via O
sequential O
decision O
making O
. O
They O
allow O
better O
control O
over O
the O
level O
of O
abstraction O
desired O
to O
approach O
a O
task O
( O
i.e. O
, O
go O
to O
kitchen O
, O
vs. O
put O
hand O
on O
door O
's O
knob O
, O
turn O
knob O
clockwise O
, O
pull O
door O
, O
let O
go O
of O
the O
knob O
, O
walk O
through O
the O
door O
) O
. O
We O
believe O
making O
a O
plan O
in O
this O
abstract O
language O
space O
is O
simpler O
and O
more O
interpretable O
. O

With O
respect O
to O
risks O
, O
we O
consider O
this O
current O
work O
as O
exploratory O
only O
. O
ScienceWorld B-DatasetName
is O
primarily O
intended O
for O
training O
agents O
to O
learn O
reasoning O
capabilities O
in O
the O
science O
domain O
, O
with O
limited O
immediate O
utility O
to O
human O
science O
students O
. O
Agents O
trained O
on O
ScienceWorld B-DatasetName
should O
not O
be O
used O
to O
provide O
advice O
for O
the O
real O
world O
. O
The O
environment O
in O
ScienceWorld B-DatasetName
has O
been O
made O
safer O
compared O
to O
the O
real O
world O
. O
For O
instance O
, O
the O
agent O
ca O
n't O
accidentally O
burn O
itself O
while O
boiling O
a O
substance O
on O
a O
campfire O
, O
and O
its O
actions O
should O
not O
be O
taken O
as O
demonstrations O
of O
safe O
procedures O
for O
students O
. O

A.1 O
Object O
Model O

Objects O
in O
SCIENCEWORLD B-DatasetName
are O
represented O
using O
an O
object O
- O
oriented O
model O
and O
are O
implemented O
as O
classes O
. O
SCIENCEWORLD B-DatasetName
objects O
can O
be O
thought O
of O
as O
collections O
of O
sets O
of O
properties O
( O
e.g. O
, O
material O
properties O
, O
life O
properties O
, O
device O
properties O
, O
etc O
. O
) O
. O
All O
objects O
implement O
common O
functions O
, O
such O
as O
those O
that O
produce O
textual O
descriptions O
of O
the O
object O
, O
or O
that O
provide O
one O
or O
more O
possible O
referents O
for O
the O
object O
based O
on O
its O
current O
state O
( O
e.g. O
, O
the O
water O
substance O
in O
the O
solid O
state O
could O
generate O
the O
referents O
ice O
, O
solid O
water O
, O
and O
substance O
, O
each O
of O
which O
could O
be O
used O
by O
the O
agent O
to O
refer O
to O
that O
object O
in O
an O
action O
) O
. O
Similar O
to O
Z O
- O
machine O
games O
( O
Infocom O
, O
1989 O
) O
, O
objects O
are O
stored O
in O
an O
object O
tree O
representing O
the O
object O
's O
current O
container O
( O
its O
immediate O
parent O
object O
in O
the O
tree O
) O
, O
and O
any O
objects O
it O
contains O
( O
child O
nodes O
in O
the O
tree O
) O
. O

A.2 O
Environment O
and O
Objects O

SCIENCEWORLD B-DatasetName
is O
composed O
of O
a O
map O
of O
10 O
locations O
centered O
around O
a O
house O
theme O
( O
kitchen O
, O
bathroom O
, O
workshop O
, O
art O
studio O
, O
greenhouse O
, O
outside O
, O
etc O
. O
) O
, O
as O
shown O
in O
Figure O
1 O
. O
While O
the O
rooms O
and O
how O
they O
interconnect O
is O
static O
, O
the O
environment O
is O
randomly O
populated O
with O
different O
combinations O
of O
relevant O
contextual O
items O
each O
time O
it O
is O
initialized O
-for O
example O
, O
in O
one O
run O
, O
the O
living O
room O
may O
have O
a O
bookcase O
with O
three O
books O
. O
In O
other O
runs O
, O
the O
bookcase O
may O
have O
different O
books O
, O
no O
books O
, O
or O
not O
be O
present O
in O
the O
environment O
. O
This O
parametric O
variation O
discourages O
agents O
from O
memorizing O
the O
specific O
environment O
, O
and O
encourages O
robustness O
in O
task O
performance O
. O

The O
environment O
is O
populated O
with O
up O
to O
195 O
specific O
types O
of O
objects O
( O
excluding O
variations O
of O
those O
objects O
that O
change O
names O
or O
task O
properties O
, O
i.e. O
, O
red O
wires O
and O
black O
wires O
belong O
to O
the O
same O
object O
type O
) O
. O
This O
includes O
23 O
animals O
, O
11 O
plants O
, O
25 O
substances O
, O
10 O
canonical O
liquid O
containers O
( O
like O
tin O
cups O
or O
glass O
jars O
) O
, O
13 O
electrical O
components O
( O
such O
as O
light O
bulbs O
, O
motors O
, O
wires O
, O
and O
generators O
) O
, O
16 O
devices O
( O
including O
a O
stove O
, O
thermometer O
, O
and O
stopwatch O
) O
, O
and O
15 O
common O
pieces O
of O
furniture O
. O
To O
support O
these O
objects O
, O
the O
simulator O
includes O
a O
variety O
of O
other O
properties O
, O
including O
( O
for O
example O
) O
plant O
/ O
animal O
life O
cycles O
, O
and O
80 O
material O
properties O
( O
including O
water O
, O
glass O
, O
iron O
, O
and O
wood O
) O
that O
pure O
substances O
or O
physical O
objects O
( O
e.g. O
, O
tables O
) O
can O
be O
made O
from O
. O

A.3 O
Action O
Space O

The O
simulator O
implements O
25 O
actions O
, O
shown O
in O
Table O
3 O
, O
including O
generic O
actions O
common O
in O
interactive O
text O
environments O
( O
e.g. O
, O
opening O
a O
door O
, O
moving O
to O
a O
location O
) O
, O
as O
well O
as O
science O
- O
domain O
specific O
actions O
( O
e.g. O
, O
connecting O
electrical O
components O
, O
chemically O
mixing O
items O
, O
pouring O
liquids O
) O
. O
Five O
actions O
take O
two O
arguments O
, O
16 O
take O
one O
argument O
, O
and O
four O
actions O
take O
zero O
arguments O
. O
Given O
the O
approximately O
200 O
possible O
objects O
( O
excluding O
parametric O
variations O
) O
in O
SCIENCEWORLD B-DatasetName
, O
the O
action O
space O
can O
naively O
be O
estimated O
to O
be O
approximately O
200,000 O
possible O
unique O
action O
possibilities O
at O
each O
step O
, O
though O
only O
a O
small O
subset O
of O
these O
would O
be O
meaningful O
. O
Similar O
to O
the O
Jericho O
framework O
, O
the O
simulator O
can O
provide O
valid O
action O
detection O
as O
an O
aid O
to O
agents O
( O
such O
as O
the O
DRRN O
) O
that O
require O
selecting O
their O
next O
action O
from O
a O
list O
of O
possible O
known O
- O
valid O
actions O
at O
a O
given O
step O
. O

Input O
Parser O
At O
each O
step O
, O
an O
input O
parser O
attempts O
to O
parse O
user O
or O
agent O
input O
into O
a O
single O
unique O
action O
. O
Actions O
are O
specified O
as O
templates O
that O
can O
take O
on O
a O
variety O
of O
surface O
forms O
( O
e.g. O
, O
move O
to O
LOCATION O
or O
go O
to O
LOCATION O
) O
, O
and O
that O
include O
placeholders O
for O
object O
referents O
. O
At O
runtime O
, O
the O
parser O
examines O
all O
valid O
referents O
for O
visible O
objects O
from O
the O
agent O
's O
point O
of O
view O
, O
and O
if O
a O
given O
input O
string O
can O
produce O
more O
than O
one O
valid O
action O
, O
the O
parser O
will O
ask O
for O
clarification O
5 O
. O

A.4 O
Simulation O
Engines O

SCIENCEWORLD B-DatasetName
supports O
actions O
commonly O
found O
in O
interactive O
text O
environments O
-for O
example O
, O
objects O
can O
be O
moved O
or O
examined O
, O
foods O
can O
be O
eaten O
, O
and O
books O
can O
be O
read O
. O
In O
addition O
, O
the O
environment O
contains O
a O
number O
of O
elementary O
science O
- O
domain O
specific O
processes O
that O
either O
occur O
automatically O
( O
e.g. O
, O
thermodynamics O
) O
or O
are O
coupled O
to O
actions O
( O
e.g. O
, O
using O
devices O
, O
mixing O
chemicals O
) O
. O
Those O
simulation O
engines O
are O
: O

6 O

Thermodynamics O
: O
All O
objects O
have O
temperatures O
and O
other O
thermal O
properties O
based O
on O
their O
materials O
. O
All O
objects O
within O
a O
container O
are O
considered O
in O
thermal O
contact O
with O
each O
other O
, O
and O
transfer O
heat O
energy O
using O
a O
simplified O
conductive O
heat O
model O
. O
The O
proportion O
of O
heat O
transferred O
between O
objects O
at O
each O
step O
is O
mediated O
by O
the O
object O
's O
thermal O
conduction O
coefficient O
, O
allowing O
thermal O
conductors O
( O
like O
metal O
pots O
) O
and O
insulators O
( O
like O
ceramics O
) O
to O
be O
modelled O
. O
Every O
material O
has O
phase O
transition O
points O
( O
i.e. O
, O
melting O
point O
, O
boiling O
point O
) O
and O
combustion O
points O
populated O
based O
on O
the O
best O
- O
known O
or O
approximate O
physical O
values O
for O
those O
materials O
. O

Objects O
that O
move O
past O
these O
thresholds O
will O
change O
state O
of O
matter O
( O
e.g. O
, O
from O
a O
solid O
to O
a O
liquid O
) O
, O
or O
be- O
5 O
For O
example O
, O
if O
the O
agent O
is O
in O
a O
room O
with O
two O
apples O
, O
one O
on O
a O
table O
and O
one O
in O
a O
bowl O
, O
the O
command O
take O
apple O
will O
cause O
the O
parser O
to O
ask O
the O
agent O
to O
clarify O
which O
apple O
they O
mean O
by O
selecting O
possible O
alternatives O
from O
a O
numbered O
list O
. O
6 O
To O
maintain O
tractability O
in O
implementation O
, O
simulation O
engines O
are O
implemented O
with O
a O
fidelity O
at O
the O
level O
of O
an O
elementary O
science O
curriculum O
. O
Thermal O
transfer O
uses O
a O
simplified O
equation O
, O
biological O
changes O
happen O
in O
stages O
rather O
than O
gradually O
, O
only O
series O
instead O
of O
arbitrary O
electrical O
circuits O
are O
simulated O
( O
and O
without O
the O
concepts O
of O
resistance O
, O
inductance O
or O
other O
advanced O
topics O
) O
, O
etc O
. O
gin O
a O
combustion O
process O
that O
ultimately O
ends O
in O
the O
object O
turning O
to O
ash O
unless O
its O
fire O
is O
put O
out O
. O
Convective O
heat O
transfer O
is O
also O
modelled O
in O
the O
form O
of O
heat O
sources O
( O
e.g. O
, O
oven O
, O
stove O
) O
and O
heat O
sinks O
( O
e.g. O
, O
fridge O
, O
freezer O
) O
that O
transfer O
heat O
energy O
to O
or O
from O
objects O
. O
Rooms O
also O
transfer O
ambient O
heat O
energy O
to O
/ O
from O
the O
objects O
they O
contain O
. O

Electricity O
: O
The O
simulator O
models O
simple O
series O
electrical O
circuits O
, O
where O
electrically O
- O
powered O
devices O
( O
e.g. O
, O
light O
bulb O
, O
motor O
) O
can O
be O
powered O
by O
being O
connected O
to O
electrical O
energy O
sources O
( O
e.g. O
, O
battery O
, O
solar O
panel O
) O
through O
electrical O
conductors O
( O
nominally O
, O
wires O
) O
. O
Polarized O
and O
unpolarized O
components O
are O
modelled O
, O
with O
each O
object O
having O
exactly O
two O
terminals O
( O
either O
an O
anode O
and O
cathode O
for O
polarized O
components O
, O
or O
terminals O
1 O
and O
2 O
for O
unpolarized O
components O
) O
. O
Connection O
happens O
through O
explicit O
terminal O
- O
to O
- O
terminal O
connection O
actions O
( O
e.g. O
, O
connect O
battery O
anode O
to O
blue O
wire O
terminal O
1 O
) O
. O
Every O
non O
- O
electrical O
object O
in O
SCIENCEWORLD B-DatasetName
has O
virtual O
unpolarized O
terminals O
, O
allowing O
circuits O
to O
be O
build O
with O
valid O
electrical O
conductors O
( O
e.g. O
, O
using O
a O
metal O
fork O
in O
place O
of O
a O
wire O
) O
, O
and O
for O
the O
agent O
to O
build O
circuits O
that O
test O
conductivity O
by O
( O
for O
example O
) O
observing O
if O
a O
light O
bulb O
illuminates O
when O
a O
plastic O
versus O
metal O
fork O
is O
used O
in O
the O
circuit O
. O

Devices O
: O
Many O
objects O
are O
also O
considered O
devices O
, O
that O
can O
be O
activated O
or O
deactivated O
by O
the O
agent O
( O
e.g. O
, O
stove O
, O
electrical O
switch O
) O
, O
or O
may O
have O
environment O
- O
specific O
conditions O
to O
being O
activated O
( O
e.g. O
, O
a O
light O
bulb O
will O
only O
activate O
if O
it O
is O
properly O
electrically O
connected O
; O
a O
solar O
panel O
will O
only O
produce O
power O
if O
it O
is O
outside O
) O
. O
Objects O
can O
also O
be O
used O
with O
other O
objects O
in O
specific O
contexts O
( O
e.g. O
, O
a O
thermometer O
, O
to O
measure O
an O
object O
's O
temperature O
; O
a O
shovel O
, O
to O
dig O
soil O
from O
the O
ground O
) O
. O

Chemistry O
: O
A O
subset O
of O
specific O
chemical O
reactions O
are O
modelled O
, O
where O
mixing O
a O
set O
of O
substances O
together O
in O
a O
container O
will O
produce O
a O
resultant O
substance O
( O
e.g. O
, O
salt O
and O
water O
mix O
to O
produce O
salt O
water O
) O
. O
Common O
chemical O
reactions O
described O
in O
elementary O
science O
questions O
( O
e.g. O
, O
water O
reactions O
, O
rust O
, O
food O
reactions O
, O
paint O
mixing O
) O
are O
modelled O
. O

Life O
Stages O
: O
Living O
things O
( O
plants O
and O
animals O
) O
progress O
through O
life O
stages O
( O
e.g. O
, O
seed O
, O
seedling O
, O
juvenile O
plant O
, O
adult O
plant O
, O
reproducing O
plant O
, O
dead O
plant O
) O
. O
Progression O
through O
life O
stages O
happens O
over O
time O
by O
continuing O
to O
meet O
the O
needs O
of O
that O
living O
thing O
( O
e.g. O
, O
water O
, O
soil O
) O
. O
If O
the O
needs O
are O
not O
met O
( O
e.g. O
, O
a O
plant O
is O
not O
watered O
, O
is O
removed O
from O
soil O
, O
or O
becomes O
too O
hot O
) O
, O
then O
it O
dies O
. O

Reproduction O
and O
Genetics O
: O
Living O
things O
can O
have O
genes O
that O
express O
traits O
( O
e.g. O
, O
flower O
colour O
, O
seed O
shape O
, O
leaf O
size O
) O
. O
Genes O
are O
inherited O
from O
the O
alleles O
of O
both O
parents O
, O
and O
genotype O
is O
determined O
at O
the O
time O
of O
reproduction O
using O
a O
Punnett O
square O
. O
Phenotype O
( O
expressed O
, O
visible O
traits O
) O
are O
determined O
based O
on O
which O
genes O
are O
dominant O
versus O
recessive O
. O
Currently O
, O
traits O
are O
only O
populated O
for O
selected O
plants O
to O
reproduce O
Mendeliangenetics O
experiments O
. O
Plants O
reproduce O
by O
exchanging O
pollen O
( O
containing O
their O
genes O
) O
between O
flowers O
, O
typically O
by O
a O
pollinator O
( O
such O
as O
a O
bee O
) O
. O
Pollinated O
flowers O
eventually O
wilt O
and O
turn O
into O
fruits O
containing O
seeds O
of O
genetic O
descendants O
. O

Friction O
( O
Inclined O
Plane O
) O
: O
Forces O
are O
a O
significant O
part O
of O
an O
elementary O
science O
curriculum O
, O
but O
difficult O
to O
incorporate O
without O
2D O
or O
3D O
simulation O
. O
SCIENCEWORLD B-DatasetName
models O
the O
forces O
of O
gravity O
and O
friction O
in O
the O
specific O
context O
of O
1 O
- O
dimensional O
inclined O
plane O
experiments O
. O
Objects O
placed O
at O
the O
top O
of O
an O
inclined O
plane O
will O
slide O
down O
the O
plane O
at O
a O
speed O
proportional O
to O
the O
plane O
's O
angle O
, O
and O
the O
friction O
coefficient O
of O
its O
surface O
material O
. O
The O
position O
is O
described O
to O
the O
agent O
( O
e.g. O
, O
" O
an O
inclined O
plant O
with O
a O
block O
60 O
% O
of O
the O
way O
down O
the O
plane O
" O
) O
, O
allowing O
experiments O
to O
determine O
either O
the O
relative O
angle O
or O
friction O
coefficients O
of O
different O
inclined O
planes O
based O
on O
the O
speed O
the O
object O
moves O
down O
a O
given O
plane O
. O

Containers O
: O
Containers O
can O
be O
always O
open O
( O
e.g. O
, O
a O
metal O
pot O
) O
or O
closeable O
( O
e.g. O
, O
a O
cupboard O
) O
. O
Objects O
contained O
inside O
containers O
are O
not O
visible O
until O
the O
container O
is O
open O
. O
Some O
effects O
spread O
beyond O
a O
container O
-for O
example O
, O
a O
wooden O
cupboard O
with O
a O
hot O
object O
inside O
may O
combust O
, O
causing O
other O
objects O
in O
the O
kitchen O
to O
also O
increase O
temperature O
. O

B O
Tasks O
and O
Competencies O

To O
support O
our O
goal O
of O
generating O
a O
diverse O
set O
of O
tasks O
, O
we O
identified O
a O
candidate O
set O
of O
10 O
broad O
science O
exam O
topics O
from O
the O
list O
of O
400 O
fine O
- O
grained O
science O
curriculum O
topics O
of O
Xu O
et O
al O
. O
( O
2020 O
) O
. O
Topics O
were O
chosen O
that O
would O
be O
amenable O
to O
textbased O
simulation O
, O
and O
that O
did O
not O
have O
critical O
fine O
- O
grained O
spatial O
reasoning O
requirements O
. O
These O
topics O
and O
tasks O
are O
described O
in O
Section O
B.2 O
. O

Subtasks O
and O
Masked O
Objects O
: O
Each O
of O
the O
10 O
broad O
curriculum O
topics O
is O
further O
subdivided O
into O
between O
2 O
and O
4 O
specific O
subtasks O
that O
test O
specific O
reasoning O
capacities O
( O
e.g. O
, O
melting O
, O
boiling O
, O
and O
freezing O
subtasks O
for O
the O
change O
- O
of O
- O
state O
task O
) O
, O
or O
ask O
the O
agent O
to O
perform O
the O
same O
task O
but O
with O
names O
of O
critical O
task O
objects O
masked O
. O
Some O
tasks O
are O
possible O
to O
partially O
solve O
by O
looking O
up O
critical O
task O
information O
( O
e.g. O
, O
knowing O
that O
white O
flowers O
are O
a O
dominant O
trait O
of O
pea O
plants O
for O
the O
Mendelian O
genetics O
task O
) O
. O
We O
include O
two O
versions O
of O
tasks O
, O
one O
with O
using O
masked O
names O
( O
e.g. O
, O
growing O
Unknown O
Plant O
B O
instead O
of O
a O
Pea O
Plant O
) O
while O
simultaneously O
randomly O
generating O
the O
properties O
of O
those O
objects O
to O
provide O
an O
instrument O
to O
measure O
when O
agents O
are O
solving O
tasks O
by O
performing O
the O
experimental O
procedure O
, O
and O
when O
they O
are O
directly O
looking O
up O
answers O
. O

Task O
Formats O
: O
Task O
goals O
are O
structured O
with O
the O
broad O
goal O
of O
both O
( O
a O
) O
accomplishing O
a O
task O
, O
and O
( O
b O
) O
doing O
so O
intentionally O
. O
Tasks O
typically O
include O
preliminary O
subgoals O
where O
the O
agent O
must O
signal O
their O
intent O
to O
perform O
the O
task O
on O
a O
specific O
object O
by O
first O
" O
focusing O
" O
on O
the O
object O
they O
intend O
to O
perform O
the O
task O
with O
( O
e.g. O
, O
for O
a O
boiling O
task O
, O
focusing O
on O
water O
they O
intend O
to O
boil O
) O
before O
they O
perform O
the O
task O
. O

Tasks O
take O
on O
two O
main O
forms O
: O
Perform O
a O
task O
: O
the O
agent O
must O
directly O
perform O
a O
task O
, O
that O
produces O
some O
measurable O
change O
in O
the O
environment O
( O
e.g. O
, O
growing O
a O
fruit O
through O
pollination O
) O
that O
can O
be O
directly O
measured O
as O
an O
end O
- O
state O
. O
Forcedchoice O
: O
The O
agent O
must O
perform O
a O
task O
that O
requires O
making O
an O
inference O
( O
e.g. O
, O
whether O
an O
object O
is O
an O
electrical O
conductor O
or O
insulator O
) O
, O
and O
provide O
their O
answer O
by O
placing O
the O
task O
object O
in O
a O
specific O
container O
( O
i.e. O
, O
an O
" O
answer O
box O
" O
) O
if O
the O
object O
is O
conductive O
, O
and O
a O
different O
container O
if O
it O
is O
an O
insulator O
. O

Task O
Variations O
: O
To O
prevent O
overfitting O
and O
encourage O
generalization O
, O
each O
subtask O
contains O
between O
10 O
and O
1400 O
parametric O
variations O
of O
that O
subtask O
( O
with O
7200 O
total O
variations O
across O
all O
30 O
subtasks O
) O
. O
Variations O
change O
critical O
task O
objects O
( O
e.g. O
, O
the O
specific O
substance O
to O
be O
melted O
) O
, O
the O
agent O
's O
starting O
location O
in O
the O
environment O
, O
as O
well O
as O
randomly O
vary O
the O
contents O
of O
the O
environment O
itself O
( O
e.g. O
, O
whether O
the O
living O
room O
contains O
a O
bookshelf O
, O
or O
a O
painting O
, O
or O
both O
) O
. O

Task O
Simplifications O
: O
Agents O
find O
different O
competencies O
that O
SCIENCEWORLD B-DatasetName
tests O
to O
be O
chal O
- O
lenging O
. O
Tasks O
can O
be O
made O
easier O
by O
enabling O
any O
of O
5 O
environment O
simplifications O
( O
or O
choosing O
" O
easy O
" O
mode O
, O
which O
enables O
all O
the O
simplifications O
) O
. O
Examples O
of O
simplifications O
include O
a O
teleport O
action O
that O
lets O
agents O
instantly O
move O
to O
any O
location O
, O
and O
having O
all O
containers O
open O
by O
default O
. O

B.1 O
Scoring O
and O
Evaluation O
Protocol O

Goals O
and O
Reward O
Shaping O
: O
Each O
subtask O
contains O
a O
small O
number O
of O
method O
- O
agnostic O
required O
goals O
to O
be O
met O
( O
such O
as O
focusing O
on O
the O
substance O
to O
melt O
, O
and O
causing O
that O
substance O
's O
state O
of O
matter O
to O
change O
from O
a O
solid O
to O
a O
liquid O
for O
the O
melting O
task O
) O
. O
In O
addition O
, O
to O
make O
rewards O
less O
sparse O
for O
agents O
learning O
these O
tasks O
, O
each O
task O
includes O
between O
2 O
and O
15 O
optional O
subgoals O
( O
such O
as O
turning O
on O
the O
stove O
, O
or O
the O
substance O
increasing O
in O
temperature O
by O
10C O
) O
that O
help O
nudge O
agents O
in O
the O
direction O
of O
canonical O
solutions O
, O
if O
desired O
. O
Meeting O
required O
and O
optional O
subgoals O
increases O
the O
agent O
's O
score O
on O
a O
given O
subtask O
. O
Scores O
for O
all O
tasks O
are O
normalized O
to O
between O
0 B-MetricValue
and O
1 B-MetricValue
. O

B.2 O
Specific O
Tasks O

Changes O
of O
State O
: O
The O
agent O
must O
find O
a O
named O
substance O
( O
e.g. O
, O
ice O
) O
, O
and O
change O
the O
state O
of O
matter O
of O
that O
substance O
( O
solid O
, O
liquid O
, O
gas O
) O
using O
the O
heating O
and O
cooling O
devices O
( O
e.g. O
, O
stove O
, O
freezer O
) O
available O
in O
the O
environment O
. O
Subtasks O
require O
specific O
phase O
changes O
( O
melting O
, O
boiling O
, O
freezing O
, O
or O
the O
agent O
's O
choice O
) O
. O
Variations O
change O
the O
substance O
, O
and O
ablate O
common O
devices O
( O
e.g. O
, O
the O
stove O
becomes O
disabled O
) O
so O
that O
the O
agent O
must O
find O
alternate O
methods O
of O
heating O
or O
cooling O
. O

Measurement O
Instrument O
: O
The O
agent O
must O
find O
a O
thermometer O
and O
use O
it O
to O
measure O
the O
temperature O
of O
a O
named O
object O
. O
In O
two O
additional O
subtasks O
, O
the O
agent O
must O
use O
the O
thermometer O
to O
measure O
the O
melting O
point O
of O
a O
named O
substance O
by O
heating O
it O
and O
continually O
monitoring O
the O
temperature O
. O
Answers O
are O
modelled O
as O
a O
forced O
- O
choice O
task O
, O
where O
the O
agent O
is O
given O
a O
predetermined O
temperature O
threshold O
at O
the O
start O
of O
the O
task O
( O
e.g. O
, O
50 O

• O
C O
) O
, O
and O
must O
focus O
on O
one O
answer O
box O
if O
the O
melting O
point O
is O
above O
the O
threshold O
, O
and O
the O
other O
answer O
box O
if O
the O
melting O
point O
is O
below O
the O
threshold O
. O
Variations O
change O
the O
substance O
to O
be O
measured O
, O
and O
the O
preset O
temperature O
threshold O
. O

Electrical O
Circuits O
: O
The O
agent O
must O
build O
a O
working O
series O
electrical O
circuit O
by O
connecting O
various O
electrical O
components O
including O
power O
sources O
( O
e.g. O
, O
battery O
, O
wind O
mill O
, O
gas O
generator O
) O
, O
different O
coloured O
wires O
, O
and O
active O
components O
( O
e.g. O
, O
lights O
, O
motors O
) O
. O
Subtasks O
include O
( O
a O
) O
powering O
a O
named O
component O
, O
( O
b O
) O
powering O
using O
renewable O
versus O
nonrenewable O
energy O
, O
and O
( O
c O
) O
measuring O
whether O
named O
or O
unknown O
substances O
are O
electrically O
conductive O
. O
Variations O
change O
available O
components O
, O
colours O
of O
wire O
, O
and O
specific O
task O
objects O
required O
to O
be O
used O
in O
the O
circuit O
. O

Classification O
: O
In O
four O
subtasks O
, O
the O
agent O
must O
find O
an O
object O
in O
the O
environment O
belonging O
to O
a O
specific O
category O
( O
living O
things O
, O
non O
- O
living O
things O
, O
plants O
, O
or O
animals O
) O
, O
and O
place O
it O
in O
an O
answer O
box O
. O

Variations O
change O
the O
location O
and O
description O
of O
the O
answer O
box O
. O

Growing O
plants O
: O

The O
agent O
must O
grow O
a O
named O
plant O
( O
e.g. O
, O
a O
peach O
tree O
) O
from O
seed O
. O
To O
do O
this O
, O
they O
must O
place O
the O
seed O
and O
soil O
in O
a O
flower O
pot O
, O
and O
provide O
regular O
water O
as O
the O
plant O
progresses O
through O
life O
stages O
into O
adulthood O
. O
Failure O
to O
water O
appropriately O
causes O
the O
plant O
to O
perish O
. O
In O
a O
subtask O
, O
the O
agent O
must O
grow O
a O
fruit O
by O
growing O
several O
plants O
, O
then O
releasing O
pollinators O
( O
i.e. O
, O
bees O
) O
that O
cross O
- O
pollinate O
flowers O
on O
the O
plants O
, O
which O
will O
eventually O
produce O
fruits O
. O
Variations O
change O
seed O
type O
, O
and O
soil O
location O
( O
either O
provided O
in O
the O
pot O
, O
provided O
in O
the O
room O
, O
or O
must O
be O
gathered O
outside O
using O
a O
shovel O
) O
. O

Chemistry O
: O

The O
agent O
must O
create O
a O
specific O
substance O
by O
mixing O
two O
or O
more O
input O
substances O
in O
a O
container O
. O
In O
the O
generic O
subtask O
, O
a O
recipe O
document O
that O
can O
be O
read O
by O
the O
agent O
is O
provided O
somewhere O
in O
the O
environment O
. O
In O
two O
paint O
- O
themed O
subtasks O
, O
the O
agent O
is O
given O
primary O
colours O
of O
paint O
( O
red O
, O
green O
, O
yellow O
) O
, O
and O
must O
mix O
secondary O
( O
e.g. O
, O
orange O
) O
or O
tertiary O
( O
e.g. O
, O
orange O
- O
yellow O
) O
colours O
through O
several O
steps O
. O
Variations O
change O
the O
required O
output O
substance O
. O

Life O
Spans O
: O
In O
three O
task O
variations O
, O
the O
agent O
must O
find O
3 O
animals O
in O
the O
environment O
, O
then O
select O
either O
the O
shortest O
- O
lived O
( O
e.g. O
, O
bee O
) O
, O
longest O
- O
lived O
( O
e.g. O
, O
turtle O
) O
, O
or O
shortest O
- O
then O
- O
longest O
lived O
( O
beethen O
- O
turtle O
) O
. O
Variations O
change O
which O
animals O
are O
populated O
in O
the O
environment O
from O
a O
list O
of O
long O
, O
medium O
, O
and O
short O
- O
lived O
animals O
. O

Life O
Stages O
: O
The O
agent O
must O
find O
a O
named O
plant O
or O
animal O
, O
and O
focus O
on O
its O
life O
stages O
, O
from O
earliest O
( O
e.g. O
, O
seed O
) O
to O
latest O
( O
e.g. O
, O
reproducing O
adult O
plant O
) O
. O

Variations O
change O
the O
plant O
or O
animal O
involved O
. O

Forces O
: O
The O
agent O
must O
use O
inclined O
planes O
and O
masses O
( O
e.g. O
, O
a O
block O
) O
for O
experiments O
about O
forces O
. O

In O
one O
subtask O
the O
agent O
is O
given O
two O
planes O
, O
and O
must O
determine O
which O
has O
the O
steepest O
or O
shallowest O
angle O
based O
on O
the O
time O
the O
block O
takes O
to O
move O
down O
the O
plane O
. O
In O
two O
other O
subtasks O
, O
the O
agent O
must O
find O
which O
of O
two O
planes O
has O
a O
surface O
of O
highest O
or O
least O
friction O
, O
from O
either O
named O
( O
e.g. O
, O
plastic O
, O
steel O
) O
or O
unknown O
surfaces O
. O
The O
agent O
can O
measure O
time O
internally O
( O
in O
terms O
of O
number O
of O
steps O
for O
a O
block O
to O
fall O
) O
, O
or O
measure O
this O
explicitly O
with O
a O
provided O
stopwatch O
. O
Variations O
change O
inclined O
plane O
angles O
( O
first O
task O
) O
or O
surface O
material O
types O
( O
remaining O
tasks O
) O
. O

Mendelian O
Genetics O
: O
The O
agent O
must O
determine O
whether O
a O
named O
trait O
( O
e.g. O
, O
white O
flower O
colour O
) O
is O
a O
dominant O
or O
recessive O
trait O
in O
a O
plant O
. O
Two O
seeds O
are O
provided O
( O
one O
with O
the O
trait O
as O
dominant O
, O
one O
recessive O
) O
, O
and O
the O
agent O
must O
grow O
two O
generations O
of O
plants O
and O
count O
how O
often O
it O
observes O
a O
given O
trait O
in O
successive O
generations O
to O
determine O
whether O
it O
is O
dominant O
or O
recessive O
. O
Subtasks O
change O
whether O
the O
plant O
is O
known O
( O
the O
pea O
plant O
, O
as O
in O
Mendel O
's O
experiments O
) O
or O
a O
randomly O
generated O
plant O
, O
while O
variations O
change O
the O
trait O
under O
investigation O
. O

B.3 O
Commonsense O
Competencies O

In O
addition O
to O
science O
- O
domain O
competencies O
, O
the O
agent O
must O
demonstrate O
fluency O
with O
commonsense O
knowledge O
and O
procedures O
to O
complete O
tasks O
successfully O
. O
Agents O
must O
know O
the O
locations O
of O
common O
objects O
( O
e.g. O
, O
water O
comes O
from O
a O
sink O
, O
orange O
juice O
is O
typically O
found O
in O
a O
fridge O
) O
, O
the O
affordances O
of O
common O
objects O
( O
a O
sink O
can O
be O
turned O
on O
to O
create O
water O
, O
a O
cup O
can O
be O
used O
to O
carry O
a O
liquid O
) O
, O
navigation O
( O
a O
world O
is O
made O
of O
discrete O
rooms O
that O
can O
be O
traversed O
through O
doors O
) O
, O
containers O
need O
to O
be O
opened O
to O
observe O
or O
use O
their O
contents O
, O
and O
so O
forth O
. O

B.4 O
Scoring O
and O
Evaluation O
Protocol O

Goals O
and O
Reward O
Shaping O
: O
Each O
subtask O
contains O
a O
small O
number O
of O
method O
- O
agnostic O
required O
goals O
to O
be O
met O
( O
such O
as O
focusing O
on O
the O
substance O
to O
melt O
, O
and O
causing O
that O
substance O
's O
state O
of O
matter O
to O
change O
from O
a O
solid O
to O
a O
liquid O
for O
the O
melting O
task O
) O
. O
In O
addition O
, O
to O
make O
rewards O
less O
sparse O
for O
agents O
learning O
these O
tasks O
, O
each O
task O
includes O
between O
2 O
and O
15 O
optional O
subgoals O
( O
such O
as O
turning O
on O
the O
stove O
, O
or O
the O
substance O
increasing O
in O
temperature O
by O
10C O
) O
that O
help O
nudge O
agents O
in O
the O
direction O
of O
canonical O
solutions O
, O
if O
desired O
. O
Meeting O
required O
and O
optional O
subgoals O
increases O
the O
agent O
's O
score O
on O
a O
given O
subtask O
. O
Scores O
for O
all O
tasks O
are O
normalized O
to O
between O
0 B-MetricValue
and O
1 B-MetricValue
. O
We O
did O
evaluation O
on O
the O
test O
variations O
every O
1000 O
steps O
per O
environment O
thread O
. O
We O
randomly O
chose O
10 O
test O
variations O
and O
run O
1 O
episode O
of O
testing O
for O
each O
chosen O
variation O
during O
each O
evaluation O
period O
and O
reported O
the O
average O
score O
of O
the O
10 O
% O
test O
steps O
scores O
. O

DRRN B-MethodName
: O

We O
use O
the O
DRRN B-MethodName
architecture O
from O
https O
: O
/ O
/ O
github.com O
/ O
microsoft O
/ O
tdqn O
, O
with O
embedding B-HyperparameterName
size I-HyperparameterName
and O
hidden B-HyperparameterName
size I-HyperparameterName
set O
as O
128 B-HyperparameterValue
. O
The O
learning B-HyperparameterName
rate I-HyperparameterName
we O
use O
to O
train O
DRRN B-MethodName
is O
0.0001 B-HyperparameterValue
. O
The O
memory B-HyperparameterName
size I-HyperparameterName
is O
100k B-HyperparameterValue
, O
and O
priority B-HyperparameterName
fraction I-HyperparameterName
is O
0.50 B-HyperparameterValue
. O
The O
tokenizer O
for O
the O
input O
text O
is O
a O
uni O
- O
gram O
subword O
tokenizer O
model O
adapted O
from O
Kudo O
( O
2018 O
) O
. O

KG B-MethodName
- I-MethodName
A2C I-MethodName
: O

We O
make O
two O
major O
changes O
to O
the O
original O
KG B-MethodName
- I-MethodName
A2C I-MethodName
model O
to O
function O
with O
SCIENCE B-DatasetName
- I-DatasetName
WORLD I-DatasetName
. O
( O
1 O
) O
We O
replace O
the O
OpenIE O
knowledge O
graph O
extractor O
with O
a O
heuristic O
extractor O
. O
The O
heuristic O
extractor O
uses O
regular O
expressions O
to O
parse O
the O
text O
of O
the O
" O
look O
around O
" O
and O
" O
agent O
inventory O
" O
information O
into O
( O
subject O
, O
relation O
, O
object O
) O
triples O
. O
This O
heuristic O
functionally O
extracts O
the O
ground O
truth O
knowledge O
graph O
representation O
of O
the O
observable O
environment O
for O
the O
agent O
. O
( O
2 O
) O
We O
change O
the O
KG O
- O
A2C O
agent O
to O
generate O
object O
types O
instead O
of O
references O
to O
specific O
objects O
. O
After O
selecting O
the O
action O
template O
and O
object O
type O
fillers O
that O
the O
agent O
has O
the O
highest O
confidence O
in O
, O
we O
ground O
those O
object O
types O
( O
e.g. O
apple O
) O
with O
specific O
object O
referents O
in O
the O
agent O
's O
current O
visible O
environment O
. O
If O
more O
than O
one O
referent O
meets O
that O
type O
, O
one O
is O
chosen O
at O
random O
. O
The O
learning B-HyperparameterName
rate I-HyperparameterName
we O
use O
to O
train O
the O
KG B-MethodName
- I-MethodName
A2C I-MethodName
agent O
is O
0.003 B-HyperparameterValue
and O
the O
tokenizer O
used O
is O
the O
same O
as O
the O
DRRN B-MethodName
agent O
. O

CALM B-MethodName
- I-MethodName
GPT2 I-MethodName
: O
Following O
the O
original O
CALM B-MethodName
paper O
( O
Yao O
et O
al O
. O
, O
2020 O
) O
, O
we O
use O
a O
12 B-HyperparameterValue
- O
layer B-HyperparameterName
, O
768hidden B-HyperparameterValue
, O
12 B-HyperparameterValue
- O
head B-HyperparameterName
GPT-2 B-MethodName
model O
. O
We O
use O
the O
default O
pre O
- O
trained O
weight O
offered O
by O
the O
Huggingface O
Transformers O
library O
( O
Wolf O
et O
al O
. O
, O
2020 O
) O
. O
We O
fine O
- O
tune O
this O
GPT-2 B-MethodName
model O
on O
complete O
action O
sequences O
generated O
by O
the O
oracle O
agents O
. O
The O
GPT-2 B-MethodName
input O
prompt O
is O
formed O
as O
" O

[ O
CLS O
] O
d O
[ O
SEP O
] O
o O
t O
[ O
SEP O
] O
o O
look O
t O
[ O
SEP O
] O
o O
inv O
t O
[ O
SEP O
] O
o O
t−1 O
[ O
SEP O
] O
a O
t−1 O
[ O
SEP O
] O

" O
and O
targets O
to O
predict O
a O
t O
, O
where O
d O
stands O
for O
the O
task O
description O
and O
o O
t O
, O
o O
look O
t O
, O
o O
inv O
t O
, O
and O
a O
t O
stand O
for O
the O
observation O
( O
excluding O
the O
look O
around O
and O
inventory O
information O
) O
, O
the O
output O
of O
a O
" O
look O
around O
" O
action O
at O
the O
agent O
's O
current O
location O
, O
the O
agent O
inventory O
, O
and O
the O
action O
at O
time O
step O
t. O
We O
use O
beam O
search O
for O
generation O
, O
generating O
16 O
beams O
representing O
candidate O
actions O
for O
the O
agent O
to O
select O
from O
. O
We O
set O
the O
diversity B-HyperparameterName
penalty I-HyperparameterName
to O
50.0 B-HyperparameterValue
to O
encourage O
the O
GPT-2 B-MethodName
model O
to O
generate O
different O
actions O
. O
For O
the O
GPT-2 B-MethodName
training O
we O
use O
a O
batch B-HyperparameterName
size I-HyperparameterName
of O
12 B-HyperparameterValue
and O
train O
for O
20 B-HyperparameterValue
epochs B-HyperparameterName
with O
a O
learning B-HyperparameterName
rate I-HyperparameterName
of O
0.00002 B-HyperparameterValue
. O
The O
learning B-HyperparameterName
rate I-HyperparameterName
we O
use O
to O
train O
the O
CALM B-MethodName
agent O
is O
0.0001 B-HyperparameterValue
and O
the O
input O
tokenizer O
is O
the O
same O
as O
that O
used O
in O
the O
original O
GPT-2 B-MethodName
paper O
. O

Due O
to O
the O
high O
computation O
cost O
of O
the O
CALM B-MethodName
model O
, O
and O
modest O
overall O
performance O
, O
performance O
for O
each O
task O
is O
the O
average O
of O
only O
3 B-HyperparameterValue
random O
seeds B-HyperparameterName
instead O
of O
the O
5 B-HyperparameterValue
used O
for O
training O
the O
DRRN B-MethodName
and O
KGA2C B-MethodName
models O
. O
During O
development O
, O
a O
small O
error O
was O
found O
in O
the O
prompt O
. O
Pilot O
experiments O
suggested O
this O
resulted O
in O
a O
negligible O
( O
±0.01 O
) O
change O
in O
performance O
, O
so O
the O
full O
experiments O
( O
requiring O
up O
to O
6000 O
GPU O
hours O
) O
were O
not O
regenerated O
. O

Episode O
reward O
curves O
: O
Episode O
reward O
curves O
for O
each O
model O
across O
all O
30 O
subtasks O
in O
SCIENCE B-DatasetName
- I-DatasetName
WORLD I-DatasetName
are O
shown O
in O
Figure O
3 O
. O

C.2 O
Behavior B-MethodName
Cloning I-MethodName
and O
Text B-MethodName
Decision I-MethodName
Transformer I-MethodName

The O
T5 B-MethodName
models O
used O
to O
train O
both O
of O
these O
models O
are O
initialized O
with O
weights O
and O
tokenizers O
from O
the O
trained O
Macaw-11b B-MethodName
model O
released O
at O
https O
: O
/ O
/ O
github.com O
/ O
allenai O
/ O
macaw O
. O
They O
are O
trained O
on O
a O
v3 O
- O
32 O
TPU O
pod O
with O
a O
batch B-HyperparameterName
size I-HyperparameterName
of O
16 B-HyperparameterValue
and O
32 O
- O
way O
model O
parallelism O
for O
100k B-HyperparameterValue
gradient O
update O
steps B-HyperparameterName
. O

At O
inference O
time O
, O
we O
use O
the O
model O
to O
generate O
actions O
given O
the O
observation O
of O
current O
and O
previous O
step O
. O
We O
use O
beam O
search O
with O
a O
beam B-HyperparameterName
size I-HyperparameterName
16 B-HyperparameterValue
to O
get O
the O
top O
16 O
generations O
. O
We O
set O
the O
diversity B-HyperparameterName
penalty I-HyperparameterName
to O
50.0 B-HyperparameterValue
to O
encourage O
diversity O
in O
generation O
. O
For O
each O
subtask O
, O
we O
run O
one O
episode O
on O
each O
of O
its O
test O
variations O
and O
report O
the O
average O
score O
. O
We O
do O
not O
update O
weights O
of O
the O
T5 B-MethodName
model O
during O
evaluation O
. O
size O
across O
all O
tasks O
and O
random O
seeds O
. O
For O
SCIENCEWORLD B-DatasetName
tasks O
, O
larger O
models O
do O
not O
necessarily O
perform O
better O
, O
and O
in O
some O
cases O
appear O
to O
show O
inverse O
scaling O
. O
* O
signifies O
that O
the O
value O
of O
131 O
M O
parameters O
includes O
the O
number O
of O
the O
parameters O
of O
the O
pre O
- O
trained O
GPT-2 B-MethodName
action O
generator O
model O
. O
Only O
6.9 O
million O
policy O
parameters O
are O
updated O
in O
RL O
training O
. O

C.3 O
Resources O

To O
examine O
the O
effect O
of O
model O
size O
on O
behavior O
cloned O
and O
decision O
transformer O
model O
performance O
, O
we O
ran O
two O
model O
sizes O
for O
the O
T5 B-MethodName
models O
, O
shown O
in O
Table O
5 O
. O
We O
first O
observe O
that O
pre O
- O
training O
specifically O
for O
scientific O
question O
answering O
on O
a O
curated O
dataset O
( O
Macaw B-MethodName
) O
outperforms O
T5 B-MethodName
general O
language O
model O
pre O
- O
training O
. O
Further O
, O
we O
note O
that O
T5 B-MethodName
- I-MethodName
Large I-MethodName
and O
Macaw B-MethodName
- I-MethodName
Large I-MethodName
, O
with O
14 O
times O
fewer O
parameters O
( O
770 O
m O
each O
) O
, O
out O
- O
perform O
the O
larger O
11 O
billion O
parameter O
models O
by O
approximately O
a O
factor O
of O
two O
. O
These O
results O
suggest O
that O
while O
SCI B-DatasetName
- I-DatasetName
ENCEWORLD I-DatasetName
can O
benefit O
from O
external O
scientific O
knowledge O
, O
it O
may O
also O
present O
an O
inverse O
scaling O
problem O
7 O
, O
where O
increasing O
model O
size O
can O
sometimes O
decrease O
overall O
task O
performance O
. O
However O
, O
these O
results O
are O
only O
suggestive O
of O
an O
inverse O
scaling O
problem O
rather O
than O
a O
concrete O
demonstration O
. O
Due O
to O
the O
high O
cost O
of O
training O
and O
inference O
for O
these O
models O
, O
we O
ca O
n't O
currently O
rule O
out O
that O
these O
differences O
may O
be O
due O
to O
hyperparameter O
differences O
. O
We O
leave O
this O
verification O
as O
future O
work O
. O

C.5 O
Attribution O

Graphical O
visualization O
makes O
use O
of O
RPG O
game O
assets O
developed O
by O
@ O
Noiracide O
. O

Acknowledgements O

This O
work O
supported O
in O
part O
by O
National O
Science O
Foundation O
( O
NSF O
) O
award O
# O
1815948 O
to O
PJ O
, O
Google O
Cloud O
Compute O
, O
and O
the O
Allen O
Institute O
for O
AI O
. O
The O
authors O
would O
also O
like O
to O
thank O
Liwei O
Jiang O
, O
Jack O
Hessel O
, O
and O
Oyvind O
Tafjord O
for O
their O
very O
timely O
technical O
assistance O
and O
advice O
, O
giving O
us O
the O
ability O
to O
train O
our O
larger O
, O
transformer O
- O
based O
agents O
. O
We O
'd O
also O
like O
to O
thank O
Michal O
Guerquin O
for O
technical O
assistance O
with O
the O
project O
website O
. O

KaFSP B-MethodName
: O
Knowledge B-MethodName
- I-MethodName
Aware I-MethodName
Fuzzy I-MethodName
Semantic I-MethodName
Parsing I-MethodName
for O
Conversational B-TaskName
Question I-TaskName
Answering I-TaskName
over O
a O
Large O
- O
Scale O
Knowledge O
Base O

In O
this O
paper O
, O
we O
study O
two O
issues O
of O
semantic O
parsing O
approaches O
to O
conversational B-TaskName
question I-TaskName
answering I-TaskName
over O
a O
large O
- O
scale O
knowledge O
base O
: O
( O
1 O
) O
The O
actions O
defined O
in O
grammar O
are O
not O
sufficient O
to O
handle O
uncertain O
reasoning O
common O
in O
real O
- O
world O
scenarios O
. O
( O
2 O
) O
Knowledge O
base O
information O
is O
not O
well O
exploited O
and O
incorporated O
into O
semantic O
parsing O
. O
To O
mitigate O
the O
two O
issues O
, O
we O
propose O
a O
knowledge B-MethodName
- I-MethodName
aware I-MethodName
fuzzy I-MethodName
semantic I-MethodName
parsing I-MethodName
framework O
( O
KaFSP B-MethodName
) O
. O
It O
defines O
fuzzy O
comparison O
operations O
in O
the O
grammar O
system O
for O
uncertain O
reasoning O
based O
on O
the O
fuzzy O
set O
theory O
. O
In O
order O
to O
enhance O
the O
interaction O
between O
semantic O
parsing O
and O
knowledge O
base O
, O
we O
incorporate O
entity O
triples O
from O
the O
knowledge O
base O
into O
a O
knowledgeaware O
entity O
disambiguation O
module O
. O
Additionally O
, O
we O
propose O
a O
multi B-TaskName
- I-TaskName
label I-TaskName
classification I-TaskName
framework O
to O
not O
only O
capture O
correlations O
between O
entity O
types O
and O
relations O
but O
also O
detect O
knowledge O
base O
information O
relevant O
to O
the O
current O
utterance O
. O
Both O
enhancements O
are O
based O
on O
pre O
- O
trained O
language O
models O
. O
Experiments O
on O
a O
large O
- O
scale O
conversational O
question O
answering O
benchmark O
demonstrate O
that O
the O
proposed O
KaFSP B-MethodName
achieves O
significant O
improvements O
over O
previous O
state O
- O
of O
- O
the O
- O
art O
models O
, O
setting O
new O
SOTA O
results O
on O
8 O
out O
of O
10 O
question O
types O
, O
gaining O
improvements O
of O
over O
10 B-MetricValue
% I-MetricValue
F1 B-MetricName
or O
accuracy B-MetricName
on O
3 O
question O
types O
, O
and O
improving O
overall O
F1 B-MetricName
from O
83.01 B-MetricValue
% I-MetricValue
to O
85.33 B-MetricValue
% I-MetricValue
. O
The O
source O
code O
of O
KaFSP B-MethodName
is O
available O
at O
https O
: O
/ O
/ O
github.com O
/ O
tjunlp O
- O
lab O
/ O
KaFSP O
. O

Introduction O

With O
the O
growing O
popularity O
of O
intelligent O
virtual O
assistants O
( O
e.g. O
, O
Alexa O
, O
Siri O
, O
Cortana O
) O
and O
the O
availability O
of O
large O
- O
scale O
knowledge O
bases O
( O
e.g. O
, O
DBPedia O
( O
Auer O
et O
al O
. O
, O
2007 O
) O
, O
Wikidata O
( O
Vrandečić O
and O
Krötzsch O
, O
2014 O
) O
, O
YAGO O
( O
Rebele O
et O
al O
. O
, O
2016 O
) O
) O
, O
conversational B-TaskName
question I-TaskName
answering I-TaskName
( O
QA B-TaskName
) O
over O
knowledge O
bases O
( O
KB O
) O
has O
attracted O
broad O
interests O
. O
It O
aims O
to O
satisfy O
users O
' O
information O
needs O
by O
retrieving O
answers O
from O
a O
given O
knowledge O
graph O
to O
users O
' O
questions O
in O
a O
multi O
- O
turn O
conversational O
setting O
with O
a O
wide O
range O
of O
discourse O
phenomena O
( O
e.g. O
, O
ellipsis O
, O
coreference O
, O
lexical O
cohesion O
) O
. O

While O
conversational B-TaskName
QA I-TaskName
over O
large O
- O
scale O
KBs O
can O
be O
realized O
without O
explicit O
semantic O
parsing O
( O
e.g. O
, O
HRED B-MethodName
- I-MethodName
KVM I-MethodName
( O
Kacupaj O
et O
al O
. O
, O
2021 O
) O
) O
, O
the O
majority O
of O
effort O
is O
dedicated O
to O
the O
exploration O
of O
contextual O
semantic O
parsers O
( O
Guo O
et O
al O
. O
, O
2018 O
; O
Shen O
et O
al O
. O
, O
2019 O
; O
Thirukovalluru O
et O
al O
. O
, O
2021 O
; O
Kacupaj O
et O
al O
. O
, O
2021 O
; O
Lan O
and O
Jiang O
, O
2021 O
) O
. O
The O
semantic O
parsing O
based O
approaches O
usually O
project O
an O
utterance O
into O
a O
logical O
form O
that O
can O
be O
executed O
on O
a O
given O
knowledge O
base O
. O
Early O
semantic O
parsing O
method O
D2A B-MethodName
( O
Guo O
et O
al O
. O
, O
2018 O
) O
suffers O
from O
the O
stepwise O
error O
propagation O
issue O
, O
which O
is O
improved O
by O
MaSP B-MethodName
( O
Shen O
et O
al O
. O
, O
2019 O
) O
that O
jointly O
learns O
pointer O
- O
equipped O
semantic O
parsing O
and O
typeaware O
entity O
detection O
in O
a O
multi O
- O
task O
learning O
framework O
. O
The O
very O
recent O
work O
LASAGNE B-MethodName
( O
Kacupaj O
et O
al O
. O
, O
2021 O
) O
further O
enhances O
MaSP B-MethodName
via O
a O
graph O
attention O
network O
that O
exploits O
the O
correlation O
( O
missing O
in O
MaSP O
) O
between O
entity O
types O
and O
relations O
and O
achieves O
the O
state O
- O
of O
- O
the O
- O
art O
results O
on O
the O
CSQA B-DatasetName
benchmark O
( O
Saha O
et O
al O
. O
, O
2018 O
) O
. O

Despite O
the O
aforementioned O
progress O
, O
we O
argue O
that O
current O
semantic O
parsing O
approaches O
to O
conversational B-TaskName
QA I-TaskName
over O
large O
- O
scale O
KBs O
still O
suffer O
from O
two O
critical O
issues O
. O
First O
, O
grammar O
rules O
that O
form O
the O
base O
for O
the O
mapping O
of O
questions O
to O
logical O
forms O
, O
although O
being O
constantly O
updated O
in O
D2A B-MethodName
, O
MaSP B-MethodName
, O
and O
LASAGNE B-MethodName
, O
are O
still O
not O
sufficient O
to O
cover O
all O
real O
- O
world O
situations O
, O
e.g. O
, O
fuzzy O
inference O
on O
numbers O
. O
Consider O
the O
question O
" O
Which O
nutrients O
can O
interact O
with O
approximately O
89 O
chemical O
substances O
and O
drugs O
? O
" O
. O
It O
is O
difficult O
for O
existing O
grammar O
to O
represent O
" O
approximately O
89 O
" O
. O
Second O
, O
the O
interaction O
between O
questions O
and O
knowledge O
base O
is O
not O
adequate O
for O
entity O
disambiguation O
and O
redundancy O
detection O
in O
semantic O
parsing O
. O
For O
the O
question O
" O
Which O
educational O
institution O
is O
the O
alma O
mater O
of O
Pierre O
Lefebvre O
? O
" O
, O
without O
using O
relevant O
information O
from O
KB O
, O
it O
is O
difficult O
for O
semantic O
parsing O
to O
distinguish O
whether O
" O
Pierre O
Lefebvre O
" O
is O
a O
French O
military O
physician O
or O
a O
French O
politician O
as O
more O
than O
one O
persons O
named O
" O
Pierre O
Lefebvre O
" O
are O
in O
the O
knowledge O
base O
. O

To O
address O
these O
two O
issues O
, O
we O
propose O
a O
Knowledge B-MethodName
- I-MethodName
aware I-MethodName
Fuzzy I-MethodName
Semantic I-MethodName
Parsing I-MethodName
( O
KaFSP B-MethodName
) O
model O
to O
enhance O
both O
grammar O
rules O
and O
the O
interaction O
between O
KB O
and O
semantic O
parsing O
. O
Particularly O
, O
we O
introduce O
fuzzy O
operations O
into O
the O
grammar O
system O
used O
in O
previous O
work O
, O
enabling O
the O
system O
to O
perform O
uncertainty O
reasoning O
on O
numbers O
. O
Such O
updates O
have O
a O
significant O
impact O
on O
answering O
quantitative O
and O
comparative O
questions O
. O
In O
order O
to O
make O
the O
knowledge O
base O
well O
facilitate O
semantic O
parsing O
, O
we O
incorporate O
deep O
entity O
knowledge O
in O
the O
given O
knowledge O
base O
into O
different O
modules O
in O
the O
proposed O
semantic O
parsing O
framework O
. O
In O
the O
entity O
disambiguation O
module O
, O
entity O
triples O
from O
the O
knowledge O
base O
are O
exploited O
to O
disambiguate O
candidate O
entities O
. O
In O
the O
entity O
type O
and O
relation O
prediction O
module O
, O
a O
multi O
- O
label O
classification O
framework O
is O
proposed O
to O
capture O
correlations O
between O
entity O
types O
and O
relations O
and O
to O
pinpoint O
KB O
information O
relevant O
to O
the O
current O
utterance O
. O

Contributions O
Our O
main O
contributions O
are O
as O
follows O
: O

• O
We O
propose O
a O
knowledge B-MethodName
- I-MethodName
aware I-MethodName
fuzzy I-MethodName
semantic I-MethodName
parsing I-MethodName
framework O
for O
conversational B-TaskName
QA I-TaskName
over O
large O
- O
scale O
KBs O
, O
which O
enables O
the O
grammar O
system O
to O
model O
uncertainty O
reasoning O
based O
on O
the O
fuzzy O
set O
theory O
, O
and O
enhances O
the O
interaction O
between O
KB O
and O
semantic O
parsing O
with O
two O
knowledge O
- O
aware O
modules O
. O

• O
Experiment O
results O
demonstrate O
that O
our O
proposed O
model O
achieves O
new O
state O
- O
of O
- O
the O
- O
art O
results O
on O
8 O
out O
of O
10 O
question O
types O
on O
the O
CSQA B-DatasetName
dataset O
( O
Saha O
et O
al O
. O
, O
2018 O
) O
, O
which O
is O
to O
date O
the O
largest O
dataset O
for O
complex O
conversational B-TaskName
question I-TaskName
answering I-TaskName
over O
a O
large O
- O
scale O
knowledge O
base O
. O

Related O
Work O

Semantic O
parsing O
approaches O
have O
conventionally O
been O
used O
for O
knowledge B-TaskName
base I-TaskName
question I-TaskName
answering I-TaskName
( O
KBQA B-TaskName
) O
. O
Early O
efforts O
parse O
natural O
language O
questions O
into O
logical O
forms O
typically O
via O
dictionary O
- O
based O
parsers O
or O
similarity O
models O
( O
Wong O
and O
Mooney O
, O
2007 O
; O
Collins O
, O
2007 O
, O
2009 O
; O
Kwiatkowski O
et O
al O
. O
, O
2011 O
; O
Andreas O
et O
al O
. O
, O
2013 O
; O
Artzi O
and O
Zettlemoyer O
, O
2013 O
; O
Reddy O
et O
al O
. O
, O
2014 O
; O
Zhao O
and O
Huang O
, O
2015 O
; O
Dubey O
et O
al O
. O
, O
2016 O
; O
Long O
et O
al O
. O
, O
2016 O
) O
. O

Recent O
years O
have O
witnessed O
that O
semantic O
parsing O
has O
been O
shifted O
from O
traditional O
statistical O
models O
with O
feature O
engineering O
to O
neural O
approaches O
that O
learn O
continuous O
representations O
for O
generating O
logical O
forms O
( O
Yih O
et O
al O
. O
, O
2014 O
; O
Jia O
and O
Liang O
, O
2016 O
; O
Xiao O
et O
al O
. O
, O
2016 O
; O
Bao O
et O
al O
. O
, O
2016 O
; O
Lapata O
, O
2018 O
, O
2016 O
; O
Bhutani O
et O
al O
. O
, O
2020 O
; O
Jiang O
, O
2020 O
, O
2021 O
) O
. O
For O
example O
, O
Dong O
and O
Lapata O
( O
2016 O
) O
use O
the O
encoder O
- O
decoder O
framework O
equipped O
with O
a O
neural O
attention O
mechanism O
to O
cast O
semantic O
parsing O
into O
Seq2Seq B-MethodName
generation O
. O

As O
knowledge O
bases O
are O
becoming O
large O
, O
semantic O
parsing O
for O
KBQA B-TaskName
is O
usually O
performed O
in O
a O
stepwise O
, O
modular O
framework O
. O
Guo O
et O
al O
. O
( O
2018 O
) O
recognize O
entities O
in O
questions O
and O
link O
them O
to O
the O
given O
large O
- O
scale O
knowledge O
graph O
at O
the O
first O
stage O
and O
then O
learn O
to O
map O
the O
entity O
- O
linked O
questions O
into O
logical O
forms O
. O
Dong O
and O
Lapata O
( O
2018 O
) O
propose O
a O
coarse O
- O
to O
- O
fine O
two O
- O
stage O
decoding O
method O
for O
semantic O
parsing O
, O
which O
generates O
a O
coarse O
sketch O
for O
a O
question O
with O
low O
- O
level O
features O
at O
the O
first O
stage O
and O
then O
continues O
to O
decode O
the O
final O
logical O
form O
based O
on O
the O
output O
of O
the O
first O
stage O
as O
well O
as O
the O
question O
itself O
. O

As O
mentioned O
in O
Section O
1 O
, O
such O
stepwise O
methods O
are O
confronted O
with O
error O
propagation O
across O
stages O
( O
e.g. O
, O
from O
entity O
linking O
to O
mapping O
, O
from O
coarse O
parse O
to O
fine O
parse O
) O
. O
In O
order O
to O
alleviate O
such O
problem O
, O
Shen O
et O
al O
. O
( O
2019 O
) O
and O
Kacupaj O
et O
al O
. O
( O
2021 O
) O
use O
a O
multi O
- O
task O
learning O
framework O
to O
jointly O
learn O
entity O
detection O
, O
linking O
, O
and O
semantic O
parsing O
in O
a O
single O
model O
. O
Kacupaj O
et O
al O
. O
( O
2021 O
) O
also O
use O
a O
graph O
attention O
network O
( O
Veličković O
et O
al O
. O
, O
2018 O
) O
to O
explore O
entity O
type O
and O
relation O
information O
in O
the O
knowledge O
base O
. O

Due O
to O
the O
superiority O
of O
multi O
- O
task O
learning O
for O
semantic O
parsing O
tailored O
for O
KBQA B-TaskName
, O
our O
work O
is O
also O
based O
on O
the O
multi O
- O
task O
learning O
framework O
. O
However O
, O
our O
model O
is O
significantly O
different O
from O
existing O
works O
in O
both O
fuzzy O
grammar O
rules O
and O
knowledge O
- O
aware O
entity O
disambiguation O
together O
with O
entity O
type O
and O
relation O
prediction O
. O

KaFSP B-MethodName

We O
use O
a O
multi O
- O
task O
learning O
framework O
to O
map O
an O
input O
( O
current O
question O
concatenated O
with O
context O
) O
into O
a O
logical O
form O
where O
entities O
are O
detected O
and O
linked O
to O
the O
given O
knowledge O
base O
. O
Figure O
1 O
shows O
the O
architecture O
of O
KaFSP B-MethodName
. O
The O
backbone O
network O
of O
KaFSP B-MethodName
follows O
LASAGNE B-MethodName
( O
Kacupaj O
et O
al O
. O
, O
2021 O
) O
consisting O
of O
a O
seq2seq B-MethodName
network O
, O
an O
entity O
recognition O
module O
and O
a O
graph O
attention O
network O
module O
( O
Section O
3.2 O
) O
. O
Our O
contributions O
lie O
in O
the O
fuzzy O
grammar O
( O
Section O
3.1 O
) O
, O
the O
knowledgeaware O
entity O
disambiguation O
module O
( O
Section O
3.3 O
) O
, O
and O
the O
entity O
type O
and O
relation O
prediction O
module O
( O
Section O
3.4 O
) O
. O
The O
two O
knowledge O
- O
aware O
modules O
are O
shown O
in O
the O
black O
dashed O
box O
in O
Figure O
1 O
. O

Fuzzy O
Grammar O

In O
semantic O
parsing O
approaches O
tailored O
for O
conversational B-TaskName
KBQA I-TaskName
, O
a O
grammar O
with O
the O
minimum O
number O
of O
actions O
is O
usually O
defined O
to O
construct O
KB O
- O
executable O
logical O
forms O
( O
i.e. O
, O
semantic O
parse O
trees O
) O
. O
The O
actions O
defined O
in O
the O
previous O
grammar O
system O
( O
Guo O
et O
al O
. O
, O
2018 O
; O
Kacupaj O
et O
al O
. O
, O
2021 O
) O
are O
all O
deterministic O
operations O
. O
However O
, O
vague O
and O
fuzzy O
questions O
are O
common O
in O
real O
- O
world O
scenarios O
, O
e.g. O
, O
" O
How O
many O
works O
of O
art O
did O
approximately O
the O
same O
number O
of O
people O
do O
the O
dubbing O
for O
as O
Another O
? O
" O
, O
which O
can O
not O
be O
answered O
by O
previous O
deterministic O
grammars O
. O
The O
grammar O
of O
LASAGNE B-MethodName
includes O
an O
action O
termed O
" O
approx O
" O
, O
which O
aims O
to O
perform O
the O
operation O
of O
" O
approximately O
equal O
to O
" O
. O
However O
, O
how O
two O
numbers O
are O
measured O
to O
be O
roughly O
equal O
to O
each O
other O
is O
not O
defined O
. O
Therefore O
, O
we O
take O
the O
grammar O
of O
LASAGNE B-MethodName
as O
a O
starting O
point O
for O
building O
our O
own O
grammar O
and O
add O
fuzzy O
actions O
to O
the O
grammar O
to O
make O
it O
to O
adapt O
to O
real O
- O
world O
vague O
questions O
mentioned O
above O
. O
The O
new O
grammar O
is O
briefly O
summarized O
in O
Table O
1 O
. O

We O
further O
give O
a O
" O
precise O
" O
( O
measurable O
) O
definition O
for O
these O
added O
fuzzy O
actions O
based O
on O
the O
fuzzy O
set O
theory O
( O
Zadeh O
, O
1965 O
) O
. O
For O
a O
number O
a O
, O
we O
define O
its O
fuzzy O
set O
as O
A O
= O
{ O
x O
, O
µ O
( O
x O
) O
|x O
∈ O
R O
} O
. O
µ O
( O
x O
) O
is O
the O
membership O
function O
of O
set O
A O
, O
which O
indicates O
the O
degree O
of O
similarity O
between O
x O
and O
a O
, O
and O
is O
defined O
based O
on O
a O
generalized O
bell O
- O
shaped O
membership O
function O
as O
: O

µ O
( O
x O
) O
= O
1 O
1 O
+ O
| O
x−a O
c O
| O
2b O
, O
( O
1 O
) O

where O
c O
∈ O
R O
and O
b O
∈ O
N O
+ O
. O
When O
µ O
( O
x O
) O
= O
1 O
, O
x O
and O
a O
are O
strictly O
equal O
; O
and O
when O
µ O
( O
x O
) O
= O
0 O
, O
x O
and O
a O
are O
strictly O
not O
equal O
. O

A O
threshold O
λ O
∈ O
( O
0 O
, O
1 O
] O
can O
be O
defined O
to O
get O
three O
fuzzy O
sets O
: O

A O
≈ O
λ O
= O
{ O
µ O
( O
x O
) O
> O
λ|x O
∈ O
R O
} O
, O
A O
≳ O
λ O
= O
{ O
x O
> O
a|x O
∈ O
R O
} O
∪ O
A O
≈ O
λ O
, O
A O
≲ O
λ O
= O
{ O
x O
< O
a|x O
∈ O
R O
} O
∪ O
A O
≈ O
λ O
. O

( O
2 O
) O

When O
µ O
( O
x O
) O
> O
λ O
, O
then O
x O
∈ O
A O
≈ O
λ O
, O
which O
denotes O
that O
x O
and O
a O
is O
approximately O
equal O
to O
each O
other O
. O
When O
x O
∈ O
A O
≳ O
λ O
, O
x O
is O
considered O
to O
be O
greater O
than O
or O
approximately O
equal O
to O
a. O
When O
x O
∈ O
A O
≲ O
λ O
, O
x O
is O
considered O
to O
be O
less O
than O
or O
approximately O
equal O
to O
a O
. O

It O
is O
worth O
noting O
that O
all O
the O
parameters O
in O
Eq O
. O
( O
1 O
) O
and O
the O
threshold O
λ O
can O
be O
flexibly O
predefined O
, O
which O
makes O
our O
grammar O
adjustable O
to O
different O
fuzzy O
scenarios O
. O

Backbone O
Network O

We O
follow O
the O
multi O
- O
task O
learning O
framework O
of O
LASAGNE B-MethodName
( O
Kacupaj O
et O
al O
. O
, O
2021 O
) O
to O
build O
the O
backbone O
network O
for O
our O
KaFSP B-MethodName
. O

Encoder O
and O
Decoder O

The O
skeleton O
of O
the O
entire O
model O
is O
a O
Transformer O
- O
based O
encoder O
- O
decoder O
network O
. O
The O
input O
x O
fed O
into O
the O
encoder O
is O
formed O
in O
a O
way O
similar O
to O
LASAGNE B-MethodName
, O
which O
is O
composed O
of O
the O
previous O
question O
, O
the O
answer O
to O
the O
previous O
question O
, O
and O
the O
current O
question O
separated O
by O
a O
symbol O
" O
[ O
SEP O
] O
" O
. O
A O
special O
token O
" O
[ O
CTX O
] O
" O
is O
appended O
to O
the O
input O
for O
encoding O
the O
input O
representation O
h O
enc O
ctx O
, O
as O
shown O
in O
Figure O
1 O
. O
Both O
the O
encoder O
and O
decoder O
use O
a O
two O
- O
layer O
multi O
- O
head O
attention O
Transformer O
block O
, O
which O
can O
be O
formulated O
as O
: O

h O
enc O
= O
encoder O
( O
x O
; O
θ O
enc O
) O
, O
z O
dec O
= O
decoder O
( O
h O
enc O
; O
θ O
dec O
) O
, O
P O
( O
y O
dec O
|x O
) O
= O
t O
softmax O
( O
W O
dec O
z O
dec O
t O
) O
, O
( O
3 O
) O

where O

z O
dec O
t O
∈ O
R O
|V O
dec O
| O

is O
the O
hidden O
state O
of O
the O
decoder O
at O
time O
step O
t O
, O
and O
W O
dec O
is O
the O
linear O
projection O
matrix O
at O
the O
targe O
side O
. O
The O
key O
task O
of O
the O
decoder O
is O
to O
generate O
an O
action O
( O
listed O
in O
Table O
1 O
) O
at O
each O
time O
step O
to O
obtain O
the O
logical O
form O
y O
dec O
corresponding O
to O
the O
input O
x O
. O

Entity O
Recognition O
Inspired O
by O
Shen O
et O
al O
. O
( O
2019 O
) O
, O
we O
jointly O
detect O
entities O
and O
their O
types O
in O
a O
BIO O
sequence O
labeling O
way O
. O
The O
labels O
for O
the O
input O
sequence O
x O
are O
in O
{ O
O O
, O
{ O
B O
, O
I O
} O
× O
{ O
T O
i O
} O
Ntp O
1 O
} O
. O
T O
i O
stands O
for O
the O
i O
- O
th O
entity O
type O
label O
, O
and O
N O
tp O
denotes O
the O
number O
of O
the O
distinct O
entity O
types O
in O
the O
knowledge O
base O
. O
An O
LSTM O
network O
, O
stacked O
over O
the O
encoder O
, O
is O
used O
to O
perform O
the O
sequence O
labeling O
task O
. O
To O
make O
the O
outputs O
of O
the O
sequence O
labeling O
task O
compatible O
with O
logical O
forms O
, O
we O
follow O
LASAGNE O
to O
use O
a O
feedforward O
layer O
stacked O
over O
the O
LSTM O
layer O
. O
The O
entire O
module O
of O
entity O
recognition O
is O
hence O
formulated O
as O
follows O
: O

h O
LSTM O
= O
LSTM O
( O
h O
enc O
; O
θ O
LSTM O
) O
, O
h O
FFN O
= O
LeakyReLU O
( O
W O
FFN O
1 O
[ O
h O
enc O
; O
h O
LSTM O
] O
) O
, O
P O
( O
y O
ER O
|x O
) O
= O
t O
softmax O
( O
W O
FFN O
2 O
h O
FFN O
t O
) O
, O
( O
4 O
) O

where O
h O
LSTM O
is O
the O
LSTM O
hidden O
state O
at O
time O
step O
t O
, O
h O
FFN O
is O
the O
FFN O
- O
transformed O
version O
of O
h O
LSTM O
, O
and O
P O
( O
y O
ER O
|x O
) O
denotes O
the O
probability O
distribution O
over O
entity O
tags O
. O

Graph O
Attention O
Network O
( O
GAT O
) O
We O
follow O
LASAGNE B-MethodName
to O
use O
the O
GAT O
module O
to O
learn O
the O
correlations O
between O
entity O
types O
and O
their O
relations O
in O
the O
knowledge O
base O
. O
It O
can O
be O
defined O
as O
: O

h O
GAT O
= O
GAT O
( O
e O
node O
; O
θ O
GAT O
) O
, O
( O
5 O
) O

where O
e O
node O
are O
the O
embeddings O
of O
nodes O
in O
the O
type O
- O
relation O
graph O
constructed O
from O
the O
knowledge O
base O
. O
Please O
refer O
to O
Kacupaj O
et O
al O
. O
( O
2021 O
) O
for O
more O
details O
on O
the O
GAT O
module O
. O

Entity O
Disambiguation O

In O
a O
large O
- O
scale O
knowledge O
base O
, O
it O
is O
common O
that O
entities O
with O
different O
meanings O
share O
the O
same O
surface O
forms O
. O
Predicting O
entity O
types O
could O
help O
differentiate O
them O
. O
However O
, O
when O
candidates O
have O
both O
the O
same O
type O
and O
surface O
form O
, O
it O
is O
difficult O
for O
entity O
type O
prediction O
to O
distinguish O
them O
again O
. O

In O
order O
to O
address O
this O
issue O
, O
we O
incorporate O
more O
information O
about O
these O
ambiguous O
entities O
from O
the O
knowledge O
base O
to O
disambiguate O
them O
. O
We O
model O
the O
entity O
disambiguation O
problem O
as O
a O
binary O
classification O
problem O
: O

y O
= O
f O
( O
c O
, O
s O
, O
K O
( O
e O
) O
) O
, O
( O
6 O
) O

where O
s O
is O
the O
surface O
form O
of O
a O
candidate O
entity O
e O
, O
c O
is O
the O
context O
where O
e O
occurs O
, O
and O
K O
( O
e O
) O
denotes O
relevant O
information O
of O
the O
candidate O
entity O
e O
from O
the O
knowledge O
base O
. O
If O
y O
= O
1 O
the O
entity O
e O
is O
disambiguated O
and O
linked O
to O
the O
true O
entity O
in O
the O
knowledge O
base O
defined O
by O
K O
( O
e O
) O
. O
The O
purpose O
of O
this O
is O
to O
maximize O
both O
the O
true O
positive O
and O
true O
negative O
. O

We O
define O
the O
context O
of O
e O
as O
the O
entire O
input O
x. O
To O
define O
K O
( O
e O
) O
, O
we O
use O
all O
triples O
that O
are O
relevant O
to O
e O
in O
the O
knowledge O
base O
, O
regardless O
of O
whether O
the O
entity O
is O
a O
subject O
or O
an O
object O
in O
triples O
. O
That O
is O
, O
K O
( O
e O
) O
is O
an O
ordered O
set O
of O
KB O
triples O
. O
Each O
triple O
in O
K O
( O
e O
) O
can O
be O
formulated O
as O
( O
e O
h O
, O
r O
, O
e O
t O
) O
, O
where O
the O
candidate O
entity O
e O
is O
either O
the O
head O
entity O
( O
e O
h O
) O
or O
tail O
entity O
( O
e O
t O
) O
. O

In O
Eq O
. O
( O
6 O
) O
, O
f O
is O
the O
classifier O
to O
disambiguate O
candidate O
entities O
. O
We O
use O
a O
pre O
- O
trained O
language O
model O
XLNet O
( O
Yang O
et O
al O
. O
, O
2019 O
) O
fine O
- O
tuned O
in O
the O
training O
dataset O
as O
the O
classifier O
. O

In O
order O
to O
feed O
s O
, O
c O
, O
and O
K O
( O
e O
) O
into O
the O
pretrained O
and O
fine O
- O
tuned O
classifier O
, O
we O
reorganize O
them O
into O
a O
concatenated O
textual O
sequence O
, O
with O
components O
be O
separated O
by O
the O
token O
" O
[ O
SEP O
] O
" O
. O
KB O
triples O
are O
all O
instantiated O
with O
corresponding O
words O
in O
the O
knowledge O
base O
, O
where O
e O
h O
, O
r O
, O
and O
e O
t O
are O
separated O
by O
blanks O
. O
We O
use O
the O
top O
3 O
triples O
in O
K O
( O
e O
) O
and O
feed O
them O
into O
the O
classifier O
, O
where O
the O
triples O
are O
sorted O
by O
their O
IDs O
. O
Such O
a O
choice O
is O
a O
trade O
- O
off O
between O
knowledge O
graph O
coverage O
and O
memory O
consumption O
in O
practice O
. O
If O
the O
number O
of O
relevant O
triples O
retrieved O
from O
the O
knowledge O
base O
is O
less O
than O
3 O
, O
we O
use O
the O
candidate O
entity O
itself O
to O
fill O
in O
the O
empty O
triples O
. O

Type O
and O
Relation O
Prediction O

This O
module O
mainly O
performs O
two O
subtasks O
: O
the O
unified O
recognition O
of O
entity O
types O
and O
relations O
, O
and O
the O
KB O
- O
guided O
prediction O
of O
correct O
entity O
types O
and O
relations O
stacked O
over O
the O
first O
subtask O
, O
as O
shown O
in O
the O
Type O
& O
Relation O
Prediction O
module O
in O
Figure O
1 O
. O

Let O
G O
⊆ O
E O
× O
R O
× O
E O
denote O
the O
knowledge O
base O
, O
where O
E O
is O
the O
entity O
set O
and O
R O
is O
the O
relation O
set O
. O

Each O
entity O
e O
∈ O
E O
has O
an O
entity O
type O
τ O
∈ O
T O
( O
entity O
type O
set O
) O
. O

We O
model O
the O
type O
and O
relation O
recognition O
subtask O
as O
a O
multi B-TaskName
- I-TaskName
label I-TaskName
classification I-TaskName
task O
and O
use O
a O
classifier O
to O
predict O
the O
probability O
of O
an O
output O
sequence O
from O
a O
given O
input O
sequence O
. O

To O
obtain O
neural O
representations O
of O
both O
entity O
types O
and O
entity O
relations O
for O
the O
recognition B-TaskName
subtask O
, O
we O
use O
a O
pre O
- O
trained O
language O
model O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
. O
The O
input O
fed O
into O
BERT B-MethodName
is O
formed O
in O
a O
way O
similar O
to O
the O
entity O
disambiguation O
module O
. O
The O
difference O
is O
that O
we O
replace O
the O
entity O
with O
its O
entity O
type O
. O
Formally O
, O
the O
neural O
representation O
e O
τ O
of O
an O
entity O
type O
is O
computed O
as O
follows O
: O

e O
τ O
= O
BERT O
[ O
CLS O
] O
( O
[ O
CLS O
] O
s O
( O
τ O
) O
[ O
SEP O
] O
K O
( O
τ O
) O
[ O
SEP O
] O
) O
, O

where O
[ O
CLS O
] O
indicates O
that O
we O
use O
the O
representation O
of O
the O
prepended O
artificial O
[ O
CLS O
] O
token O
as O
the O
representation O
of O
the O
entity O
type O
τ O
, O
s O
( O
τ O
) O
and O
K O
( O
τ O
) O
represent O
the O
surface O
form O
and O
triples O
of O
τ O
, O
respectively O
. O
Similarly O
, O
the O
neural O
representation O
e O
r O
of O
a O
relation O
is O
formulated O
as O
: O
Kacupaj O
et O
al O
. O
( O
2021 O
) O
find O
that O
modeling O
the O
correlations O
between O
entity O
types O
and O
relations O
is O
crucial O
for O
semantic O
parsing O
. O
In O
our O
KaFSP B-MethodName
, O
we O
use O
a O
single O
classifier O
to O
predict O
both O
entity O
types O
and O
relations O
, O
instead O
of O
using O
two O
separate O
classifiers O
that O
share O
no O
common O
information O
( O
Shen O
et O
al O
. O
, O
2019 O
; O
Kacupaj O
et O
al O
. O
, O
2021 O
) O
. O
Hence O
, O
the O
prediction O
space O
of O
our O
classifier O
is O
T O
∪R O
, O
and O
the O
correlations O
between O
types O
and O
relations O
are O
naturally O
captured O
in O
the O
same O
single O
classifier O
. O
We O
use O
a O
sigmoid O
function O
to O
output O
probabilities O
as O
follows O
: O

e O
r O
= O
BERT O
[ O
CLS O
] O
( O
[ O
CLS O
] O
s O
( O
r O
) O
[ O
SEP O
] O
K O
( O
r O
) O
[ O
SEP O
] O
) O
. O

P O
( O
y O
MLC O
|x O
) O
= O
Sigmoid O
( O
h O
enc O
ctx O
× O
W O
MLC O
( O
e O
τ O
r O
) O
⊤ O
) O
, O
( O
7 O
) O

where O
W O
MLC O
∈ O
R O
|T O
∪R|×d O
is O
a O
linear O
projection O
matrix O
, O
and O
e O
τ O
r O
are O
the O
concatenation O
of O
the O
embeddings O
of O
τ O
∈ O
T O
and O
r O
∈ O
R O
. O

The O
KB O
- O
guided O
prediction O
of O
entity O
types O
and O
relations O
is O
actually O
to O
make O
final O
decisions O
on O
them O
with O
relevant O
information O
from O
the O
knowledge O
base O
. O
Since O
KB O
contains O
a O
lot O
of O
triples O
irrelevant O
to O
the O
current O
utterance O
u O
, O
in O
order O
to O
make O
the O
knowledge O
graph O
embedding O
provide O
the O
information O
related O
to O
u O
, O
we O
use O
the O
output O
probabilities O
from O
the O
proposed O
multi O
- O
label O
classifier O
to O
pinpoint O
relevant O
information O
from O
the O
knowledge O
base O
encoded O
by O
GAT O
. O
Particularly O
, O
we O
calculate O
the O
Hadamard O
product O
of O
P O
( O
y O
MLC O
|x O
) O
and O
h O
GAT O
: O

h O
MLC O
= O
W O
TRP O
( O
h O
GAT O
⊙ O
P O
( O
y O
MLC O
|x O
) O
) O
, O
( O
8 O
) O

where O
W O
TRP O
∈ O
R O
2d×d O
is O
a O
linear O
projection O
matrix O
. O

Given O
the O
hidden O
states O
of O
the O
decoder O
z O
dec O
and O
last O
hidden O
state O
of O
the O
encoder O
h O
enc O
ctx O
, O
we O
use O
a O
feedforward O
network O
to O
predict O
the O
sequence O
of O
types O
and O
relations O
: O

P O
( O
y O
TRP O
|x O
) O
= O
t O
softmax O
( O
( O
h O
MLC O
) O
⊤ O
FFN O
( O
h O
enc O
ctx O
; O
z O
dec O
t O
) O
) O
, O
( O
9 O

) O

where O
FFN O
( O
h O
enc O
ctx O
; O
z O
dec O
t O
) O
is O
the O
projection O
of O
the O
concatenation O
of O
the O
context O
representation O
and O
the O
hidden O
state O
of O
the O
decoder O
at O
time O
step O
t O
. O

Learning O
and O
Inference O

KaFSP B-MethodName
Training O

Before O
training O
KaFSP B-MethodName
, O
we O
use O
weak O
supervisions O
( O
only O
the O
final O
answers O
) O
to O
obtain O
golden O
standard O
logical O
forms O
of O
questions O
in O
the O
training O
set O
through O
BFS O
, O
following O
Guo O
et O
al O
. O
( O
2018 O
) O
. O

In O
KaFSP B-MethodName
, O
we O
have O
6 O
subtasks O
: O
the O
encoderdecoder B-TaskName
subtask O
( O
DEC B-TaskName
) O
, O
the O
entity B-TaskName
recognition I-TaskName
subtask O
( O
ER B-TaskName
) O
, O
the O
filtering B-TaskName
and I-TaskName
permutation I-TaskName
subtask O
from O
LASAGNE B-MethodName
( O
FP B-TaskName
) O
, O
the O
multi B-TaskName
- I-TaskName
label I-TaskName
classification I-TaskName
subtask O
( O
MLC B-TaskName
) O
described O
in O
Section O
3.4 O
, O
the O
type B-TaskName
and I-TaskName
relation I-TaskName
prediction I-TaskName
subtask O
( O
TRP B-TaskName
) O
and O
the O
entity B-TaskName
disambiguation I-TaskName
subtask O
( O
ED B-TaskName
) O
. O
We O
a O
mixed O
training O
strategy O
to O
train O
these O
subtasks O
. O
The O
first O
5 O
subtasks O
are O
jointly O
trained O
in O
a O
multi O
- O
task O
learning O
way O
while O
the O
last O
subtask O
is O
separately O
trained O
. O
Reasons O
for O
this O
strategy O
are O
twofold O
: O
1 O
) O
Entity B-TaskName
disambiguation I-TaskName
is O
a O
relatively O
independent O
subtask O
compared O
with O
other O
subtasks O
. O
2 O
) O
We O
fine O
- O
tune O
a O
huge O
pre O
- O
trained O
language O
model O
XLNet B-MethodName
( O
Yang O
et O
al O
. O
, O
2019 O
) O
on O
this O
subtask O
. O
Direct O
incorporation O
of O
the O
fine O
- O
tuning O
procedure O
into O
multi O
- O
task O
learning O
may O
make O
it O
difficult O
for O
the O
entire O
model O
to O
converge O
. O

The O
joint O
loss O
J O
for O
the O
multi O
- O
task O
learning O
training O
is O
formulated O
as O
: O

J O
= O
m∈M O
γ O
m O
L O
m O
, O
( O
10 O
) O

where O
M O
= O
{ O
DEC B-TaskName
, O
ER B-TaskName
, O
FP B-TaskName
, O
MLC B-TaskName
, O
TRP B-TaskName
} O
is O
the O
set O
of O
subtasks O
and O
γs O
are O
the O
weights O
of O
these O
subtasks O
, O
which O
are O
learned O
during O
training O
. O
In O
learning O
these O
weights O
, O
we O
take O
into O
account O
the O
difference O
in O
magnitude O
among O
the O
5 O
losses O
according O
to O
the O
log O
standard O
deviation O
( O
Kendall O
et O
al O
. O
, O
2018 O
) O
. O
L B-MetricName
DEC I-MetricName
, O
L B-MetricName
ER I-MetricName
, O
L B-MetricName
FP I-MetricName
and O
L B-MetricName
TRP I-MetricName
are O
the O
negative B-MetricName
log I-MetricName
- I-MetricName
likelihood I-MetricName
losses I-MetricName
of O
4 O
subtasks O
, O
which O
are O
defined O
as O
follows O
: O

L B-MetricName
DEC I-MetricName
= O
− O
m O
k=1 O
log O
P O
( O
y O
DEC B-TaskName
k O
|x O
) O
, O
L B-MetricName
ER I-MetricName
= O
− O
n O
j=1 O
log O
P O
( O
y O
ER B-TaskName
j O
|x O
) O
, O
L B-MetricName
FP I-MetricName
= O
− O
n O
j=1 O
log O
P O
( O
y O
FP B-TaskName
j O
|x O
) O
, O
L B-MetricName
TRP I-MetricName
= O
− O
m O
k=1 O
, O
y O
dec O
k O
∈P O
log O
P O
( O
y O
TRP B-TaskName
k O
|x O
) O
, O
( O
11 O
) O

where O
n O
and O
m O
are O
the O
length O
of O
the O
input O
utterance O
x O
and O
the O
golden O
standard O
logical O
form O
, O
respec O
- O
tively O
. O
P O
is O
the O
set O
of O
placeholders O
for O
relations O
and O
types O
. O
y O
* O
are O
ground O
- O
truth O
labels O
for O
corresponding O
subtasks O
. O

The O
loss O
for O
the O
multi O
- O
label O
classification O
L B-MetricName
MLC I-MetricName
is O
a O
binary O
cross O
- O
entropy O
loss O
, O
defined O
as O
: O

L O
MLC B-TaskName
= O
− O
1 O
l O
l O
i=1 O
y O
MLC B-TaskName
i O
log O
( O
P O
( O
y O
MLC B-TaskName
i O
|x O
) O
) O
+ O
ȳ O
i O
MLC B-TaskName
log O
( O
P O
( O
ȳ O
i O
MLC B-TaskName
|x O
) O
) O
, O
( O
12 O

) O

where O
l O
is O
the O
size O
of O
T O
∪ O
R O
, O
ȳ O
MLC B-TaskName
= O
1 O
− O
y O
MLC B-TaskName
, O
y O
MLC B-TaskName
is O
defined O
in O
Eq O
. O
( O
7 O
) O
. O
The O
entity B-TaskName
disambiguation I-TaskName
is O
trained O
separately O
, O
and O
its O
loss O
function O
is O
defined O
as O
: O

L O
ED B-TaskName
= O
e∈Ex O
y O
ED B-TaskName
i O
log O
( O
P O
( O
y O
ED B-TaskName
i O
|x O
) O
) O
+ O
ȳ O
i O
ED B-TaskName
log O
( O
P O
( O
ȳ O
i O
ED B-TaskName
|x O
) O
) O
, O
( O
13 O
) O

where O
E O
x O
is O
the O
set O
of O
entities O
that O
appear O
in O
x O
and O
y O
ED B-TaskName
is O
defined O
in O
Eq O
. O
( O
6 O
) O
. O

To O
train O
this O
subtask O
, O
we O
retrieve O
all O
entities O
that O
are O
present O
in O
the O
current O
input O
from O
the O
knowledge O
base O
. O
Note O
that O
we O
only O
construct O
500,000 O
and O
40,000 O
samples O
respectively O
for O
training O
and O
validation O
of O
the O
entity O
disambiguation O
module O
. O

Grammar O
- O
Guided O
Inference O

The O
grammar O
defined O
in O
Table O
1 O
is O
used O
to O
guide O
the O
decoding O
step O
. O
The O
decoder O
generates O
a O
sequence O
mixed O
with O
actions O
and O
placeholders O
. O
Placeholders O
are O
instantiated O
with O
specific O
entities O
, O
types O
, O
relations O
, O
and O
numbers O
. O
The O
decoding O
process O
for O
a O
logical O
form O
terminates O
when O
no O
nonterminals O
remain O
. O
After O
decoding O
, O
we O
use O
a O
shift O
- O
reduce O
method O
to O
check O
the O
logical O
form O
sequence O
and O
delete O
or O
correct O
wrong O
placeholders O
. O

Once O
the O
BIO O
tags O
and O
entity O
types O
are O
identified O
, O
entity O
spans O
can O
be O
located O
from O
the O
input O
utterance O
. O
We O
search O
from O
the O
inverted O
index O
constructed O
for O
the O
knowledge O
base O
for O
each O
predicted O
entity O
span O
to O
obtain O
an O
entity O
candidate O
list O
. O
After O
filtering O
the O
retrieved O
entity O
candidate O
list O
according O
to O
the O
corresponding O
entity O
type O
, O
if O
there O
are O
still O
multiple O
candidate O
entities O
, O
the O
entity O
disambiguation O
module O
is O
activated O
to O
calculate O
the O
conditional O
probability O
of O
each O
candidate O
entity O
. O
The O
candidate O
entity O
with O
the O
highest O
probability O
is O
selected O
. O

Finally O
, O
we O
use O
the O
relation B-TaskName
and I-TaskName
type I-TaskName
prediction I-TaskName
results O
and O
disambiguated B-TaskName
entities I-TaskName
to O
instantiate O
the O
placeholders O
to O
get O
final O
logical O
forms O
. O

Experiments O

We O
carried O
out O
experiments O
and O
analyses O
to O
validate O
the O
effectiveness O
of O
the O
proposed O
KaFSP B-MethodName
. O

Experimental O
Settings O
Dataset O

We O
evaluated O
the O
proposed O
model O
on O
the O
CSQA B-DatasetName
dataset O
( O
Saha O
et O
al O
. O
, O
2018 O
) O
, O
a O
standard O
dataset O
for O
complex O
sequential O
question O
answering O
. O
The O
dataset O
is O
composed O
of O
200 O
K O
dialogues O
with O
1.6 O
M O
turns O
, O
and O
over O
12.8 O
M O
entities O
from O
Wikidata O
, O
where O
153 O
K O
, O
16 O
K O
, O
and O
28 O
K O
dialogues O
are O
used O
for O
training O
, O
verification O
, O
and O
test O
, O
respectively O
. O
The O
questions O
cover O
a O
wide O
range O
of O
linguistic O
phenomena O
, O
such O
as O
co O
- O
reference O
, O
ellipsis O
, O
and O
reasoning O
. O

Evaluation O
Metrics O

We O
used O
the O
same O
evaluation O
metrics O
as O
Saha O
et O
al O
. O
( O
2018 O
) O
. O
When O
answers O
are O
composed O
of O
one O
or O
more O
entities O
, O
F1 B-MetricName
score O
is O
used O
as O
the O
evaluation O
metric O
. O
When O
answers O
are O
a O
Boolean O
value O
or O
number O
, O
accuracy B-MetricName
is O
used O
as O
the O
metric O
. O
Following O
previous O
works O
( O
Guo O
et O
al O
. O
, O
2018 O
; O
Shen O
et O
al O
. O
, O
2019 O
; O
Kacupaj O
et O
al O
. O
, O
2021 O
) O
, O
we O
also O
calculated O
overall O
scores O
for O
all O
types O
of O
questions O
under O
each O
evaluation O
metric O
. O

Baselines O
We O
compared O
KaFSP B-MethodName
against O
5 O
stateof O
- O
the O
- O
art O
baselines O
on O
the O
CSQA B-DatasetName
. O
The O
first O
baseline O
is O
HRED+KVM B-MethodName
( O
Saha O
et O
al O
. O
, O
2018 O
) O
, O
which O
combines O
the O
HRED B-MethodName
model O
with O
the O
key O
- O
value O
memory O
network O
. O
The O
other O
four O
baselines O
are O
D2A B-MethodName
( O
Guo O
et O
al O
. O
, O
2018 O
) O
, O
MaSP B-MethodName
( O
Shen O
et O
al O
. O
, O
2019 O
) O
, O
KISP B-MethodName
( O
Thirukovalluru O
et O
al O
. O
, O
2021 O
) O
, O
LASAGNE B-MethodName
( O
Kacupaj O
et O
al O
. O
, O
2021 O
, O
which O
achieve O
state O
- O
of O
- O
the O
- O
art O
results O
on O
different O
types O
of O
questions O
on O
the O
CSQA B-DatasetName
dataset O
. O
More O
details O
for O
model O
settings O
can O
be O
found O
in O
Appendix O
A O
. O

Results O

Table O
2 O
shows O
experiment O
results O
on O
the O
CSQA B-DatasetName
dataset O
. O
Our O
model O
outperforms O
LASAGNE B-MethodName
on O
all O
types O
of O
questions O
and O
achieves O
new O
SOTA O
results O
in O
8 O
out O
of O
10 O
question O
types O
. O
Additionally O
, O
our O
model O
outperforms O
all O
previous O
baselines O
in O
terms O
of O
" O
overall O
" O
results O
. O

For O
question O
types O
that O
involve O
one O
or O
more O
entities O
, O
namely O
Logical O
Reasoning O
( O
All O
) O
, O
Simple O
Question O
( O
Direct O
) O
, O
and O
Verification O
( O
Boolean O
) O
, O
the O
improvements O
over O
LASAGNE B-MethodName
on O
these O
question O
types O
are O
3.14 B-MetricValue
% I-MetricValue
, O
2.78 B-MetricValue
% I-MetricValue
, O
and O
1.29 B-MetricValue
% I-MetricValue
respectively O
. O
This O
is O
mainly O
because O
we O
have O
added O
a O
knowledgeaware O
entity B-TaskName
disambiguation I-TaskName
module O
to O
improve O
the O
accuracy O
of O
entity O
linking O
. O
For O
question O
types O
Clarification O
, O
Comparative O
Reasoning O
( O
All O
) O
, O
and O
Comparative O
Reasoning O
( O
Count O
) O
, O
they O
usually O
involve O
multiple O
entity O
types O
and O
relations O
. O
KaFSP B-MethodName
achieves O
huge O
improvements O
of O
11.91 B-MetricValue
% I-MetricValue
, O
16.23 B-MetricValue
% I-MetricValue
, O
and O
19.45 B-MetricValue
% I-MetricValue
on O
these O
question O
types O
over O
LASAGNE B-MethodName
. O
This O
is O
mainly O
due O
to O
fuzzy O
comparison O
rules O
in O
the O
new O
grammar O
system O
and O
the O
proposed O
knowledge O
- O
aware O
type B-TaskName
and I-TaskName
relation I-TaskName
prediction I-TaskName
module O
. O
The O
module O
benefits O
from O
the O
multi B-TaskName
- I-TaskName
label I-TaskName
classification I-TaskName
with O
a O
single O
classifier O
that O
not O
only O
helps O
to O
capture O
correlations O
between O
entity O
types O
and O
relations O
but O
also O
pinpoints O
and O
incorporates O
only O
relevant O
information O
from O
the O
knowledge O
base O
into O
relation B-TaskName
and I-TaskName
type I-TaskName
prediction I-TaskName
, O
which O
makes O
the O
predictions O
of O
types O
and O
relations O
more O
accurate O
. O

Our O
model O
does O
not O
outperform O
previous O
SOTA O
results O
on O
only O
2 O
question O
types O
, O
i.e. O
, O
Simple O
Question O
( O
Co O
- O
referenced O
) O
and O
Simple O
Question O
( O
Ellipsis O
) O
. O
Although O
KaFSP B-MethodName
is O
lower O
than O
KISP B-MethodName
on O
these O
two O
question O
types O
, O
it O
is O
0.55 B-MetricValue
% I-MetricValue
and O
1.66 B-MetricValue
% I-MetricValue
higher O
than O
LASAGNE B-MethodName
. O
We O
conjecture O
that O
the O
reasons O
for O
being O
not O
superior O
to O
KISP B-MethodName
on O
these O
question O
types O
are O
twofold O
. O
First O
, O
spurious O
logical O
forms O
may O
have O
a O
negative O
impact O
on O
the O
decoder O
when O
it O
is O
trained O
on O
data O
indeed O
with O
false O
logical O
forms O
. O
Second O
, O
in O
conversational B-TaskName
QA I-TaskName
, O
not O
only O
entities O
but O
also O
entity O
relations O
can O
be O
omitted O
in O
questions O
. O
For O
example O
, O
" O
How O
many O
people O
acted O
as O
an O
influence O
on O
Thomas O
Aquinas O
? O
And O
also O
tell O
me O
about O
Walt O
Whitman O
? O
" O
. O
In O
KaFSP B-MethodName
, O
we O
replace O
the O
real O
ID O
of O
an O
omitted O
entity O
with O
" O
previous O
- O
entity O
" O
. O
However O
, O
this O
strategy O
is O
not O
used O
for O
omitted O
relations O
when O
producing O
logic O
forms O
, O
which O
may O
have O
neg- O
ative O
impacts O
on O
the O
two O
question O
types O
mentioned O
above O
. O
Furthermore O
, O
although O
KaFSP B-MethodName
increases O
the O
number O
of O
parameters O
, O
most O
added O
parameters O
are O
from O
the O
pretrained O
XLNet B-MethodName
( O
base O
) O
model O
included O
for O
entity B-TaskName
disambiguation I-TaskName
. O
This O
does O
not O
have O
a O
big O
impact O
on O
the O
inference O
speed O
of O
KaFSP B-MethodName
compared O
to O
LASAGNE B-MethodName
. O

Ablation O
Study O

Table O
3 O
summarizes O
experiment O
results O
of O
ablation O
study O
on O
our O
major O
contributions O
: O
fuzzy O
grammar O
, O
the O
knowledge O
- O
aware O
entity B-TaskName
disambiguation I-TaskName
module O
, O
and O
the O
multi B-TaskName
- I-TaskName
label I-TaskName
classification I-TaskName
framework O
. O
We O
observe O
that O
all O
three O
key O
components O
make O
substantial O
contributions O
to O
our O
proposed O
model O
. O

For O
the O
ablation O
study O
on O
the O
entity B-TaskName
disambiguation I-TaskName
module O
, O
we O
compared O
KaFSP B-MethodName
against O
" O
w O
/ O
o O
ED B-TaskName
" O
that O
directly O
selects O
the O
first O
entity O
from O
the O
ordered O
candidate O
list O
retrieved O
from O
the O
knowledge O
base O
as O
the O
disambiguated O
entity O
. O
When O
ED O
module O
is O
n't O
used O
, O
candidate O
entities O
are O
sorted O
lexicographically O
by O
their O
IDs O
. O
This O
was O
done O
to O
be O
consistent O
with O
previous O
approaches O
in O
our O
baselines O
. O
We O
find O
that O
for O
all O
types O
of O
questions O
, O
the O
application O
of O
the O
proposed O
knowledge O
- O
aware O
ED B-TaskName
improves O
the O
results O
to O
various O
degrees O
. O
This O
is O
because O
entity O
ambiguity O
is O
present O
in O
a O
wide O
range O
of O
questions O
. O

For O
Simple O
Question O
( O
Direct O
) O
questions O
, O
our O
further O
analysis O
shows O
that O
14.11 O
% O
of O
entities O
are O
updated O
by O
our O
knowledge O
- O
aware O
ED B-TaskName
, O
which O
leads O
to O
an O
improvement O
of O
2.60 B-MetricValue
% I-MetricValue
. O
Both O
natural O
language O
questions O
and O
the O
knowledge O
base O
contain O
information O
that O
can O
be O
used O
to O
disambiguate O
entities O
. O
The O
proposed O
knowledge O
- O
aware O
ED B-TaskName
incorporates O
both O
types O
of O
information O
for O
disambiguation O
. O
Table O
4 O
shows O
the O
total O
number O
of O
entities O
, O
the O
total O
number O
of O
disambiguated O
entities O
, O
and O
their O
proportions O
in O
the O
logical O
forms O
of O
different O
types O
of O
questions O
. O
It O
can O
be O
seen O
that O
overall O
, O
the O
disambiguated O
entities O
account O
for O
13.98 O
% O
. O
For O
Logical O
Reasoning O
and O
Verification O
questions O
where O
the O
proportion O
of O
disambiguated O
entities O
is O
relatively O
high O
, O
correspondingly O
, O
the O
improvements O
achieved O
by O
adding O
the O
entity B-TaskName
disambiguation I-TaskName
module O
is O
high O
. O
This O
further O
validates O
the O
effectiveness O
of O
the O
proposed O
entity B-TaskName
disambiguation I-TaskName
module O
. O

Similarly O
, O
our O
ablation O
study O
validates O
the O
effectiveness O
of O
both O
the O
fuzzy O
grammar O
and O
the O
knowledge O
- O
aware O
multi B-TaskName
- I-TaskName
label I-TaskName
classification I-TaskName
( O
case O
study O
on O
the O
multi B-TaskName
- I-TaskName
label I-TaskName
classification I-TaskName
can O
be O
found O
in O
Appendix O
B O
) O
. O

Error O
Analysis O

For O
error O
analysis O
, O
we O
randomly O
sampled O
100 O
incorrect O
predictions O
and O
summarized O
the O
following O
two O
types O
of O
typical O
errors O
: O
Entity B-MetricName
Ambiguity I-MetricName
( O
54 B-MetricValue
% I-MetricValue
) O
Although O
our O
entity B-TaskName
disambiguation I-TaskName
model O
can O
achieve O
a O
prediction O
accuracy B-MetricName
of O
95.16 B-MetricValue
% I-MetricValue
, O
ambiguous O
entities O
still O
exist O
in O
some O
questions O
. O
Take O
the O
question O
" O
What O
lead O
to O
the O
death O
of O
Jerry O
Stephenson O
? O
" O
as O
an O
example O
. O
Both O
entity O
Q6184489 O
and O
Q100927364 O
are O
found O
in O
the O
knowledge O
base O
, O
which O
matches O
the O
surface O
form O
" O
Jerry O
Stephenson O
" O
. O
However O
, O
it O
is O
difficult O
to O
determine O
whether O
the O
real O
entity O
in O
the O
question O
is O
Q100927364 O
( O
college O
basketball O
player O
Austin O
Peay O
) O
or O
Q6184489 O
( O
American O
baseball O
player O
) O
with O
only O
information O
of O
three O
triples O
and O
insufficient O
context O
. O
Spurious O
Logical O
Forms O
( O
6 O
% O
) O
Similar O
to O
previous O
works O
( O
Shen O
et O
al O
. O
, O
2019 O
; O
Kacupaj O
et O
al O
. O
, O
2021 O
) O
, O
we O
find O
that O
our O
model O
can O
infer O
correct O
answers O
even O
with O
wrong O
" O
ground O
- O
truth O
" O
logical O
forms O
generated O
with O
the O
algorithm O
taken O
from O
previous O
work O
( O
Guo O
et O
al O
. O
, O
2018 O
) O
. O
This O
will O
affect O
the O
overall O
performance O
of O
the O
model O
. O
Such O
a O
phenomenon O
is O
especially O
common O
in O
complex O
reasoning O
questions O
. O

Conclusion O

In O
this O
paper O
, O
we O
have O
presented O
a O
knowledgeaware B-MethodName
fuzzy I-MethodName
semantic I-MethodName
parsing I-MethodName
framework O
KaFSP B-MethodName
for O
conversational B-TaskName
question I-TaskName
answering I-TaskName
over O
a O
largescale O
knowledge O
base O
. O
KaFSP B-MethodName
defines O
fuzzy O
comparison O
actions O
in O
grammar O
based O
on O
the O
fuzzy O
set O
theory O
to O
cover O
approximately O
comparative O
reasoning O
. O
In O
addition O
to O
this O
, O
we O
propose O
two O
knowledgeaware O
components O
in O
KaFSP B-MethodName
to O
incorporate O
information O
from O
the O
knowledge O
base O
for O
entity B-TaskName
disambiguation I-TaskName
and O
entity B-TaskName
type I-TaskName
& I-TaskName
relation I-TaskName
prediction I-TaskName
. O
Experiment O
results O
demonstrate O
that O
KaFSP B-MethodName
is O
substantially O
better O
than O
all O
previous O
state O
- O
of O
- O
the O
- O
art O
models O
, O
setting O
new O
SOTA O
results O
on O
8 O
out O
of O
10 O
question O
types O
on O
the O
CSQA B-DatasetName
dataset O
and O
achieving O
over O
90 B-MetricValue
% I-MetricValue
F1 B-MetricName
or O
accuracy B-MetricName
in O
3 O
question O
types O
for O
the O
first O
time O
. O

A O
Hyperparamters O
and O
Module O
Configurations O

Table O
5 O
summarizes O
the O
hyperparameters O
used O
in O
the O
KaFSP B-MethodName
framework O
. O
For O
the O
Transformer O
module O
, O
we O
used O
the O
standard O
configurations O
from O
Vaswani O
et O
al O
. O
( O
2017 O
) O
. O
Following O
the O
setting O
for O
Transformer O
base O
, O
we O
used O
residual B-HyperparameterName
dropout I-HyperparameterName
( O
Srivastava O
et O
al O
. O
, O
2014 O
) O
in O
the O
summation O
of O
word O
embeddings O
and O
positional O
encodings O
in O
both O
the O
encoder O
and O
decoder O
with O
a O
rate O
of O
0.1 B-HyperparameterValue
. O

B O
Case O
Study O
on O
the O
Multi O
- O
Label O
Classification O

We O
sample O
12 O
questions O
and O
list O
the O
ground O
- O
truth O
labels O
( O
i.e. O
, O
entity O
types O
and O
relations O
) O
corresponding O
to O
each O
question O
in O
Table O
6 O
. O
Through O
the O
proposed O
knowledge O
- O
aware O
multi B-TaskName
- I-TaskName
label I-TaskName
classification I-TaskName
module O
, O
we O
get O
the O
probabilities O
of O
different O
labels O
for O
each O
question O
, O
which O
are O
visualized O
in O
Figure O
2 O
. O
We O
observe O
that O
the O
knowledge O
- O
aware O
multilabel B-TaskName
classification I-TaskName
module O
can O
effectively O
recognize O
entity O
types O
and O
relations O
in O
the O
questions O
. O
Take O
Question O
10 O
and O
11 O
as O
examples O
. O
Both O
questions O
contain O
type O
T2 O
and O
relation O
R1 O
, O
while O
Question O
11 O
has O
another O
relation O
R4 O
. O
In O
Figure O
2 O
, O
we O
observe O
that O
this O
module O
can O
recognize O
these O
types O
and O
relations O
correctly O
. O
Applying O
the O
proposed O
multi B-TaskName
- I-TaskName
label I-TaskName
classification I-TaskName
module O
to O
the O
type B-TaskName
and I-TaskName
relation I-TaskName
prediction I-TaskName
module O
will O
hence O
effectively O
filter O
information O
irrelevant O
to O
the O
current O
question O
6 O
. O

and O
help O
the O
model O
make O
better O
predictions O
. O
We O
also O
find O
that O
some O
questions O
have O
a O
higher O
probability O
on O
a O
very O
small O
number O
of O
irrelevant O
labels O
. O
However O
, O
as O
the O
number O
of O
labels O
in O
KBs O
is O
large O
, O
having O
a O
high O
probability O
of O
only O
a O
few O
irrelevant O
labels O
will O
not O
greatly O
affect O
the O
results O
of O
the O
entire O
model O
. O

ID O

Question O

Ground O
- O
truth O
Labels O

Acknowledgments O

The O
present O
research O
was O
supported O
by O
Zhejiang O
Lab O
( O
No O
. O
2022KH0AB01 O
) O
and O
Huawei O
( O
No O
. O
TC20210528011 O
) O
. O
We O
would O
like O
to O
thank O
the O
anonymous O
reviewers O
for O
their O
insightful O
comments O
. O
We O
also O
want O
to O
thank O
MindSpore O
1 O
for O
the O
partial O
suppoort O
of O
this O
work O
, O
which O
is O
a O
new O
deep O
learning O
computing O
framework O
. O

Early B-TaskName
exit I-TaskName
mechanism O
aims O
to O
accelerate O
the O
inference O
speed O
of O
large O
- O
scale O
pre O
- O
trained O
language O
models O
. O
The O
essential O
idea O
is O
to O
exit O
early O
without O
passing O
through O
all O
the O
inference O
layers O
at O
the O
inference O
stage O
. O
To O
make O
accurate O
predictions O
for O
downstream O
tasks O
, O
the O
hierarchical O
linguistic O
information O
embedded O
in O
all O
layers O
should O
be O
jointly O
considered O
. O
However O
, O
much O
of O
the O
research O
up O
to O
now O
has O
been O
limited O
to O
use O
local O
representations O
of O
the O
exit O
layer O
. O
Such O
treatment O
inevitably O
loses O
information O
of O
the O
unused O
past O
layers O
as O
well O
as O
the O
high O
- O
level O
features O
embedded O
in O
future O
layers O
, O
leading O
to O
sub O
- O
optimal O
performance O
. O
To O
address O
this O
issue O
, O
we O
propose O
a O
novel O
Past O
- O
Future O
method O
to O
make O
comprehensive O
predictions O
from O
a O
global O
perspective O
. O
We O
first O
take O
into O
consideration O
all O
the O
linguistic O
information O
embedded O
in O
the O
past O
layers O
and O
further O
engage O
the O
future O
information O
which O
is O
originally O
inaccessible O
for O
predictions O
. O
Extensive O
experiments O
demonstrate O
that O
our O
method O
outperforms O
previous O
early B-TaskName
exit I-TaskName
methods O
by O
a O
large O
margin O
, O
yielding O
better O
and O
robust O
performance O
1 O
. O

Introduction O

Pre O
- O
trained O
language O
models O
( O
PLMs O
) O
, O
e.g. O
, O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
, O
RoBERTa B-MethodName
and O
XLNet B-MethodName
( O
Yang O
et O
al O
. O
, O
2019 O
) O
, O
have O
obtained O
remarkable O
success O
in O
a O
wide O
range O
of O
NLP O
tasks O
. O
Despite O
their O
impressive O
performance O
, O
PLMs O
are O
usually O
associated O
with O
large O
memory O
requirement O
and O
high O
computational O
cost O
. O
Such O
drawbacks O
slow O
down O
the O
inference O
and O
further O
encumber O
the O
application O
of O
PLMs O
in O
the O
scenarios O
where O
inference O
time O
and O
computation O
budget O
are O
restricted O
. O

To O
address O
this O
issue O
, O
a O
growing O
number O
of O
studies O
focusing O
on O
improving O
model O
efficiency O
have O
emerged O
recently O
. O
Particularly O
, O
Kaya O
et O
al O
. O
( O
2019 O
) O
point O
out O
that O
the O
current O
over O
- O
parameterized O
models O
conduct O
excessive O
computation O
for O
simple O
instances O
, O
which O
is O
actually O
undesirable O
and O
computationally O
wasteful O
. O
In O
light O
of O
this O
observation O
, O
an O
increasing O
amount O
of O
work O
seeks O
various O
early O
exit O
methods O
, O
of O
which O
the O
basic O
idea O
is O
to O
exit O
early O
without O
passing O
through O
the O
entire O
model O
during O
inference O
. O
Concretely O
, O
for O
NLP O
tasks O
, O
they O
couple O
branch O
classifiers O
with O
each O
layer O
of O
the O
pre O
- O
trained O
language O
models O
and O
stop O
forward O
propagation O
at O
an O
intermediate O
layer O
. O
Then O
the O
current O
branch O
classifier O
makes O
a O
prediction O
based O
on O
the O
representation O
of O
the O
token O
that O
is O
used O
as O
the O
aggregated O
sequence O
representation O
for O
classification O
tasks O
and O
is O
referred O
to O
as O
the O
state O
of O
the O
layer O
in O
this O
work O
. O

However O
, O
existing O
work O
on O
early O
exit O
has O
two O
major O
drawbacks O
. O
First O
, O
existing O
work O
( O
Xin O
et O
al O
. O
, O
2020 O
; O
uses O
only O
local O
states O
in O
the O
early O
exit O
framework O
. O
They O
inevitably O
lose O
valuable O
features O
that O
are O
captured O
by O
passed O
layers O
but O
are O
ignored O
for O
prediction O
, O
leading O
to O
less O
reliable O
prediction O
results O
. O
Moreover O
, O
these O
methods O
abandon O
the O
potentially O
useful O
features O
captured O
by O
the O
future O
layers O
that O
have O
not O
been O
passed O
, O
which O
may O
hurt O
the O
performance O
of O
the O
instances O
requiring O
high O
- O
level O
features O
embedded O
in O
the O
deep O
layers O
. O
Consequently O
, O
their O
performance O
dramatically O
declines O
when O
the O
inference O
exits O
earlier O
for O
a O
higher O
speed O
- O
up O
ratio O
. O

These O
two O
major O
drawbacks O
hinder O
the O
progress O
of O
early O
exit O
research O
and O
motivate O
us O
to O
develop O
a O
new O
mechanism O
using O
the O
hierarchical O
linguistic O
information O
embedded O
in O
all O
layers O
( O
Jawahar O
et O
al O
. O
, O
2019 O
) O
from O
a O
global O
perspective O
. O
However O
, O
up O
to O
now O
, O
a O
global O
early O
exit O
mechanism O
remains O
a O
under O
- O
explored O
challenging O
problem O
. O
We O
extend O
the O
existing O
methods O
to O
their O
corresponding O
global O
versions O
and O
find O
that O
naive O
global O
strategies O
only O
result O
in O
poor O
performance O
. O
Meanwhile O
, O
the O
future O
states O
are O
originally O
inaccessible O
in O
the O
early O
exit O
framework O
, O
which O
also O
remains O
a O
bottleneck O
for O
a O
global O
prediction O
considering O
both O
past O
and O
future O
states O
. O

In O
this O
paper O
, O
we O
focus O
on O
the O
aforementioned O
problems O
and O
first O
put O
into O
practice O
a O
global O
Past O
- O
Future O
early B-TaskName
exit I-TaskName
mechanism O
. O
The O
term O
global O
is O
two O
- O
fold O
: O
( O
1 O
) O
instead O
of O
using O
one O
or O
several O
local O
state O
( O
s O
) O
for O
prediction O
in O
previous O
work O
, O
all O
the O
available O
past O
states O
are O
effectively O
incorporated O
in O
our O
method O
; O
( O
2 O
) O
furthermore O
, O
to O
grasp O
the O
features O
embedded O
in O
the O
deep O
layers O
, O
the O
originally O
inaccessible O
future O
states O
are O
approximated O
by O
imitation O
learning O
and O
are O
also O
engaged O
for O
prediction O
. O
The O
comparison O
of O
the O
previous O
method O
and O
our O
method O
is O
illustrated O
in O
Figure O
1 O
. O
By O
combining O
both O
past O
and O
future O
states O
, O
our O
model O
is O
able O
to O
make O
more O
accurate O
predictions O
for O
downstream O
tasks O
. O

Extensive O
experiments O
reveal O
that O
the O
proposal O
significantly O
outperforms O
previous O
early O
exit O
methods O
. O
Particularly O
, O
it O
surpasses O
the O
previous O
methods O
by O
a O
large O
margin O
when O
the O
speed O
- O
up O
ratio O
is O
relatively O
high O
. O
In O
addition O
, O
extensive O
experiments O
with O
different O
pre O
- O
trained O
language O
models O
as O
backbones O
demonstrate O
consistent O
improvement O
over O
the O
baseline O
methods O
, O
which O
verifies O
the O
generality O
of O
our O
method O
. O

To O
summarize O
, O
our O
contributions O
are O
as O
follows O
: O

• O
We O
propose O
a O
set O
of O
global O
strategies O
which O
effectively O
incorporate O
all O
available O
states O
and O
they O
achieve O
better O
performance O
compared O
to O
the O
existing O
naive O
global O
strategies O
. O

• O
Our O
early B-TaskName
exit I-TaskName
method O
first O
utilizes O
the O
future O
states O
which O
are O
originally O
inaccessible O
at O
the O
inference O
stage O
, O
enabling O
more O
comprehensive O
global O
predictions O
. O

• O
Experiments O
show O
that O
our O
proposal O
achieves O
better O
performance O
compared O
to O
the O
previous O
state O
- O
of O
- O
the O
- O
art O
early B-TaskName
exit I-TaskName
methods O
. O

Related O
Work O

Large O
- O
scale O
pre O
- O
trained O
language O
models O
( O
Devlin O
et O
al O
. O
, O
2019 O
; O
based O
on O
the O
Transformer O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
architecture O
demonstrate O
superior O
performance O
in O
various O
NLP O
tasks O
. O
However O
, O
the O
impressive O
performance O
is O
on O
the O
basis O
of O
massive O
parameters O
, O
leading O
to O
large O
memory O
requirement O
and O
computational O
cost O
during O
inference O
. O
To O
overcome O
this O
bottleneck O
, O
increasing O
studies O
work O
on O
improving O
the O
efficiency O
of O
overparameterized O
pre O
- O
trained O
language O
models O
. O
Knowledge O
distillation O
( O
Hinton O
et O
al O
. O
, O
2015 O
; O
Turc O
et O
al O
. O
, O
2019 O
; O
Jiao O
et O
al O
. O
, O
2019 O
; O
Li O
et O
al O
. O
, O
2020a O
) O
compacts O
the O
model O
architecture O
to O
obtain O
a O
smaller O
model O
that O
remains O
static O
for O
all O
instances O
at O
the O
inference O
stage O
. O
Sanh O
et O
al O
. O
( O
2019 O
) O
focus O
on O
reducing O
the O
number O
of O
layers O
since O
their O
investigation O
reveals O
variations O
on O
hidden O
size O
dimension O
have O
a O
smaller O
impact O
on O
computation O
efficiency O
. O
Sun O
et O
al O
. O
( O
2019 O
) O
learn O
from O
multiple O
intermediate O
layers O
of O
the O
teacher O
model O
for O
incremental O
knowledge O
extraction O
instead O
of O
only O
learning O
from O
the O
last O
hidden O
representations O
. O
Further O
, O
Wang O
et O
al O
. O
( O
2020 O
) O
design O
elaborate O
techniques O
to O
drive O
the O
student O
model O
to O
mimic O
the O
self O
- O
attention O
module O
of O
teacher O
models O
. O
compress O
model O
by O
progressive O
module O
replacing O
, O
showing O
a O
new O
perspective O
of O
model O
compression O
. O
However O
, O
these O
static O
model O
compression O
methods O
treat O
the O
instances O
requiring O
different O
computational O
cost O
without O
distinction O
. O
Moreover O
, O
they O
have O
to O
distill O
a O
model O
from O
scratch O
to O
meet O
the O
varying O
speed O
- O
up O
ratio O
requirements O
. O

To O
meet O
different O
constraints O
for O
acceleration O
, O
another O
line O
of O
work O
studies O
instance O
- O
adaptive O
methods O
to O
adjust O
the O
number O
of O
executed O
layers O
for O
different O
instances O
. O
Li O
et O
al O
. O
( O
2020b O
) O
select O
models O
in O
different O
sizes O
depending O
on O
the O
difficulty O
of O
input O
instance O
. O
Besides O
, O
early O
exit O
is O
a O
practical O
method O
to O
adaptively O
accelerate O
inference O
and O
is O
first O
proposed O
for O
computer O
vision O
tasks O
( O
Kaya O
et O
al O
. O
, O
2019 O
; O
Teerapittayanon O
et O
al O
. O
, O
2016 O
) O
. O
Elbayad O
et O
al O
. O
( O
2020 O
) O
; O
Xin O
et O
al O
. O
( O
2020 O
) O
; O
Schwartz O
et O
al O
. O
( O
2020 O
) O
follow O
the O
essential O
idea O
and O
leverage O
the O
method O
in O
NLP O
tasks O
. O
To O
prevent O
the O
error O
from O
one O
single O
classifier O
, O
make O
the O
model O
stop O
inference O
when O
a O
cross O
- O
layer O
consistent O
prediction O
is O
achieved O
. O
However O
, O
researches O
on O
the O
subject O
has O
been O
mostly O
restricted O
to O
only O
use O
the O
local O
states O
around O
the O
exit O
layer O
. O

Method O

We O
first O
introduce O
the O
strategies O
to O
incorporate O
multiple O
states O
and O
the O
imitation O
learning O
method O
for O
generating O
approximations O
of O
future O
states O
. O
Then O
we O
introduce O
the O
merging O
gate O
to O
adaptively O
fuse O
past O
and O
future O
states O
. O
At O
last O
, O
we O
show O
the O
training O
process O
and O
the O
exit O
condition O
during O
inference O
. O

Incorporation O
of O
Past O
States O

Existing O
work O
( O
Xin O
et O
al O
. O
, O
2020 O
) O
focuses O
on O
making O
exit O
decision O
based O
on O
a O
single O
branch O
classifier O
. O
The O
consequent O
unreliable O
result O
motivates O
the O
recent O
advance O
that O
uses O
consecutive O
states O
to O
improve O
the O
accuracy O
and O
robustness O
. O
However O
, O
the O
model O
prediction O
is O
still O
limited O
to O
use O
several O
local O
states O
. O
In O
contrast O
, O
we O
investigate O
how O
to O
incorporate O
all O
the O
past O
states O
from O
a O
global O
perspective O
. O
The O
existing O
strategy O
using O
consecutive O
consistent O
prediction O
labels O
can O
be O
easily O
extended O
to O
a O
global O
version O
that O
counts O
the O
majority O
of O
the O
predicted O
labels O
which O
is O
regarded O
as O
a O
voting O
strategy O
. O
Another O
alternative O
is O
the O
commonly O
- O
used O
ensemble O
strategy O
that O
averages O
the O
output O
probabilities O
for O
prediction O
. O
Besides O
these O
naive O
solutions O
, O
we O
explore O
the O
following O
strategies O
to O
integrate O
multiple O
states O
into O
a O
single O
one O
: O

• O
Max O
- O
Pooling O
: O
The O
max O
- O
pooling O
operation O
is O
performed O
on O
all O
available O
states O
, O
resulting O
in O
the O
integrated O
state O
. O

• O
Avg O
- O
Pooling O
: O
The O
average O
- O
pooling O
operation O
is O
performed O
on O
all O
available O
states O
, O
resulting O
in O
the O
integrated O
state O
. O

• O
Attn O
- O
Pooling O
: O
The O
attentive O
- O
pooling O
takes O
the O
weighted O
summation O
of O
all O
available O
states O
as O
the O
integrated O
state O
. O
The O
attention O
weights O
are O
computed O
with O
the O
last O
state O
as O
the O
query O
. O

• O
Concatenation O
: O
All O
available O
states O
are O
concatenated O
and O
then O
fed O
into O
a O
linear O
transformation O
layer O
to O
obtain O
the O
compressed O
state O
. O

• O
Sequential O
Neural O
Network O
: O
All O
available O
states O
are O
sequentially O
fed O
into O
an O
LSTM O
and O
the O
hidden O
output O
of O
the O
last O
time O
- O
step O
is O
regarded O
as O
the O
integrated O
state O
. O

Formally O
, O
the O
state O
of O
the O
i O
- O
th O
layer O
is O
denoted O
as O
s O
i O
. O
When O
forward O
propagation O
proceeds O
to O
the O
i O
- O
th O
intermediate O
layer O
, O
all O
the O
past O
states O
s O
1 O
: O
i O
are O
incorporated O
into O
a O
global O
past O
state O
s O
p O
: O

s O
p O
= O
G O
( O
s O
1 O
: O
i O
) O
( O
1 O
) O

where O
G O
( O
• O
) O
refers O
to O
one O
of O
the O
state O
incorporation O
strategies O
. O

Imitation O
of O
Future O
States O

Existing O
work O
for O
early B-TaskName
exit I-TaskName
stops O
inference O
at O
an O
intermediate O
layer O
and O
ignores O
the O
underlying O
valuable O
features O
captured O
by O
the O
future O
layers O
. O
Such O
treatment O
is O
partly O
rationalized O
by O
the O
recent O
claim O
( O
Kaya O
et O
al O
. O
, O
2019 O
) O
that O
shallow O
layers O
are O
adequate O
to O
make O
a O
correct O
prediction O
. O
However O
, O
Jawahar O
et O
al O
. O
( O
2019 O
) O
reveal O
that O
the O
pre O
- O
trained O
language O
models O
capture O
a O
hierarchy O
of O
linguistic O
information O
from O
the O
lower O
to O
the O
upper O
layers O
, O
e.g. O
, O
the O
lower O
layers O
learn O
the O
surface O
or O
syntactic O
features O
while O
the O
upper O
layers O
capture O
high O
- O
level O
information O
like O
the O
semantic O
features O
. O
We O
hypothesize O
that O
some O
instances O
not O
only O
rely O
on O
syntactic O
features O
but O
also O
require O
semantic O
features O
. O
It O
is O
actually O
undesirable O
to O
only O
consider O
features O
captured O
by O
shallow O
layers O
. O
Therefore O
, O
we O
propose O
to O
take O
advantage O
of O
both O
past O
and O
future O
states O
. O
Normally O
, O
we O
can O
directly O
fetch O
the O
past O
states O
, O
while O
using O
future O
information O
is O
intractable O
how O
since O
the O
future O
states O
are O
inaccessible O
before O
passing O
through O
the O
future O
layers O
. O
To O
bridge O
this O
gap O
, O
we O
propose O
a O
simple O
method O
to O
approximate O
the O
future O
states O
in O
light O
of O
imitation O
learning O
( O
Ross O
et O
al O
. O
, O
2011 O
; O
Nguyen O
, O
2016 O
; O
Ho O
and O
Ermon O
, O
2016 O
) O
. O
We O
couple O
each O
layer O
with O
an O
imitation O
learner O
. O
During O
training O
, O
the O
imitation O
learner O
is O
encouraged O
to O
mimic O
the O
representation O
of O
the O
real O
state O
of O
that O
layer O
. O
Through O
this O
layer O
- O
wise O
imitation O
, O
we O
can O
obtain O
approximations O
of O
the O
future O
states O
with O
minimum O
cost O
. O
The O
illustration O
of O
the O
future O
imitation O
learning O
during O
inference O
is O
shown O
in O
Figure O
2 O
. O

To O
be O
precise O
, O
we O
intend O
to O
obtain O
a O
state O
approximation O
of O
the O
j O
- O
th O
layer O
if O
the O
forward O
pass O
exits O
at O
the O
intermediate O
i O
- O
th O
layer O
for O
any O
j O
> O
i. O
During O
training O
, O
we O
pass O
through O
the O
entire O
n O
- O
layer O
model O
but O
we O
simulate O
the O
situation O
that O
the O
forward O
pass O
ends O
up O
at O
the O
i O
- O
th O
layer O
for O
any O
i O
< O
n. O
The O
j O
- O
th O
learner O
corresponding O
to O
the O
j O
- O
th O
layer O
takes O
s O
i O
as O
input O
and O
outputs O
an O
approximationŝ O
i O
j O
of O
the O
real O
state O
s O
j O
. O
Then O
s O
j O
serves O
as O
a O
teacher O
to O
guide O
the O
jth O
imitation O
learner O
. O
We O
adopt O
cosine O
similarity O
as O
the O
distance O
measurement O
and O
penalize O
the O
discrepancy O
between O
the O
real O
state O
s O
j O
and O
the O
learned O
statê O
s O
i O
j O
. O
Let O
L O
i O
cos O
denotes O
the O
imitation O
loss O
of O
the O
situation O
that O
the O
forward O
pass O
exits O
at O
the O
i O
- O
th O
layer O
, O
it O
is O
computed O
as O
the O
average O
of O
the O
similarity O
loss O
for O
any O
j O
> O
i. O
Since O
the O
exit O
layer O
i O
can O
be O
any O
number O
between O
2 O
to O
n O
during O
inference O
, O
we O
go O
through O
all O
possible O
number O
i O
and O
average O
the O
corresponding O
L O
i O
cos O
, O
resulting O
the O
overall O
loss O
L O
cos O
: O

s O
i O
j O
= O
Learner O
j O
( O
s O
i O
) O
( O
2 O
) O
l O
i O
, O
j O
cos O
( O
s O
j O
, O
ŝ O
i O
j O
) O
= O
1 O
−ŝ O
i O
j O
• O
s O
j O
ŝ O
i O
j O
s O
j O
( O
3 O
) O
L O
i O
cos O
= O
1 O
n O
− O
i O
n O
j O
= O
i+1 O
l O
i O
, O
j O
cos O
( O
s O
j O
, O
ŝ O
i O
j O
) O
( O
4 O
) O
L O
cos O
= O
1 O
n O
− O
1 O
n O
i=2 O
L O
i O
cos O
( O
5 O
) O

where O
• O
denotes O
the O
L O
2 O
norm O
. O
Learner O
j O
( O
• O
) O
is O
a O
simple O
feed O
- O
forward O
layer O
with O
learnable O
parameters O
W O
i O
and O
b O
i O
. O

During O
training O
, O
the O
forward O
propagation O
is O
computed O
on O
all O
layers O
and O
all O
imitation O
learners O
are O
encouraged O
to O
generate O
representations O
close O
to O
the O
real O
states O
. O
During O
inference O
, O
the O
forward O
propagation O
proceeds O
to O
the O
i O
- O
th O
intermediate O
layer O
and O
the O
subsequent O
imitation O
learners O
take O
the O
i O
- O
th O
real O
state O
as O
input O
to O
generate O
the O
approximations O
of O
future O
states O
. O
Then O
the O
approximations O
are O
incorporated O
into O
a O
comprehensive O
future O
state O
s O
f O
with O
one O
of O
the O
global O
strategies O
introduced O
before O
: O

s O
f O
= O
G O
( O
ŝ O
i O
i+1 O
: O
n O
) O
( O
6 O
) O

whereŝ O
i O
i+1 O
: O
n O
denotes O
the O
approximations O
of O
the O
states O
from O
the O
( O
i+1 O
) O
-th O
layer O
to O
the O
n O
- O
th O
layer O
. O

Adaptive O
Merging O
Gate O

We O
then O
explore O
how O
to O
adaptively O
merge O
the O
past O
information O
and O
future O
information O
. O
Intuitively O
, O
the O
past O
state O
s O
p O
and O
the O
future O
state O
s O
f O
are O
of O
different O
importance O
since O
the O
authentic O
past O
states O
are O
more O
reliable O
than O
our O
imitated O
future O
states O
. O
In O
addition O
, O
different O
instances O
depend O
differently O
on O
high O
- O
level O
features O
learned O
by O
future O
layers O
. O
Therefore O
, O
it O
is O
indispensable O
to O
develop O
an O
adaptive O
method O
to O
automatically O
combine O
the O
past O
state O
s O
p O
and O
the O
future O
state O
s O
f O
. O
In O
our O
work O
, O
we O
design O
an O
adaptive O
merging O
gate O
to O
automatically O
fuse O
the O
past O
state O
s O
p O
and O
the O
future O
state O
s O
f O
. O
As O
the O
forward O
propagation O
proceeds O
to O
the O
i O
- O
th O
layer O
, O
we O
compute O
the O
reliability O
of O
the O
past O
state O
s O
p O
, O
and O
the O
final O
merged O
representation O
is O
a O
trade O
- O
off O
between O
these O
two O
states O
: O

α O
= O
sigmoid O
( O
FFN O
( O
s O
p O
) O
) O
( O
7 O
) O

z O
i O
= O
αs O
p O
+ O
( O
1 O
− O
α O
) O
s O
f O
( O
8 O
) O

During O
training O
, O
each O
layer O
can O
generate O
the O
approximated O
states O
of O
future O
and O
obtain O
a O
merged O
final O
state O
which O
is O
used O
for O
prediction O
. O
Then O
the O
model O
will O
be O
updated O
with O
the O
layer O
- O
wise O
crossentropy O
loss O
against O
the O
ground O
- O
truth O
label O
y. O
The O
merging O
gate O
adaptively O
learns O
to O
adjust O
the O
balance O
under O
the O
supervision O
signal O
given O
by O
ground O
- O
truth O
labels O
. O
However O
, O
with O
the O
layer O
- O
wise O
optimization O
objectives O
, O
the O
shallow O
layers O
will O
be O
updated O
more O
frequently O
since O
they O
receive O
more O
updating O
signals O
from O
higher O
layers O
. O
To O
address O
this O
issue O
, O
we O
heuristically O
re O
- O
weight O
the O
cross O
entropy O
loss O
of O
each O
layer O
depending O
on O
its O
depth O
i O
and O
get O
its O
weight O
w O
i O
. O
The O
updating O
procedure O
is O
formalized O
as O
: O

w O
i O
= O
i O
n O
j=1 O
j O
( O
9 O
) O
p O
i O
= O
softmax O
( O
z O
i O
) O
( O
10 O
) O

L O
i O
ce O
= O
− O
l∈labels O
y O
( O
l O
) O
log O
( O
p O
i O
( O
l O
) O
) O
( O
11 O
) O

L O
ce O
= O
n O
i=1 O
w O
i O
L O
i O
ce O
( O
12 O
) O

The O
overall O
loss B-HyperparameterName
is O
computed O
as O
follows O
: O

L O
= O
L O
ce O
+ O
L O
cos O
( O
13 O
) O

Fine O
- O
tuning O
and O
Inference O

Here O
we O
introduce O
the O
fine O
- O
tuning O
technique O
and O
the O
exit O
condition O
at O
the O
inference O
stage O
. O

Fine O
- O
tuning O
The O
representations O
learned O
by O
shallow O
layers O
have O
a O
big O
impact O
on O
performance O
in O
the O
early O
exit O
framework O
since O
the O
prediction O
largely O
depends O
on O
the O
states O
of O
shallow O
layers O
. O
Most O
existing O
work O
updates O
all O
of O
the O
model O
layers O
at O
each O
step O
during O
fine O
- O
tuning O
to O
adapt O
to O
the O
data O
of O
downstream O
tasks O
. O
However O
, O
we O
argue O
that O
such O
an O
aggressive O
updating O
strategy O
may O
undermine O
the O
well O
- O
generalized O
features O
learned O
in O
the O
pretraining O
stage O
. O
In O
our O
work O
, O
we O
try O
to O
balance O
the O
requirements O
of O
maintaining O
features O
learned O
in O
pre O
- O
training O
and O
adapting O
to O
data O
at O
the O
fine O
- O
tuning O
stage O
. O
Specifically O
, O
the O
parameters O
of O
a O
layer O
will O
be O
frozen O
with O
a O
probability O
p O
and O
the O
probability O
p O
linearly O
decreases O
from O
the O
first O
layer O
to O
the O
L O
- O
th O
layer O
in O
a O
range O
of O
1 O
to O
0 O
. O

Inference O
Following O
Xin O
et O
al O
. O
( O
2020 O
) O
, O
we O
quantify O
the O
prediction O
confidence O
e O
with O
the O
entropy O
of O
the O
output O
distribution O
p O
i O
of O
i O
- O
th O
layer O
: O

e O
( O
p O
i O
) O
= O
Entropy O
( O
p O
i O
) O
( O
14 O
) O

The O
inference O
stops O
once O
the O
confidence O
e O
( O
p O
i O
) O
is O
lower O
than O
a O
predefined O
threshold B-HyperparameterName
τ B-HyperparameterName
. O
The O
hyperparameter O
τ B-HyperparameterName
is O
adjusted O
according O
to O
the O
required O
speed O
- O
up O
ratios O
. O
If O
the O
exit O
condition O
is O
never O
reached O
, O
our O
model O
degrades O
into O
the O
common O
case O
of O
inference O
that O
the O
complete O
forward O
propagation O
is O
accomplished O
. O

Experiments O

Experimental O
Setup O

Experimental O
Settings O
Following O
previous O
work O
( O
Xin O
et O
al O
. O
, O
2020 O
) O
, O
we O
evaluate O
our O
proposed O
method O
on O
six O
classification O
datasets O
from O
the O
GLUE B-DatasetName
benchmark O
: O
SST-2 B-DatasetName
, O
MRPC B-DatasetName
, O
QNLI B-DatasetName
, O
RTE B-DatasetName
, O
QQP B-DatasetName
, O
and O
MNLI B-DatasetName
. O
We O
perform O
a O
grid O
search O
over O
the O
sets O
of O
learning B-HyperparameterName
rate I-HyperparameterName
as O
{ O
1e-5 B-HyperparameterValue
, O
2e-5 B-HyperparameterValue
, O
3e-5 B-HyperparameterValue
, O
5e-5 B-HyperparameterValue
} O
, O
batch B-HyperparameterName
size I-HyperparameterName
as O
{ O
16 B-HyperparameterValue
, O
32 B-HyperparameterValue
, O
128 B-HyperparameterValue
} O
and O
number O
of O
frozen B-HyperparameterName
layers I-HyperparameterName
during O
fine O
- O
tuning O
as O
{ O
0,1,2,3 B-HyperparameterValue
} O
. O
The O
maximum O
sequence B-HyperparameterName
length I-HyperparameterName
is O
fixed O
to O
128 B-HyperparameterValue
. O
We O
employ O
a O
linear O
decay O
learning O
rate O
scheduler O
and O
the O
AdamW O
optimizer O
. O
In O
addition O
, O
we O
use O
the O
concatenation O
strategy O
to O
incorporate O
all O
available O
states O
for O
its O
best O
performance O
on O
the O
GLUE B-DatasetName
dev O
set O
. O

Speed O
Measurement O
Since O
the O
measurement O
of O
runtime O
might O
not O
be O
stable O
, O
following O
Xin O
et O
al O
. O
( O
2020 O
) O
; O
, O
we O
manually O
adjust O
the O
exit O
threshold O
τ B-HyperparameterName
and O
calculate O
the O
speed O
- O
up O
ratio O
by O
comparing O
the O
actually O
executed O
layers O
in O
forward O
propagation O
and O
the O
required O
complete O
layers O
. O
For O
a O
n O
- O
layer B-HyperparameterName
model O
, O
the O
speed O
- O
up O
ratio O
is O
: O

speed O
- O
up O
ratio O
= O
n O
i=1 O
n O
* O
m O
i O
n O
i=1 O
i O
* O
m O
i O
( O
15 O
) O

where O
m O
i O
is O
the O
number O
of O
examples O
that O
exit O
at O
the O
i O
- O
th O
layer O
of O
the O
model O
. O

Baselines O

The O
proposed O
method O
can O
be O
practical O
for O
a O
range O
of O
existing O
pre O
- O
trained O
language O
models O
. O
Without O
losing O
generality O
, O
we O
conduct O
experiments O
with O
several O
well O
- O
known O
PLMs O
as O
backbones O
, O
namely O
, O
BERT B-MethodName
, O
RoBERTa B-MethodName
, O
and O
ALBERT B-MethodName
( O
Lan O
et O
al O
. O
, O
2019 O
) O
. O
Both O
BERT B-MethodName
and O
RoBERTa B-MethodName
suffer O
from O
the O
problem O
of O
over O
- O
parameterization O
. O
ALBERT B-MethodName
largely O
alleviates O
this O
problem O
and O
is O
very O
efficient O
in O
terms O
of O
model O
size O
, O
the O
results O
on O
which O
verify O
the O
effectiveness O
on O
such O
parameter O
- O
efficient O
models O
. O
We O
mainly O
compare O
our O
method O
with O
other O
methods O
targeting O
on O
reducing O
the O
depth O
of O
models O
, O
including O
the O
recent O
early O
exit O
methods O
and O
the O
method O
directly O
reducing O
model O
depth O
to O
m O
layers B-HyperparameterName
which O
is O
denoted O
as O
( B-MethodName
AL I-MethodName
) I-MethodName
BERT I-MethodName
- I-MethodName
mL I-MethodName
. O

Overall O
Comparison O

We O
compare O
our O
model O
performance O
with O
the O
baseline O
methods O
when O
different O
backbone O
models O
are O
adopted O
and O
show O
the O
result O
in O
method O
maintains O
a O
comparable O
result O
with O
the O
original O
models O
on O
most O
datasets O
. O
We O
also O
notice O
that O
directly O
reducing O
layers O
performs O
well O
and O
serves O
as O
a O
strong O
baseline O
. O
Nevertheless O
, O
our O
proposal O
significantly O
outperforms O
such O
a O
method O
as O
well O
as O
the O
other O
two O
early B-TaskName
exit I-TaskName
methods O
. O

We O
then O
adopt O
a O
more O
aggressive O
3.00× O
speedup O
ratio O
to O
verify O
the O
effectiveness O
of O
our O
method O
. O
According O
to O
Table O
2 O
, O
the O
performance O
of O
PABEE B-MethodName
and O
DeeBERT B-MethodName
deteriorates O
badly O
. O
In O
contrast O
, O
our O
model O
exhibits O
more O
robust O
and O
stable O
performance O
, O
showing O
its O
superiority O
over O
previous O
early O
exit O
methods O
. O
Particularly O
, O
ALBERT B-MethodName
is O
already O
very O
efficient O
in O
model O
size O
owing O
to O
its O
layer O
- O
sharing O
mechanism O
. O
Results O
shown O
in O
the O
bottom O
of O
Table O
2 O
suggest O
that O
our O
model O
can O
obtain O
a O
good O
result O
with O
minimum O
performance O
loss O
on O
such O
a O
parameterefficient O
model O
. O
The O
success O
of O
our O
proposal O
might O
be O
attributed O
to O
the O
global O
perspective O
for O
prediction O
. O
DeeBERT B-MethodName
makes O
prediction O
with O
the O
help O
of O
the O
state O
of O
a O
single O
branch O
classifier O
, O
leading O
to O
less O
reliable O
results O
. O
Although O
PABEE B-MethodName
employs O
cross O
- O
layer O
prediction O
to O
prevent O
error O
from O
one O
single O
classifier O
, O
they O
ignore O
much O
available O
information O
of O
past O
states O
as O
well O
as O
the O
high O
- O
level O
semantic O
features O
captured O
by O
future O
layers O
. O
Different O
from O
those O
methods O
, O
our O
method O
jointly O
takes O
into O
consideration O
the O
hierarchical O
linguistic O
information O
embedded O
in O
all O
layers O
and O
thus O
is O
able O
to O
produce O
more O
accurate O
results O
. O

Performance O
- O
Efficiency O
Trade O
- O
Off O

To O
further O
verify O
the O
robustness O
and O
efficiency O
of O
our O
method O
, O
we O
visualize O
the O
performanceefficiency O
trade O
- O
off O
curves O
in O
Figure O
3 O
on O
a O
representative O
subset O
of O
the O
GLUE B-DatasetName
dev O
set O
. O
The O
backbone O
model O
is O
BERT B-MethodName
. O
Please O
refer O
to O
the O
Appendix O
A O
for O
results O
of O
RoBERTa B-MethodName
and O
ALBERT B-MethodName
. O
As O
can O
be O
seen O
from O
Figure O
3 O
, O
the O
performance O
of O
previous O
stateof O
- O
the O
- O
art O
early O
exit O
methods O
drops O
dramatically O
when O
the O
speed O
- O
up O
ratio O
increases O
, O
which O
limits O
their O
practicality O
for O
higher O
acceleration O
requirements O
. O
By O
comparison O
, O
our O
method O
demonstrates O
more O
tolerance O
of O
speed O
- O
up O
ratio O
. O
It O
significantly O
improves O
performance O
compared O
to O
previous O
bestperforming O
early O
exit O
models O
under O
the O
same O
speedup O
ratio O
, O
especially O
in O
the O
case O
that O
the O
speed O
- O
up O
ratio O
is O
high O
, O
indicating O
that O
it O
can O
be O
applied O
in O
a O
wider O
range O
of O
acceleration O
scenarios O
. O

Analysis O

Effect O
of O
Global O
Strategies O

The O
results O
of O
different O
global O
strategies O
on O
a O
representative O
subset O
of O
GLUE B-DatasetName
dev O
are O
shown O
in O
Table O
3 O
. O
The O
naive O
global O
strategies O
including O
voting O
and O
ensemble O
perform O
poorly O
, O
which O
demonstrates O
that O
existing O
global O
strategies O
can O
only O
achieve O
suboptimal O
performance O
. O
In O
contrast O
, O
we O
design O
simple O
yet O
effective O
global O
strategies O
to O
incorporate O
past O
states O
which O
bring O
significant O
improvement O
compared O
to O
baselines O
. O
In O
addition O
, O
we O
empirically O
find O
that O
the O
concatenation O
strategy O
works O
best O
from O
an O
overall O
point O
of O
view O
. O
We O
assume O
that O
such O
a O
strategy O
allows O
interaction O
among O
different O
states O
, O
yielding O
better O
performance O
. O
In O
addition O
, O
the O
effect O
of O
the O
merging O
gate O
can O
be O
found O
in O
Appendix O
B O
. O

Analysis O
of O
Future O
Information O

To O
assess O
whether O
and O
how O
future O
information O
contributes O
to O
the O
prediction O
, O
we O
first O
evaluate O
the O
Global O
Future O
version O
of O
our O
early O
exit O
method O
where O
all O
the O
approximations O
of O
futures O
states O
are O
incorporated O
through O
the O
concatenation O
strategy O
. O
Effect O
of O
future O
information O
is O
backed O
with O
the O
results O
shown O
in O
Table O
4 O
. O
We O
observe O
that O
the O
Global O
Future O
mechanism O
brings O
improvement O
on O
most O
datasets O
for O
both O
2× O
speed O
- O
up O
ratio O
and O
3× O
speedup O
ratio O
, O
which O
confirms O
that O
the O
approximations O
of O
future O
states O
help O
enhance O
the O
model O
ability O
in O
prediction O
. O
Beyond O
that O
, O
the O
future O
states O
can O
be O
especially O
advantageous O
for O
the O
models O
with O
a O
higher O
speed O
- O
up O
ratio O
. O
Recall O
that O
approximations O
of O
future O
states O
complement O
the O
high O
- O
level O
semantic O
information O
and O
the O
exit O
at O
shallow O
layers O
loses O
more O
semantic O
information O
in O
comparison O
with O
the O
exit O
at O
deep O
layers O
. O
Therefore O
, O
the O
benefit O
of O
future O
information O
is O
more O
significant O
compared O
to O
the O
exit O
at O
shallow O
layers O
, O
which O
is O
validated O
by O
the O
larger O
improvement O
gap O
with O
a O
3× O
speed O
- O
up O
ratio O
. O

We O
also O
investigate O
the O
effect O
of O
future O
information O
on O
exit O
time O
. O
Figure O
4 O
demonstrates O
the O
distribution O
of O
exit O
layers O
with O
and O
without O
future O
information O
. O
When O
future O
information O
is O
engaged O
, O
we O
observe O
that O
the O
proportion O
of O
exit O
at O
shallow O
layers O
increases O
. O
The O
observation O
conforms O
with O
our O
intuition O
: O
with O
the O
approximations O
of O
future O
states O
supplemented O
for O
prediction O
, O
the O
merged O
state O
at O
a O
shallow O
layer O
is O
able O
to O
make O
a O
confident O
and O
correct O
prediction O
. O
Thus O
the O
exit O
time O
is O
earlier O
compared O
to O
situations O
without O
future O
states O
, O
result- O
ing O
in O
a O
higher O
speed O
- O
up O
ratio O
. O
To O
be O
more O
specific O
, O
for O
MRPC B-DatasetName
, O
the O
speed B-MetricName
- I-MetricName
up I-MetricName
ratios I-MetricName
with O
and O
without O
future O
states O
are O
1.69 B-MetricValue
and O
1.99 B-MetricValue
, O
and O
are O
1.92 B-MetricValue
and O
2.04 B-MetricValue
for O
MNLI B-DatasetName
, O
respectively O
. O
Meanwhile O
, O
we O
observe O
a O
performance O
boost O
with O
future O
states O
involved O
. O
It O
confirms O
our O
assumption O
that O
the O
high O
- O
level O
semantic O
features O
embedded O
in O
future O
states O
help O
improve O
performance O
in O
early B-TaskName
exit I-TaskName
framework O
. O

Comparison O
with O
Distillation O
Methods O

As O
an O
alternative O
method O
to O
accelerate O
inference O
, O
knowledge O
distillation O
also O
exhibits O
promising O
performance O
for O
NLP O
tasks O
. O
We O
provide O
comparison O
with O
typical O
knowledge O
distillation O
methods O
in O
Table O
5 O
. O
Existing O
model O
TinyBERT B-MethodName
( O
Jiao O
et O
al O
. O
, O
2019 O
) O
exerts O
multiple O
elaborate O
strategies O
to O
achieve O
the O
state O
- O
of O
- O
the O
- O
art O
results O
, O
including O
the O
expensive O
general O
distillation O
process O
and O
a O
vast O
amount O
of O
augmented O
data O
for O
fine O
- O
tuning O
. O
We O
remove O
these O
two O
techniques O
to O
exclude O
the O
effect O
of O
extra O
training O
data O
. O
Under O
the O
same O
settings O
, O
we O
observe O
that O
our O
method O
outperforms O
the O
distillation O
methods O
with O
the O
same O
speed O
- O
up O
ratio O
. O

In O
general O
, O
early O
exit O
and O
distillation O
methods O
improve O
inference O
efficiency O
from O
different O
perspec- O
tives O
. O
The O
distillation O
methods O
are O
more O
efficient O
in O
saving O
memory O
usage O
, O
but O
the O
downside O
is O
that O
such O
static O
methods O
suffer O
from O
high O
computation O
cost O
to O
adapt O
to O
different O
speed O
- O
up O
ratios O
. O
A O
new O
student O
model O
has O
to O
be O
trained O
from O
scratch O
if O
the O
speedup O
requirement O
changes O
. O
By O
contrast O
, O
dynamic O
methods O
are O
more O
flexible O
to O
meet O
different O
acceleration O
requirements O
. O
Concretely O
, O
simple O
instances O
will O
be O
processed O
by O
passing O
through O
fewer O
layers O
and O
complex O
instances O
may O
require O
more O
layers O
. O
Moreover O
, O
the O
speed O
- O
up O
ratio O
can O
be O
easily O
adjusted O
depending O
on O
the O
acceleration O
requests O
. O
Nevertheless O
, O
early B-TaskName
exit I-TaskName
and O
distillation O
accelerate O
inference O
from O
different O
perspectives O
and O
these O
two O
kinds O
of O
techniques O
can O
be O
integrated O
to O
further O
compress O
the O
model O
size O
and O
accelerate O
the O
inference O
time O
. O

Conclusions O

We O
propose O
a O
novel O
Past O
- O
Future O
early B-TaskName
exit I-TaskName
method O
from O
a O
global O
perspective O
. O
Unlike O
previous O
work O
using O
only O
local O
states O
for O
prediction O
, O
our O
model O
employs O
all O
available O
past O
states O
for O
prediction O
and O
propose O
a O
novel O
approach O
to O
engage O
the O
future O
states O
which O
are O
originally O
inaccessible O
for O
prediction O
. O
Experiments O
illustrate O
that O
our O
method O
achieves O
significant O
improvement O
over O
baseline O
methods O
with O
different O
models O
as O
backbones O
, O
suggesting O
the O
superiority O
of O
our O
early B-TaskName
exit I-TaskName
method O
. O

Acknowledgements O

We O
thank O
all O
the O
anonymous O
reviewers O
for O
their O
constructive O
comments O
. O
This O
work O
is O
partly O
supported O
by O
National O
Key O
R O
& O
D O
Program O
of O
China O
No O
. O
2019YFC1521200 O
and O
Beijing O
Academy O
of O
Artificial O
Intelligence O
( O
BAAI O
) O
. O
Xu O
Sun O
is O
the O
corresponding O
author O
. O

A O
More O
Performance O
- O
Efficiency O

Trade O
- O
Off O
Curves O

Performance O
- O
efficiency O
curves O
with O
RoBERTa B-MethodName
and O
ALBERT B-MethodName
as O
backbones O
are O
shown O
in O
Figure O
5 O
and O
Figure O
6 O
respectively O
. O
Similar O
to O
the O
observation O
with O
BERT B-MethodName
as O
backbone O
, O
the O
performance O
of O
Dee B-MethodName
- I-MethodName
BERT I-MethodName
and O
PABEE B-MethodName
becomes O
progressively O
worse O
as O
the O
speed O
- O
up O
ratio O
increases O
. O
In O
contrast O
, O
our O
past O
- O
future O
early O
exit O
method O
shows O
more O
robust O
results O
. O
We O
conduct O
ablation O
study O
to O
show O
the O
effect O
of O
the O
merging O
gate O
and O
report O
the O
result O
in O
Table O
6 O
. O
We O
can O
see O
that O
the O
performance O
drops O
when O
we O
remove O
the O
merging O
gate O
from O
our O
model O
, O
suggesting O
that O
the O
merging O
gate O
plays O
an O
important O
role O
in O
keeping O
the O
balance O
between O
past O
information O
and O
future O
information O
. O

COLD B-DatasetName
: O
A O
Benchmark O
for O
Chinese B-TaskName
Offensive I-TaskName
Language I-TaskName
Detection I-TaskName

Offensive B-TaskName
language I-TaskName
detection I-TaskName
is O
increasingly O
crucial O
for O
maintaining O
a O
civilized O
social O
media O
platform O
and O
deploying O
pre O
- O
trained O
language O
models O
. O
However O
, O
this O
task O
in O
Chinese O
is O
still O
under O
exploration O
due O
to O
the O
scarcity O
of O
reliable O
datasets O
. O
To O
this O
end O
, O
we O
propose O
a O
benchmark O
-COLD B-DatasetName
for O
Chinese B-TaskName
offensive I-TaskName
language I-TaskName
analysis I-TaskName
, O
including O
a O
Chinese B-DatasetName
Offensive I-DatasetName
Language I-DatasetName
Dataset I-DatasetName
-COLDATASET B-DatasetName
and O
a O
baseline O
detector O
-COLDETECTOR B-MethodName
which O
is O
trained O
on O
the O
dataset O
. O
We O
show O
that O
the O
COLD B-DatasetName
benchmark O
contributes O
to O
Chinese B-TaskName
offensive I-TaskName
language I-TaskName
detection I-TaskName
which O
is O
challenging O
for O
existing O
resources O
. O
We O
then O
deploy O
the O
COLDETECTOR B-MethodName
and O
conduct O
detailed O
analyses O
on O
popular O
Chinese O
pre O
- O
trained O
language O
models O
. O
We O
first O
analyze O
the O
offensiveness O
of O
existing O
generative O
models O
and O
show O
that O
these O
models O
inevitably O
expose O
varying O
degrees O
of O
offensive O
issues O
. O
Furthermore O
, O
we O
investigate O
the O
factors O
that O
influence O
the O
offensive O
generations O
, O
and O
we O
find O
that O
anti O
- O
bias O
contents O
and O
keywords O
referring O
to O
certain O
groups O
or O
revealing O
negative O
attitudes O
trigger O
offensive O
outputs O
easier O
. O
1 O

Introduction O

Offensive B-TaskName
language I-TaskName
detection I-TaskName
task O
plays O
an O
essential O
role O
in O
maintaining O
social O
platforms O
and O
promoting O
civilized O
communication O
Noever O
, O
2018 O
; O
Dinan O
et O
al O
. O
, O
2019 O
; O
Jahan O
and O
Oussalah O
, O
2021 O
) O
. O
With O
the O
rise O
of O
large O
- O
scale O
language O
models O
( O
Zhang O
et O
al O
. O
, O
2020 O
; O
Roller O
et O
al O
. O
, O
2020 O
; O
Wang O
et O
al O
. O
, O
2020 O
; O
Zhang O
et O
al O
. O
, O
2021b O
) O
, O
the O
safety O
issues O
due O
to O
offensive O
generation O
continue O
to O
be O
exposed O
( O
Gehman O
et O
al O
. O
, O
2020 O
; O
Bender O
et O
al O
. O
, O
2021 O
; O
Mi O
et O
al O
. O
, O
2022 O
) O
, O
attracting O
widespread O
attention O
from O
researchers O
and O
pushing O
the O
research O
boom O
on O
this O
task O
to O
new O
heights O
( O
Sheng O
et O
al O
. O
, O
2021 O
; O
. O

To O
tackle O
the O
problem O
of O
offensive O
language O
detection O
, O
a O
reliable O
and O
versatile O
benchmark O
is O
a O
needed O
basis O
to O
accelerate O
in O
- O
depth O
research O
. O
The O
datasets O
, O
including O
WTC B-DatasetName
( O
Wulczyn O
et O
al O
. O
, O
2017 O
) O
, O
OLID B-DatasetName
( O
Zampieri O
et O
al O
. O
, O
2019 O
) O
, O
BAD B-DatasetName
and O
RealToxicPrompts B-DatasetName
( O
Gehman O
et O
al O
. O
, O
2020 O
) O
, O
are O
proposed O
to O
study O
the O
safety O
issues O
from O
different O
dimensions O
and O
granularities O
. O
The O
publicly O
available O
detector O
, O
PerspectiveAPI B-MethodName
2 I-MethodName
, O
is O
widely O
used O
for O
toxicity O
evaluation O
and O
contributes O
to O
creating O
safer O
environments O
for O
online O
communication O
( O
Han O
and O
Tsvetkov O
, O
2020 O
; O
. O
However O
, O
most O
existing O
works O
focus O
on O
English O
. O
The O
issue O
of O
Chinese O
offensive O
language O
detection O
has O
not O
been O
well O
studied O
due O
to O
the O
lack O
of O
labeled O
datasets O
and O
reliable O
detectors O
. O

In O
addition O
, O
large O
- O
scale O
language O
models O
often O
lean O
biases O
in O
pre O
- O
training O
data O
and O
generates O
offensive O
or O
unethical O
contents O
( O
Sheng O
et O
al O
. O
, O
2021 O
; O
Zhang O
et O
al O
. O
, O
2021a O
) O
, O
which O
substantially O
hinders O
the O
deployment O
of O
models O
in O
practice O
. O
Meanwhile O
, O
limited O
by O
reliable O
benchmarks O
, O
the O
offensiveness O
of O
Chinese O
language O
models O
has O
not O
yet O
been O
thoroughly O
studied O
. O
How O
offensive O
can O
Chinese O
language O
models O
be O
? O
What O
contents O
influence O
the O
triggering O
of O
offensive O
generation O
? O
Diving O
deeper O
into O
these O
questions O
will O
facilitate O
building O
more O
reliable O
and O
deployable O
language O
models O
. O
This O
paper O
proposes O
a O
benchmark O
named O
COLD B-DatasetName
to O
tackle O
the O
above O
challenges O
in O
Chinese O
offensive O
language O
research O
. O
The O
COLDATASET B-DatasetName
( O
Chinese B-DatasetName
Offensive I-DatasetName
Language I-DatasetName
Dataset I-DatasetName
) O
, O
contains O
37,480 O
comments O
with O
binary O
offensive O
labels O
and O
covers O
diverse O
topics O
of O
race O
, O
gender O
, O
and O
region O
. O
To O
gain O
further O
insights O
into O
the O
data O
types O
and O
characteristics O
, O
we O
annotate O
the O
test O
set O
at O
a O
fine O
- O
grained O
level O
with O
four O
categories O
: O
attacking O
individuals O
, O
attacking O
groups O
, O
anti O
- O
bias O
and O
other O
non O
- O
offensive O
. O
We O
present O
a O
baseline O
detector O
, O
COLDETECTOR B-MethodName
, O
for O
offensive O
language O
detection O
, O
which O
adopts O
pretrained O
Chinese B-MethodName
BERT I-MethodName
and O
is O
fine O
- O
tuned O
on O
the O
proposed O
dataset O
and O
performs O
satisfactorily O
compared O
to O
other O
methods O
using O
existing O
resources O
and O
technology O
. O

With O
the O
proposed O
benchmark O
COLD B-DatasetName
, O
we O
evaluate O
the O
offensiveness O
of O
popular O
Chinese O
generation O
models O
, O
including O
CDialGPT B-MethodName
( O
Wang O
et O
al O
. O
, O
2020 O
) O
, O
CPM B-MethodName
( O
Zhang O
et O
al O
. O
, O
2021b O
) O
, O
and O
EVA B-MethodName
, O
to O
investigate O
their O
strengths O
and O
weaknesses O
in O
terms O
of O
safety O
. O
Experimental O
results O
show O
that O
both O
offensive O
and O
non O
- O
offensive O
inputs O
have O
the O
risk O
of O
inducing O
safety O
issues O
. O
Additionally O
, O
some O
types O
of O
prompts O
, O
including O
anti O
- O
bias O
contents O
, O
certain O
target O
group O
keywords O
and O
negative O
attitude O
words O
, O
can O
more O
easily O
trigger O
offensive O
outputs O
than O
other O
inputs O
. O
Figure O
1 O
The O
contributions O
of O
this O
work O
are O
threefold O
: O

• O
We O
present O
, O
to O
the O
best O
of O
our O
knowledge O
, O
the O
first O
publicly O
available O
Chinese B-DatasetName
Offensive I-DatasetName
Language I-DatasetName
Dataset I-DatasetName
: O
COLDATASET B-DatasetName
. O
It O
contains O
37,480 O
sentences O
and O
covers O
the O
topics O
of O
race O
, O
gender O
and O
region O
. O

• O
We O
provide O
the O
a O
baseline O
detector O
, O
COLDE B-MethodName
- I-MethodName
TECTOR I-MethodName
, O
together O
with O
discussions O
on O
existing O
detection O
methods O
. O
We O
show O
the O
contribution O
of O
the O
proposed O
benchmark O
to O
offensive O
language O
detection O
. O

• O
We O
evaluate O
popular O
open O
- O
source O
generative O
models O
and O
reveal O
their O
varying O
degrees O
of O
offensiveness O
. O
We O
also O
show O
that O
the O
safety O
issue O
can O
be O
triggered O
by O
even O
non O
- O
offensive O
inputs O
, O
such O
as O
anti O
- O
bias O
languages O
. O

2 O
Related O
Work O

Offensive B-TaskName
Language I-TaskName
Detection I-TaskName

Offensive O
language O
, O
toxic O
language O
, O
and O
hate O
speech O
are O
highly O
related O
terms O
with O
blurred O
boundaries O
( O
Jahan O
and O
Oussalah O
, O
2021 O
) O
. O
In O
this O
paper O
, O
we O
do O
not O
distinguish O
them O
and O
use O
them O
interchangeably O
. O
The O
contents O
with O
any O
form O
of O
targeted O
offense O
to O
individuals O
or O
groups O
are O
considered O
offensive O
language O
. O
It O
includes O
veiled O
or O
direct O
offensive O
content O
expressing O
rudeness O
, O
disrespect O
, O
insults O
, O
threats O
and O
profanity O
based O
on O
aspects O
such O
as O
race O
, O
religion O
, O
sex O
, O
or O
sexual O
orientation O
( O
Zampieri O
et O
al O
. O
, O
2019 O
; O
Cambrigdge O
dictionary O
; O
. O
Automatic O
offensive O
language O
detection O
can O
help O
detoxify O
the O
online O
communities O
and O
safely O
deploy O
large O
- O
scale O
language O
models O
( O
Warner O
and O
Hirschberg O
, O
2012 O
; O
Schmidt O
and O
Wiegand O
, O
2017 O
) O
, O
which O
is O
an O
important O
task O
. O
Abundant O
efforts O
are O
seeking O
to O
detect O
hate O
speech O
based O
on O
automatic O
identification O
, O
such O
as O
topic O
analysis O
and O
keywordbased O
detection O
( O
Warner O
and O
Hirschberg O
, O
2012 O
; O
MacAvaney O
et O
al O
. O
, O
2019 O
) O
. O
Due O
to O
the O
development O
of O
deep O
learning O
and O
pre O
- O
trained O
models O
like O
BERT O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
, O
data O
- O
driven O
methods O
are O
gradually O
becoming O
mainstream O
for O
detecting O
hate O
speech O
( O
Wulczyn O
et O
al O
. O
, O
2017 O
; O
Zampieri O
et O
al O
. O
, O
2019 O
) O
. O
Meanwhile O
, O
numerous O
works O
have O
released O
large O
- O
scale O
resources O
like O
Kaggle O
Challenges O
on O
toxicity O
and O
bias O
3 O
, O
which O
offers O
significant O
support O
for O
training O
a O
strong O
and O
robust O
detector O
. O
However O
, O
offensive O
language O
detection O
in O
Chinese O
greatly O
lags O
behind O
English O
( O
Jahan O
and O
Oussalah O
, O
2021 O
) O
. O
Moreover O
, O
due O
to O
the O
specificity O
of O
Chinese O
culture O
and O
linguistics O
, O
translation O
- O
based O
methods O
contain O
inherent O
defects O
( O
Sohn O
and O
Lee O
, O
2019 O
) O
. O
In O
this O
paper O
, O
we O
release O
an O
open O
- O
source O
Chinese O
offensive O
language O
dataset O
and O
corresponding O
automatic O
detection O
methods O
, O
which O
aims O
to O
guide O
the O
development O
of O
related O
Chinese O
community O
. O

Model O
Safety O
Analysis O

With O
the O
emergence O
of O
large O
- O
scale O
pre O
- O
trained O
models O
( O
Devlin O
et O
al O
. O
, O
2019 O
; O
Roller O
et O
al O
. O
, O
2020 O
; O
Radford O
et O
al O
. O
, O
2019 O
) O
, O
their O
security O
ethics O
have O
raised O
widespread O
attention O
. O
Numerous O
previous O
research O
follow O
the O
language O
model O
analysis O
paradigm O
( O
Petroni O
et O
al O
. O
, O
2019 O
) O
and O
attempt O
to O
mine O
the O
relational O
knowledge O
presented O
in O
training O
data O
and O
stored O
in O
pre O
- O
trained O
language O
models O
. O
They O
construct O
templates O
like O
the O
" O
fill O
- O
in O
- O
the O
- O
black O
" O
cloze O
statement O
to O
analyze O
different O
safety O
issues O
, O
including O
social O
bias O
( O
Nadeem O
et O
al O
. O
, O
2020 O
; O
Nangia O
et O
al O
. O
, O
2020 O
; O
Schick O
et O
al O
. O
, O
2021 O
) O
, O
toxicity O
( O
Ousidhoum O
et O
al O
. O
, O
2021 O
) O
and O
morality O
( O
Schramowski O
et O
al O
. O
, O
2021 O
) O
. O
Another O
popular O
approach O
evaluates O
model O
safety O
by O
simulating O
the O
conversation O
and O
evaluating O
the O
generated O
responses O
in O
terms O
of O
bias O
and O
fairness O
( O
Liu O
et O
al O
. O
, O
2019 O
) O
, O
political O
prudence O
( O
Bang O
et O
al O
. O
, O
2021 O
) O
, O
and O
toxicity O
agreement O
( O
Baheti O
et O
al O
. O
, O
2021 O
) O
. O
This O
method O
requires O
proper O
prompts O
to O
probe O
the O
safety O
issues O
. O
Gehman O
et O
al O
. O

( O
2020 O
) O
claims O
that O
prompts O
with O
varying O
degrees O
of O
toxicity O
can O
all O
trigger O
toxic O
outputs O
. O
This O
paper O
follows O
the O
above O
approach O
to O
explore O
model O
's O
internal O
knowledge O
for O
offensive O
language O
detection O
and O
thoroughly O
analyze O
the O
offensiveness O
of O
generative O
language O
models O
. O

Offensiveness O
in O
Chinese O

Data O
- O
driven O
methods O
for O
offensive O
language O
detection O
and O
safety O
evaluation O
are O
proven O
effective O
in O
practice O
. O
However O
, O
there O
remains O
a O
dire O
scarcity O
of O
relevant O
resources O
in O
Chinese O
. O
In O
Table O
1 O
, O
we O
list O
, O
to O
the O
best O
of O
our O
knowledge O
, O
all O
relevant O
existing O
datasets O
in O
Chinese O
. O
Yang O
and O
Lin O
( O
2020 O
) O
introduced O
a O
dataset O
for O
detecting O
and O
rephrasing O
Chinese O
profanity O
, O
which O
is O
an O
extension O
of O
their O
previous O
version O
containing O
2k O
sentences O
( O
Su O
et O
al O
. O
, O
2017 O
) O
. O
Tang O
et O
al O
. O
( O
2020 O
) O
released O
a O
Chinese O
dataset O
COLA B-DatasetName
for O
categorizing O
offensive O
language O
, O
but O
it O
is O
not O
( O
yet O
) O
available O
at O
the O
time O
of O
writing O
of O
this O
paper O
. O
proposed O
the O
first O
Chinese O
sexism O
dataset O
for O
identifying O
gender O
- O
related O
abusive O
language O
. O
More O
recently O
, O
Zhou O
et O
al O
. O
( O
2022a O
, O
b O
) O
presented O
a O
Chinese O
dialog O
bias O
dataset O
and O
studied O
the O
implicit O
attitudes O
toward O
targeted O
groups O
in O
dialogues O
. O
To O
the O
best O
of O
our O
knowledge O
, O
there O
is O
no O
open O
- O
source O
Chinese O
dataset O
for O
offensive O
language O
detection O
. O
Detoxifying O
in O
online O
communities O
and O
language O
model O
generations O
still O
rely O
mostly O
on O
the O
blacklisting O
mechanism O
, O
which O
severely O
limits O
the O

Dataset O
Construction O

We O
present O
COLDATASET B-DatasetName
, O
a O
Chinese O
dataset O
containing O
37k O
sentences O
and O
covering O
the O
topics O
of O
racial O
, O
gender O
, O
and O
regional O
bias O
. O
Our O
data O
collection O
process O
is O
in O
line O
with O
the O
suggestions O
provided O
by O
Vidgen O
and O
Derczynski O
( O
2020 O
) O
to O
achieve O
standardized O
and O
accountable O
research O
benchmarks O
. O

Data O
Source O

We O
investigate O
offensive O
language O
on O
Chinese O
social O
platforms O
and O
popular O
generative O
language O
models O
during O
the O
preliminary O
research O
stage O
. O
We O
find O
that O
name O
- O
calling O
, O
verbal O
violence O
, O
and O
other O
types O
of O
offensiveness O
frequently O
occurs O
in O
discussions O
of O
social O
bias O
- O
related O
topics O
of O
racial O
, O
gender O
, O
and O
regional O
issues O
. O
Therefore O
, O
we O
study O
offensiveness O
of O
these O
topics O
in O
this O
paper O
. O
We O
crawl O
real O
- O
world O
data O
posted O
on O
social O
media O
platforms O
, O
including O
Zhihu O
and O
Weibo O
. O
We O
analyze O
the O
data O
and O
find O
that O
the O
proportion O
of O
offensive O
data O
is O
sparse O
because O
the O
platform O
maintains O
civilized O
speech O
. O
This O
way O
, O
we O
collect O
data O
by O
two O
strategies O
: O
( O
1 O
) O
keyword O
querying O
and O
( O
2 O
) O
crawling O
from O
related O
sub O
- O
topics O
. O

Keyword O
querying O

To O
narrow O
down O
the O
search O
scope O
and O
increase O
the O
density O
of O
the O
target O
data O
, O
we O
use O
the O
keyword O
querying O
method O
. O
Under O
each O
topic O
, O
we O
pre O
- O
collect O
keywords O
that O
occur O
frequently O
, O
such O
as O
racism O
, O
gender O
bias O
, O
and O
regional O
discrimination O
, O
as O
well O
as O
various O
descriptive O
words O
for O
target O
groups O
, O
such O
as O
black O
man O
( O
黑人 O
) O
and O
ni**r O
/ O
ni**a O
( O
黑鬼 O
) O
.The O
collected O
keywords O
are O
shown O
in O
Appendix O
B.1 O
. O
Using O
them O
, O
high O
- O
density O
data O
relating O
to O
each O
topic O
can O
be O
obtained O
from O
the O
crawled O
mass O
data O
. O

Crawling O
from O
related O
sub O
- O
topics O
We O
search O
some O
widely O
discussed O
sub O
- O
topics O
in O
Zhihu O
and O
directly O
crawl O
data O
from O
the O
follow O
- O
up O
comments O
. O
Compared O
to O
keyword O
queries O
, O
these O
data O
are O
not O
limited O
by O
pre O
- O
collected O
keywords O
and O
can O
provide O
a O
more O
comprehensive O
look O
at O
user O
discussions O
on O
the O
topic O
, O
resulting O
in O
a O
broader O
range O
of O
content O
and O
expressions O
. O

The O
collected O
data O
are O
post O
- O
processed O
( O
refer O
to O
Appendix O
B.2 O
) O
and O
then O
mixed O
as O
candidate O
data O
for O
further O
annotation O
during O
the O
model O
- O
in O
- O
the O
- O
loop O
collection O
. O

Model O
- O
in O
- O
the O
- O
loop O
Collection O

To O
improve O
the O
collection O
efficiency O
, O
we O
follow O
the O
model O
- O
in O
- O
the O
- O
loop O
setup O
and O
train O
a O
classifier O
to O
discover O
target O
data O
from O
the O
candidates O
. O
We O
adopt O
different O
labeling O
strategies O
for O
training O
and O
test O
set O
to O
improve O
labeling O
efficiency O
. O

Training O
Set O
Collection O
For O
the O
construction O
of O
training O
set O
, O
we O
semi O
- O
automatically O
label O
the O
data O
based O
on O
the O
model O
- O
in O
- O
the O
- O
loop O
setup O
. O
Firstly O
, O
We O
initialize O
a O
classifier O
by O
manually O
labeling O
500 O
samples O
( O
Offen O
. O
or O
Non O
- O
Offen O
. O
) O
as O
training O
data O
. O
Secondly O
, O
we O
adopt O
the O
classifier O
on O
a O
bunch O
of O
unlabeled O
data O
and O
predict O
their O
offensiveness O
. O
Then O
, O
the O
data O
are O
ranked O
by O
predicted O
scores O
and O
divided O
into O
multiple O
bins O
for O
sample O
checking O
. O
We O
sample O
around O
10 O
% O
data O
from O
each O
bin O
and O
manually O
label O
them O
with O
the O
following O
strategy O
: O
( O
1 O
) O
If O
the O
accuracy B-MetricName
of O
the O
predicted O
labels O
is O
up O
to O
90 B-MetricValue
% I-MetricValue
, O
data O
in O
the O
bin O
is O
directly O
added O
to O
the O
training O
set O
; O
Otherwise O
, O
( O
2 O
) O
the O
bin O
is O
manually O
relabeled O
entirely O
and O
then O
added O
to O
the O
training O
set O
. O
By O
this O
means O
, O
we O
iteratively O
update O
the O
classifier O
and O
training O
set O
for O
6 O
rounds O
. O
Details O
can O
be O
found O
at O
Appendix O
B.3 O
. O

Test O
Set O
Collection O

To O
ensure O
the O
reliability O
of O
test O
set O
annotation O
, O
we O
pick O
data O
from O
different O
probability O
intervals O
and O
manually O
annotate O
them O
. O
To O
give O
annotators O
a O
deeper O
understanding O
of O
our O
task O
, O
we O
further O
categorize O
the O
data O
and O
conduct O
more O
fine O
- O
grained O
annotation O
. O
The O
category O
of O
Offensive O
is O
subdivided O
into O
( O
1 O
) O
Attack O
Individuals O
and O
( O
2 O
) O
Attack O
Groups O
according O
to O
what O
is O
targeted O
/ O
attacked O
in O
the O
content O
( O
Waseem O
et O
al O
. O
, O
2017 O
; O
Vidgen O
and O
Derczynski O
, O
2020 O
) O
Offensive O
is O
subdivided O
into O
( O
3 O
) O
Anti O
- O
Bias O
and O
( O
4 O
) O
Other O
Non O
- O
Offensive O
. O
( O
Definitions O
of O
fine O
- O
grained O
categories O
are O
detailed O
in O
Appendix O
C O
) O

Human O
Annotation O

We O
employed O
17 O
native O
Chinese O
native O
workers O
for O
labeling O
. O
They O
are O
evenly O
distributed O
by O
gender O
( O
9 O
males O
and O
8 O
females O
) O
and O
come O
from O
various O
regions O
of O
China O
. O
Following O
the O
annotation O
suggestions O
provided O
by O
Vidgen O
and O
Derczynski O
( O
2020 O
) O
, O
we O
iteratively O
develop O
guidelines O
and O
train O
annotators O
to O
ensure O
the O
quality O
of O
annotation O
. O
The O
remuneration O
for O
annotators O
is O
60 O
CNY O
per O
hour O
. O
For O
higher O
efficiency O
, O
auto O
- O
labeled O
training O
data O
in O
each O
bin O
is O
checked O
and O
corrected O
by O
one O
annotator O
. O
For O
quality O
assurance O
, O
each O
sample O
in O
test O
set O
is O
assigned O
to O
three O
annotators O
, O
and O
the O
label O
with O
the O
most O
votes O
becomes O
the O
final O
. O
We O
compute O
the O
Inter O
- O
Annotator O
Agreement O
of O
the O
test O
set O
. O
The O
Fleiss O
' O
κ O
( O
Fleiss O
, O
1971 O
) O
of O
2 O
- O
class O
( O
Offen O
. O
or O
Non O
- O
Offen O
. O
) O
is O
0.819 O
( O
almost O
perfect O
agreement O
) O
and O
4 O
- O
class O
( O
Attack O
Individuals O
/ O
Groups O
, O
Anti O
- O
Bias O
, O
and O
Other O
Non O
- O
Offen O
. O
) O
is O
0.757 O
( O
substantial O
agreement O
) O
. O
More O
details O
of O
Data O
Collection O
and O
Annotation O
Guidelines O
are O
given O
in O
Appendix O
B O
and O
C. O
If O
the O
sample O
contains O
keywords O
under O
a O
certain O
topic O
, O
it O
is O
considered O
topic O
- O
related O
. O
We O
show O
the O
number O
of O
sentences O
under O
each O
topic O
in O
Table O
4 O
. O

Data O
Analysis O

As O
Table O
4 O
shows O
, O
the O
collected O
data O
is O
relatively O
evenly O
distributed O
among O
the O
three O
topics O
. O
About O
10 O
% O
of O
the O
data O
do O
not O
contain O
topic O
- O
related O
keywords O
( O
None O
) O
. O
These O
data O
are O
collected O
by O
subtopic O
crawling O
, O
making O
our O
data O
distribution O
closer O
to O
actual O
scenario O
. O
Table O
4 O
also O
reveals O
the O
presence O
of O
overlap O
between O
topics O
. O
For O
example O
, O
the O
overlap O
of O
race O
and O
gender O
exists O
in O
the O
sentences O
discussing O
African O
American O
Women O
, O
and O
the O
overlap O
of O
region O
and O
gender O
exists O
when O
discussing O
rural O
girls O
. O
This O
overlap O
makes O
this O
dataset O
more O
diverse O
and O
consistent O
with O
the O
real O
scenarios O
. O

Offensive B-TaskName
Language I-TaskName
Detection I-TaskName

The O
experiments O
of O
offensive O
language O
detection O
are O
designed O
to O
verify O
the O
following O
two O
questions O
: O
Can O
offensive O
language O
be O
detected O
with O
existing O
resources O
and O
technology O
alone O
? O
Does O
proposed O
COLDATASET B-DatasetName
effectively O
advance O
the O
offensive O
language O
detection O
task O
? O

Experimental O
Setup O

The O
aim O
of O
the O
offensive O
language O
detection O
task O
is O
to O
assign O
the O
label O
y O
( O
Offen O
. O
or O
Non O
- O
Offen O
. O
) O
to O
the O
given O
text O
x. O
To O
investigate O
how O
well O
offensive O
language O
is O
detected O
with O
the O
proposed O
dataset O
and O
other O
existing O
resources O
, O
several O
detection O
methods O
are O
evaluated O
. O

COLDETECTOR B-MethodName
We O
train O
COLDETECTOR B-MethodName
on O
the O
proposed O
COLDATASET B-DatasetName
for O
offensive O
language O
detection O
. O
COLDETECTOR B-MethodName
adopts O
transformerbased O
architecture O
and O
is O
based O
on O
pre O
- O
trained O
model O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
. O
We O
use O
the O
version O
of O
bert B-MethodName
- I-MethodName
base I-MethodName
- I-MethodName
chinese I-MethodName
4 O
, O
which O
has O
12 B-HyperparameterValue
layers B-HyperparameterName
and O
12 B-HyperparameterValue
attention B-HyperparameterName
heads I-HyperparameterName
, O
as O
the O
backbone O
of O
our O
detector O
. O
According O
to O
the O
classical O
fashion O
, O
given O
a O
text O
x O
i O
, O
we O
add O
a O
special O
token O
[ O
CLS O
] O
before O
the O
text O
and O
input O
to O
the O
BERT O
model O
. O
We O
take O
out O
the O
first O
hidden O
- O
state O
( O
corresponding O
to O
[ O
CLS O
] O
) O
in O
the O
last O
layer O
of O
BERT B-MethodName
as O
the O
sentence O
representation O
: O

E O
x O
i O
= O
f O
BERT B-MethodName
( O
x O
i O
) O

, O
and O
then O
it O
is O
further O
processed O
by O
a O
Linear O
layer O
: O

p O
i O
= O
f O
θ O
( O
E O
x O
) O
= O
sigmoid O
( O
W O
• O
E O
x O
i O
+ O
b O
) O

to O
generate O
the O
final O
prediction O
. O
All O
the O
parameters O
in O
f O
BERT B-MethodName
( O
• O
) O
and O
f O
θ O
( O
• O
) O
are O
trained O
on O
COLDATASET B-DatasetName
with O
cross B-MetricName
- I-MetricName
entropy I-MetricName
loss I-MetricName
function O
: O

L O
= O
1 O
N O
i O
− O
[ O
y O
i O
• O
log O
( O
p O
i O
) O
+ O
( O
1 O
− O
y O
i O
) O
• O
log O
( O
1 O
− O
p O
i O
) O
] O
. O

TranslJigsaw B-MethodName
Detector I-MethodName
( O
TJIGDET B-MethodName
) O
Considering O
the O
lack O
of O
Chinese O
dataset O
, O
we O
explore O
the O
performance O
of O
translated O
data O
from O
English O
to O
Chinese O
( O
Shi O
et O
al O
. O
, O
2010 O
; O
Nozza O
, O
2021 O
; O
Markov O
et O
al O
. O
, O
2021 O
) O
. O
We O
pick O
a O
subset O
of O
59k O
data O
labeled O
with O
toxicity O
and O
a O
variety O
of O
identity O
attributes O
from O
the O
Kaggle O
competition O
. O
( O
Jigsaw O
, O
2018 O
) O
. O
Then O
they O
are O
translated O
to O
Chinese O
with O
the O
Baidu O
General O
Translation O
API O
, O
and O
the O
statistical O
information O
is O
shown O
in O
Appendix O
D.2 O
. O
Similar O
with O
COLDETECTOR B-MethodName
, O
TranslJigsaw B-MethodName
detector I-MethodName
is O
obtained O
by O
fine O
- O
tuning O
BERT B-MethodName
model O
on O
the O
translated O
data O
. O

Prompt B-MethodName
- I-MethodName
based I-MethodName
Self I-MethodName
- I-MethodName
Detection I-MethodName
( O
PSELFDET B-MethodName
) O
Internal O
knowledge O
stored O
in O
the O
model O
during O
pretraining O
can O
be O
used O
to O
detect O
offensive O
language O
( O
Schick O
et O
al O
. O
, O
2021 O
; O
Prabhumoye O
et O
al O
. O
, O
2021 O
) O
. O
We O
explore O
the O
performance O
of O
prompt O
- O
based O
selfdetection O
under O
the O
zero O
- O
shot O
setting O
. O
Given O
the O
text O
TEXT O
to O
evaluate O
, O
the O
input O
of O
the O
model O
is O
formulated O
as O
: O
" O
TEXT O
" O
Please O
judge O
whether O
the O
above O
content O
is O
malicious O
or O
offensive O
? O
Answer O
: O
[ O
MASK O
] O
. O
BERT B-MethodName
Model O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
is O
used O
to O
predict O
the O
scores O
of O
[ O
MASK O
] O
token O
. O
We O
take O
the O
scores O
of O
candidate O
words O
of O
可 O
( O
yes O
) O
and O
否 O
( O
no O
) O
as O
the O
self O
- O
detection O
results O
. O
An O
example O
is O
given O
in O
the O
Appendix O
D.1 O
. O

Baidu B-MethodName
Text I-MethodName
Censor I-MethodName
( O
BAIDUTC B-MethodName
) O
As O
a O
public O
API O
, O
Baidu B-MethodName
Text I-MethodName
Censor I-MethodName
aims O
to O
identify O
harmful O
content O
including O
pornography O
, O
violence O
, O
terrorism O
, O
political O
sensitivity O
, O
and O
abuse O
5 O
. O
Keyword B-MethodName
Matching I-MethodName
( O
KEYMAT B-MethodName
) O
Keyword B-MethodName
matching I-MethodName
is O
frequently O
used O
in O
offensive O
language O
filtering O
for O
safety O
maintenance O
of O
social O
platforms O
. O
In O
this O
work O
, O
we O
use O
14k O
sensitive O
words O
released O
on O
Github O
6 O
, O
and O
the O
text O
containing O
any O
word O
in O
this O
word O
list O
is O
considered O
offensive O
. O
Random O
In O
the O
random O
setting O
, O
the O
label O
of O
offensive O
or O
non O
- O
offensive O
is O
randomly O
assigned O
. O

Performance O
of O
COLDETECTOR B-MethodName

We O
present O
the O
results O
on O
the O
test O
set O
of O
COL B-DatasetName
- I-DatasetName
DATASET I-DatasetName
in O
Table O
5 O
. O
The O
proposed O
COLDETEC B-MethodName
- I-MethodName
TOR I-MethodName
obtains O
the O
best O
performance O
( O
81 B-MetricValue
% I-MetricValue
accuracy B-MetricName
) O
among O
all O
the O
methods O
and O
outperforms O
the O
second O
place O
( O
BAIDUTC B-MethodName
) O
by O
a O
large O
margin O
( O
18 B-MetricValue
% I-MetricValue
absolute O
improvement O
in O
accuracy B-MetricName
) O
. O
These O
comparison O
results O
indicate O
that O
our O
benchmark O
can O
effectively O
advance O
the O
offensive O
detection O
task O
in O
online O
communities O
. O

To O
further O
explore O
the O
detection O
performance O
, O
we O
compare O
the O
three O
best O
- O
performing O
methods O
on O
recognizing O
the O
labeled O
four O
subcategories O
, O
the O
results O
are O
shown O
in O
Table O
6 O
. O
COLDETECTOR B-MethodName
performs O
well O
in O
detecting O
the O
sub O
- O
categories O
of O
Offen O
. O
( O
79.51 B-MetricValue
% I-MetricValue
and O
85.49 B-MetricValue
% I-MetricValue
accuracy B-MetricName
of O
Attack O
individual O
and O
Attack O
group O
) O
, O
indicating O
that O
COLDETECTOR B-MethodName
is O
able O
to O
discover O
offensive O
samples O
well O
compared O
to O
the O
other O
methods O
, O
contributing O
to O
higher O
recall B-MetricName
of O
Offen O
. O
( O
85 B-MetricValue
% I-MetricValue
) O
. O
The O
higher O
accuracy B-MetricName
of O
Other O
Non O
- O
Offen O
. O
( O
81.06 B-MetricValue
% I-MetricValue
) O
indicates O
that O
COLD B-MethodName
- I-MethodName
ETECTOR I-MethodName
can O
well O
distinguish O
Offen O
. O
from O
Other O
Non O
- O
Offen O
. O
However O
, O
the O
accuracy B-MetricName
of O
Anti O
- O
Bias O
is O
only O
38.32 B-MetricValue
% I-MetricValue
, O
indicating O
that O
COLDETECTOR B-MethodName
are O
easily O
tricked O
by O
Anti O
- O
Bias O
data O
and O
mis O
- O
classify O
them O
as O
Offen O
. O
, O
affecting O
the O
precision B-MetricName
of O
recalled O
Offen O
. O
samples O
( O
72 B-MetricValue
% I-MetricValue
) O
. O

In O
light O
of O
the O
challenges O
to O
classify O
Anti O
- O
Bias O
data O
, O
we O
further O
analyzed O
the O
samples O
that O
successfully O
fooled O
COLDETECTOR B-MethodName
. O
We O
observe O
that O
a O
common O
form O
of O
expression O
in O
Anti O
- O
Bias O
contents O
is O
acknowledgment O
followed O
by O
denial O
, O
e.g. O
, O
" O
Women O
are O
often O
discriminated O
against O
in O
the O
workplace O
, O
but O
I O
do O
n't O
think O
it O
's O
right O
. O
" O
. O
Such O
expressions O
can O
easily O
deceive O
the O
classifier O
into O
focusing O
solely O
on O
the O
first O
half O
of O
the O
content O
and O
ignoring O
the O
anti O
- O
bias O
statements O
following O
, O
leading O
to O
incorrect O
predictions O
. O

Though O
achieving O
satisfying O
performance O
( O
81 B-MetricValue
% I-MetricValue
accuracy B-MetricName
) O
, O
COLDETECTOR O
still O
lags O
far O
behind O
the O
performance O
of O
human O
experts O
as O
well O
as O
English O
toxic O
detectors O
( O
Hanu O
and O
Unitary O
team O
, O
2020 O
) O
. O
First O
, O
the O
proposed O
detector O
is O
obtained O
by O
simply O
fine O
- O
tuning O
the O
BERT B-MethodName
model O
and O
thus O
performs O
slightly O
worse O
on O
discovering O
the O
covert O
offensiveness O
and O
anti O
- O
bias O
samples O
, O
which O
depends O
more O
on O
the O
support O
from O
labeled O
implicit O
offensive O
data O
( O
Lees O
et O
al O
. O
, O
2021 O
) O
. O
Second O
, O
our O
training O
data O
is O
collected O
semi O
- O
automatically O
. O
Although O
sample O
checking O
can O
ensure O
the O
accuracy O
of O
assigned O
labels O
to O
a O
certain O
extent O
, O
it O
will O
inevitably O
introduce O
noise O
through O
unchecked O
data O
. O
We O
believe O
that O
if O
all O
data O
in O
the O
training O
set O
can O
be O
manually O
annotated O
in O
the O
future O
, O
there O
will O
be O
improvements O
in O
detection O
performance O
. O

Offensive B-TaskName
Language I-TaskName
Detection I-TaskName
with O
Existing O
Resources O

We O
analyze O
the O
performances O
of O
baselines O
based O
on O
existing O
resources O
and O
find O
that O
it O
is O
challenging O
to O
achieve O
satisfactory O
performance O
on O
this O
task O
only O
relying O
on O
existing O
resources O
. O

Discussion O
of O
Baidu B-MethodName
Text I-MethodName
Censor I-MethodName

As O
the O
results O
in O
Table O
5 O
and O
Even O
mixing O
TranslJigsaw B-MethodName
and O
COLDATASET B-DatasetName
as O
training O
data O
, O
the O
performance O
has O
no O
improvement O
compared O
to O
the O
only O
COLDATASET B-DatasetName
case O
( O
both O
are O
81 B-MetricValue
% I-MetricValue
accuracy B-MetricName
) O
. O
It O
shows O
a O
significant O
gap O
between O
the O
translated O
data O
and O
the O
original O
Chinese O
data O
. O
Firstly O
, O
there O
are O
language O
- O
specific O
characteristics O
due O
to O
different O
cultural O
backgrounds O
( O
Nozza O
, O
2021 O
) O
. O
Secondly O
, O
there O
is O
a O
noise O
produced O
during O
machine O
translation O
process O
. O
The O
dataset O
proposed O
in O
this O
paper O
relieves O
the O
resource O
limitations O
, O
contributing O
to O
Chinese O
offensive O
language O
research O
. O

Discussion O
of O
Prompt B-MethodName
- I-MethodName
based I-MethodName
Self I-MethodName
- I-MethodName
Detection I-MethodName

As O
shown O
in O
the O
results O
, O
the O
performance O
of O
PSELFDET B-MethodName
( O
59 B-MetricValue
% I-MetricValue
accuracy B-MetricName
) O
is O
better O
than O
RAN B-MethodName
- I-MethodName
DOM I-MethodName
and O
KEYMAT B-MethodName
, O
demonstrating O
the O
potential O
of O
mining O
the O
internal O
knowledge O
of O
the O
language O
model O
for O
detection O
tasks O
. O
However O
, O
its O
contribution O
is O
far O
inferior O
to O
supervised O
learning O
- O
based O
approaches O
( O
81 B-MetricValue
% I-MetricValue
accuracy B-MetricName
of O
COLDETECTOR B-MethodName
) O
. O
Previous O
work O
show O
that O
exploring O
the O
appropriate O
word O
pair O
and O
given O
prompt O
can O
effectively O
contribute O
to O
the O
performance O
of O
self O
- O
detection O
( O
Schick O
et O
al O
. O
, O
2021 O
; O
Prabhumoye O
et O
al O
. O
, O
2021 O
) O
. O
We O
compare O
different O
ways O
of O
prompt O
construction O
and O
present O
results O
of O
the O
best O
practice O
in O
Table O
5 O
. O
Detailed O
exploration O
of O
other O
prompts O
and O
word O
- O
pairs O
are O
included O
in O
Appendix O
D.1 O
. O

Discussion O
of O
Keyword B-MethodName
Matching I-MethodName

The O
results O
in O
Table O
5 O
show O
the O
unsatisfactory O
performance O
of O
keyword B-MethodName
matching I-MethodName
( O
54 B-MetricValue
% I-MetricValue
accuracy B-MetricName
) O
. O
Firstly O
, O
the O
coverage O
and O
quality O
of O
the O
keyword O
list O
are O
decisive O
for O
the O
detection O
accuracy O
. O
However O
, O
with O
the O
continuous O
emergence O
of O
new O
words O
and O
the O
diversification O
of O
vocabulary O
, O
achieving O
complete O
coverage O
is O
almost O
impossible O
, O
leading O
to O
the O
low O
recall B-MetricName
of O
Offen O
. O
( O
63 B-MetricValue
% I-MetricValue
) O
. O
Secondly O
, O
it O
is O
inaccurate O
to O
filter O
potentially O
sensitive O
samples O
by O
matching O
keywords O
due O
to O
the O
potential O
occurrence O
of O
those O
words O
in O
both O
Offen O
. O
and O
Non O
- O
Offen O
. O
samples O
. O
Therefore O
, O
even O
if O
the O
text O
contains O
a O
sensitive O
word O
, O
it O
does O
not O
necessarily O
express O
toxicity O
, O
which O
leads O
to O
low O
precision B-MetricName
( O
44 B-MetricValue
% I-MetricValue
) O
. O
Detailed O
analyses O
of O
the O
occurrence O
of O
sensitive O
words O
in O
Offen. O
/ O
Non O
- O
Offen O
. O
contents O
are O
presented O
in O
Appendix O
D.3 O
. O

Evaluation O
of O
Generative O
LMs O

With O
the O
proposed O
COLDATASET B-DatasetName
and O
COLDE B-MethodName
- I-MethodName
TECTOR I-MethodName
, O
we O
evaluate O
the O
offensiveness O
of O
popular O
Chinese O
generative O
language O
models O
. O
We O
mainly O
investigate O
the O
following O
research O
questions O
. O
RQ1 O
: O
How O
offensive O
are O
the O
Chinese O
generative O
language O
models O
? O
RQ2 O
: O
What O
type O
of O
prompts O
can O
trigger O
offensive O
generation O
? O

Evaluation O
Metrics O

We O
use O
the O
sentences O
in O
COLDATASET B-DatasetName
as O
input O
prompts O
and O
COLDETECTOR B-MethodName
as O
detector O
to O
evaluate O
the O
offensiveness O
of O
generated O
content O
from O
the O
evaluated O
models O
. O
We O
calculate O
the O
offensive B-MetricName
rate I-MetricName
of O
each O
model O
, O
which O
is O
the O
proportion O
of O
offensive O
generations O
among O
the O
total O
generations O
. O
A O
lower O
offensive B-MetricName
rate I-MetricName
indicates O
lower O
offensiveness O
of O
the O
model O
. O

Evaluated O
Models O

We O
evaluate O
the O
following O
publicly O
available O
Chinese O
generative O
language O
models O
for O
offensiveness O
: O
• O
CDialGPT B-MethodName
( O
Wang O
et O
al O
. O
, O
2020 O
) O
, O
a O
Chinese O
dialog O
model O
( O
with O
104 O
M O
parameters O
) O
trained O
on O
a O
cleaned O
conversational O
dataset O
LCCC B-DatasetName
. O
We O
evaluate O
CDialGPT B-MethodName
- I-MethodName
Base I-MethodName
and O
and O
CDialGPT B-MethodName
- I-MethodName
Large I-MethodName
models O
. O

• O
EVA B-MethodName
, O
the O
largest O
Chinese O
dialogue O
model O
( O
with O
2.8B O
parameters O
) O
trained O
on O
1.4B O
Chinese O
dialogue O
data O
. O

Evaluation O
Results O

The O
automatic O
and O
human O
evaluation O
results O
of O
language O
models O
are O
shown O
in O
Table O
8 O
and O
Table O
9 O
Bias O
inputs O
show O
a O
shockingly O
high O
risk O
of O
triggering O
offensiveness O
. O
To O
investigate O
what O
contents O
trigger O
risk O
, O
we O
conduct O
further O
studies O
of O
CPM B-MethodName
- O
Generation O
model O
by O
designing O
template O
- O
based O
inputs O
. O
The O
details O
are O
shown O
in O
Appendix O
E.1 O
. O
We O
find O
that O
offensive O
generation O
is O
sensitive O
to O
the O
following O
factors：1 O
) O
Target O
group O
keywords O
. O
The O
model O
is O
significantly O
biased O
against O
some O
groups O
such O
as O
feminist O
and O
black O
man O
, O
and O
tends O
to O
generate O
more O
toxic O
outputs O
with O
these O
inputs O
than O
others O
such O
as O
teenage O
girls O
, O
indicating O
the O
inherent O
bias O
of O
the O
model O
. O
2 O
) O
Negative O
attitude O
words O
. O
There O
is O
a O
higher O
offensive O
ratio O
when O
negative O
attitude O
words O
appear O
in O
the O
prompt O
. O
For O
example O
, O
there O
are O
higher O
ratios O
of O
both O
disgust O
and O
not O
disgust O
than O
not O
like O
. O
Anti O
- O
bias O
contents O
promote O
fairness O
and O
oppose O
bias O
. O
They O
are O
more O
likely O
to O
contain O
the O
above O
- O
mentioned O
target O
group O
keywords O
and O
negative O
attitude O
words O
than O
other O
non O
- O
offensive O
inputs O
, O
which O
explains O
why O
anti O
- O
bias O
inputs O
trigger O
more O
offensive O
generations O
. O

Conclusion O

We O
present O
a O
new O
dataset O
named O
COLDATASET B-DatasetName
for O
Chinese O
offensive O
language O
analysis O
. O
We O
show O
that O
the O
proposed O
COLDETECTOR B-MethodName
trained O
on O
our O
data O
can O
effectively O
detect O
offensive O
content O
. O
It O
can O
also O
be O
used O
as O
a O
benchmark O
for O
the O
offensiveness O
evaluation O
of O
language O
models O
. O
We O
evaluate O
some O
popular O
used O
models O
and O
reveal O
that O
they O
have O
different O
degrees O
of O
risk O
in O
generating O
offesive O
contents O
. O
Besides O
, O
our O
work O
shows O
that O
, O
for O
language O
models O
, O
non O
- O
offensive O
input O
can O
also O
induce O
safety O
problems O
as O
offensive O
input O
, O
and O
is O
worth O
the O
same O
attention O
. O
In O
particular O
, O
anti O
- O
bias O
language O
, O
which O
is O
non O
- O
offensive O
but O
has O
hazards O
comparable O
to O
offensive O
input O
, O
is O
often O
overlooked O
in O
existing O
work O
. O

We O
hope O
this O
new O
benchmark O
can O
provide O
the O
basis O
for O
safety O
research O
in O
Chinese O
and O
shed O
light O
on O
further O
studies O
. O
We O
call O
for O
more O
research O
to O
expand O
the O
scope O
of O
offensive O
and O
other O
unsafe O
language O
detection O
. O
Besides O
, O
we O
believe O
that O
, O
further O
investigating O
on O
what O
types O
of O
input O
successfully O
induce O
unsafe O
generation O
, O
will O
facilitate O
the O
safer O
deployment O
of O
language O
models O
. O

Ethical O
Considerations O

Our O
work O
is O
a O
forerunner O
of O
a O
relatively O
comprehensive O
benchmark O
for O
the O
study O
of O
offensive O
speech O
in O
Chinese O
. O
However O
, O
our O
proposal O
may O
have O
the O
following O
omissions O
and O
shortcomings O
. O

• O
Our O
dataset O
may O
contain O
mislabeled O
data O
due O
to O
the O
subjectivity O
of O
manual O
annotation O
. O
In O
addition O
, O
our O
training O
set O
adopts O
the O
semiautomatic O
annotation O
strategy O
and O
the O
incomplete O
data O
annotation O
also O
increases O
the O
labeling O
error O
. O
We O
appeal O
to O
data O
users O
to O
optionally O
re O
- O
annotate O
the O
semi O
- O
automated O
labeled O
training O
data O
if O
required O
. O

• O
We O
clearly O
understand O
that O
our O
dataset O
focuses O
only O
on O
common O
topics O
of O
race O
, O
gender O
, O
and O
region O
, O
with O
limited O
data O
coverage O
and O
a O
simple O
annotation O
schema O
. O
We O
do O
believe O
that O
constructing O
a O
greater O
dataset O
covering O
more O
topics O
with O
a O
more O
fine O
- O
grained O
taxonomy O
would O
contribute O
to O
a O
more O
robust O
Chinese O
offensive O
detector O
, O
deserving O
more O
effort O
in O
future O
work O
. O

• O
We O
are O
mindful O
that O
our O
benchmark O
detector O
can O
not O
detect O
all O
types O
of O
offensiveness O
due O
to O
the O
limitation O
of O
data O
coverage O
and O
the O
training O
techniques O
of O
the O
neural O
network O
. O

All O
the O
data O
in O
the O
proposed O
benchmark O
is O
collected O
from O
publicly O
available O
social O
platforms O
. O
We O
strictly O
follow O
the O
protocols O
for O
the O
use O
of O
data O
sources O
. O
The O
contents O
in O
our O
dataset O
do O
NOT O
represent O
our O
views O
or O
opinions O
. O

Our O
resources O
and O
analyses O
are O
intended O
to O
help O
create O
more O
harmonious O
online O
communities O
and O
promote O
the O
safer O
deployment O
of O
language O
models O
. O
We O
acknowledge O
that O
it O
would O
also O
be O
misused O
in O
problematic O
scenarios O
to O
create O
more O
offensive O
language O
or O
make O
someone O
uncomfortable O
. O
However O
, O
we O
believe O
that O
the O
proposed O
benchmark O
creates O
more O
value O
than O
risks O
towards O
creating O
more O
harmonious O
online O
communities O
and O
building O
more O
reliable O
language O
models O
. O

Limitations O

This O
paper O
tackles O
the O
issues O
of O
Chinese B-TaskName
offensive I-TaskName
language I-TaskName
detection I-TaskName
. O
In O
the O
section O
of O
Ethical O
Considerations O
, O
we O
claim O
that O
the O
proposed O
dataset O
has O
potentially O
mislabeled O
data O
and O
is O
limited O
in O
data O
coverage O
, O
and O
the O
detectors O
fine O
- O
tuned O
on O
this O
dataset O
can O
not O
ideally O
detect O
all O
offensive O
categories O
. O
We O
also O
discuss O
the O
ethical O
considerations O
of O
data O
collection O
and O
data O
usage O
. O
Besides O
the O
above O
- O
mentioned O
ethical O
concerns O
, O
we O
acknowledge O
the O
following O
limitations O
of O
our O
work O
. O

Limitation O
of O
contextual O
information O

Our O
work O
is O
mainly O
devoted O
to O
studying O
the O
offensiveness O
at O
sentence O
level O
and O
therefore O
contextual O
information O
is O
not O
included O
in O
the O
proposed O
COLDATASET B-DatasetName
. O
We O
do O
believe O
that O
offensive O
expression O
in O
contextsensitive O
scenarios O
( O
e.g. O
, O
dialogue O
) O
would O
be O
more O
challenging O
and O
require O
further O
exploration O
. O

Limitation O
of O
baseline O
models O

In O
the O
offensive O
language O
detection O
experiments O
( O
Section O
4 O
) O
, O
we O
take O
BERT B-MethodName
- I-MethodName
base I-MethodName
- I-MethodName
Chinese I-MethodName
as O
the O
backbone O
model O
for O
the O
three O
baseline O
models O
( O
COLDETECTOR B-MethodName
, O
TJIGDET B-MethodName
, O
and O
PSELFDET B-MethodName
) O
to O
demonstrate O
the O
contribution O
of O
our O
dataset O
. O
We O
acknowledge O
that O
adopting O
more O
backbone O
models O
( O
e.g. O
, O
mBART O
and O
xlm O
- O
Roberta O
) O
would O
contribute O
to O
a O
more O
solid O
comparison O
, O
which O
is O
worth O
exploring O
in O
more O
depth O
in O
the O
future O
. O

A O
Data O
Statement O

To O
enable O
researchers O
to O
better O
understand O
and O
use O
our O
dataset O
, O
we O
present O
the O
data O
statement O
following O
the O
professional O
practice O
for O
NLP O
systems O
developed O
by O
Bender O
and O
Friedman O
( O
2018 O
) O
. O

Dataset O
We O
present O
a O
Chinese O
Offensive O
Language O
Dataset O
( O
COLDATASET B-DatasetName
) O
in O
this O
paper O
, O
which O
containing O
37,480 O
sentences O
and O
covering O
the O
topics O
of O
racial O
, O
gender O
, O
and O
regional O
bias O
. O
There O
are O
32,157 O
training O
data O
, O
which O
are O
semi O
- O
automatically O
labeled O
with O
offensiveness O
labels O
( O
Offen O
. O
or O
Not O
- O
Offen O
. O
) O
. O
The O
test O
set O
contains O
5,323 O
data O
, O
and O
they O
are O
manually O
labeled O
with O
fine O
- O
grained O
categories O
, O
including O
Attack O
Individual O
, O
Attack O
Groups O
, O
Anti O
- O
Bias O
and O
Other O
Non O
- O
Offen O
. O

Speaker O

The O
data O
in O
COLDATASET B-DatasetName
is O
collected O
from O
social O
platforms O
of O
Zhihu O
and O
Weibo O
and O
the O
users O
who O
post O
on O
these O
platforms O
are O
the O
Speakers O
who O
generate O
the O
data O
. O

Annotator O
We O
employ O
17 O
native O
Chinese O
native O
workers O
for O
labeling O
, O
including O
9 O
males O
and O
8 O
females O
come O
from O
various O
regions O
of O
China O
, O
including O
Henan O
Province O
, O
Beijing O
, O
and O
Northeast O
part O
that O
are O
widely O
talked O
about O
in O
region O
discrimination O
and O
other O
less O
discussed O
regions O
. O
They O
are O
highly O
trained O
on O
our O
offensive O
language O
annotation O
task O
. O

Curator O
The O
authors O
act O
as O
curators O
, O
determine O
the O
scope O
of O
data O
collection O
, O
define O
the O
taxonomy O
, O
design O
annotation O
guidelines O
, O
train O
annotators O
, O
and O
control O
the O
quality O
of O
annotated O
data O
. O

NLP O
System O

We O
design O
the O
rule O
- O
based O
methods O
for O
data O
crawling O
and O
post O
- O
processing O
. O
Our O
annotation O
task O
is O
aided O
by O
an O
iteratively O
- O
optimized O
classifier O
, O
which O
picks O
out O
candidate O
data O
that O
need O
to O
be O
further O
manually O
annotated O
. O

Stakeholders O
The O
researchers O
engaged O
in O
the O
Chinese O
offensive O
language O
study O
will O
be O
the O
direct O
stakeholders O
, O
and O
proposed O
COLDATASET B-DatasetName
will O
effectively O
support O
their O
further O
research O
. O
The O
managers O
of O
social O
platforms O
can O
use O
this O
dataset O
to O
optimize O
their O
detector O
, O
contributing O
to O
better O
offensive O
language O
filtering O
. O
Meanwhile O
, O
COLDATASET B-DatasetName
contributes O
to O
language O
model O
developers O
evaluating O
their O
models O
' O
offensiveness O
and O
facilitating O
safer O
deployment O
. O

B O
Details O
of O
Dataset O
Construction O
B.1 O
Keyword O
query O

The O
collected O
keywords O
under O
each O
topic O
are O
given O
in O
Table O
10 O
. O
They O
are O
used O
to O
obtain O
high O
- O
density O
data O
from O
the O
crawled O
mass O
data O
. O

B.2 O
Post O
- O
Processing O

For O
the O
crawled O
data O
, O
we O
only O
keep O
samples O
of O
length O
between O
5 O
and O
200 O
tokens O
. O
We O
clean O
the O
noise O
and O
unusual O
characters O
, O
including O
emojis O
, O
URLs O
, O
usernames O
and O
white O
space O
, O
and O
then O
deduplicate O
the O
samples O
so O
that O
the O
collected O
data O
is O
more O
conducive O
to O
the O
data O
analysis O
in O
this O
task O
. O
After O
post O
- O
processing O
, O
the O
remaining O
data O
will O
be O
automatically O
selected O
and O
labeled O
in O
the O
model O
- O
inthe O
- O
loop O
setup O
. O

B.3 O
Model O
- O
in O
- O
the O
- O
loop O
Collection O

We O
adopt O
the O
model O
- O
in O
- O
the O
- O
loop O
setup O
to O
discover O
the O
target O
data O
and O
optimize O
the O
classifier O
performance O
. O
The O
main O
flow O
is O
shown O
in O
Figure O
3 O
. O
A O
small O
amount O
of O
data O
is O
manually O
labeled O
as O
the O
initial O
data O
( O
500 O
sentences O
) O
, O
and O
the O
following O
steps O
are O
iteratively O
performed O
in O
each O
round O
: O
Classifier O
We O
use O
BERT B-MethodName
model O
with O
12 B-HyperparameterValue
layers B-HyperparameterName
7 B-HyperparameterValue
in O
data O
collection O
, O
which O
has O
shown O
strong O
power O
in O
natural O
language O
processing O
tasks O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
. O
Parameters O
of O
COLDETECTOR B-DatasetName
are O
optimized O
by O
BertAdam O
optimizer O
with O
a O
linear O
warmup O
( O
proportion B-HyperparameterName
0.05 B-HyperparameterValue
) O
and O
a O
decay B-HyperparameterName
schedule I-HyperparameterName
. O
We O
set O
the O
learning B-HyperparameterName
rate I-HyperparameterName
as O
5e-5 B-HyperparameterValue
, O
batch B-HyperparameterName
size I-HyperparameterName
as O
64 B-HyperparameterValue
, O
and O
max B-HyperparameterName
training I-HyperparameterName
epoch I-HyperparameterName
as O
30 B-HyperparameterValue
. O
Early O
- O
stopping O
mechanism O
is O
used O
to O
avoid O
overfitting O
. O
In O
each O
round O
, O
the O
classifier O
is O
fine O
- O
tuned O
with O
updated O
data O
from O
previous O
rounds O
. O
The O
performance O
of O
the O
classifier O
is O
given O
in O
Figure O
4 O
, O
which O
shows O
that O
after O
the O
second O
round O
, O
the O
performance O
tends O
to O
increase O
steadily O
as O
the O
scale O
of O
data O
increases O
. O

Dataset O

The O
expansion O
of O
the O
dataset O
is O
performed O
in O
6 O
rounds O
. O
In O
the O
first O
five O
rounds O
, O
both O
training O
and O
test O
data O
are O
expanded O
, O
while O
only O
the O
training O
data O
is O
expanded O
in O
the O
sixth O
round O
, O
as O
shown O
in O
Figure O
6 O
and O
7 O
. O
It O
should O
note O
that O
the O
classifier O
is O
not O
reliable O
in O
the O
beginning O
, O
and O
it O
is O
difficult O
to O
obtain O
highconfidence O
predictions O
. O
For O
example O
, O
the O
accuracy B-MetricName
is O
only O
58 B-MetricValue
% I-MetricValue
in O
the O
first O
round O
. O
It O
is O
challenging O
to O
learn O
a O
good O
decision O
surface O
due O
to O
the O
limitation O
of O
the O
data O
scale O
. O
The O
prediction O
probability O
is O
concentrated O
between O
0.2 O
and O
0.5 O
, O
and O
the O
classifier O
tends O
to O
predict O
all O
data O
as O
Non O
- O
Offensive O
. O
So O
, O
we O
pick O
data O
from O
this O
interval O
for O
annotation O
and O
the O
returned O
data O
will O
boost O
the O
performance O
of O
the O
classifier O
. O

After O
the O
third O
round O
, O
the O
classifier O
's O
performance O
gradually O
stabilized O
, O
and O
the O
accuracy O
of O
the O
predicted O
high O
- O
scoring O
samples O
steadily O
increased O
. O
Therefore O
, O
we O
tend O
to O
select O
more O
data O
from O
highscoring O
samples O
to O
improve O
the O
efficiency O
of O
data O
collection O
. O

C O
Annotation O
Guideline O

We O
provide O
annotators O
with O
annotation O
guidelines O
, O
as O
shown O
in O
Figure O
5 O
. O
Annotators O
are O
first O
requested O
to O
judge O
whether O
a O
given O
sample O
is O
offensive O
( O
Q1 O
) O
. O
Then O
, O
The O
offensive O
samples O
are O
further O
divided O
into O
Attack O
individuals O
or O
Attack O
Groups O
according O
to O
the O
target O
offended O
( O
Q2 O
) O
, O
while O
the O
Non O
- O
offensive O
samples O
are O
divided O
into O
Anti O
- O
Bias O
or O
Other O
Non O
- O
Offensive O
. O
For O
the O
training O
set O
, O
the O
annotator O
is O
only O
required O
to O
answer O
the O
first O
question O
( O
Q1 O
) O
to O
check O
and O
relabel O
the O
automatically O
annotated O
samples O
. O

We O
consider O
different O
categories O
referred O
to O
in O
annotation O
guidelines O
as O
follows O
. O
More O
examples O
can O
be O
found O
in O
Figure O
5 O
. O

Offensive O
In O
this O
paper O
, O
we O
consider O
any O
form O
of O
targeted O
attacks O
on O
individuals O
or O
groups O
to O
be O
regarded O
as O
Offensive O
language O
. O
It O
includes O
implicit O
or O
direct O
offensive O
content O
that O
is O
rude O
, O
disrespectful O
, O
insulting O
, O
threatening O
, O
profane O
, O
as O
well O
as O
any O
other O
toxic O
content O
that O
makes O
others O
uncomfortable O
or O
provokes O
further O
intense O
offensive O
feedback O
. O
( O
Zampieri O
et O
al O
. O
, O
2019 O
; O
Cambrigdge O
dictionary O
; O
. O
Further O
, O
based O
on O
the O
target O
, O
Offensive O
is O
subdivided O
into O
Attack O
Individuals O
and O
Attack O
Groups O
following O
Waseem O
et O
al O
. O
( O
2017 O
) O
. O

• O
Attack O
Individuals O
, O
mainly O
refers O
to O
offensive O
content O
directed O
at O
individuals O
, O
and O
the O
target O
is O
often O
referred O
to O
by O
a O
specific O
name O
or O
a O
pronoun O
. O

• O
Attacking O
groups O
, O
mainly O
refers O
to O
offensive O
content O
towards O
generalized O
groups O
based O
on O
their O
social O
identities O
related O
to O
race O
, O
religion O
, O
gender O
, O
etc O
. O

Non O
- O
Offensive O
Non O
- O
Offensive O
is O
subdivided O
into O
Anti O
- O
bias O
and O
Other O
Non O
- O
Offensive O
. O
We O
make O
this O
division O
because O
Anti O
- O
Bias O
is O
beneficial O
to O
fight O
offensive O
language O
and O
maintain O
a O
harmonious O
communication O
environment O
, O
which O
deserves O
further O
study O
than O
other O
non O
- O
offensive O
speech O
. O
Figure O
6 O
: O
Training O
data O
collected O
in O
each O
round O
. O
The O
x O
axis O
is O
the O
probability O
interval O
of O
predicting O
as O
" O
offensive O
" O
and O
the O
y O
axis O
is O
the O
sample O
number O
. O

Figure O
7 O
: O
Test O
data O
collected O
in O
each O
round O
. O
The O
x O
axis O
is O
the O
probability O
interval O
of O
predicting O
as O
" O
offensive O
" O
and O
the O
y O
axis O
is O
the O
sample O
number O
. O

• O
Anti O
- O
Bias O
, O
mainly O
refers O
to O
the O
expression O
countering O
offensiveness O
, O
which O
is O
usually O
considered O
fairness O
, O
fact O
- O
based O
contents O
expressed O
in O
a O
positive O
or O
neutral O
mood O
. O

• O
Other O
Non O
- O
Offensive O
, O
refers O
to O
the O
nonoffensive O
contents O
other O
than O
anti O
- O
bias O
speech O
. O

D O
Details O
of O
Offensive O
Detection O

D.1 O
Prompt B-MethodName
- I-MethodName
based I-MethodName
Self I-MethodName
- I-MethodName
Detection I-MethodName
Figure O
8 O
gives O
an O
example O
of O
self O
detection O
. O
Bertbase B-MethodName
- I-MethodName
chinese I-MethodName
is O
taken O
as O
the O
model O
to O
predict O
the O
scores O
of O
[ O
MASK O
] O
token O
and O
we O
take O
the O
scores O
of O
candidate O
words O
of O
可 O
( O
yes O
) O
and O
否 O
( O
no O
) O
as O
the O
results O
of O
self O
- O
detection O
. O

We O
call O
for O
further O
research O
to O
explore O
the O
internal O
knowledge O
of O
language O
models O
to O
facilitate O
this O
task O
, O
and O
the O
following O
tips O
can O
be O
considered O
. O
The O
first O
is O
exploring O
appropriate O
word O
pairs O
. O
" O
Yes O
/ O
No O
" O
is O
often O
used O
in O
English O
( O
Schick O
et O
al O
. O
, O
2021 O
) O
, O
but O
the O
candidate O
word O
pairs O
in O
Chinese O
are O
more O
varied O
. O
We O
have O
explored O
the O
alternative O
word O
pairs O
in O
Chinese O
, O
and O
the O
results O
are O
shown O
in O
Table O
11 O
, O
indicating O
that O
different O
word O
pairs O
have O
significant O
impacts O
on O
the O
results O
. O
Second O
, O
the O
detection O
performance O
is O
directly O
related O
to O
the O
given O
prompt O
. O
Under O
the O
few O
- O
shot O
setting O
, O
it O
was O
found O
that O
prompt O
- O
based O
methods O
can O
achieve O
results O
similar O
to O
, O
even O
better O
than O
, O
fine O
- O
tuned O
models O
( O
Prabhumoye O
et O
al O
. O
, O
2021 O
) O
. O
We O
call O
for O
more O
research O
to O
investigate O
prompt O
- O
based O
self O
- O
detection O
methods O
to O
further O
enhance O
their O
ability O
of O
offensive O
language O
detection O
. O

D.2 O
TranslJigsaw B-MethodName
Detector O

To O
explore O
the O
performance O
of O
translated O
data O
on O
this O
task O
, O
we O
pick O
the O
data O
released O
for O
the O
Kaggle O
competition O
Jigsaw B-MethodName
Unintended I-MethodName
Bias I-MethodName
in I-MethodName
Toxicity I-MethodName
Classification I-MethodName
8 O
( O
Jigsaw O
, O
2018 O
) O
. O
This O
dataset O
contains O
1.8 O
million O
data O
annotated O
by O
human O
raters O
. O
A O
subset O
of O
the O
data O
is O
labeled O
with O
various O
identity O
attributes O
related O
to O
sexual O
, O
religious O
, O
racial O
, O
and O
disability O
bias O
. O
We O
pick O
59k O
data O
according O
to O
whether O
it O
is O
toxic O
and O
bias O
- O
topic O
related O
, O
and O
then O
translate O
them O
from O
English O
to O
Chinese O
. O
The O
statistical O
information O
of O
translated O
data O
is O
shown O
in O
Table O
12 O
. O

D.3 O
Keyword B-MethodName
Matching I-MethodName

The O
keyword O
matching O
method O
shows O
unsatisfactory O
performance O
in O
the O
offensive O
detection O
task O
on O
the O
proposed O
COLDATASET B-DatasetName
. O
The O
main O
reason O
is O
that O
sensitive O
words O
may O
appear O
in O
both O
offensive O
and O
non O
- O
offensive O
sentences O
, O
as O
shown O
in O
Figure O
9 O
. O
Some O
cases O
are O
given O
in O
Table O
13 O
. O

As O
can O
be O
seen O
from O
Figure O
9 O
, O
most O
of O
the O
sensitive O
words O
appear O
in O
both O
the O
offensive O
and O
nonoffensive O
samples O
, O
as O
shown O
in O
the O
region O
①. O
Even O
some O
sensitive O
words O
with O
strong O
offensive O
frequently O
appear O
in O
anti O
- O
bias O
( O
non O
- O
offensive O
) O
content O
, O
as O
shown O
in O
region O
②. O
Although O
there O
are O
some O
sensitive O
words O
that O
appear O
only O
in O
the O
offensive O
samples O
, O
as O
shown O
in O
region O
③ O
, O
we O
believe O
that O
these O
keywords O
will O
likewise O
appear O
in O
the O
nonoffensive O
sample O
when O
the O
scale O
and O
coverage O
of O
the O
COLDATASET B-DatasetName
are O
large O
enough O
. O
Such O
results O
suggest O
that O
it O
is O
challenging O
to O
rely O
solely O
on O
keyword O
matching O
for O
offensive O
speech O
detection O
. O

Figure O
9 O
: O
For O
the O
sensitive O
words O
used O
in O
keyword O
matching O
, O
we O
analyzed O
their O
occurrences O
in O
the O
test O
set O
of O
COLDATASET B-DatasetName
. O
The O
y O
- O
axis O
denotes O
the O
number O
of O
texts O
containing O
the O
keywords O
. O

E O
Details O
of O
Evaluation O
E.1 O
Impact O
factors O
of O
offensive O
generation O

To O
further O
explore O
the O
impact O
factors O
of O
offensive O
generations O
, O
we O
collected O
103 O
target O
group O
keywords O
and O
9 O
templates O
, O
and O
constructed O
a O
total O
of O
927 O
prompts O
. O
For O
each O
prompt O
, O
20 O
responses O
are O
generated O
by O
CPM B-MethodName
- O
Generate O
model O
( O
max O
length O
is O
set O
to O
200 O
tokens O
) O
. O
The O
offensive O
ratio O
of O
each O
keywords O
is O
shown O
in O
Figure O
11 O
and O
that O
of O
each O
prompt O
is O
shown O
in O
Table O
14 O
. O
We O
also O
analyze O
the O
influence O
of O
the O
length O
of O
generated O
contents O
and O
the O
results O
in O
Figure O
10 O
indicate O
that O
the O
longer O
generations O
bring O
greater O
offensive O
risk O
. O

Figure O
10 O
: O
The O
offensive O
ratio O
of O
generations O
varies O
with O
the O
max O
- O
length O
of O
generated O
contents O
( O
CPM B-MethodName
- O
Generate O
model O
) O
. O

E.2 O
Case O
study O

Offensive O
generations O
detected O
by O
COLDETEC B-MethodName
- I-MethodName
TOR I-MethodName
As O
shown O
in O
Table O
15 O
, O
we O
list O
some O
examples O
of O
offensive O
generations O
discovered O
by O
proposed O
COLDETECTOR B-MethodName
. O
These O
examples O
show O
that O
both O
Offen O
. O
and O
Non O
- O
offen O
. O
contents O
can O
trigger O
Offen O
. O
generations O
. O

Failure O
cases O
of O
offensive O
generation O
detection O

The O
proposed O
COLDETECTOR B-MethodName
effectively O
discovers O
offensive O
languages O
in O
generated O
texts O
. O
However O
, O
as O
pointed O
out O
, O
in O
dialogue O
scenarios O
, O
the O
system O
tends O
to O
cater O
to O
users O
and O
generate O
responses O
of O
toxicity O
agreement O
. O
Our O
COLDETECTOR B-MethodName
focuses O
on O
sentence O
- O
level O
offensive O
language O
and O
is O
insufficient O
to O
detect O
context O
- O
sensitive O
cases O
. O
Some O
failure O
cases O
are O
shown O
in O
Table O
16 O
. O
Further O
research O
will O
be O
conducted O
on O
offensive O
analysis O
in O
dialog O
scenarios O
, O
along O
with O
the O
proposed O
sentence O
- O
level O
COLD B-MethodName
- I-MethodName
ETECTOR I-MethodName
, O
to O
formulate O
more O
rigorous O
strategies O
to O
ensure O
the O
safe O
deployment O
of O
generative O
models O
. O

Modeling O
Bilingual O
Conversational O
Characteristics O
for O
Neural B-TaskName
Chat I-TaskName
Translation I-TaskName

Neural B-TaskName
chat I-TaskName
translation I-TaskName
aims O
to O
translate O
bilingual O
conversational O
text O
, O
which O
has O
a O
broad O
application O
in O
international O
exchanges O
and O
cooperation O
. O
Despite O
the O
impressive O
performance O
of O
sentence O
- O
level O
and O
context O
- O
aware O
Neural B-TaskName
Machine I-TaskName
Translation I-TaskName
( O
NMT B-TaskName
) O
, O
there O
still O
remain O
challenges O
to O
translate O
bilingual O
conversational O
text O
due O
to O
its O
inherent O
characteristics O
such O
as O
role O
preference O
, O
dialogue O
coherence O
, O
and O
translation O
consistency O
. O
In O
this O
paper O
, O
we O
aim O
to O
promote O
the O
translation O
quality O
of O
conversational O
text O
by O
modeling O
the O
above O
properties O
. O
Specifically O
, O
we O
design O
three O
latent O
variational O
modules O
to O
learn O
the O
distributions O
of O
bilingual O
conversational O
characteristics O
. O
Through O
sampling O
from O
these O
learned O
distributions O
, O
the O
latent O
variables O
, O
tailored O
for O
role O
preference O
, O
dialogue O
coherence O
, O
and O
translation O
consistency O
, O
are O
incorporated O
into O
the O
NMT B-TaskName
model O
for O
better O
translation O
. O
We O
evaluate O
our O
approach O
on O
the O
benchmark O
dataset O
BConTrasT B-DatasetName
( O
English⇔German O
) O
and O
a O
self O
- O
collected O
bilingual O
dialogue O
corpus O
, O
named O
BMELD B-DatasetName
( O
English⇔Chinese O
) O
. O
Extensive O
experiments O
show O
that O
our O
approach O
notably O
boosts O
the O
performance O
over O
strong O
baselines O
by O
a O
large O
margin O
and O
significantly O
surpasses O
some O
state O
- O
of O
- O
the O
- O
art O
context O
- O
aware O
NMT B-TaskName
models O
in O
terms O
of O
BLEU B-MetricName
and O
TER B-MetricName
. O
Additionally O
, O
we O
make O
the O
BMELD B-DatasetName
dataset O
publicly O
available O
for O
the O
research O
community O
. O
1 O

Introduction O

A O
conversation O
may O
involve O
participants O
that O
speak O
in O
different O
languages O
( O
e.g. O
, O
one O
speaking O
in O
English O
and O
another O
in O
Chinese O
) O
. O
Fig O
. O
1 O
shows O
an O
example O
, O
where O
the O
English O
role O
R O
1 O
and O
the O
Chinese O
role O
R O
2 O
are O
talking O
about O
the O
" O
boat O
" O
. O
The O
, O
where O
the O
Chinese O
utterances O
are O
presented O
in O
pinyin O
style O
. O
R O
i O
: O
Role O
i. O
The O
dashed O
arrows O
mark O
the O
translation O
direction O
. O
The O
green O
and O
red O
arrows O
represent O
the O
monolingual O
and O
bilingual O
conversation O
flow O
, O
respectively O
. O
Although O
the O
translation O
of O
Y O
5 O
produced O
by O
the O
" O
S B-TaskName
- I-TaskName
NMT I-TaskName
" O
( O
a O
context O
- O
free O
sentencelevel O
NMT O
system O
) O
is O
reasonable O
at O
the O
sentence O
level O
, O
the O
coherence O
of O
the O
entire O
dialogue O
translation O
is O
poor O
. O

goal O
of O
chat O
translation O
is O
to O
translate O
bilingual O
conversational O
text O
, O
i.e. O
, O
converting O
one O
participant O
's O
language O
( O
e.g. O
, O
English O
) O
to O
another O
's O
( O
e.g. O
, O
Chinese O
) O
and O
vice O
versa O
( O
Farajian O
et O
al O
. O
, O
2020 O
) O
. O
It O
enables O
multiple O
speakers O
to O
communicate O
with O
each O
other O
in O
their O
native O
languages O
, O
which O
has O
a O
wide O
application O
in O
industry O
- O
level O
services O
. O

Although O
sentence B-TaskName
- I-TaskName
level I-TaskName
Neural I-TaskName
Machine I-TaskName
Translation I-TaskName
( O
NMT B-TaskName
) O
( O
Sutskever O
et O
al O
. O
, O
2014 O
; O
Vaswani O
et O
al O
. O
, O
2017 O
; O
Hassan O
et O
al O
. O
, O
2018 O
; O
Yan O
et O
al O
. O
, O
2020 O
; O
has O
achieved O
promising O
progress O
, O
it O
still O
faces O
challenges O
in O
accurately O
translating O
conversational O
text O
due O
to O
abandoning O
the O
dialogue O
history O
, O
which O
leads O
to O
role O
- O
irrelevant O
, O
incoherent O
and O
inconsistent O
translations O
( O
Mirkin O
et O
al O
. O
, O
2015 O
; O
Wang O
et O
al O
. O
, O
2017a O
; O
Läubli O
et O
al O
. O
, O
2018 O
; O
Toral O
et O
al O
. O
, O
2018 O
) O
. O
Further O
, O
context O
- O
aware O
NMT O
( O
Tiedemann O
and O
Scherrer O
, O
2017 O
; O
Voita O
et O
al O
. O
, O
2018Voita O
et O
al O
. O
, O
, O
2019aMaruf O
et O
al O
. O
, O
2019 O
; O
Ma O
et O
al O
. O
, O
2020 O
) O
can O
be O
directly O
applied O
to O
chat O
translation O
through O
incorporating O
the O
dialogue O
history O
but O
can O
not O
obtain O
satisfactory O
results O
in O
this O
sce O
- O
nario O
( O
Moghe O
et O
al O
. O
, O
2020 O
) O
. O
One O
important O
reason O
is O
the O
lack O
of O
explicitly O
modeling O
the O
inherent O
bilingual O
conversational O
characteristics O
, O
e.g. O
, O
role O
preference O
, O
dialogue O
coherence O
, O
and O
translation O
consistency O
, O
as O
pointed O
out O
by O
Farajian O
et O
al O
. O
( O
2020 O
) O
. O

For O
a O
conversation O
, O
its O
dialogue O
history O
contains O
rich O
role O
preference O
information O
such O
as O
emotion O
, O
style O
, O
and O
humor O
, O
which O
is O
beneficial O
to O
rolerelevant O
utterance O
generation O
( O
Wu O
et O
al O
. O
, O
2020 O
) O
. O
As O
shown O
in O
Fig O
. O
1 O
, O
the O
utterances O
X O
1 O
, O
X O
3 O
and O
X O
5 O
from O
role O
R O
1 O
always O
have O
strong O
emotions O
( O
i.e. O
, O
joy O
) O
because O
of O
his O
/ O
her O
preference O
, O
and O
preserving O
the O
same O
preference O
information O
across O
languages O
can O
help O
raise O
emotional O
resonance O
and O
mutual O
understanding O
( O
Moghe O
et O
al O
. O
, O
2020 O
) O
. O
Meanwhile O
, O
there O
exists O
semantic O
coherence O
in O
the O
conversation O
, O
as O
the O
solid O
green O
arrow O
in O
Fig O
. O
1 O
, O
where O
the O
utterance O
X O
5 O
naturally O
and O
semantically O
connects O
with O
the O
dialogue O
history O
( O
X O
1∼4 O
) O
on O
the O
topic O
" O
boat O
" O
. O
In O
addition O
, O
the O
bilingual O
conversation O
exhibits O
translation O
consistency O
, O
where O
the O
correct O
lexical O
choice O
to O
translate O
the O
current O
utterance O
might O
have O
appeared O
in O
preceding O
turns O
. O
For O
instance O
, O
the O
word O
" O
sail O
" O
in O
X O
1 O
is O
translated O
into O
" O
jiàchuán O
" O
, O
and O
thus O
the O
word O
" O
sailing O
" O
in O
X O
3 O
should O
be O
mapped O
into O
" O
jiàchuán O
" O
rather O
than O
other O
words O
( O
e.g. O
, O
" O
hángxíng O
" O
2 O
) O
to O
maintain O
translation O
consistency O
. O
On O
the O
contrary O
, O
if O
we O
ignore O
these O
characteristics O
, O
translations O
might O
be O
role O
- O
irrelevant O
, O
incoherent O
, O
inconsistent O
, O
and O
detrimental O
to O
further O
communication O
like O
the O
translation O
produced O
by O
the O
" O
S B-TaskName
- I-TaskName
NMT I-TaskName
" O
in O
Fig O
. O
1 O
. O
Although O
the O
translation O
is O
acceptable O
at O
the O
sentence O
level O
, O
it O
is O
abrupt O
at O
the O
bilingual O
conversation O
level O
. O

Apparently O
, O
how O
to O
effectively O
exploit O
these O
bilingual O
conversational O
characteristics O
is O
one O
of O
the O
core O
issues O
in O
chat O
translation O
. O
And O
it O
is O
challenging O
to O
implicitly O
capture O
these O
properties O
by O
just O
incorporating O
the O
complex O
dialogue O
history O
into O
encoders O
due O
to O
lacking O
the O
relevant O
information O
guidance O
( O
Farajian O
et O
al O
. O
, O
2020 O
) O
. O
On O
the O
other O
hand O
, O
the O
Conditional O
Variational O
Auto O
- O
Encoder O
( O
CVAE O
) O
( O
Sohn O
et O
al O
. O
, O
2015 O
) O
has O
shown O
its O
superiority O
in O
learning O
distributions O
of O
data O
properties O
, O
which O
is O
often O
utilized O
to O
model O
the O
diversity O
( O
Zhao O
et O
al O
. O
, O
2017 O
) O
, O
coherence O
( O
Wang O
and O
Wan O
, O
2019 O
) O
and O
users O
' O
personalities O
( O
Bak O
and O
Oh O
, O
2019 O
) O
, O
etc O
. O
In O
spite O
of O
its O
success O
, O
adapting O
it O
to O
chat O
translation O
is O
non O
- O
trivial O
, O
especially O
involving O
multiple O
tailored O
latent O
variables O
. O
Therefore O
, O
in O
this O
paper O
, O
we O
propose O
a O
model O
, O
named O
CPCC B-MethodName
, O
to O
capture O
role O
preference O
, O
dialogue O
coherence O
, O
and O
translation O
consistency O
with O
latent O
variables O
learned O
by O
the O
CVAE O
for O
neural O
chat O
translation O
. O
CPCC B-MethodName
contains O
three O
specific O
latent O
variational O
modules O
to O
learn O
the O
distributions O
of O
role O
preference O
, O
dialogue O
coherence O
, O
and O
translation O
consistency O
, O
respectively O
. O
Specifically O
, O
we O
firstly O
use O
one O
role O
- O
tailored O
latent O
variable O
, O
sampled O
from O
the O
learned O
distribution O
conditioned O
only O
on O
the O
utterances O
from O
this O
role O
, O
to O
preserve O
preference O
. O
Then O
, O
we O
utilize O
another O
latent O
variable O
, O
generated O
by O
the O
distribution O
conditioned O
on O
source O
- O
language O
dialogue O
history O
, O
to O
maintain O
coherence O
. O
Finally O
, O
we O
leverage O
the O
last O
latent O
variable O
, O
generated O
by O
the O
distribution O
conditioned O
on O
paired O
bilingual O
conversational O
utterances O
, O
to O
keep O
translation O
consistency O
. O
As O
a O
result O
, O
these O
tailored O
latent O
variables O
allow O
our O
CPCC B-MethodName
to O
produce O
role O
- O
specific O
, O
coherent O
, O
and O
consistent O
translations O
, O
and O
hence O
make O
the O
bilingual O
conversation O
go O
fluently O
. O

We O
conduct O
experiments O
on O
WMT20 O
Chat O
Translation O
dataset O
: O
BConTrasT B-DatasetName
( O
En⇔De O
3 O
) O
( O
Farajian O
et O
al O
. O
, O
2020 O
) O
and O
a O
self O
- O
collected O
dialogue O
corpus O
: O
BMELD B-DatasetName
( O
En⇔Ch O
) O
. O
Results O
demonstrate O
that O
our O
model O
achieves O
consistent O
improvements O
in O
four O
directions O
in O
terms O
of O
BLEU B-MetricValue
( O
Papineni O
et O
al O
. O
, O
2002 O
) O
and O
TER B-MetricValue
( O
Snover O
et O
al O
. O
, O
2006 O
) O
, O
showing O
its O
effectiveness O
and O
generalizability O
. O
Human O
evaluation O
further O
suggests O
that O
our O
model O
effectively O
alleviates O
the O
issue O
of O
role O
- O
irrelevant O
, O
incoherent O
and O
inconsistent O
translations O
compared O
to O
other O
methods O
. O
Our O
contributions O
are O
summarized O
as O
follows O
: O

• O
To O
the O
best O
of O
our O
knowledge O
, O
we O
are O
the O
first O
to O
incorporate O
the O
role O
preference O
, O
dialogue O
coherence O
, O
and O
translation O
consistency O
into O
neural O
chat O
translation O
. O
• O
We O
are O
the O
first O
to O
build O
a O
bridge O
between O
the O
dialogue O
and O
machine O
translation O
via O
conditional O
variational O
auto O
- O
encoder O
, O
which O
effectively O
models O
three O
inherent O
characteristics O
in O
bilingual O
conversation O
for O
neural B-TaskName
chat I-TaskName
translation I-TaskName
. O
• O
Our O
approach O
gains O
consistent O
and O
significant O
performance O
over O
the O
standard O
context O
- O
aware O
baseline O
and O
remarkably O
outperforms O
some O
state O
- O
of O
- O
the O
- O
art O
context O
- O
aware O
NMT B-TaskName
models O
. O
• O
We O
contribute O
a O
new O
bilingual O
dialogue O
corpus O
( O
BMELD B-DatasetName
, O
En⇔Ch O
) O
with O
manual O
translations O
and O
our O
codes O
to O
the O
research O
community O
. O

We O
aim O
to O
learn O
a O
model O
that O
can O
capture O
inherent O
characteristics O
in O
the O
bilingual O
dialogue O
history O
for O
producing O
high O
- O
quality O
translations O
, O
i.e. O
, O
using O
the O
context O
for O
better O
translations O
( O
Farajian O

|T O
|−1 O
2 O
] O

and O
T O
is O
the O
total O
number O
of O
turns O
( O
assumed O
to O
be O
odd O
here O
) O
. O
et O
al O
. O
, O
2020 O
) O
. O
Following O
, O
we O
define O
paired O
bilingual O
utterances O
( O
X O
i O
, O
Y O
i O
) O
as O
a O
turn O
in O
Fig O
. O
2 O
, O
where O
we O
will O
translate O
the O
current O
utterance O
X O
2k+1 O
at O
the O
( O
2k O
+ O
1 O
) O
-th O
turn O
. O
Here O
, O
we O
denote O
the O
utterance O
X O
2k+1 O
as O
X O
u O
and O
its O
translation O
Y O
2k+1 O
as O
Y O
u O
for O
simplicity O
, O
where O
X O
u O
= O
{ O
x O
i O
} O
m O
i=1 O
with O
m O
tokens O
and O
Y O
u O
= O
{ O
y O
i O
} O
n O
i=1 O
with O
n O
tokens O
. O
Formally O
, O
the O
conditional O
distribution O
for O
the O
current O
utterance O
is O

p O
θ O
( O
Y O
u O
|X O
u O
, O
C O
) O
= O
n O
t=1 O
p O
θ O
( O
y O
t O
|X O
u O
, O
y O
1 O
: O
t−1 O
, O
C O
) O
, O

where O
C O
is O
the O
bilingual O
dialogue O
history O
. O

Before O
we O
dig O
into O
the O
details O
of O
how O
to O
utilize O
C O
, O
we O
define O
three O
types O
of O
context O
in O
C O
( O
as O
shown O
in O
Fig O
. O
2 O
) O
: O
( O
1 O
) O
the O
set O
of O
previous O
role O
- O
specific O
source O
- O
language O
turns O
, O
denoted O
as O
C O
role O
X O
= O
{ O
X O
1 O
, O
X O
3 O
, O
X O
5 O
, O
... O
, O
X O
2k+1 O
} O
4 O
where O
k O
∈ O
[ O
0 O
, O
|T O
|−3 O
2 O
] O
and O
T O
is O
the O
total O
number O
of O
turns O
; O
( O
2 O
) O
the O
set O
of O
previous O
source O
- O
language O
turns O
, O
denoted O
as O
C O
X O
= O
{ O
X O
1 O
, O
X O
2 O
, O
X O
3 O
, O
... O
, O
X O
2k O
} O
; O
and O
( O
3 O
) O
the O
set O
of O
previous O
target O
- O
language O
turns O
, O
denoted O
as O

C O
Y O
= O
{ O
Y O
1 O
, O
Y O
2 O
, O
Y O
3 O
, O
... O
, O
Y O
2k O
} O
. O

4 O
Our O
Methodology O
Fig O
. O
3 O
demonstrates O
an O
overview O
of O
our O
model O
, O
consisting O
of O
five O
components O
: O
input O
representation O
, O
encoder O
, O
latent O
variational O
modules O
, O
decoder O
, O
and O
training O
objectives O
. O
Specifically O
, O
we O
aim O
to O
model O
both O
dialogue O
and O
translation O
simultaneously O
. O
Therefore O
, O
for O
the O
input O
representation O
( O
§ O
4.1 O
) O
, O
we O
incorporate O
dialogue O
- O
level O
embeddings O
, O
i.e. O
, O
role O
and O
dialogue O
turn O
embeddings O
, O
into O
the O
encoder O
( O
§ O
4.2 O
) O
. O
Then O
, O
we O
introduce O
three O
specific O
latent O
variational O
modules O
( O
§ O
4.3 O
) O
to O
learn O
the O
distributions O
for O
varied O
inherent O
bilingual O
characteristics O
. O
Finally O
, O
we O
elaborate O
on O
how O
to O
incorporate O
the O
three O
tailored O
latent O
variables O
sampled O
from O
4 O
C O
role O
Y O
= O
{ O
Y2 O
, O
Y4 O
, O
Y6 O
, O
... O
, O
Y O
2k O
} O
is O
also O
role O
- O
specific O
utterances O
of O
the O
interlocutor O
, O
which O
is O
used O
to O
model O
the O
interlocutor O
's O
consistency O
in O
the O
reverse O
translation O
direction O
. O
Here O
, O
we O
take O
one O
translation O
direction O
( O
i.e. O
, O
En⇒Ch O
) O
as O
an O
example O
. O
the O
distributions O
into O
the O
decoder O
( O
§ O
4.4 O
) O
and O
our O
two O
- O
stage O
training O
objectives O
( O
§ O
4.5 O
) O
. O

Input O
Representation O

The O
CPCC B-MethodName
contains O
three O
types O
of O
inputs O
: O
source O
input O
X O
u O
, O
target O
input O
Y O
u O
, O
and O
context O
inputs O
{ O
C O
role O
X O
, O
C O
X O
, O
C O
Y O
} O
. O
Apart O
from O
the O
conventional O
word O
embeddings O
WE O
and O
position O
embeddings O
PE O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
, O
we O
also O
introduce O
role O
embeddings O
RE O
and O
dialogue O
turn O
embeddings O
TE O
to O
identify O
different O
utterances O
. O
Specifically O
, O
for O
X O
u O
, O
we O
firstly O
project O
it O
into O
these O
embeddings O
. O
Then O
, O
we O
perform O
a O
sum O
operation O
to O
unify O
them O
into O
a O
single O
input O
for O
each O
token O
x O
i O
: O

h O
0 O
i O
= O
WE O
( O
x O
i O
) O
+ O
PE O
( O
x O
i O
) O
+ O
RE O
( O
x O
i O
) O
+ O
TE O
( O
x O
i O
) O
, O

( O
2 O
) O
where O
1 O
≤ O
i O
≤ O
m O
and O
WE O
∈ O
R O
|V O
|×d O
, O
RE O
∈ O
R O
|R|×d O
and O
SE O
∈ O
R O
|T O
|×d O
. O
|V O
| O
, O
|R| O
, O
|T O
| O
, O
and O
d O
denote O
the O
size O
of O
shared O
vocabulary O
, O
number O
of O
roles O
, O
max O
turns O
of O
dialogue O
, O
and O
hidden O
size O
, O
respectively O
. O
h O
0 O
∈ O
R O
m×d O
, O
similarly O
for O
Y O
u O
. O
For O
each O
of O
{ O
C O
role O
X O
, O
C O
X O
, O
C O
Y O
} O
, O
we O
add O
' O
[ O
cls O
] O
' O
tag O
at O
the O
head O
of O
it O
and O
use O
' O
[ O
sep O
] O
' O
tag O
to O
separate O
its O
utterances O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
, O
and O
then O
get O
its O
embeddings O
via O
Eq O
. O
2 O
. O

Encoder O

The O
Transformer O
encoder O
consists O
of O
N B-HyperparameterName
e I-HyperparameterName
stacked O
layers O
and O
each O
layer O
includes O
two O
sub O
- O
layers O
: O
5 O
a O
multi O
- O
head O
self O
- O
attention O
( O
SelfAtt O
) O
sub O
- O
layer O
and O
a O
position O
- O
wise O
feed O
- O
forward O
network O
( O
FFN O
) O
sublayer O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
: O

s O
e O
= O
SelfAtt O
( O
h O
−1 O
e O
) O
+ O
h O
−1 O
e O
, O
h O
−1 O
e O
∈ O
R O
m×d O
, O

h O
e O
= O
FFN O
( O
s O
e O
) O
+ O
s O
e O
, O
{ O
h O
e O
, O
s O
e O
} O
∈ O
R O
m×d O
, O
5 O
We O
omit O
the O
layer O
normalization O
for O
simplicity O
, O
and O
you O
may O
refer O
to O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
for O
more O
details O
. O

where O
h O
e O
denotes O
the O
state O
of O
the O
-th O
encoder O
layer O
and O
h O
0 O
e O
denotes O
the O
initialized O
feature O
h O
0 O
. O
We O
prepare O
the O
representations O
of O
X O
u O
and O
{ O
C O
role O
X O
, O
C O
X O
, O
C O
Y O
} O
for O
training O
prior O
and O
recognition O
networks O
. O
For O
X O
u O
, O
we O
apply O
mean O
- O
pooling O
with O
mask O
operation O
over O
the O
output O
h O
Ne B-HyperparameterName
, O
X O
e O
of O
the O
N B-HyperparameterName
eth I-HyperparameterName
encoder O
layer O
, O
i.e. O
, O
h O

X O
= O
1 O
m O
m O
i=1 O
( O
M O
X O
i O
h O
Ne B-HyperparameterName
, O
X O
e O
, O
i O

) O
, O
h O
X O
∈ O
R O
d O
, O
where O
M O
X O
∈ O
R O
m O
denotes O
the O
mask O
matrix O
, O
whose O
value O
is O
either O
1 O
or O
0 O
indicating O
whether O
the O
token O
is O
padded O
. O
For O
C O
role O
X O
, O
as O
shown O
in O
Fig O
. O
3 O
, O
we O
follow O
( O
Ma O
et O
al O
. O
, O
2020 O
) O
and O
share O
the O
first O
encoder O
layer O
to O
obtain O
the O
context O
representation O
. O
Here O
, O
we O
take O
the O
hidden O
state O
of O
' O
[ O
cls O
] O
' O
as O
its O
representation O
, O
denoted O
as O
h O
ctx O
role O
∈ O
R O
d O
. O
Similarly O
, O
we O
obtain O
representations O
of O
C O
X O
and O
C O
Y O
, O
denoted O
as O
h O
ctx O
X O
∈ O
R O
d O
and O
h O
ctx O
Y O
∈ O
R O
d O
, O
respectively O
. O
For O
training O
recognition O
networks O
, O
we O
obtain O
the O
representation O
of O

Y O
u O
as O
h O
Y O
= O
1 O
n O
n O
i=1 O
( O
M O
Y O
i O
h O
Ne B-HyperparameterName
, O
Y O
e O
, O
i O
) O
, O
h O
Y O
∈ O
R O
d O
, O
where O
M O
Y O
∈ O
R O
n O
, O
similar O
to O
M O
X O
. O

Latent O
Variational O
Modules O

We O
design O
three O
tailored O
latent O
variational O
modules O
to O
learn O
the O
distributions O
of O
inherent O
bilingual O
conversational O
characteristics O
, O
i.e. O
, O
role O
preference O
, O
dialogue O
coherence O
, O
and O
translation O
consistency O
. O

Role O
Preference O
. O
To O
preserve O
the O
role O
preference O
when O
translating O
the O
role O
's O
current O
utterance O
, O
we O
only O
encode O
the O
previous O
utterances O
of O
this O
role O
and O
produce O
a O
role O
- O
tailored O
latent O
variable O
z O
role O
∈ O
R O
dz O
, O
where O
d O
z O
is O
the O
latent O
size O
. O
Inspired O
by O
( O
Wang O
and O
Wan O
, O
2019 O
) O
, O
we O
use O
isotropic O
Gaussian O
distribution O
as O
the O
prior O
distribution O
of O
z O
role O
: O
p O
θ O
( O
z O
role O
|X O
u O
, O
C O
role O
X O
) O
∼ O
N O
( O
µ O
role O
, O
σ O
2 O
role O
I O
) O
, O
where O
I O
denotes O
the O
identity O
matrix O
and O
we O
have O
µ O
role O
= O
MLP O
role O
θ O
( O
h O
X O
; O
h O
ctx O
role O
) O
, O
σ O
role O
= O
Softplus O
( O
MLP O
role O
θ O
( O
h O
X O
; O
h O
ctx O
role O
) O
) O
, O
where O
MLP O
( O
• O
) O
and O
Softplus O
( O
• O
) O
are O
multi O
- O
layer O
perceptron O
and O
approximation O
of O
ReLU O
function O
, O
respectively O
. O
( O
• O
; O
• O
) O
indicates O
concatenation O
operation O
. O

At O
training O
, O
the O
posterior O
distribution O
conditions O
on O
both O
role O
- O
specific O
utterances O
and O
the O
current O
translation O
, O
which O
contain O
rich O
role O
preference O
information O
. O
Therefore O
, O
the O
prior O
network O
can O
learn O
a O
role O
- O
tailored O
distribution O
by O
approaching O
the O
posterior O
network O
via O
KL B-MetricName
divergence I-MetricName
( O
Sohn O
et O
al O
. O
, O
2015 O
) O
: O

q O
φ O
( O
z O
role O
|X O
u O
, O
C O
role O
X O
, O
Y O
u O
) O
∼ O
N O
( O
µ O
role O
, O
σ O
2 O
role O
I O

) O
and O
{ O
µ O
role O
, O
σ O
role O
} O
are O
calculated O
as O
: O

µ O
role O
= O
MLP O
role O
φ O
( O
h O
X O
; O
h O
ctx O
role O
; O
h O
Y O
) O
, O
σ O
role O
= O
Softplus O
( O
MLP O
role O
φ O
( O
h O
X O
; O
h O
ctx O
role O
; O
h O
Y O
) O
) O
. O

Dialogue O
Coherence O
. O
To O
maintain O
the O
coherence O
in O
chat O
translation O
, O
we O
encode O
the O
entire O
sourcelanguage O
utterances O
and O
then O
generate O
a O
latent O
variable O
z O
dia O
∈ O
R O
dz O
. O
Similar O
to O
z O
role O
, O
we O
define O
its O
prior O
distribution O
as O
: O

p O
θ O
( O
z O
dia O
|X O
u O
, O
C O
X O
) O
∼ O
N O
( O
µ O
dia O
, O
σ O
2 O
dia O
I O
) O
and O
{ O
µ O
dia O
, O
σ O
dia O
} O
are O
calculated O
as O
: O
µ O
dia O
= O
MLP O
dia O
θ O
( O
h O
X O
; O
h O
ctx O
X O
) O
, O
σ O
dia O
= O
Softplus O
( O
MLP O
dia O
θ O
( O
h O
X O
; O
h O
ctx O
X O
) O
) O
. O

At O
training O
, O
the O
posterior O
distribution O
conditions O
on O
both O
the O
entire O
source O
- O
language O
utterances O
and O
the O
translation O
that O
provide O
a O
dialoguelevel O
coherence O
clue O
, O
and O
is O
responsible O
for O
guiding O
the O
learning O
of O
the O
prior O
distribution O
. O
Specifically O
, O
we O
define O
the O
posterior O
distribution O
as O
: O

q O
φ O
( O
z O
dia O
|X O
u O
, O
C O
X O
, O
Y O
u O
) O
∼ O
N O
( O
µ O
dia O
, O
σ O
2 O
dia O
I O
) O

, O
where O
µ O
dia O
and O
σ O
dia O
are O
calculated O
as O
: O

µ O
dia O
= O
MLP O
dia O
φ O
( O
h O
X O
; O
h O
ctx O
X O
; O
h O
Y O
) O
, O
σ O
dia O
= O
Softplus O
( O
MLP O
dia O
φ O
( O
h O
X O
; O
h O
ctx O
X O
; O
h O
Y O
) O
) O
. O

Translation O
Consistency O
. O
To O
keep O
the O
lexical O
choice O
of O
translation O
consistent O
with O
those O
of O
previous O
utterances O
, O
we O
encode O
the O
paired O
sourcetarget O
utterances O
and O
then O
sample O
a O
latent O
variable O
z O
tra O
∈ O
R O
dz O
. O
We O
define O
its O
prior O
distribution O
as O
: O
p O
θ O
( O
z O
tra O
|X O
u O
, O
C O
X O
, O
C O
Y O
) O
∼ O
N O
( O
µ O
tra O
, O
σ O
2 O
tra O
I O
) O
and O
{ O
µ O
tra O
, O
σ O
tra O
} O
are O
calculated O
as O
: O

µ O
tra O
= O
MLP O
tra O
θ O
( O
h O
X O
; O
h O
ctx O
X O
; O
h O
ctx O
Y O
) O
, O
σ O
tra O
= O
Softplus O
( O
MLP O
tra O
θ O
( O
h O
X O
; O
h O
ctx O
X O
; O
h O
ctx O
Y O
) O
) O
. O

At O
training O
, O
the O
posterior O
distribution O
conditions O
on O
all O
paired O
bilingual O
dialogue O
utterances O
that O
contain O
implicit O
and O
aligned O
information O
, O
and O
serves O
as O
learning O
of O
the O
prior O
distribution O
. O
Specifically O
, O
we O
define O
the O
posterior O
distribution O
as O
: O
q O
φ O
( O
z O
tra O
|X O
u O
, O
C O
X O
, O
C O
Y O
, O
Y O
u O
) O
∼ O
N O
( O
µ O
tra O
, O
σ O
2 O
tra O
I O
) O
, O
where O
µ O
tra O
and O
σ O
tra O
are O
calculated O
as O
: O

µ O
tra O
= O
MLP O
tra O
φ O
( O
h O
X O
; O
h O
ctx O
X O
; O
h O
ctx O
Y O
; O
h O
Y O
) O
, O
σ O
tra O
= O
Softplus O
( O
MLP O
tra O
φ O
( O
h O
X O
; O
h O
ctx O
X O
; O
h O
ctx O
Y O
; O
h O
Y O
) O
) O
. O

Decoder O

The O
decoder O
adopts O
a O
similar O
structure O
to O
the O
encoder O
, O
and O
each O
of O
N B-HyperparameterValue
d I-HyperparameterValue
decoder O
layers O
contains O
an O
additional O
cross O
- O
attention O
sub O
- O
layer O
( O
CrossAtt O
) O
: O

s O
d O
= O
SelfAtt O
( O
h O
−1 O
d O
) O
+ O
h O
−1 O
d O
, O
h O
−1 O
d O
∈ O
R O
n×d O
, O
c O
d O
= O
CrossAtt O
( O
s O
d O
, O
h O
Ne B-HyperparameterName
e O
) O
+ O
s O
d O
, O
s O
d O
∈ O
R O
n×d O
, O
h O
d O
= O
FFN O
( O
c O
d O
) O
+ O
c O
d O
, O
{ O
c O
d O
, O
h O
d O
} O
∈ O
R O
n×d O
, O

where O
h O
d O
denotes O
the O
state O
of O
the O
-th O
decoder O
layer O
. O

As O
shown O
in O
Fig O
. O
3 O
, O
we O
obtain O
the O
latent O
variables O
{ O
z O
role O
, O
z O
dia O
, O
z O
tra O
} O
either O
from O
the O
posterior O
distribution O
predicted O
by O
recognition O
networks O
( O
training O
process O
as O
the O
solid O
grey O
lines O
) O
or O
from O
prior O
distribution O
predicted O
by O
prior O
networks O
( O
inference O
process O
as O
the O
dashed O
red O
lines O
) O
. O
Finally O
, O
we O
incorporate O
{ O
z O
role O
, O
z O
dia O
, O
z O
tra O
} O
into O
the O
state O
of O
the O
top O
layer O
of O
the O
decoder O
with O
a O
projection O
layer O
: O

o O
t O
= O
Tanh O
( O
W O
p O
[ O
h O
N O
d O
d O
, O
t O
; O
z O
role O
; O
z O
dia O
; O
z O
tra O
] O
+ O
b O
p O
) O
, O
o O
t O
∈ O
R O
d O
, O

where O
W O
p O
∈ O
R O
d× O
( O
d+3dz O
) O
and O
b O
p O
∈ O
R O
d O
are O
training O
parameters O
, O
h O
N B-HyperparameterName
d I-HyperparameterName
d O
, O
t O
is O
the O
hidden O
state O
at O
time O
- O
step O
t O
of O
the O
N B-HyperparameterName
d I-HyperparameterName
-th O
decoder O
layer O
. O
Then O
, O
o O
t O
is O
fed O
to O
a O
linear O
transformation O
and O
softmax O
layer O
to O
predict O
the O
probability O
distribution O
of O
the O
next O
target O
token O
: O

p O
t O
= O
Softmax O
( O
W O
o O
o O
t O
+ O
b O
o O
) O
, O
p O
t O
∈ O
R O
|V O
| O
, O

where O
W O
o O
∈ O
R O
|V O
|×d O
and O
b O
o O
∈ O
R O
|V O
| O
are O
training O
parameters O
. O

Training O
Objectives O

We O
apply O
a O
two O
- O
stage O
training O
strategy O
Ma O
et O
al O
. O
, O
2020 O
) O
. O
Firstly O
, O
we O
train O
our O
model O
on O
large O
- O
scale O
sentence O
- O
level O
NMT B-TaskName
data O
to O
minimize O
the O
cross B-MetricName
- I-MetricName
entropy I-MetricName
objective O
: O

L O
( O
θ O
; O
X O
, O
Y O
) O
= O
− O
N O
t=1 O
logp O
θ O
( O
y O
t O
|X O
, O
y O
1 O
: O
t−1 O
) O
. O

Secondly O
, O
we O
fine O
- O
tune O
it O
on O
the O
chat O
translation O
data O
to O
maximize O
the O
following O
objective O
: O

J O
( O
θ O
, O
φ O
; O
X O
u O
, O
C O
role O
X O
, O
C O
X O
, O
C O
Y O
, O
Y O
u O
) O
= O
− O
KL O
( O
q O
φ O
( O
z O
role O
|X O
u O
, O
C O
role O
X O
, O
Y O
u O
) O
p O
θ O
( O
z O
role O
|X O
u O
, O
C O
role O
X O
) O
) O
− O
KL O
( O
q O
φ O
( O
z O
dia O
|X O
u O
, O
C O
X O
, O
Y O
u O
) O
p O
θ O
( O
z O
dia O
|X O
u O
, O
C O
X O
) O
) O
− O
KL O
( O
q O
φ O
( O
z O
tra O
|X O
u O
, O
C O
X O
, O
C O
Y O
, O
Y O
u O
) O
p O
θ O
( O
z O
tra O
|X O
u O
, O
C O
X O
, O
C O
Y O
) O
) O
+ O
E O
q O
φ O
[ O
logp O
θ O
( O
Y O
u O
|X O
u O
, O
z O
role O
, O
z O
dia O
, O
z O
tra O
) O
] O
. O

We O
use O
the O
reparameterization O
trick O
( O
Kingma O
and O
Welling O
, O
2013 O
) O
to O
estimate O
the O
gradients O
of O
the O
prior O
and O
recognition O
networks O
( O
Zhao O
et O
al O
. O
, O
2017 O
) O
. O

Experiments O

Datasets O
and O
Metrics O

Datasets O
. O
We O
apply O
a O
two O
- O
stage O
training O
strategy O
, O
i.e. O
, O
firstly O
training O
on O
a O
large O
- O
scale O
sentence O
- O
level O
NMT B-TaskName
corpus O
( O
WMT20 O
6 O
) O
and O
then O
fine O
- O
tuning O
on O
chat O
translation O
corpus O
( O
BConTrasT B-DatasetName
( O
Farajian O
et O
al O
. O
, O
2020 O
) O
7 O
and O
BMELD B-DatasetName
) O
. O
The O
details O
( O
WMT20 O
data O
and O
results O
of O
the O
first O
stage O
) O
are O
shown O
in O
Appendix O
A O
. O

BConTrasT. B-DatasetName
The O
dataset O
8 O
is O
first O
provided O
by O
WMT O
2020 O
Chat B-TaskName
Translation I-TaskName
Task O
( O
Farajian O
et O
al O
. O
, O
2020 O
) O
, O
which O
is O
translated O
from O
English O
into O
German O
and O
is O
based O
on O
the O
monolingual O
Taskmaster-1 O
corpus O
( O
Byrne O
et O
al O
. O
, O
2019 O
) O
. O
The O
conversations O
( O
originally O
in O
English O
) O
were O
first O
automatically O
translated O
into O
German O
and O
then O
manually O
postedited O
by O
Unbabel O
editors O
, O
9 O
who O
are O
native O
German O
speakers O
. O
Having O
the O
conversations O
in O
both O
languages O
allows O
us O
to O
simulate O
bilingual O
conversations O
in O
which O
one O
speaker O
, O
the O
customer O
, O
speaks O
in O
German O
and O
the O
other O
speaker O
, O
the O
agent O
, O
answers O
in O
English O
. O

BMELD B-DatasetName
. O
Similarly O
, O
based O
on O
the O
dialogue O
dataset O
in O
the O
MELD B-DatasetName
( O
originally O
in O
English O
) O
( O
Poria O
et O
al O
. O
, O
2019 O
) O
, O
10 O
we O
firstly O
crawled O
the O
corresponding O
Chinese O
translations O
from O
this O
11 O
and O
then O
manually O
post O
- O
edited O
them O
according O
to O
the O
dialogue O
history O
by O
native O
Chinese O
speakers O
, O
who O
are O
postgraduate O
students O
majoring O
in O
English O
. O
Finally O
, O
following O
( O
Farajian O
et O
al O
. O
, O
2020 O
) O
, O
we O
assume O
50 O
% O
speakers O
as O
Chinese O
speakers O
to O
keep O
data O
balance O
for O
Ch⇒En O
translations O
and O
build O
the O
bilingual B-DatasetName
MELD I-DatasetName
( O
BMELD B-DatasetName
) O
. O
For O
the O
Chinese O
, O
we O
segment O
the O
sentence O
using O
Stanford O
CoreNLP O
toolkit O
12 O
. O

Metrics O
. O
For O
fair O
comparison O
, O
we O
use O
the O
Sacre B-MetricName
- I-MetricName
BLEU I-MetricName
13 O
( O
Post O
, O
2018 O
) O
and O
v0.7.25 O
for O
TER B-MetricName
( O
Snover O
6 O
http O
: O
/ O
/ O
www.statmt.org O
/ O
wmt20 O
/ O
translation-task.html O
7 O
http O
: O
/ O
/ O
www.statmt.org O
/ O
wmt20 O
/ O
chat-task.html O
8 O
https O
: O
/ O
/ O
github.com O
/ O
Unbabel O
/ O
BConTrasT B-DatasetName
9 O
www.unbabel.com O
10 O
The O
MELD B-DatasetName
is O
a O
multimodal O
emotionLines O
dialogue O
dataset O
, O
each O
utterance O
of O
which O
corresponds O
to O
a O
video O
, O
voice O
, O
and O
text O
, O
and O
is O
annotated O
with O
detailed O
emotion O
and O
sentiment O
. O
11 O
( O
Koehn O
, O
2004 O
) O
. O
For O
En⇔De O
, O
we O
report O
case O
- O
sensitive O
score O
following O
the O
WMT20 O
chat O
task O
( O
Farajian O
et O
al O
. O
, O
2020 O
) O
. O

For O
Ch⇒En O
, O
we O
report O
case O
- O
insensitive O
score O
. O
For O
En⇒Ch O
, O
we O
report O
the O
character O
- O
level O
BLEU B-MetricName
score I-MetricName
. O

Implementation O
Details O

For O
all O
experiments O
, O
we O
follow O
the O
Transformer B-MethodName
- I-MethodName
Base I-MethodName
and O
Transformer B-MethodName
- I-MethodName
Big I-MethodName
settings O
illustrated O
in O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
. O
In O
Transformer B-MethodName
- I-MethodName
Base I-MethodName
, O
we O
use O
512 B-HyperparameterValue
as O
hidden B-HyperparameterName
size I-HyperparameterName
( O
i.e. O
, O
d B-HyperparameterName
) O
, O
2048 B-HyperparameterValue
as O
filter B-HyperparameterName
size I-HyperparameterName
and O
8 B-HyperparameterValue
heads B-HyperparameterName
in O
multi O
- O
head O
attention O
. O
In O
Transformer B-MethodName
- I-MethodName
Big I-MethodName
, O
we O
use O
1024 B-HyperparameterValue
as O
hidden B-HyperparameterName
size I-HyperparameterName
, O
4096 B-HyperparameterValue
as O
filter B-HyperparameterName
size I-HyperparameterName
, O
and O
16 B-HyperparameterValue
heads B-HyperparameterName
in O
multi O
- O
head O
attention O
. O

All O
our O
Transformer O
models O
contain O
N O
e O
= O
6 O
encoder O
layers O
and O
N O
d O
= O
6 O
decoder O
layers O
and O
all O
models O
are O
trained O
using O
THUMT O
( O
Tan O
et O
al O
. O
, O
2020 O
) O
framework O
. O
We O
conduct O
experiments O
on O
the O
validation O
set O
of O
En⇒De O
to O
select O
the O
hyperparameters O
of O
context O
length O
and O
latent O
dimension O
, O
which O
are O
then O
shared O
for O
all O
tasks O
. O
For O
the O
results O
and O
more O
details O
( O
other O
hyperparameters O
setting O
and O
average O
running O
time O
) O
, O
please O
refer O
to O
Appendix O
B O
, O
C O
, O
and O
D O
. O

Comparison O
Models O

Baseline O
NMT B-TaskName
Models O
. O
Transformer B-MethodName
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
: O
the O
de O
- O
facto O
NMT B-TaskName
model O
that O
does O
not O
fine O
- O
tune O
on O
chat O
translation O
data O
. O
Transformer+FT B-MethodName
: O
fine O
- O
tuning O
on O
the O
chat O
translation O
data O
after O
being O
pre O
- O
trained O
on O
sentence O
- O
level O
NMT B-TaskName
corpus O
. O

Main O
Results O

Overall O
, O
we O
separate O
the O
models O
into O
two O
parts O
in O
Tab O
. O
2 O
: O
the O
Base O
setting O
and O
the O
Big O
setting O
. O
In O
each O
part O
, O
we O
show O
the O
results O
of O
our O
re O
- O
implemented O
Transformer B-MethodName
baselines O
, O
the O
context O
- O
aware O
NMT B-TaskName
systems O
, O
and O
our O
approach O
on O
En⇔De O
and O
En⇔Ch O
. O

Results O
on O
En⇔De O
. O
Under O
the O
Base O
setting O
, O
CPCC B-MethodName
substantially O
outperforms O
the O
baselines O
( O
e.g. O
, O
" O
Transformer+FT B-MethodName
" O
) O
by O
a O
large O
margin O
with O
1.70↑ B-MetricValue
and O
1.48↑ B-MetricValue
BLEU B-MetricName
scores I-MetricName
on O
En⇒De O
and O
De⇒En O
, O
respectively O
. O
On O
the O
TER B-MetricName
, O
our O
CPCC B-MethodName
achieves O
a O
significant O
improvement O
of O
1.3 B-MetricValue
points I-MetricValue
in O
both O
language O
pairs O
. O
Under O
the O
Big O
setting O
, O
our O
CPCC B-MethodName
also O
consistently O
boosts O
the O
performance O
in O
both O
direc O
- O
tions O
( O
i.e. O
, O
1.22↑ B-MetricValue
and O
1.47↑ B-MetricValue
BLEU B-MetricName
scores I-MetricName
, O
0.4↓ B-MetricValue
and O
1.1↓ B-MetricValue
TER B-MetricName
scores I-MetricName
) O
, O
showing O
its O
effectiveness O
. O

Compared O
against O
the O
strong O
context O
- O
aware O
NMT B-TaskName
systems O
( O
underlined O
results O
) O
, O
our O
CPCC B-MethodName
significantly O
surpasses O
them O
( O
about O
1.39∼1.59↑ B-MetricValue
BLEU B-MetricName
scores I-MetricName
and O
0.6∼0.9↓ B-MetricValue
TER B-MetricName
scores O
) O
in O
both O
language O
directions O
under O
both O
Base O
and O
Big O
settings O
, O
demonstrating O
the O
superiority O
of O
our O
model O
. O

Results O
on O
En⇔Ch O
. O

We O
also O
conduct O
experiments O
on O
our O
self O
- O
collected O
data O
to O
validate O
the O
generalizability O
across O
languages O
in O
Tab O
. O
2 O
. O

Our O
CPCC B-MethodName
presents O
remarkable O
BLEU B-MetricName
improvements O
over O
the O
" O
Transformer+FT B-MethodName
" O
by O
a O
large O
margin O
in O
two O
directions O
by O
2.33↑ B-MetricValue
and O
0.91↑ B-MetricValue
BLEU B-MethodName
gains O
under O
the O
Base O
setting O
, O
respectively O
, O
and O
by O
2.03↑ B-MetricValue
and O
0.83↑ B-MetricValue
BLEU B-MetricValue
gains O
in O
both O
directions O
under O
the O
Big O
setting O
. O
These O
results O
suggest O
that O
CPCC B-MethodName
consistently O
performs O
well O
across O
languages O
. O

Compared O
with O
strong O
context O
- O
aware O
NMT B-TaskName
systems O
( O
e.g. O
, O
" O
V O
- O
Transformer+FT B-MethodName
" O
) O
, O
our O
approach O
notably O
surpasses O
them O
in O
both O
language O
directions O
under O
both O
Base O
and O
Big O
settings O
, O
which O
shows O
the O
generalizability O
and O
superiority O
of O
our O
model O
. O

Analysis O

Ablation O
Study O

We O
conduct O
ablation O
studies O
to O
investigate O
how O
well O
each O
tailored O
latent O
variable O
of O
our O
model O
works O
. O
When O
removing O
latent O
variables O
listed O
in O
Tab O
. O
3 O
, O
we O
have O
the O
following O
findings O
. O

( O
1 O
) O
All O
latent O
variables O
make O
substantial O
contributions O
to O
performance O
, O
proving O
the O
importance O
of O
modeling O
role O
preference O
, O
dialogue O
coherence O
, O
and O
translation O
consistency O
, O
which O
is O
consistent O
with O
our O
intuition O
that O
the O
properties O
should O
be O
beneficial O
to O
better O
translations O
( O
rows O
1∼3 O
vs. O
row O
0 O
) O
. O

( O
2 O
) O
Results O
of O
rows O
4∼7 O
show O
the O
combination O
effect O
of O
three O
latent O
variables O
, O
suggesting O
that O
the O
combination O
among O
three O
latent O
variables O
has O
a O
cumulative O
effect O
( O
rows O
4∼7 O
vs. O
rows O
0∼3 O
) O
. O

( O
3 O
) O
Row O
7 O
vs. O
row O
0 O
shows O
that O
explicitly O
modeling O
the O
bilingual O
conversational O
characteristics O
significantly O
outperforms O
implicit O
modeling O
( O
i.e. O
, O
just O
incorporating O
the O
dialogue O
history O
into O
encoders O
) O
, O
which O
lacks O
the O
relevant O
information O
guidance O
. O

Dialogue O
Coherence O

Following O
( O
Lapata O
and O
Barzilay O
, O
2005 O
; O
Xiong O
et O
al O
. O
, O
2019 O
) O
, O
we O
measure O
dialogue O
coherence O
as O
sentence O
similarity O
. O
Specifically O
, O
the O
representation O
of O
each O
sentence O
is O
the O
mean O
of O
the O
distributed O
vectors O
of O
its O
words O
, O
and O
the O
dialogue O
coherence O
between O
two O
sentences O
s O
1 O
and O
s O
2 O
is O
determined O
by O
the O
cosine O
similarity O
: O

sim O
( O
s O
1 O
, O
s O
2 O
) O
= O
cos O
( O
f O
( O
s O
1 O
) O
, O
f O
( O
s O
2 O
) O
) O
, O
f O
( O
s O
i O
) O
= O
1 O
|s O
i O
| O
w∈s O
i O
( O
w O
) O
, O

where O
w O
is O
the O
vector O
for O
word O
w O
. O

We O
use O
Word2Vec O
14 O
( O
Mikolov O
et O
al O
. O
, O
2013 O
) O
to O
learn O
the O
distributed O
vectors O
of O
words O
by O
training O
on O
the O
monolingual O
dialogue O
dataset O
: O
Taskmaster-1 O
( O
Byrne O
et O
al O
. O
, O
2019 O
) O
. O
And O
we O
set O
the O
dimensionality O
of O
word O
embeddings O
to O
100 B-HyperparameterValue
. O

Tab O
. O
4 O
shows O
the O
cosine B-MetricName
similarity I-MetricName
on O
the O
test O
set O
of O
De⇒En O
. O
It O
reveals O
that O
our O
model O
encouraged O
by O
tailor O
- O
made O
latent O
variables O
produces O
better O
coherence O
in O
chat O
translation O
than O
contrast O
systems O
. O

Human O
Evaluation O

Inspired O
by O
( O
Bao O
et O
al O
. O
, O
2020 O
; O
Farajian O
et O
al O
. O
, O
2020 O
) O
, O
we O
use O
four O
criteria O
for O
human O
evaluation O
: O
( O
1 O
) O
Preference B-MetricName
measures O
whether O
the O
translation O
preserves O
the O
role O
preference O
information O
; O
( O
2 O
) O
Coherence B-MetricName
denotes O
whether O
the O
translation O
is O
semantically O
coherent O
with O
the O
dialogue O
history O
; O
( O
3 O
) O
Consistency B-MetricName
measures O
whether O
the O
lexical O
choice O
of O
translation O
is O
consistent O
with O
the O
preceding O
utterances O
; O
( O
4 O
) O
Fluency B-MetricName
measures O
whether O
the O
translation O
is O
logically O
reasonable O
and O
grammatically O
correct O
. O
Table O
4 O
: O
Results O
of O
dialogue O
coherence O
in O
terms O
of O
sentence O
similarity O
( O
De⇒En O
, O
Base O
) O
. O
The O
" O
# O
-th O
Pr O
. O
" O
denotes O
the O
# O
-th O
preceding O
utterance O
to O
the O
current O
one O
. O
" O
† O
† O
" O
indicates O
that O
statistically O
significant O
better O
than O
the O
best O
result O
of O
all O
contrast O
NMT B-TaskName
models O
( O
p O
< O
0.01 O
) O
. O
We O
firstly O
randomly O
sample O
200 O
examples O
from O
the O
test O
set O
of O
Ch⇒En O
. O
Then O
, O
we O
assign O
each O
bilingual O
dialogue O
history O
and O
corresponding O
6 O
generated O
translations O
to O
three O
human O
annotators O
without O
order O
, O
and O
ask O
them O
to O
evaluate O
whether O
each O
translation O
meets O
the O
criteria O
defined O
above O
. O
All O
annotators O
are O
postgraduate O
students O
and O
not O
involved O
in O
other O
parts O
of O
our O
experiments O
. O

Models O

Tab O
. O
5 O
shows O
that O
our O
CPCC B-MethodName
effectively O
alleviates O
the O
problem O
of O
role O
- O
irrelevant O
, O
incoherent O
and O
inconsistent O
translations O
compared O
with O
other O
models O
( O
significance O
test O
( O
Koehn O
, O
2004 O
) O
, O
p O
< O
0.05 O
) O
, O
indicating O
the O
superiority O
of O
our O
model O
. O
The O
interannotator B-MetricName
agreement I-MetricName
is O
0.527 B-MetricValue
, O
0.491 B-MetricValue
, O
0.556 B-MetricValue
and O
0.485 B-MetricValue
calculated O
by O
the O
Fleiss B-MetricName
' I-MetricName
kappa I-MetricName
( O
Fleiss O
and O
Cohen O
, O
1973 O
) O
, O
for O
preference B-MetricName
, O
coherence B-MetricName
, O
consistency B-MetricName
and O
fluency B-MetricName
, O
respectively O
, O
indicating O
" O
Moderate O
Agreement O
" O
for O
all O
four O
criteria O
. O
We O
also O
present O
some O
case O
studies O
in O
Appendix O
H O
. O

Related O
Work O

Chat O
NMT B-TaskName
. O
It O
only O
involves O
several O
researches O
due O
to O
the O
lack O
of O
human O
- O
annotated O
publicly O
available O
data O
( O
Farajian O
et O
al O
. O
, O
2020 O
) O
. O
Therefore O
, O
some O
existing O
work O
( O
Wang O
et O
al O
. O
, O
2016 O
; O
Zhang O
and O
Zhou O
, O
2019 O
; O
Rikters O
et O
al O
. O
, O
2020 O
) O
mainly O
pays O
attention O
to O
designing O
methods O
to O
automatically O
construct O
the O
subtitles O
corpus O
, O
which O
may O
contain O
noisy O
bilingual O
utterances O
. O
Recently O
, O
Farajian O
et O
al O
. O
( O
2020 O
) O
organize O
the O
WMT20 O
chat O
translation O
task O
and O
first O
provide O
a O
human O
postedited O
corpus O
, O
where O
some O
teams O
investigate O
the O
effect O
of O
dialogue O
history O
and O
finally O
ensemble O
their O
models O
for O
higher O
ranks O
( O
Berard O
et O
al O
. O
, O
2020 O
; O
Mohammed O
et O
al O
. O
, O
2020 O
; O
Bao O
et O
al O
. O
, O
2020 O
; O
Moghe O
et O
al O
. O
, O
2020 O
) O
. O
As O
a O
synchronizing O
study O
, O
Wang O
et O
al O
. O
( O
2021 O
) O
use O
multitask O
learning O
to O
auto O
- O
correct O
the O
translation O
error O
, O
such O
as O
pronoun O
dropping O
, O
punctuation O
dropping O
, O
and O
typos O
. O
Unlike O
them O
, O
we O
focus O
on O
explicitly O
modeling O
role O
preference O
, O
dialogue O
coherence O
, O
and O
translation O
consistency O
with O
tailored O
latent O
variables O
to O
promote O
the O
translation O
quality O
. O

Context O
- O
Aware O
NMT B-TaskName
. O

Chat O
NMT B-TaskName
can O
be O
viewed O
as O
a O
special O
case O
of O
context O
- O
aware O
NMT B-TaskName
, O
which O
has O
attracted O
many O
researchers O
( O
Gong O
et O
al O
. O
, O
2011 O
; O
Jean O
et O
al O
. O
, O
2017 O
; O
Wang O
et O
al O
. O
, O
2017b O
; O
Bawden O
et O
al O
. O
, O
2018 O
; O
Miculicich O
et O
al O
. O
, O
2018 O
; O
Kuang O
et O
al O
. O
, O
2018 O
; O
Tu O
et O
al O
. O
, O
2018 O
; O
Kang O
et O
al O
. O
, O
2020 O
; O
Ma O
et O
al O
. O
, O
2020 O
) O
to O
extend O
the O
encoder O
or O
decoder O
for O
exploring O
the O
context O
impact O
on O
translation O
quality O
. O
Although O
these O
models O
can O
be O
directly O
applied O
to O
chat O
translation O
, O
they O
can O
not O
explicitly O
capture O
the O
bilingual O
conversational O
characteristics O
and O
thus O
lead O
to O
unsatisfactory O
translations O
( O
Moghe O
et O
al O
. O
, O
2020 O
) O
. O
Different O
from O
these O
studies O
, O
we O
focus O
on O
explicitly O
modeling O
these O
bilingual O
conversational O
characteristics O
via O
CVAE O
for O
better O
translations O
. O

Conditional O

Variational O
Auto O
- O
Encoder O
. O
CVAE O
has O
verified O
its O
superiority O
in O
many O
fields O
( O
Sohn O
et O
al O
. O
, O
2015 O
) O
. O
In O
NMT O
, O
and O
Su O
et O
al O
. O
( O
2018 O
) O
extend O
CVAE O
to O
capture O
the O
global O
/ O
local O
information O
of O
source O
sentence O
for O
better O
results O
. O
McCarthy O
et O
al O
. O
( O
2020 O
) O
focus O
on O
addressing O
the O
posterior O
collapse O
with O
mutual O
information O
. O
Besides O
, O
some O
studies O
use O
CVAE O
to O
model O
the O
correlations O
between O
image O
and O
text O
for O
multimodal O
NMT B-TaskName
( O
Toyama O
et O
al O
. O
, O
2016 O
; O
Calixto O
et O
al O
. O
, O
2019 O
) O
. O
Although O
the O
CVAE O
has O
been O
widely O
used O
in O
NLP O
tasks O
, O
its O
adaption O
and O
utilization O
to O
chat O
translation O
for O
modeling O
inherent O
bilingual O
conversational O
characteristics O
are O
non O
- O
trivial O
, O
and O
to O
the O
best O
of O
our O
knowledge O
, O
has O
never O
been O
investigated O
before O
. O

Conclusion O
and O
Future O
Work O

We O
propose O
to O
model O
bilingual O
conversational O
characteristics O
through O
tailored O
latent O
variables O
for O
neural B-TaskName
chat I-TaskName
translation I-TaskName
. O
Experiments O
on O
En⇔De O
and O
En⇔Ch O
directions O
show O
that O
our O
model O
notably O
improves O
translation O
quality O
on O
both O
BLEU B-MetricName
and O
TER B-MetricName
metrics O
, O
showing O
its O
superiority O
and O
generalizability O
. O
Human O
evaluation O
further O
verifies O
that O
our O
model O
yields O
role O
- O
specific O
, O
coherent O
, O
and O
consistent O
translations O
by O
incorporating O
tailored O
latent O
variables O
into O
NMT B-TaskName
. O
Moreover O
, O
we O
contribute O
a O
new O
bilingual O
dialogue O
data O
( O
BMELD B-DatasetName
, O
En⇔Ch O
) O
with O
manual O
translations O
to O
the O
research O
community O
. O
In O
the O
future O
, O
we O
would O
like O
to O
explore O
the O
effect O
of O
multimodality O
and O
emotion O
on O
chat O
translation O
, O
which O
has O
been O
well O
studied O
in O
dialogue O
field O
. O
Corpus O
, O
and O
WikiMatrix O
for O
the O
En⇔Ch O
. O
We O
firstly O
filter O
noisy O
sentence O
pairs O
according O
to O
their O
characteristics O
in O
terms O
of O
duplication O
and O
length O
( O
whose O
length O
exceeds O
80 O
) O
. O
To O
pre O
- O
process O
the O
raw O
data O
, O
we O
employ O
a O
series O
of O
open O
- O
source O
/ O
in O
- O
house O
scripts O
, O
including O
full- O
/ O
half O
- O
width O
conversion O
, O
unicode O
conversation O
, O
punctuation O
normalization O
, O
and O
tokenization O
. O
After O
filtering O
steps O
, O
we O
generate O
subwords O
via O
joint O
BPE O
( O
Sennrich O
et O
al O
. O
, O
2016 O
) O
with O
32 O
K O
merge O
operations O
. O
Finally O
, O
we O
obtain O
45,541,367 O
sentence O
pairs O
for O
En⇔De O
and O
22,244,006 O
sentence O
pairs O
for O
En⇔Ch O
, O
respectively O
. O

We O
test O
the O
model O
performance O
of O
the O
first O
stage O
on O
newstest2019 B-DatasetName
. O
The O
results O
are O
shown O
in O
Tab O
. O
6 O
. O

B O
Implementation O
Details O

For O
all O
experiments O
, O
we O
follow O
two O
model O
settings O
illustrated O
in O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
, O
namely O
Transformer B-MethodName
- I-MethodName
Base I-MethodName
and O
Transformer B-MethodName
- I-MethodName
Big I-MethodName
. O
The O
training B-HyperparameterName
step I-HyperparameterName
is O
set O
to O
200,000 B-HyperparameterValue
and O
2,000 B-HyperparameterValue
for O
the O
first O
stage O
and O
the O
fine O
- O
tuning O
stage O
, O
respectively O
. O
The O
batch B-HyperparameterName
size I-HyperparameterName
for O
each O
GPU O
is O
set O
to O
4096 B-HyperparameterValue
tokens O
. O
The O
beam B-HyperparameterName
size I-HyperparameterName
is O
set O
to O
4 B-HyperparameterValue
, O
and O
the O
length B-HyperparameterName
penalty I-HyperparameterName
is O
0.6 B-HyperparameterValue
among O
all O
experiments O
. O
All O
experiments O
in O
the O
first O
stage O
are O
conducted O
utilizing O
8 O
NVIDIA O
Tesla O
V100 O
GPUs O
, O
while O
we O
use O
2 O
GPUs O
for O
the O
second O
stage O
, O
i.e. O
, O
fine O
- O
tuning O
. O
That O
gives O
us O
about O
8 O
* O
4096 O
and O
2 O
* O
4096 O
tokens O
per O
update O
for O
all O
experiments O
in O
the O
first O
- O
stage O
and O
second O
- O
stage O
, O
respectively O
. O
All O
models O
are O
optimized O
using O
Adam O
( O
Kingma O
and O
Ba O
, O
2015 O
) O
with O
β B-HyperparameterName
1 I-HyperparameterName
= O
0.9 B-HyperparameterValue
and O
β B-HyperparameterName
2 I-HyperparameterName
= O
0.998 B-HyperparameterValue
, O
and O
learning B-HyperparameterName
rate I-HyperparameterName
is O
set O
to O
1.0 B-HyperparameterValue
for O
all O
experiments O
. O
Label B-HyperparameterName
smoothing I-HyperparameterName
is O
set O
to O
0.1 B-HyperparameterValue
. O
We O
use O
dropout B-HyperparameterName
of O
0.1 B-HyperparameterValue
/ O
0.3 B-HyperparameterValue
for O
Base O
and O
Big O
setting O
, O
respectively O
. O
To O
alleviate O
the O
degeneration O
problem O
of O
the O
variational O
framework O
, O
we O
apply O
KL O
annealing O
. O
The O
KL B-HyperparameterName
multiplier I-HyperparameterName
λ B-HyperparameterName
gradually O
increases O
from O
0 B-HyperparameterValue
to O
1 B-HyperparameterValue
over O
10 O
, O
000 O
steps O
. O
|R| B-HyperparameterName
is O
set O
to O
2 B-HyperparameterValue
for O
En⇔De O
and O
7 B-HyperparameterValue
for O
En⇔Ch O
, O
respectively O
. O
|T B-HyperparameterName
| I-HyperparameterName
is O
set O
to O
10 B-HyperparameterValue
. O
The O
criterion O
for O
selecting O
hyperparameters O
is O
the O
BLEU B-MetricName
score O
on O
validation O
sets O
for O
both O
tasks O
. O
The O
average O
running O
time O
is O
shown O
in O
Tab O
. O
7 O
. O

Stages O

En⇒De O
De⇒En O
En⇒Ch O
Ch⇒En O
In O
the O
case O
of O
blind O
testing O
or O
online O
use O
( O
assumed O
dealing O
with O
En⇒De O
) O
, O
since O
translations O
of O
target O
utterances O
( O
i.e. O
, O
English O
) O
will O
not O
be O
given O
, O
an O
inverse O
De⇒En O
model O
is O
simultaneously O
trained O
and O
used O
to O
back O
- O
translate O
target O
utterances O
( O
Bao O
et O
al O
. O
, O
2020 O
) O
, O
similar O
to O
all O
tasks O
. O

C O
Effect O
of O
Context B-HyperparameterName
Length I-HyperparameterName

We O
firstly O
investigate O
the O
effect O
of O
context B-HyperparameterName
length I-HyperparameterName
( O
i.e. O
, O
the O
number B-HyperparameterName
of I-HyperparameterName
preceding I-HyperparameterName
utterances I-HyperparameterName
) O
on O
our O
approach O
under O
the O
Transformer B-MethodName
Base O
setting O
. O
As O
shown O
in O
the O
left O
of O
Fig O
. O
4 O
, O
using O
three O
preceding O
source O
sentences O
as O
dialogue O
history O
achieves O
the O
best O
translation O
performance O
on O
the O
validation O
set O
( O
En⇒De O
) O
. O
Using O
more O
preceding O
sentences O
does O
not O
bring O
any O
improvement O
and O
increases O
the O
computational O
cost O
. O
This O
confirms O
the O
finding O
of O
Tu O
et O
al O
. O
( O
2018 O
) O
and O
that O
longdistance O
context O
only O
has O
limited O
influence O
. O
Therefore O
, O
we O
set O
the O
number B-HyperparameterName
of I-HyperparameterName
preceding I-HyperparameterName
sentences I-HyperparameterName
to O
3 B-HyperparameterValue
in O
all O
experiments O
. O

D O
Effect O
of O
Latent B-HyperparameterName
Dimension I-HyperparameterName

The O
right O
of O
Fig O
. O
4 O
shows O
the O
effect O
of O
the O
latent B-HyperparameterName
dimension I-HyperparameterName
on O
translation O
quality O
under O
the O
Transformer B-MethodName
Base O
setting O
. O
Obviously O
, O
using O
latent B-HyperparameterName
dimension I-HyperparameterName
32 B-HyperparameterValue
suffices O
to O
achieve O
superior O
performance O
. O
Increasing O
the O
dimension O
does O
not O
lead O
to O
any O
improvements O
. O
Therefore O
, O
we O
set O
the O
latent B-HyperparameterName
dimension I-HyperparameterName
to O
32 B-HyperparameterValue
in O
all O
experiments O
. O

E O
KL B-MetricName
Divergence I-MetricName

Generally O
, O
KL B-MetricName
divergence I-MetricName
measures O
the O
amount O
of O
information O
encoded O
in O
a O
latent O
variable O
. O
In O
the O
extreme O
case O
where O
the O
KL B-MetricName
divergence I-MetricName
of O
latent O
variable O
z O
equals O
to O
zero O
, O
the O
model O
completely O
ignores O
z O
, O
i.e. O
, O
it O
degenerates O
. O
Fig O
. O
5 O
shows O
that O
the O
total O
KL O
divergence O
of O
our O
model O
maintains O
around O
0.2∼0.5 B-MetricValue
indicating O
that O
the O
degeneration O
problem O
does O
not O
exist O
in O
our O
model O
and O
latent O
variables O
can O
play O
their O
corresponding O
roles O
. O

F O
Case O
Study O

In O
this O
section O
, O
we O
show O
some O
cases O
in O
Fig O
. O
6 O
and O
Fig O
. O
7 O
to O
investigate O
the O
effect O
of O
different O
models O
. O

Role O
Preference O
and O
Dialogue O
Coherence O
. O
As O
shown O
in O
Fig O
. O
6 O
, O
we O
observe O
that O
the O
baseline O
models O
and O
the O
context O
- O
aware O
models O
except O
" O
V O
- O
Transformer+FT B-MethodName
" O
can O
not O
preserve O
the O
role O
preference O
information O
, O
e.g. O
, O
joy O
emotion O
, O
even O
these O
" O
* O
-Transformer+FT B-MethodName
" O
models O
incorporate O
the O
bilingual O
conversational O
history O
into O
the O
encoder O
. O
The O
" O
V O
- O
Transformer+FT B-MethodName
" O
model O
produces O
very O
slightly O
emotional O
elements O
( O
e.g. O
, O
" O
zěnme O
? O
" O
) O
due O
to O
the O
latent O
variable O
over O
the O
source O
sentence O
capturing O
relevant O
preference O
information O
. O
Meanwhile O
, O
we O
find O
that O
all O
comparison O
models O
can O
not O
generate O
a O
coherent O
translation O
. O
The O
reason O
may O
be O
that O
they O
fail O
to O
capture O
the O
conversation O
- O
level O
coherence O
clue O
, O
i.e. O
, O
" O
boat O
" O
. O
By O
contrast O
, O
we O
explicitly O
model O
the O
two O
characteristics O
through O
tailored O
latent O
variables O
and O
thus O
obtain O
satisfactory O
results O
. O

Translation O
Consistency O
. O
As O
shown O
in O
Fig O
. O
7 O
, O
we O
observe O
that O
all O
comparison O
models O
can O
not O
maintain O
the O
translation O
consistency O
due O
to O
the O
lack O
of O
explicitly O
modeling O
this O
characteristic O
. O
Our O
model O
has O
the O
ability O
to O
overcome O
the O
issue O
and O
can O
keep O
the O
correct O
lexical O
choice O
to O
translate O
the O
current O
utterance O
that O
might O
have O
appeared O
in O
preceding O
turns O
, O
i.e. O
, O
" O
jiàchuàn O
" O
. O
To O
sum O
up O
, O
both O
cases O
show O
that O
our O
model O
yields O
role O
- O
specific O
, O
coherent O
, O
and O
consistent O
translations O
by O
incorporating O
tailored O
latent O
variables O
into O
translators O
, O
demonstrating O
its O
effectiveness O
and O
superiority O
. O

Acknowledgments O

The O
research O
work O
descried O
in O
this O
paper O
has O
been O
supported O
by O
the O
National O
Key O
R O
& O
D O
Program O
of O
China O
( O
2020AAA0108001 O
) O
and O
the O
National O
Nature O
Science O
Foundation O
of O
China O
( O
No O
. O
61976015 O
, O
61976016 O
, O
61876198 O
and O
61370130 O
) O
. O
The O
authors O
would O
like O
to O
thank O
the O
anonymous O
reviewers O
for O
their O
valuable O
comments O
and O
suggestions O
to O
improve O
this O
paper O
. O

Appendix O

A O
Datasets O
WMT20 O
. O
For O
the O
En⇔De O
, O
we O
combine O
six O
corpora O
including O
Euporal O
, O
ParaCrawl O
, O
Common O
- O
Crawl O
, O
TildeRapid O
, O
NewsCommentary O
, O
and O
Wiki O
- O
Matrix O
, O
and O
we O
combine O
News O
Commentary O
v15 O
, O
Wiki O
Titles O
v2 O
, O
UN O
Parallel O
Corpus O
V1.0 O
, O
CCMT OIntegrating O
Semantics O
and O
Neighborhood O
Information O
with O
Graph B-MethodName
- I-MethodName
Driven I-MethodName
Generative I-MethodName
Models I-MethodName
for O
Document B-TaskName
Retrieval I-TaskName

With O
the O
need O
of O
fast O
retrieval O
speed O
and O
small O
memory O
footprint O
, O
document O
hashing O
has O
been O
playing O
a O
crucial O
role O
in O
large O
- O
scale O
information B-TaskName
retrieval I-TaskName
. O
To O
generate O
high O
- O
quality O
hashing O
code O
, O
both O
semantics O
and O
neighborhood O
information O
are O
crucial O
. O
However O
, O
most O
existing O
methods O
leverage O
only O
one O
of O
them O
or O
simply O
combine O
them O
via O
some O
intuitive O
criteria O
, O
lacking O
a O
theoretical O
principle O
to O
guide O
the O
integration O
process O
. O
In O
this O
paper O
, O
we O
encode O
the O
neighborhood O
information O
with O
a O
graph O
- O
induced O
Gaussian O
distribution O
, O
and O
propose O
to O
integrate O
the O
two O
types O
of O
information O
with O
a O
graph B-MethodName
- I-MethodName
driven I-MethodName
generative I-MethodName
model I-MethodName
. O
To O
deal O
with O
the O
complicated O
correlations O
among O
documents O
, O
we O
further O
propose O
a O
tree O
- O
structured O
approximation O
method O
for O
learning O
. O
Under O
the O
approximation O
, O
we O
prove O
that O
the O
training O
objective O
can O
be O
decomposed O
into O
terms O
involving O
only O
singleton O
or O
pairwise O
documents O
, O
enabling O
the O
model O
to O
be O
trained O
as O
efficiently O
as O
uncorrelated O
ones O
. O
Extensive O
experimental O
results O
on O
three O
benchmark O
datasets O
show O
that O
our O
method O
achieves O
superior O
performance O
over O
state O
- O
of O
- O
the O
- O
art O
methods O
, O
demonstrating O
the O
effectiveness O
of O
the O
proposed O
model O
for O
simultaneously O
preserving O
semantic O
and O
neighborhood O
information O
. O
1 O

Introduction O

Similarity O
search O
plays O
a O
pivotal O
role O
in O
a O
variety O
of O
tasks O
, O
such O
as O
image O
retrieval O
( O
Jing O
and O
Baluja O
, O
2008 O
; O
Zhang O
et O
al O
. O
, O
2018 O
) O
, O
plagiarism O
detection O
( O
Stein O
et O
al O
. O
, O
2007 O
) O
and O
recommendation O
systems O
( O
Koren O
, O
2008 O
) O
. O
If O
the O
search O
is O
carried O
out O
in O
the O
original O
continuous O
feature O
space O
directly O
, O
the O
requirements O
of O
computation O
and O
storage O
would O
be O
extremely O
high O
, O
especially O
for O
large O
- O
scale O
applications O
. O
Semantic O
hashing O
( O
Salakhutdinov O
and O
Hinton O
, O
2009b O
) O
sidesteps O
this O
problem O
by O
learning O
a O
compact O
binary O
code O
for O
every O
item O
such O
that O
similar O
items O
can O
be O
efficiently O
found O
according O
to O
the O
Hamming B-MetricName
distance I-MetricName
of O
binary O
codes O
. O

Unsupervised O
semantic O
hashing O
aims O
to O
learn O
for O
each O
item O
a O
binary O
code O
that O
can O
preserve O
the O
semantic O
similarity O
information O
of O
original O
items O
, O
without O
the O
supervision O
of O
any O
labels O
. O
Motivated O
by O
the O
success O
of O
deep O
generative O
models O
( O
Salakhutdinov O
and O
Hinton O
, O
2009a O
; O
Kingma O
and O
Welling O
, O
2013 O
; O
Rezende O
et O
al O
. O
, O
2014 O
) O
in O
unsupervised O
representation O
learning O
, O
many O
recent O
methods O
approach O
this O
problem O
from O
the O
perspective O
of O
deep O
generative O
models O
, O
leading O
to O
state O
- O
of O
- O
the O
- O
art O
performance O
on O
benchmark O
datasets O
. O
Specifically O
, O
these O
methods O
train O
a O
deep O
generative O
model O
to O
model O
the O
underlying O
documents O
and O
then O
use O
the O
trained O
generative O
model O
to O
extract O
continuous O
or O
binary O
representations O
from O
the O
original O
documents O
( O
Chaidaroon O
and O
Fang O
, O
2017 O
; O
Shen O
et O
al O
. O
, O
2018 O
; O
Dong O
et O
al O
. O
, O
2019 O
; O
Zheng O
et O
al O
. O
, O
2020 O
) O
. O
The O
basic O
principle O
behind O
these O
generative O
hashing O
methods O
is O
to O
have O
the O
hash O
codes O
retaining O
as O
much O
semantics O
information O
of O
original O
documents O
as O
possible O
so O
that O
semantically O
similar O
documents O
are O
more O
likely O
to O
yield O
similar O
codes O
. O

In O
addition O
to O
semantics O
information O
, O
it O
is O
widely O
observed O
that O
neighborhood O
information O
among O
the O
documents O
is O
also O
useful O
to O
generate O
high O
- O
quality O
hash O
codes O
. O
By O
constructing O
an O
adjacency O
matrix O
from O
the O
raw O
features O
of O
documents O
, O
neighborbased O
methods O
seek O
to O
preserve O
the O
information O
in O
the O
constructed O
adjacency O
matrix O
, O
such O
as O
the O
locality O
- O
preserving O
hashing O
( O
He O
et O
al O
. O
, O
2004 O
; O
Zhao O
et O
al O
. O
, O
2014 O
) O
, O
spectral O
hashing O
( O
Weiss O
et O
al O
. O
, O
2009 O
; O
Li O
et O
al O
. O
, O
2012 O
) O
, O
and O
etc O
. O
However O
, O
since O
the O
groundtruth O
neighborhood O
information O
is O
not O
available O
and O
the O
constructed O
one O
is O
neither O
accurate O
nor O
complete O
, O
neighbor O
- O
based O
methods O
alone O
do O
not O
perform O
as O
well O
as O
the O
semantics O
- O
based O
ones O
. O
Despite O
both O
semantics O
and O
neighborhood O
information O
are O
derived O
from O
the O
original O
documents O
, O
different O
aspects O
are O
emphasized O
in O
them O
. O
Thus O
, O
to O
obtain O
higher O
- O
quality O
hash O
codes O
, O
it O
has O
been O
proposed O
to O
incorporate O
the O
constructed O
neighborhood O
information O
into O
semantics O
- O
based O
methods O
. O
For O
examples O
, O
Chaidaroon O
et O
al O
. O
( O
2018 O
) O
and O
Hansen O
et O
al O
. O
( O
2020 O
) O
require O
the O
hash O
codes O
can O
reconstruct O
neighboring O
documents O
, O
in O
addition O
to O
the O
original O
input O
. O
Other O
works O
Hansen O
et O
al O
. O
, O
2019 O
) O
use O
an O
extra O
loss O
term O
, O
derived O
from O
the O
approximate O
neighborhood O
information O
, O
to O
encourage O
similar O
documents O
to O
produce O
similar O
codes O
. O
However O
, O
all O
of O
the O
aforementioned O
methods O
exploit O
the O
neighborhood O
information O
by O
using O
it O
to O
design O
different O
kinds O
of O
regularizers O
to O
the O
original O
semanticsbased O
models O
, O
lacking O
a O
basic O
principle O
to O
unify O
and O
leverage O
them O
under O
one O
framework O
. O

To O
fully O
exploit O
the O
two O
types O
of O
information O
, O
in O
this O
paper O
, O
we O
propose O
a O
hashing O
method O
that O
unifies O
the O
semantics O
and O
neighborhood O
information O
with O
the O
graph O
- O
driven O
generative O
models O
. O
Specifically O
, O
we O
first O
encode O
the O
neighborhood O
information O
with O
a O
multivariate O
Gaussian O
distribution O
. O
With O
this O
Gaussian O
distribution O
as O
a O
prior O
in O
a O
generative O
model O
, O
the O
neighborhood O
information O
can O
be O
naturally O
incorporated O
into O
the O
semantics O
- O
based O
hashing O
model O
. O
Despite O
the O
simplicity O
of O
the O
modeling O
, O
the O
correlation B-MetricName
introduced O
by O
the O
neighbor O
- O
encoded O
prior O
poses O
a O
significant O
challenge O
to O
the O
training O
since O
it O
invalidates O
the O
widely O
used O
identical O
- O
andindependent O
- O
distributed O
( O
i.i.d O
. O
) O
assumption O
, O
making O
all O
documents O
correlated O
. O
To O
address O
this O
issue O
, O
we O
propose O
to O
use O
a O
tree O
- O
structured O
distribution O
to O
capture O
as O
much O
as O
possible O
the O
neighborhood O
information O
. O
We O
prove O
that O
under O
the O
tree O
approximation O
, O
the O
evidence B-MetricName
lower I-MetricName
bound I-MetricName
( O
ELBO B-MetricName
) O
can O
be O
decomposed O
into O
terms O
involving O
only O
singleton O
and O
pairwise O
documents O
, O
enabling O
the O
model O
to O
be O
trained O
as O
efficiently O
as O
the O
models O
without O
considering O
the O
document O
correlations O
. O
To O
capture O
more O
neighborhood O
information O
, O
a O
more O
accurate O
approximation O
by O
using O
multiple O
trees O
is O
also O
developed O
. O
Extensive O
experimental O
results O
on O
three O
public O
datasets O
demonstrate O
that O
the O
proposed O
method O
can O
outperform O
state O
- O
of O
- O
the O
- O
art O
methods O
, O
indicating O
the O
effectiveness O
of O
the O
proposed O
framework O
in O
unifying O
the O
semantic O
and O
neighborhood O
information O
for O
document O
hashing O
. O

Preliminaries O

Semantics O
- O
Based O
Hashing O
Due O
to O
the O
similarities O
among O
the O
underlying O
ideas O
of O
these O
methods O
, O
we O
take O
the O
variational B-MethodName
deep I-MethodName
semantic I-MethodName
hashing I-MethodName
( O
VDSH B-MethodName
) O
( O
Chaidaroon O
and O
Fang O
, O
2017 O
) O
as O
an O
example O
to O
illustrate O
their O
working O
flow O
. O
Given O
a O
document O
x O
{ O
w O
j O
} O
|x| O
j=1 O
, O
VDSH B-MethodName
proposes O
to O
model O
a O
document O
by O
a O
generative O
model O
as O

p O
( O
x O
, O
z O
) O
= O
p O
θ O
( O
x|z O
) O
p O
( O
z O
) O
, O
( O
1 O
) O

where O
p O
( O
z O
) O
is O
the O
prior O
distribution O
and O
is O
chosen O
to O
be O
the O
standard O
Gaussian O
distribution O
N O
( O
z O
; O
0 O
, O
I O
d O
) O
, O
with O
I O
d O
denoting O
the O
d O
- O
dimensional O
identity O
matrix O
; O
and O
p O
θ O
( O
x|z O
) O
is O
defined O
to O
be O

p O
θ O
( O
x|z O
) O
= O
w O
i O
∈x O
p O
θ O
( O
w O
i O
|z O
) O
( O
2 O
) O
with O
p O
θ O
( O
w O
i O
|z O
) O
exp O
( O
z O
T O
Ew O
i O
+ O
b O
i O
) O
|V O
| O
j=1 O
exp O
( O
z O
T O
Ew O
j O
+ O
b O
j O
) O
, O
( O
3 O
) O

in O
which O
w O
j O
denotes O
the O
|V O
|-dimensional O
one O
- O
hot O
representation O
of O
the O
j O
- O
th O
word O
, O
with O
|x| O
and O
|V O
| O
denoting O
the O
document O
and O
vocabulary O
size O
, O
respectively O
; O
and O
E O
∈ O
R O
d×|V O
| O
represents O
the O
learnable O
embedding O
matrix O
. O
For O
a O
corpus O
containing O
N O
documents O
X O
= O
{ O
x O
1 O
, O
x O
2 O
, O
• O
• O
• O
, O
x O
N O
} O
, O
due O
to O
the O
i.i.d O
. O
assumption O
for O
documents O
, O
it O
is O
modelled O
by O
simply O
multiplying O
individual O
document O
models O
as O

p O
( O
X O
, O
Z O
) O
= O
N O
k=1 O
p O
θ O
( O
x O
k O
|z O
k O
) O
p O
( O
z O
k O
) O
, O
( O
4 O
) O

where O
Z O
[ O
z O
1 O
; O
z O
2 O
; O
• O
• O
• O
; O
z O
N O
] O
denotes O
a O
long O
vector O
obtained O
by O
concatenating O
the O
individual O
vectors O
z O
i O
. O
The O
model O
is O
trained O
by O
optimizing O
the O
evidence B-MetricName
lower I-MetricName
bound I-MetricName
( O
ELBO B-MetricValue
) O
of O
the O
log B-MetricName
- I-MetricName
likelihood I-MetricName
function O
log O
p O
( O
X O
) O
. O
After O
training O
, O
outputs O
from O
the O
trained O
encoder O
are O
used O
as O
documents O
' O
representations O
, O
from O
which O
binary O
hash O
codes O
can O
be O
obtained O
by O
thresholding O
the O
real O
- O
valued O
representations O
. O

Neighborhood O
Information O

The O
ground O
- O
truth O
semantic O
similarity O
information O
is O
not O
available O
for O
the O
unsupervised O
hashing O
task O
in O
practice O
. O
To O
leverage O
this O
information O
, O
an O
affinity O
N O
× O
N O
matrix O
A O
is O
generally O
constructed O
from O
the O
raw O
features O
( O
e.g. O
, O
the O
TFIDF O
) O
of O
original O
documents O
. O
For O
instances O
, O
we O
can O
construct O
the O
matrix O
as O

a O
ij O
= O
 O
 O
 O
e O
− O
|| O
x O
i O
−x O
j O
|| O
2 O
σ O
, O
x O
i O
∈ O
N O
k O
( O
x O
j O
) O
0 O
, O
otherwise O
( O
5 O
) O

where O
a O
ij O
denotes O
the O
( O
i O
, O
j O
) O
-th O
element O
of O
A O
; O
and O
N O
k O
( O
x O
) O
denotes O
the O
k O
- O
nearest O
neighbors O
of O
document O
x. O
Given O
the O
affinity O
matrix O
A O
, O
some O
methods O
have O
been O
proposed O
to O
incorporate O
the O
neighborhood O
information O
into O
the O
semantics O
- O
based O
hashing O
models O
. O
However O
, O
as O
discussed O
above O
, O
these O
methods O
generally O
leverage O
the O
information O
based O
on O
some O
intuitive O
criteria O
, O
lacking O
theoretical O
supports O
behind O
them O
. O

A O
Hashing O
Framework O
with O
Unified O
Semantics O
- O
Neighborhood O
Information O

In O
this O
section O
, O
we O
present O
a O
more O
effective O
framework O
to O
unify O
the O
semantic O
and O
neighborhood O
information O
for O
the O
task O
of O
document O
hashing O
. O

Reformulating O
the O
VDSH B-MethodName

To O
introduce O
the O
neighborhood O
information O
into O
the O
semantics O
- O
based O
hashing O
models O
, O
we O
first O
rewrite O
the O
VDSH B-MethodName
model O
into O
a O
compact O
form O
as O

p O
( O
X O
, O
Z O
) O
= O
p O
θ O
( O
X|Z O
) O
p O
I O
( O
Z O
) O
, O
( O
6 O
) O

where O
p O
θ O
( O
X|Z O
) O
= O
N O
k=1 O
p O
θ O
( O
x O
k O
|z O
k O
) O
; O
and O
the O
prior O
p O
I O
( O
Z O
) O
= O
N O
k=1 O
p O
( O
z O
k O
) O
, O
which O
can O
be O
shown O
to O
be O

p O
I O
( O
Z O
) O
= O
N O
( O
Z O
; O
0 O
, O
I O
N O
⊗ O
I O
d O
) O
. O
( O
7 O
) O

Here O
, O
⊗ O
denotes O
the O
Kronecker O
product O
and O
the O
subscript O
I O
indicates O
independence O
among O
z O
k O
. O
The O
ELBO B-MetricName
of O
this O
model O
can O
be O
expressed O
as O

L= O
E O
q O
φ O
( O
Z|X O
) O
[ O
log O
p O
θ O
( O
X|Z O
) O
] O
L O
1 O
−KL O
( O
q O
φ O
( O
Z|X O
) O
||p O
I O
( O
Z O
) O
) O
L O
2 O

where O
KL B-MetricName
( O
• O
) O
denotes O
the O
Kullback B-MetricName
- I-MetricName
Leibler I-MetricName
( I-MetricName
KL I-MetricName
) I-MetricName
divergence I-MetricName
. O
By O
restricting O
the O
posterior O
to O
independent O
Gaussian O
form O

q O
φ O
( O
Z|X O
) O
= O
N O
k=1 O
N O
z O
k O
; O
µ O
k O
, O
diag O
( O
σ O
2 O
k O
) O
q O
φ O
( O
z O
k O
|x O
k O
) O
, O
( O
8 O
) O

the O
L O
1 O
can O
be O
handled O
using O
the O
reparameterization O
trick O
. O
Thanks O
to O
the O
factorized O
forms O
assumed O
in O
q O
φ O
( O
Z|X O
) O
and O
p O
I O
( O
Z O
) O
, O
the O
L O
2 O
term O
can O
also O
be O
expressed O
analytically O
and O
evaluated O
efficiently O
. O

Injecting O
the O
Neighborhood O
Information O

Given O
an O
affinity O
matrix O
A O
, O
the O
covariance O
matrix O
I O
N O
+ O
λA B-HyperparameterName
can O
be O
used O
to O
reveal O
the O
neighborhood O
information O
of O
documents O
, O
where O
the O
hyperparameter O
λ B-HyperparameterName
∈ O
[ O
0 B-HyperparameterValue
, O
1 B-HyperparameterValue
) O
is O
used O
to O
control O
the O
overall O
correlation O
strength O
. O
If O
two O
documents O
are O
neighboring O
, O
then O
the O
corresponding O
correlation O
value O
in O
I O
N O
+ O
λA B-HyperparameterName
will O
be O
large O
; O
otherwise O
, O
the O
value O
will O
be O
zero O
. O

To O
have O
the O
neighborhood O
information O
reflected O
in O
document O
representations O
, O
we O
can O
require O
that O
the O
representations O
z O
i O
are O
drawn O
from O
a O
Gaussian O
distribution O
of O
the O
form O

p O
G O
( O
Z O
) O
= O
N O
( O
Z O
; O
0 O
, O
( O
I O
N O
+ O
λA B-HyperparameterName
) O
⊗ O
I O
d O
) O
, O
( O
9 O
) O

where O
the O
subscript O
G O
denotes O
that O
the O
distribution O
is O
constructed O
from O
a O
neighborhood O
graph O
. O
To O
see O
why O
the O
representations O
Z O
∼ O
p O
G O
( O
Z O
) O
have O
already O
reflected O
the O
neighborhood O
information O
, O
let O
us O
consider O
an O
example O
with O
three O
documents O

{ O
x O
1 O
, O
x O
2 O
, O
x O
3 O
} O
, O
in O
which O
x O
1 O
is O
connected O
to O
x O
2 O
, O

x O
2 O
is O
connected O
to O
x O
3 O
, O
and O
no O
connection O
exists O
between O
x O
1 O
and O
x O
3 O
. O
Under O
the O
case O
that O
z O
i O
is O
a O
two O
- O
dimensional O
vector O
z O
i O
∈ O
R O
2 O
, O
we O
have O
the O
concatenated O
representations O
[ O
z O
1 O
; O
z O
2 O
; O
z O
3 O
] O
follow O
a O
Gaussian O
distribution O
with O
covariance O
matrix O
of O

z O
1 O
z O
2 O
z O
3 O
 O
 O
 O
 O
 O
 O
 O
 O
 O
 O
 O
 O
 O
 O
z O
1 O
1 O
0 O
λa B-HyperparameterName
12 O
0 O
0 O
0 O
0 O
1 O
0 O
λa B-HyperparameterName
12 O
0 O
0 O
z O
2 O
λa B-HyperparameterName
21 O
0 O
1 O
0 O
λa B-HyperparameterName
23 O
0 O
0 O
λa B-HyperparameterName
21 O
0 O
1 O
0 O
λa B-HyperparameterName
23 O
z O
3 O
0 O
0 O
λa B-HyperparameterName
32 O
0 O
1 O
0 O
0 O
0 O
0 O
λa B-HyperparameterName
32 O
0 O
1 O

From O
the O
property O
of O
Gaussian O
distribution O
, O
it O
can O
be O
known O
that O
z O
1 O
is O
strongly O
correlated O
with O
z O
2 O
on O
the O
corresponding O
elements O
, O
but O
not O
with O
z O
3 O
. O
This O
suggests O
that O
z O
1 O
should O
be O
similar O
to O
z O
2 O
, O
but O
different O
from O
z O
3 O
, O
which O
is O
consistent O
with O
the O
neighborhood O
relation O
that O
x O
1 O
is O
a O
neighbor O
of O
x O
2 O
, O
but O
not O
of O
x O
3 O
. O
Now O
that O
the O
neighborhood O
information O
can O
be O
modeled O
by O
requiring O
Z O
being O
drawn O
from O
p O
G O
( O
Z O
) O
, O
and O
the O
semantic O
information O
can O
be O
reflected O
in O
the O
likelihood O
function O
p O
θ O
( O
X|Z O
) O
. O
The O
two O
types O
of O
information O
can O
be O
taken O
into O
account O
simultaneously O
by O
modeling O
the O
corpus O
as O

p O
( O
X O
, O
Z O
) O
= O
p O
θ O
( O
X|Z O
) O
p O
G O
( O
Z O
) O
. O
( O
10 O

) O

Comparing O
to O
the O
VDSH B-MethodName
model O
in O
( O
6 O
) O
, O
it O
can O
be O
seen O
that O
the O
only O
difference O
lies O
in O
the O
employed O
priors O
. O
Here O
, O
a O
neighborhood O
- O
preserving O
prior O
p O
G O
( O
Z O
) O
is O
employed O
, O
while O
in O
VDSH B-MethodName
, O
an O
independent O
prior O
p O
I O
( O
Z O
) O
is O
used O
. O
Although O
only O
a O
modification O
to O
the O
prior O
is O
made O
from O
the O
perspective O
of O
modeling O
, O
significant O
challenges O
are O
posed O
for O
the O
training O
. O
Specifically O
, O
by O
replacing O
p O
I O
( O
Z O
) O
with O
p O
G O
( O
Z O
) O
in O
the O
L O
2 O
of O
L O
, O
it O
can O
be O
shown O
that O
the O
expression O
of O
L O
2 O
involves O
the O
matrix O
( O
I O
N O
+ O
λA B-HyperparameterName
) O
⊗ O
I O
d O
−1 O
. O
Due O
to O
the O
introduced O
dependence O
among O
documents O
, O
for O
example O
, O
if O
the O
corpus O
contains O
over O
100,000 O
documents O
and O
the O
representation O
dimension O
is O
set O
to O
100 O
, O
the O
L O
2 O
involves O
the O
inverse O
of O
matrices O
with O
dimension O
as O
high O
as O
10 O
7 O
, O
which O
is O
computationally O
prohibitive O
in O
practice O
. O

Training O
with O
Tree O
Approximations O

Although O
the O
prior O
p O
G O
( O
Z O
) O
captures O
the O
full O
neighborhood O
information O
, O
its O
induced O
model O
is O
not O
practically O
trainable O
. O
In O
this O
section O
, O
to O
facilitate O
the O
training O
, O
we O
first O
propose O
to O
use O
a O
tree O
- O
structured O
prior O
to O
partially O
capture O
the O
neighborhood O
information O
, O
and O
then O
extend O
it O
to O
multiple O
- O
tree O
case O
for O
more O
accurate O
modeling O
. O

Approximating O
the O
Prior O
p O
G O
( O
Z O
) O
with O
a O
Tree O
- O
Structured O
Distribution O

The O
matrix O
A O
represents O
a O
graph O
G O
( O
V O
, O
E O
) O
, O
where O
V O
= O
{ O
1 O
, O
2 O
, O
• O
• O
• O
, O
N O
} O
is O
the O
set O
of O
document O
indices O
; O
and O
E O
= O
{ O
( O
i O
, O
j O
) O
|a O
ij O
= O
0 O
} O
is O
the O
set O
of O
connections O
between O
documents O
. O
From O
the O
graph O
G O
, O
a O
spanning O
tree O
T O
= O
( O
V O
, O
E O
T O
) O
can O
be O
obtained O
easily O
, O
where O
E O
T O
denotes O
the O
set O
of O
connections O
on O
the O
tree O
. O
2 O
Based O
on O
the O
spanning O
tree O
, O
we O
construct O
a O
new O
distribution O
as O

p O
T O
( O
Z O
) O
= O
i∈V O
p O
G O
( O
z O
i O
) O
( O
i O
, O
j O
) O
∈E O
T O
p O
G O
( O
z O
i O
, O
z O
j O
) O
p O
G O
( O
z O
i O
) O
p O
G O
( O
z O
j O
) O
, O
( O
11 O
) O

where O
p O
G O
( O
z O
i O
) O
and O
p O
G O
( O
z O
i O
, O
z O
j O
) O
represent O
one O
- O
and O
two O
- O
variable O
marginal O
distributions O
of O
p O
G O
( O
Z O
) O
, O
respectively O
. O
From O
the O
properties O
of O
Gaussian O
distribution O
, O
it O
is O
known O
that O

p O
G O
( O
z O
i O
) O
= O
N O
( O
z O
i O
; O
0 O
, O
I O
d O
) O
, O
p O
G O
( O
z O
i O
, O
z O
j O
) O
= O
N O
( O
[ O
z O
i O
; O
z O
j O
] O
; O
0 O
, O
( O
I O
2 O
+ O
λA B-HyperparameterName
ij O
) O
⊗I O
d O
) O
, O
( O
12 O

) O

where O

A O
ij O
0 O
a O
ij O
a O
ji O
0 O
. O
Because O
p O
T O
( O
Z O
) O
is O
de- O

fined O
on O
a O
tree O
, O
as O
proved O
in O
( O
Wainwright O
and O
Jordan O
, O
2008 O
) O
, O
it O
is O
guaranteed O
to O
be O
a O
valid O
probability O
distribution O
, O
and O
more O
importantly O
, O
it O
satisfies O
the O
following O
two O
relations O

: O
i O
) O
p O
T O
( O
z O
i O
) O
= O
p O
G O
( O
z O
i O
) O
; O
ii O
) O
p O
T O
( O
z O
i O
, O
z O
j O
) O
= O
p O
G O
( O
z O
i O
, O
z O
j O
) O
for O
any O
( O
i O
, O
j O
) O
∈ O
E O
T O
, O

where O
p O
T O
( O
z O
i O
) O
and O
p O
T O
( O
z O
i O
, O
z O
j O
) O
denote O
the O
marginal O
distributions O
of O
p O
T O
( O
Z O
) O
. O
That O
is O
, O
the O
tree O
- O
structured O
distribution O
p O
T O
( O
Z O
) O
captures O
the O
neighborhood O
information O
reflected O
on O
the O
spanning O
tree O
T. O
By O
using O
p O
T O
( O
Z O
) O
to O
replace O
p O
I O
( O
Z O
) O
of O
L O
2 O
, O
it O
can O
be O
shown O
that O
L O
2 O
can O
be O
expressed O
as O
the O
summation O
of O
terms O
involving O
only O
one O
or O
two O
variables O
, O
which O
can O
be O
handled O
easily O
. O
Due O
to O
the O
limitation O
of O
space O
, O
the O
concrete O
expression O
for O
the O
lower O
bound O
is O
given O
in O
the O
Supplementary O
Material O
. O

Imposing O
Correlations B-MetricName
on O
the O
Posterior O

The O
posterior O
distribution O
q O
φ O
( O
Z|X O
) O
in O
the O
previous O
section O
is O
assumed O
to O
be O
in O
independent O
form O
, O
as O
the O
form O
shown O
in O
( O
8 O
) O
. O
But O
since O
a O
prior O
p O
T O
( O
Z O
) O
considering O
the O
correlations B-MetricName
among O
documents O
is O
used O
, O
assuming O
an O
independent O
posterior O
is O
not O
appropriate O
. O
Hence O
, O
we O
follow O
the O
tree O
- O
structured O
prior O
and O
also O
construct O
a O
tree O
- O
structured O
posterior O

q O
T O
( O
Z|X O
) O
= O
i∈V O
q O
φ O
( O
z O
i O
|x O
i O
) O
( O
i O
, O
j O
) O
∈E O
T O
q O
φ O
( O
z O
i O
, O
z O
j O
|x O
i O
, O
x O
j O
) O
q O
φ O
( O
z O
i O
|x O
i O
) O
q O
φ O
( O
z O
j O
|x O
j O
) O
, O

where O
q O
φ O
( O
z O
i O
|x O
i O
) O
is O
the O
same O
as O
that O
in O
( O
8 O
) O
; O
and O
q O
φ O
( O
z O
i O
, O
z O
j O
|x O
i O
, O
x O
j O
) O
is O
also O
defined O
to O
be O
Gaussian O
, O
with O
its O
mean O
defined O
as O
[ O
µ O
i O
; O
µ O
j O
] O
and O
covariance O
matrix O
defined O
as O

diag O
( O
σ O
2 O
i O
) O
diag O
( O
γ O
ij O
σ O
i O
σ O
j O
) O
diag O
( O
γ O
ij O
σ O
i O
σ O
j O
) O
diag O
( O
σ O
2 O
j O
) O
, O
( O
13 O
) O

in O
which O
γ O
ij O
∈ O
R O
d O
controls O
the O
correlation B-MetricName
strength O
between O
z O
i O
and O
z O
j O
, O
whose O
elements O
are O
restricted O
in O
( O
−1 O
, O
1 O
) O
and O
denotes O
the O
Hadamard O
product O
. O
By O
taking O
the O
correlated O
posterior O
q O
T O
( O
Z|X O
) O
into O
the O
ELBO B-MetricName
, O
we O
obtain O

L O
T O
= O
i∈V O
E O
q O
φ O
[ O
log O
p O
θ O
( O
x O
i O
|z O
i O
) O
] O
−KL B-MetricName
( O
q O
φ O
( O
z O
i O
) O
||p O
G O
( O
z O
i O
) O
) O
− O
( O
i O
, O
j O
) O
∈E O
T O
KL B-MetricName
( O
q O
φ O
( O
z O
i O
, O
z O
j O
|x O
i O
, O
x O
j O
) O
||p O
G O
( O
z O
i O
, O
z O
j O
) O
) O
−KL B-MetricName
( O
q O
φ O
( O
z O
i O
) O
||p O
G O
( O
z O
i O
) O
) O
−KL B-MetricName
( O
q O
φ O
( O
z O
j O
) O
||p O
G O
( O
z O
j O
) O
) O
, O

where O
we O
briefly O
denote O
the O
variational O
distribution O
q O
φ O
( O
z O
i O
|x O
i O
) O
as O
q O
φ O
( O
z O
i O
) O
. O
Since O
p O
G O
( O
z O
i O
) O
, O
p O
G O
( O
z O
i O
, O
z O
j O
) O
, O
q O
φ O
( O
z O
i O
|x O
i O
) O
and O
q O
φ O
( O
z O
i O
, O
z O
j O
|x O
i O
, O
x O
j O
) O
are O
all O
Gaussian O
distributions O
, O
the O
KL B-MetricName
- I-MetricName
divergence I-MetricName
terms O
above O
can O
be O
derived O
in O
closed O
- O
form O
. O
Moreover O
, O
it O
can O
be O
seen O
that O
L O
T O
involves O
only O
single O
or O
pairwise O
variables O
, O
thus O
optimizing O
it O
is O
as O
efficient O
as O
the O
models O
without O
considering O
document O
correlation O
. O
With O
the O
trained O
model O
, O
hash O
codes O
can O
be O
obtained O
by O
binarizing O
the O
posterior O
mean O
µ O
i O
with O
a O
threshold O
, O
as O
done O
in O
( O
Chaidaroon O
and O
Fang O
, O
2017 O
) O
. O
However O
, O
if O
without O
any O
constraint O
, O
the O
range O
of O
mean O
lies O
in O
( O
−∞ O
, O
+ O
∞ O
) O
. O
Thus O
, O
if O
we O
binarize O
it O
directly O
, O
lots O
of O
information O
in O
the O
original O
representations O
will O
be O
lost O
. O
To O
alleviate O
this O
problem O
, O
in O
our O
implementation O
, O
we O
parameterize O
the O
posterior O
mean O
µ O
i O
by O
a O
function O
of O
the O
form O
µ O
i O
= O
sigmoid O
( O
nn O
( O
x O
i O
) O
/ O
τ O
) O
, O
where O
the O
outermost O
sigmoid O
function O
forces O
the O
mean O
to O
look O
like O
binary O
value O
and O
thus O
can O
effectively O
reduce O
the O
quantization O
loss O
, O
with O
nn O
( O
• O
) O
denoting O
a O
neural O
network O
function O
and O
τ O
controlling O
the O
slope O
of O
the O
sigmoid O
function O
. O

Extending O
to O
Multiple O
Spanning O
Trees O

Obviously O
, O
approximating O
the O
graph O
with O
a O
spanning O
tree O
may O
lose O
too O
much O
information O
. O
To O
alleviate O
this O
issue O
, O
we O
propose O
to O
capture O
the O
similarity O
information O
by O
a O
mixture O
of O
multiple O
distributions O
, O
with O
each O
built O
on O
a O
spanning O
tree O
. O
Specifically O
, O
we O
first O
construct O
a O
set O
of O
M O
spanning O
trees O

T O
G O
= O
{ O
T O
1 O
, O
T O
2 O
, O
• O
• O
• O
, O
T O
M O
} O
from O
the O
original O
graph O
G O
. O

Based O
on O
the O
set O
of O
spanning O
trees O
, O
a O
mixturedistribution O
prior O
and O
posterior O
can O
be O
constructed O
as O

p O
MT O
( O
Z O
) O
= O
1 O
M O
T O
∈T O
G O
p O
T O
( O
Z O
) O
, O
( O
14 O
) O

q O
MT O
( O
Z|X O
) O
= O
1 O
M O
T O
∈T O
G O
q O
T O
( O
Z|X O
) O
, O
( O
15 O
) O

where O
p O
T O
( O
Z O
) O
and O
q O
T O
( O
Z|X O
) O
are O
the O
prior O
and O
posterior O
defined O
on O
the O
tree O
T O
, O
as O
done O
in O
( O
11 O
) O
and O
( O
13 O
) O
. O
By O
taking O
the O
mixture O
distributions O
above O
into O
the O
ELBO B-MetricName
of O
L O
to O
replace O
the O
prior O
and O
posterior O
, O
we O
can O
obtain O
a O
new O
ELBO B-MetricName
, O
denoted O
as O
L O
MT O
. O
Obviously O
, O
it O
is O
impossible O
to O
obtain O
a O
closed O
- O
form O
expression O
for O
the O
bound O
L O
MT O
. O
But O
as O
proved O
in O
( O
Tang O
et O
al O
. O
, O
2019 O
) O
, O
by O
using O
the O
log O
- O
sum O
inequality O
, O
L O
MT O
can O
be O
further O
lower O
bounded O
by O

L O
MT O
= O
1 O
M O
T O
∈T O
G O
L O
T O
. O
( O
16 O
) O

Given O
the O
expression O
of O
L O
T O
, O
the O
lower O
bound O
of O
L O
MT O
can O
also O
be O
expressed O
in O
closed O
- O
form O
and O
optimized O
efficiently O
. O
For O
detailed O
derivations O
and O
concrete O
expressions O
, O
please O
refer O
to O
the O
Supplementary O
. O

Details O
of O
Modeling O

The O
parameters O
µ O
i O
, O
µ O
j O
, O
σ O
i O
, O
σ O
j O
and O
γ O
ij O
in O
the O
approximate O
posterior O
distribution O
q O
φ O
( O
z O
i O
|x O
i O
) O
of O
( O
8 O
) O
and O
q O
φ O
( O
z O
i O
, O
z O
j O
|x O
i O
, O
x O
j O
) O
of O
( O
13 O
) O
are O
all O
defined O
as O
the O
outputs O
of O
neural O
networks O
, O
with O
the O
parameters O
denoted O
as O
φ O
. O
Specifically O
, O
the O
entire O
model O
is O
mainly O
composed O
of O
three O
components O
: O

i O
) O
The O
variational O
encoder O
q O
φ O
( O
z O
i O
|x O
i O
) O
, O
which O
takes O
single O
document O
as O
input O
, O
and O
outputs O
the O
mean O
and O
variance O
of O
Gaussian O
distribution O
, O
i.e. O
, O
[ O
µ O
i O
; O

σ O
2 O
i O
] O
= O
f O
φ O
( O
x O
i O
) O
; O

ii O
) O
The O
correlated O
encoder O
, O
which O
takes O
pairwise O
documents O
as O
input O
, O
and O
outputs O
the O
correlation B-MetricName
coefficient O
, O
i.e. O
, O
γ O
ij O
= O
f O
φ O
( O
x O
i O
, O
x O
j O
) O
. O

Note O
that O
the O
correlation B-MetricName
encoder O
is O
required O
to O
be O
order O
- O
irrelevant O
, O
that O
is O
, O
f O
φ O
( O
x O
i O
, O
x O
j O
) O
= O
f O
φ O
( O
x O
j O
, O
x O
i O
) O
, O
which O
is O
achieved O
in O
this O
paper O
as O

f O
φ O
= O
1 O
2 O
f O
φ O
( O
x O
i O
, O
x O
j O
) O
+ O
f O
φ O
( O
x O
j O
, O
x O
i O
) O
; O

iii O
) O
The O
generative O
decoder O
p O
θ O
( O
x O
i O
|z O
i O
) O
, O
which O
takes O
the O
latent O
variable O
z O
i O
as O
input O
and O
output O
the O
document O
x O
i O
. O
The O
decoder O
is O
modeled O
by O
a O
neural O
network O
parameterized O
by O
θ O
. O

The O
model O
is O
trained O
by O
optimizing O
the O
lower O
bound O
L O
MT O
w.r.t O
. O
φ O
and O
θ O
. O
After O
training O
, O
hash O
codes O
are O
obtained O
by O
passing O
the O
documents O
through O
the O
variational O
encoder O
and O
binarizing O
the O
outputs O
on O
every O
dimension O
by O
a O
the O
threshold O
value O
, O
which O
is O
simply O
set O
as O
0.5 O
in O
our O
experiments O
. O

To O
intuitively O
understand O
the O
insight O
behind O
our O
model O
, O
an O
illustration O
is O
shown O
in O
Figure O
1 O
. O
We O
see O
that O
if O
the O
two O
documents O
are O
neighbors O
and O
semantically O
similar O
, O
the O
representations O
will O
be O
strongly O
correlated O
to O
each O
other O
. O
But O
if O
they O
are O
not O
semantically O
similar O
neighbors O
, O
the O
representations O
become O
less O
correlated O
. O
If O
they O
are O
neither O
neighbors O
nor O
semantically O
similar O
, O
the O
representations O
become O
not O
correlated O
at O
all O
. O
Since O
our O
model O
can O
simultaneously O
preserve O
semantics O
and O
neighborhood O
information O
, O
we O
name O
it O
as O
Semantics B-MethodName
- I-MethodName
Neighborhood I-MethodName
Unified I-MethodName
Hahing I-MethodName
( O
SNUH B-MethodName
) O
. O

Related O
Work O

Deep O
generative O
models O
( O
Rezende O
et O
al O
. O
, O
2014 O
) O
have O
attracted O
a O
lot O
of O
attention O
in O
semanticsbased O
hashing O
, O
due O
to O
their O
successes O
in O
unsupervised O
representation O
learning O
. O
VDSH B-MethodName
( O
Chaidaroon O
and O
Fang O
, O
2017 O
) O
first O
employed O
variational O
autoencoder O
( O
VAE O
) O
( O
Kingma O
and O
Welling O
, O
2013 O
) O
to O
learn O
continuous O
representations O
of O
documents O
and O
then O
casts O
them O
into O
binary O
codes O
. O
However O
, O
for O
the O
sake O
of O
information O
leaky O
problem O
during O
binarization O
step O
, O
such O
a O
two O
- O
stage O
strategy O
is O
prone O
to O
result O
in O
local O
optima O
and O
undermine O
the O
performance O
. O
NASH B-MethodName
( O
Shen O
et O
al O
. O
, O
2018 O
) O
tackled O
this O
issue O
by O
replacing O
the O
Gaussian O
prior O
with O
Bernoulli O
and O
adopted O
the O
straight O
- O
through O
technique O
( O
Bengio O
et O
al O
. O
, O
2013 O
) O
to O
achieve O
end O
- O
to O
- O
end O
training O
. O
To O
further O
improve O
the O
model O
's O
capability O
, O
Dong O
et O
al O
. O
( O
2019 O
) O
proposed O
to O
employ O
mixture O
distribution O
as O
a O
priori O
knowledge O
and O
Zheng O
et O
al O
. O
( O
2020 O
) O
exploited O
Boltzmann O
posterior O
to O
introduce O
correlation B-MetricName
among O
bits O
. O
Beyond O
generative O
frameworks O
, O
AMMI B-MethodName
( O
Stratos O
and O
Wiseman O
, O
2020 O
) O
achieved O
superior O
performance O
by O
maximizing O
the O
mutual O
information O
between O
codes O
and O
documents O
. O
Nevertheless O
, O
the O
aforementioned O
semantic O
hashing O
methods O
are O
consistently O
under O
the O
i.i.d O
. O
assumption O
, O
which O
means O
they O
ignore O
the O
neighborhood O
information O
. O

Spectral O
hashing O
( O
Weiss O
et O
al O
. O
, O
2009 O
) O
and O
selftaught O
hashing O
( O
Zhang O
et O
al O
. O
, O
2010 O
) O
are O
two O
typical O
methods O
of O
neighbor O
- O
based O
hashing O
models O
. O
But O
these O
algorithms O
generally O
ignore O
the O
rich O
semantic O
information O
associated O
with O
documents O
. O
Recently O
, O
some O
VAE O
- O
based O
models O
tried O
to O
concurrently O
take O
account O
of O
semantic O
and O
neighborhood O
information O
, O
such O
as O
NbrReg B-MethodName
( O
Chaidaroon O
et O
al O
. O
, O
2018 O
) O
, O
RBSH B-MethodName
( O
Hansen O
et O
al O
. O
, O
2019 O
) O
and O
PairRec B-MethodName
( O
Hansen O
et O
al O
. O
, O
2020 O
) O
. O
However O
, O
as O
mentioned O
before O
, O
all O
of O
them O
simply O
regarded O
the O
proximity O
as O
regularization O
, O
lacking O
theoretical O
principles O
to O
guide O
the O
incorporation O
process O
. O
Thanks O
to O
the O
virtue O
of O
graph O
- O
induced O
distribution O
, O
we O
effectively O
preserve O
the O
two O
types O
of O
information O
in O
a O
theoretical O
framework O
. O

Experiments O

Experiment O
Setup O

Datasets O
We O
verify O
the O
proposed O
methods O
on O
three O
public O
datasets O
which O
published O
by O
VDSH B-MethodName
3 O
: O
i O
) O
Reuters25178 B-DatasetName
, O
which O
contains O
10,788 O
news O
documents O
with O
90 O
different O
categories O
; O
ii O
) O
TMC B-DatasetName
, O
which O
is O
a O
collection O
of O
21,519 O
air O
traffic O
reports O
with O
22 O
different O
categories O
; O
iii O
) O
20Newsgroups B-DatasetName
( O
NG20 B-DatasetName
) O
, O
which O
consists O
of O
18,828 O
news O
posts O
from O
20 O
different O
topics O
. O
Note O
that O
the O
category O
labels O
of O
each O
dataset O
are O
only O
used O
to O
compute O
the O
evaluation O
metrics O
, O
as O
we O
focus O
on O
unsupervised O
scenarios O
. O

Baselines O
We O
compare O
our O
method O
with O
the O
following O
models O
: O
SpH B-MethodName
( O
Weiss O
et O
al O
. O
, O
2009 O
) O
, O
STH B-MethodName
( O
Zhang O
et O
al O
. O
, O
2010 O
) O
, O
VDSH B-MethodName
( O
Chaidaroon O
and O
Fang O
, O
2017 O
) O
, O
NASH B-MethodName
( O
Shen O
et O
al O
. O
, O
2018 O
) O
, O
GMSH B-MethodName
( O
Dong O
et O
al O
. O
, O
2019 O
) O
, O
NbrReg B-MethodName
( O
Chaidaroon O
et O
al O
. O
, O
2018 O
) O
, O
CorrSH B-MethodName
( O
Zheng O
et O
al O
. O
, O
2020 O
) O
and O
AMMI B-MethodName
( O
Stratos O
and O
Wiseman O
, O
2020 O
) O
. O
For O
all O
baselines O
, O
we O
take O
the O
reported O
performance O
from O
their O
original O
papers O
. O

Training O
Details O
For O
fair O
comparisons O
, O
we O
follow O
the O
same O
network O
architecture O
used O
in O
VDSH B-MethodName
, O
GMSH B-MethodName
and O
CorrSH B-MethodName
, O
using O
a O
one O
- O
layer O
feedforward O
neural O
network O
as O
the O
variational O
and O
the O
correlated O
encoder O
. O
The O
graph O
G O
is O
constructed O
with O
the O
K B-HyperparameterName
- O
nearest O
neighbors O
( O
KNN O
) O
algorithm O
based O
on O
cosine B-MetricName
similarity I-MetricName
on O
the O
TFIDF O
features O
of O
documents O
. O
In O
our O
experiments O
, O
the O
correlation B-HyperparameterName
strength I-HyperparameterName
coefficient I-HyperparameterName
λ B-HyperparameterName
in O
( O
12 O
) O
is O
fixed O
to O
0.99 B-HyperparameterValue
. O
According O
to O
the O
performance O
observed O
on O
the O
validation O
set O
, O
we O
choose O
the O
learning B-HyperparameterName
rate I-HyperparameterName
from O
{ O
0.0005 B-HyperparameterValue
, O
0.001 B-HyperparameterValue
, O
0.003 B-HyperparameterValue
} O
, O
batch B-HyperparameterName
size I-HyperparameterName
from O
{ O
32 B-HyperparameterValue
, O
64 B-HyperparameterValue
, O
128 B-HyperparameterValue
} O
, O
the O
temperature B-HyperparameterName
τ B-HyperparameterName
in O
sigmoid O
function O
from O
{ O
0.1 B-HyperparameterValue
, O
0.2 B-HyperparameterValue
, O
• O
• O
• O
, O
1 B-HyperparameterValue
} O
, O
the O
number B-HyperparameterName
of I-HyperparameterName
trees I-HyperparameterName
M B-HyperparameterName
and O
neighbors B-HyperparameterName
K B-HyperparameterName
both O
form O
{ O
1,2 B-HyperparameterValue
, O
. O
. O
. O
, O
20 B-HyperparameterValue
} O
, O
with O
the O
best O
used O
for O
evaluation O
on O
the O
test O
set O
. O
The O
model O
is O
trained O
using O
the O
Adam O
optimizer O
( O
Kingma O
and O
Ba O
, O
2014 O
) O
. O
More O
detailed O
experimental O
settings O
, O
along O
with O
the O
generating O
method O
of O
spanning O
trees O
, O
are O
given O
in O
the O
supplementary O
materials O
. O

Evaluation O
Metrics O

The O
retrieval B-MetricName
precision I-MetricName
is O
used O
as O
our O
evaluation O
metric O
. O
For O
each O
query O
document O
, O
we O
retrieve O
100 O
documents O
most O
similar O
to O
it O
based O
on O
the O
Hamming B-MetricName
distance I-MetricName
of O
hash O
codes O
. O
Then O
, O
the O
retrieval B-MetricName
precision I-MetricName
for O
a O
single O
sample O
is O
measured O
as O
the O
percentage O
of O
the O
retrieved O
documents O
with O
the O
same O
label O
as O
the O
query O
. O
Finally O
, O
the O
average B-MetricName
precision I-MetricName
over O
the O
whole O
test O
set O
is O
calculated O
as O
the O
performance O
of O
the O
evaluated O
method O
. O

Performance O
and O
Analysis O

Overall O
Performance O
The O
performances O
of O
all O
the O
models O
on O
the O
three O
public O
datasets O
are O
shown O
in O
Table O
1 O
. O
We O
see O
that O
our O
model O
performs O
favorably O

Method O

Reuters B-DatasetName
TMC B-DatasetName
20Newsgroups B-DatasetName
Avg O
16bits O
32bits O
64bits O
128bits O
16bits O
32bits O
64bits O
128bits O
16bits O
32bits O
64bits O
128bits O
to O
the O
current O
state O
- O
of O
- O
the O
- O
art O
method O
, O
yielding O
best O
average O
performance O
across O
different O
datasets O
and O
settings O
. O
Compared O
with O
VDSH B-MethodName
and O
NASH B-MethodName
, O
which O
simply O
employ O
isotropic O
Gaussian O
and O
Bernoulli O
prior O
, O
respectively O
, O
we O
can O
observe O
that O
our O
model O
, O
which O
leverages O
correlated O
prior O
and O
posterior O
distributions O
, O
achieves O
better O
results O
on O
all O
the O
three O
datasets O
. O
Although O
GMSH B-MethodName
improves O
performance O
by O
exploiting O
a O
more O
expressive O
Gaussian O
mixture O
prior O
, O
our O
model O
still O
outperforms O
it O
by O
a O
substantial O
margin O
, O
indicating O
the O
superiority O
of O
incorporating O
document O
correlations O
. O
It O
is O
worth O
noting O
that O
, O
by O
unifying O
semantics O
and O
neighborhood O
information O
under O
the O
generative O
models O
, O
the O
two O
types O
of O
information O
can O
be O
preserved O
more O
effectively O
. O
This O
can O
be O
validated O
by O
that O
our O
model O
performs O
significantly O
better O
than O
NbrReg B-MethodName
, O
which O
naively O
incorporates O
the O
neighborhood O
information O
by O
using O
a O
neighbor O
- O
reconstruction O
regularizer O
. O
The O
superiority O
of O
our O
unified O
method O
can O
be O
further O
corroborated O
in O
the O
comparisons O
with O
RBSH B-MethodName
and O
PairRec B-MethodName
, O
which O
are O
given O
in O
the O
Supplementary O
since O
they O
employed O
a O
different O
preprocessing O
method O
as O
the O
models O
reported O
here O
. O
Comparing O
to O
the O
current O
SOTA O
methods O
of O
AMMI B-MethodName
and O
CorrSh B-MethodName
, O
our O
method O
is O
still O
able O
to O
achieve O
better O
results O
by O
exploiting O
the O
correlation O
among O
documents O
. O
Moreover O
, O
thanks O
to O
the O
benefit O
of O
correlation O
regularization O
, O
remarkable O
gratuity O
can O
be O
acquired O
profitably O
in O
64 O
and O
128 O
bits O
. O

Impact O
of O
Introducing O
Correlations B-MethodName
in O
Prior O
and O
Posterior O

To O
understand O
the O
influences O
of O
the O
proposed O
document O
- O
correlated O
prior O
and O
posterior O
, O
we O
further O
experiment O
with O
two O
variants O
of O
our O
model O
: O
i O
) O
SNUH B-MethodName
ind I-MethodName
: O
which O
does O
not O
consider O
document O
correlations O
in O
neither O
the O
prior O
nor O
the O
posterior O
distribution O
; O
ii O
) O
SNUH B-MethodName
prior I-MethodName
: O
which O
only O
considers O
the O
correlations O
in O
the O
prior O
, O
but O
not O
in O
the O
posterior O
. O
Obviously O
, O
the O
proposed O
SNUH B-MethodName
represents O
the O
method O
that O
leverage O
the O
correlations O
in O
both O
of O
the O
prior O
and O
posterior O
. O
As O
seen O
from O
Table O
2 O
, O
SNUH B-MethodName
prior I-MethodName
achieves O
better O
performance O
than O
SNUH B-MethodName
ind I-MethodName
, O
demonstrating O
the O
benefit O
of O
considering O
the O
correlation B-MetricName
information O
of O
documents O
only O
in O
the O
prior O
. O
By O
further O
taking O
the O
correlations B-MetricName
into O
account O
in O
the O
posterior O
, O
improvements O
of O
SNUH B-MethodName
can O
be O
further O
observed O
, O
which O
fully O
corroborates O
the O
superiority O
of O
considering O
document O
correlations B-MetricName
in O
the O
prior O
and O
posterior O
. O
Another O
interesting O
observation O
is O
that O
the O
performance O
gap O
be- O
Table O
3 O
: O
Qualitative O
analysis O
of O
the O
learned O
128 O
- O
bit O
hash O
codes O
on O
the O
20Newsgroups B-DatasetName
dataset O
. O
We O
present O
the O
documents O
with O
Hamming B-MetricName
distance I-MetricName
of O
1 B-MetricValue
, O
10 B-MetricValue
, O
20 B-MetricValue
, O
50 B-MetricValue
, O
70 B-MetricValue
and O
90 B-MetricValue
to O
the O
query O
. O

tween O
SNUH B-MethodName
ind I-MethodName
and O
SNUH B-MethodName
prior I-MethodName
becomes O
small O
as O
the O
length O
of O
bits O
increases O
. O
This O
may O
be O
attributed O
to O
the O
fact O
that O
the O
increased O
generalization O
ability O
of O
models O
brought O
by O
large O
bits O
is O
inclined O
to O
alleviate O
the O
impact O
of O
priori O
knowledge O
. O
However O
, O
by O
additionally O
incorporating O
correlation B-MetricName
constraints O
on O
posterior O
, O
significant O
performance O
gains O
would O
be O
obtained O
, O
especially O
in O
large O
bits O
scenarios O
. O

Effect O
of O
Spanning O
Trees O
For O
more O
efficient O
training O
, O
spanning O
trees O
are O
utilized O
to O
approximate O
the O
whole O
graph O
by O
dropping O
out O
some O
edges O
. O
To O
understand O
its O
effects O
, O
we O
first O
investigate O
the O
impact O
of O
the O
number O
of O
trees O
. O
The O
first O
row O
of O
Figure O
2 O
shows O
the O
performance O
of O
our O
method O
as O
a O
function O
of O
different O
numbers O
of O
spanning O
trees O
. O
We O
observe O
that O
, O
compared O
to O
not O
using O
any O
correlation B-MetricName
, O
one O
tree O
alone O
can O
bring O
significant O
performance O
gains O
. O
As O
the O
tree O
number O
increases O
, O
the O
performance O
rises O
steadily O
at O
first O
and O
then O
converges O
into O
a O
certain O
level O
, O
demonstrating O
that O
the O
document O
correlations B-MetricName
can O
be O
mostly O
captured O
by O
several O
spanning O
trees O
. O
Then O
, O
we O
further O
explore O
the O
impact O
of O
the O
neighbor O
number O
when O
constructing O
the O
graphs O
using O
the O
KNN O
method O
, O
as O
shown O
in O
the O
second O
row O
of O
Figure O
2 O
. O
It O
can O
be O
seen O
that O
more O
neighbors O
contributes O
to O
better O
performance O
. O
We O
hypothesize O
that O
this O
is O
partly O
due O
to O
the O
more O
diverse O
correlation B-MetricName
information O
captured O
by O
the O
increasing O
number O
of O
neighbors O
. O
However O
, O
incorporating O
too O
many O
neighbors O
may O
lead O
to O
the O
problem O
of O
introducing O
noise O
and O
incorrect O
correlation B-MetricName
information O
to O
the O
hash O
codes O
. O
That O
explains O
why O
no O
further O
improvement O
is O
observed O
after O
the O
number O
reaches O
a O
level O
. O
VDSH B-MethodName
in O
2.038s O
, O
4.364s O
, O
1.051s O
. O
It O
can O
be O
seen O
that O
our O
model O
, O
though O
with O
much O
stronger O
performance O
, O
can O
be O
trained O
almost O
as O
efficiently O
as O
vanilla O
VDSH B-MethodName
due O
to O
the O
tree O
approximations O
. O

Case O
Study O
In O
Table O
3 O
, O
we O
present O
a O
retrieval O
case O
of O
the O
given O
query O
document O
. O
It O
can O
be O
observed O
that O
as O
the O
Hamming B-MetricName
distance I-MetricName
increases O
, O
the O
semantic O
( O
topic O
) O
of O
the O
retrieved O
document O
gradually O
becomes O
more O
irrelevant O
, O
illustrating O
that O
the O
Hamming B-MetricName
distance I-MetricName
can O
effectively O
measure O
the O
document O
relevance O
. O

Visualization O
of O
Hash O
Codes O

To O
evaluate O
the O
quality O
of O
generated O
hash O
code O
more O
intuitively O
, O
we O
project O
the O
latent O
representations O
into O
a O
2dimensional O
plane O
with O
the O
t O
- O
SNE O
( O
van O
der O
Maaten O
and O
Hinton O
, O
2008 O
) O
technique O
. O
As O
shown O
in O
Figure O
3 O
, O
the O
representations O
generated O
by O
our O
method O
are O
more O
separable O
than O
those O
of O
AMMI B-MethodName
, O
demonstrating O
the O
superiority O
of O
our O
method O
. O

Conclusion O

We O
have O
proposed O
an O
effective O
and O
efficient O
semantic O
hashing O
method O
to O
preserve O
both O
the O
semantics O
and O
neighborhood O
information O
of O
documents O
. O
Specifically O
, O
we O
applied O
a O
graph O
- O
induced O
Gaussian O
prior O
to O
model O
the O
two O
types O
of O
information O
in O
a O
unified O
framework O
. O
To O
facilitate O
training O
, O
a O
treestructure O
approximation O
was O
further O
developed O
to O
decompose O
the O
ELBO B-MetricName
into O
terms O
involving O
only O
singleton O
or O
pairwise O
variables O
. O
Extensive O
evaluations O
demonstrated O
that O
our O
model O
significantly O
outperforms O
baseline O
methods O
by O
incorporating O
both O
the O
semantics O
and O
neighborhood O
information O
. O

Appendices O
A O
Derivation O
of O
Formulas O

Derivation O
of O
KL B-MetricName
( O
q O
φ O
( O
Z|X O
) O
||p O
T O
( O
Z O
) O
) O
In O
the O
main O
paper O
, O
we O
propose O
a O
tree O
- O
type O
distribution O
to O
introduce O
partial O
neighborhood O
information O
so O
that O
the O
L O
2 O
term O
can O
be O
expressed O
as O
the O
summation O
over O
terms O
involving O
only O
one O
or O
two O
variables O
. O

Here O
, O
we O
provide O
the O
detail O
derivation O
. O

KL B-MetricName
( O
q O
φ O
( O
Z|X O
) O
||p O
T O
( O
Z O
) O
) O
= O
q O
φ O
( O
Z|X O
) O
log O
i∈V O
q O
φ O
( O
z O
i O
|x O
i O
) O
i∈V O
p O
G O
( O
z O
i O
) O
( O
i O
, O
j O
) O
∈E O
T O
p O
G O
( O
z O
i O
, O
z O
j O
) O
p O
G O
( O
z O
i O
) O
p O
G O
( O
z O
j O
) O
dZ O
= O
i∈V O
KL B-MetricName
( O
q O
φ O
( O
z O
i O
|x O
i O
) O
||p O
θ O
( O
z O
i O
) O
) O
− O
( O
i O
, O
j O
) O
∈E O
T O
E O
q O
φ O
( O
z O
i O
, O
z O
j O
|x O
i O
, O
x O
j O
) O
log O
p O
G O
( O
z O
i O
) O
p O
G O
( O
z O
j O
) O
p O
G O
( O
z O
i O
, O
z O
j O
) O
. O

Obviously O
, O
the O
KL B-MetricName
divergence I-MetricName
is O
decomposed O
into O
the O
terms O
involving O
singleton O
and O
pairwise O
variables O
, O
which O
can O
be O
calculated O
efficiently O
. O

Expressing O
L O
T O
in O
Analytical O
Form O
For O
simplification O
, O
in O
the O
following O
, O
we O
use O
µ O
1 O
, O
Σ O
1 O
to O
represent O
the O
mean O
and O
variance O
matrix O
of O
q O
T O
( O
z O
i O
, O
z O
j O
|x O
i O
, O
x O
j O
) O
, O
respectively O
, O
and O
represent O
those O
of O
p O
G O
( O
z O
i O
, O
z O
j O
) O
as O
µ O
2 O
, O
Σ O
2 O
, O
respectively O
. O
Besides O
we O
denote O
λa B-HyperparameterName
ij O
as O
τ O
ij O
so O
we O
have O
τ O
ij O
= O
λa B-HyperparameterName
ij O
= O
λa B-HyperparameterName
ji O
. O
By O
applying O
the O
Cholesky O
decomposition O
on O
the O
covariance O
matrix O
of O
Σ O
1 O
and O
Σ O
2 O

Σ O
1 O
= O
σ O
i O
0 O
d O
γ O
ij O
σ O
j O
1 O
− O
γ O
2 O
ij O
σ O
j O
σ O
i O
γ O
ij O
σ O
j O
0 O
d O
1 O
− O
γ O
2 O
ij O
σ O
j O
, O
Σ O
2 O
= O
I O
d O
0 O
τ O
ij O
I O
d O
1 O
− O
τ O
2 O
ij O
I O
d O
I O
d O
τ O
ij O
I O
d O
0 O
1 O
− O
τ O
2 O
ij O
I O
d O
, O

where O
we O
omit O
diag O
( O
• O
) O
for O
simplifying O
, O
we O
have O

KL B-MetricName
( O
q O
φ O
( O
z O
i O
, O
z O
j O
|x O
i O
, O
x O
j O
) O
||p O
G O
( O
z O
i O
, O
z O
j O
) O
) O
= O
1 O
2 O
d O
n=1 O
log O
( O
1 O
− O
τ O
2 O
ij O
) O
− O
log O
σ O
2 O
in O
+ O
log O
σ O
2 O
jn O
+ O
log O
( O
1 O
− O
γ O
2 O
ijn O
) O
− O
2 O
+ O
σ O
2 O
in O
+ O
σ O
2 O
jn O
−2τ O
ij O
γ O
ijn O
σ O
in O
σ O
jn O
+ O
µ O
2 O
in O
+ O
µ O
jn O
−2τ O
ij O
µ O
in O
µ O
jn O
1 O
− O
τ O
2 O
ij O
. O

Algorithm O
1 O
Model O
Training O
Algorithm O

Input O
: O
Document O
representations O
X O
; O
edges O
list O
of O
spanning O
trees O
E O
; O
batch B-HyperparameterName
size I-HyperparameterName
b. B-HyperparameterName
Output O
: O
Optimal O
parameters O
( O
θ O
, O
φ O
) O
. O

1 O
: O
θ O
, O
φ O
← O
Initialize O
parameters O
2 O
: O
repeat O
3 O
: O

V O
M O
← O
{ O
x1 O
, O
• O
• O
• O
, O
x O
b O
} O
∼ O
X O
Sample O
nodes O
4 O
: O
E O
M O
T O
← O
{ O
e1 O
, O
• O
• O
• O
, O
e O
b O
} O
∼ O
E O
Sample O
endges O
5 O
: O
g O
← O
∇ O
φ O
, O
θ O
L O
M O
MT O
( O
θ O
, O
φ O
; O
V O
M O
, O
E O
M O
T O
) O
6 O
: O

θ O
, O
φ O
← O
Update O
parameters O
using O
gradients O
g O
( O
e.g. O
, O
Adam O
optimizer O
) O
7 O
: O
until O
convergence O
of O
parameters O
( O
θ O
, O
φ O
) O
Then O
, O
we O
can O
express O
L O
T O
in O
an O
analytical O
form O

Input O
document O
pair O
( O
xi O
; O
xj O
) O
Variational O
Enc O
Correlated O
Enc O
Encoder O
Linear O
( O
|V O
| O
, O
d O
) O
Linear O
( O
|V O
| O
, O
d O
) O
Linear O
( O
2|V O
| O
, O
d O
) O
µ O
= O
f O
( O
• O
/ O
τ O
) O
σ O
= O
g O
( O
• O
) O
γ O
= O
2 O
* O
f O
( O
• O
) O
− O
1 O
Generator O
Linear O
( O
d O
, O
|V O
| O
) O

L O
T O
= O
i∈V O
E O
q O
φ O
( O
z O
i O
|x O
i O
) O
[ O
log O
p O
θ O
( O
x O
i O
|z O
i O
) O
] O
− O
1 O
2 O
d O
n=1 O
( O
µ O
2 O
in O
+ O
σ O
2 O
in O
−1−2 O
log O
σ O
in O
) O
− O
( O
i O
, O
j O
) O
∈E O
T O
1 O
2 O
d O
n=1 O
log O
( O
1 O
− O
τ O
2 O
ij O
) O
− O
µ O
2 O
in O
+ O
µ O
2 O
jn O
+ O
σ O
2 O
in O
+ O
σ O
2 O
jn O
+ O
log O
( O
1 O
− O
γ O
2 O
ijn O
) O
+ O
σ O
2 O
in O
+ O
σ O
2 O
jn O
−2τ O
ij O
γ O
ijn O
σ O
in O
σ O
jn O
+ O
µ O
2 O
in O
+ O
µ O
jn O
−2τ O
ij O
µ O
in O
µ O
jn O
1 O
− O
τ O
2 O
ij O

Derivation O
of O
L O
MT O
With O
L O
MT O
, O
we O
extend O
the O
single O
- O
tree O
approximation O
to O
multi O
- O
tree O
approximation O
. O
Although O
the O
KL B-MetricName
divergence I-MetricName
between O
the O
mixture O
distributions O
does O
not O
have O
a O
closed O
- O
form O
solution O
, O
we O
can O
obtain O
its O
explicit O
upper O
bound O
by O
using O
the O
log O
- O
sum O
inequality O
as O

L O
MT O
≥ O
1 O
M O
T O
∈T O
G O
E O
q O
T O
( O
Z|X O
) O
[ O
log O
p O
θ O
( O
X|Z O
) O
] O
− O
1 O
M O
T O
∈T O
G O
KL O
( O
q O
T O
( O
Z|X O
) O
||p O
T O
( O
X O
) O
) O
L O
MT O
. O

We O
can O
further O
express O
L O
MT O
in O
a O
more O
intuitive O
form O
as O
where O
w O
ij O
= O
| O
{ O
T O
∈T O
G O
| O
( O
i O
, O
j O
) O
∈E O
T O
} O
| O
M O
denotes O
the O
proportion O
of O
times O
that O
the O
edge O
( O
i O
, O
j O
) O
appears O
. O
To O
optimize O
this O
objective O
, O
we O
can O
construct O
an O
estimator O
of O
the O
ELBO B-MetricName
, O
based O
on O
the O
minibatch O

i∈V O
E O
q O
φ O
( O
z O
i O
|x O
i O
) O
[ O
log O
p O
θ O
( O
x O
i O
|z O
i O
) O
] O
−KL O
( O
q O
φ O
( O
z O
i O
|x O
i O
) O
||p O
G O
( O
z O
i O
) O
) O
− O
( O
i O
, O
j O
) O
∈E O
T O
w O
ij O
KL O
( O
q O
φ O
( O
z O
i O
, O
z O
j O
|x O
i O
, O
x O
j O
) O
||p O
G O
( O
x O
i O
, O
x O
j O
) O
) O
−KL O
( O
q O
φ O
( O
z O
i O
|x O
i O
) O
||p O
G O
( O
z O
i O
) O
) O
−KL B-MetricName
( O
q O
φ O
( O
z O
j O
|x O
j O
) O
||p O
G O
( O
z O
j O
) O
) O
, O

L O
MT O
L O
M O
MT O
= O
i∈V O
M O
L O
V O
M O
( O
x O
i O
) O
− O
( O
i O
, O
j O
) O
∈E O
M O
T O
w O
ij O
L O
E O
M O
T O
( O
x O
i O
, O
x O
j O
) O
, O

where O
V O
M O
is O
the O
subset O
of O
documents O
, O
E O
M O
T O
is O
the O
subset O
of O
edges O
and O

L O
V O
M O
( O
x O
i O
) O
E O
q O
φ O
( O
z O
i O
|x O
i O
) O
[ O
log O
p O
θ O
( O
x O
i O
|z O
i O
) O
] O
− O
KL B-MetricName
( O
q O
φ O
( O
z O
i O
|x O
i O
) O
||p O
G O
( O
z O
i O
) O
) O
; O
L O
E O
M O
T O
( O
x O
i O
, O
x O
j O
) O
KL B-MetricName
( O
q O
φ O
( O
z O
i O
, O
z O
j O
|x O
i O
, O
x O
j O
) O
||p O
G O
( O
x O
i O
, O
x O
j O
) O
) O
−KL B-MetricName
( O
q O
φ O
( O
z O
i O
|x O
i O
) O
||p O
G O
( O
z O
i O
) O
) O
−KL B-MetricName
( O
q O
φ O
( O
z O
j O
|x O
j O
) O
||p O
G O
( O
z O
j O
) O
) O
. O

Then O
we O
can O
update O
the O
parameters O
by O
using O
the O
gradient O
∇ O
φ O
, O
θ O
L O
M O
MT O
. O
The O
training O
procedure O
is O
summarized O
in O
Algorithm O
1 O
. O

B O
Tree O
Generation O
Algorithm O

Algorithm O
2 O
shows O
the O
spanning O
tree O
generation O
algorithm O
TreeGen O
( O
• O
) O
used O
in O
our O
graph O
- O
induced O
generative O
document O
hashing O
model O
. O
TreeGen O
( O
• O
) O
utilizes O
a O
depth O
- O
first O
search O
( O
DFS O
) O
algorithm O
to O
generate O
meaningful O
neighborhood O
information O
for O
each O
node O
. O
In O
this O
algorithm O
, O
RC O
[ O
• O
] O
means O
randomly O
choosing O
one O
index O
according O
to O
the O
indicator O
function O
; O
ID O
[ O
• O
] O
represents O
the O
set O
of O
node O
indexes O
satisfying O
the O
indicator O
condition O
and O
N O
( O
i O
) O
denotes O
the O
neighbors O
of O
node O
i. O
Due O
to O
the O
importance O
of O
edges O
precision O
, O
when O
choosing O
a O
neighbor O
( O
line O
16 O
in O
Algorithm O
2 O
) O
, O
instead O
of O
using O
uniform O
sampling O
, O
we O
exploit O
a O
temperature O
α O
to O
control O
the O
trade O
- O
off O
between O
the O
precision O
and O
diversity O
of O
edges O
. O
Specifically O
, O
the O
probability O
of O
sampling O
neighbor O
j O
of O
node O
i O
is O
exp O
( O
cos O
( O
x O
T O
j O
x O
i O
) O
/ O
α O
) O
n∈N O
( O
i O
) O
exp O
( O
cos O
( O
x O
T O
n O
x O
i O
) O
/ O
α O
) O

. O

We O
find O
the O
best O
configuration O
of O
α B-HyperparameterName
on O
the O
validation O
set O
with O
the O
values O
in O
{ O
0.1 B-HyperparameterValue
, O
0.2 B-HyperparameterValue
, O
• O
• O
• O
, O
1 B-HyperparameterValue
} O
. O

C O
Experiment O
Details O

For O
fair O
comparisons O
, O
we O
follow O
the O
experimental O
setting O
of O
VDSH B-MethodName
. O
Specifically O
, O
the O
vocabulary B-HyperparameterName
size I-HyperparameterName
|V B-HyperparameterName
| I-HyperparameterName
is O
7164 B-HyperparameterValue
, O
20000 B-HyperparameterValue
, O
and O
10000 B-HyperparameterValue
for O
Reuters B-DatasetName
, O
TMC B-DatasetName
and O
20Newsgroups B-DatasetName
, O
respectively O
. O
The O
split B-HyperparameterName
of O
training O
, O
validation O
, O
and O
test O
set O
is O
as O
follows O
: O
7752 B-HyperparameterValue
, O
967 B-HyperparameterValue
, O
964 B-HyperparameterValue
for O
Reuters B-DatasetName
; O
21286 B-HyperparameterValue
, O
3498 B-HyperparameterValue
, O
3498 B-HyperparameterValue
for O
TMC B-DatasetName
and O
11016 B-HyperparameterValue
, O
3667 B-HyperparameterValue
, O
3668 B-HyperparameterValue
for O
20Newsgroups B-DatasetName
, O
respectively O
. O
Moreover O
, O
the O
KL B-MetricName
term O
in O
Eq O
. O
( O
18 O
) O
of O
the O
main O
paper O
is O
weighted O
with O
a O
coefficient O
β B-HyperparameterName
to O
avoid O
posterior O
collapse O
. O
We O
find O
the O
best O
configuration O
of O
β B-HyperparameterName
on O
the O
validation O
set O
with O
the O
values O
in O
{ O
0.01 B-HyperparameterValue
, O
0.02 B-HyperparameterValue
, O
• O
• O
• O
, O
0.1 B-HyperparameterValue
} O
. O
To O
intuitively O
understand O
our O
model O
, O
we O
illustrate O
the O
whole O
architecture O
in O
Table O
4 O
. O

D O
Additional O
Experiments O

Comparing O
with O
RBSH B-MethodName
and O
PairRec B-MethodName
As O
mentioned O
before O
, O
the O
reason O
we O
do O
not O
directly O
compare O
our O
method O
with O
RBSH B-MethodName
( O
Hansen O
et O
al O
. O
, O
2019 O
) O
and O
PairRec B-MethodName
( O
Hansen O
et O
al O
. O
, O
2020 O
) O
is O
that O
their O
data O
processing O
methods O
are O
different O
from O
the O
mainstream O
methods O
( O
e.g. O
, O
VDSH B-MethodName
, O
NASH B-MethodName
, O
GMSH B-MethodName
, O
Nbr B-MethodName
- I-MethodName
Reg I-MethodName
, O
AMMI B-MethodName
and O
CorrSH B-MethodName
) O
. O
To O
further O
compare O
our O
method O
with O
them O
, O
we O
evaluate O
our O
model O
on O
three O
datasets O
that O
are O
published O
by O
RBSH B-MethodName
4 O
. O
The O
results O
are O
illustrated O
in O
Table O
5 O
. O
We O
observe O
that O
our O
method O
achieves O
the O
best O
performances O
in O
most O
experimental O
settings O
, O
which O
further O
confirms O
the O
superiority O
of O
simultaneously O
preserving O
the O
semantics O
and O
similarity O
information O
in O
a O
more O
principled O
framework O
. O

Parameter O
Sensitivity O
To O
understand O
the O
robustness O
of O
our O
model O
, O
we O
conduct O
a O
parameter O
Sensitivity O
analysis O
of O
τ B-HyperparameterName
and O
β B-HyperparameterName
in O
Figure O
4 O
. O
Compared O
with O
β B-HyperparameterName
= O
0 B-HyperparameterValue
( O
without O
using O
neighborhood O
information O
) O
, O
models O
with O
β B-HyperparameterName
= O
0 B-HyperparameterValue
improve O
performance O
significantly O
, O
but O
gradually O
performs O
steadily O
as O
β B-HyperparameterName
getting O
larger O
, O
which O
once O
again O
confirms O
the O
importance O
of O
simultaneously O
modeling O
semantic O
and O
neighborhood O
information O
. O
As O
for O
temperature B-HyperparameterName
coefficient I-HyperparameterName
τ B-HyperparameterName
used O
in O
variational O
encoder O
, O
our O
model O
performs O
steadily O
with O
various O
values O
of O
τ B-HyperparameterName
in O
the O
Reuters B-DatasetName
dataset O
. O
But O
in O
TMC B-DatasetName
and O
20Newsgroups B-DatasetName
, O
increasing O
τ B-HyperparameterName
would O
deteriorate O
the O
model O
performance O
. O
Generally O
speaking O
, O
the O
model O
can O
achieve O
better O
performance O
with O
smaller O
τ B-HyperparameterName
( O
i.e. O
, O
steeper O
sigmoid O
function O
) O
. O
As O
we O
utilize O
0.5 B-HyperparameterValue
as O
the O
threshold O
value O
, O
steeper O
sigmoid O
functions O
make O
it O
easier O
to O
distinguish O
hash O
codes O
. O

Acknowledgements O

This O
work O
is O
supported O
by O
the O
National O
Natural O
Science O
Foundation O
of O
China O
( O
No O
. O
61806223 O
, O
61906217 O
, O
U1811264 O
) O
, O
Key O
R O
& O
D O
Program O
of O
Guangdong O
Province O
( O
No O
. O
2018B010107005 O
) O
, O
National O
Natural O
Science O
Foundation O
of O
Guangdong O
Province O
( O
No O
. O
2021A1515012299 O
) O
. O
This O
work O
is O
also O
supported O
by O
MindSpore O
. O

Are O
Representations O
Built O
from O
the O
Ground O
Up O
? O
An O
Empirical O
Examination O
of O
Local B-TaskName
Composition I-TaskName
in O
Language O
Models O

Compositionality B-TaskName
, O
the O
phenomenon O
where O
the O
meaning O
of O
a O
phrase O
can O
be O
derived O
from O
its O
constituent O
parts O
, O
is O
a O
hallmark O
of O
human O
language O
. O
At O
the O
same O
time O
, O
many O
phrases O
are O
non O
- O
compositional O
, O
carrying O
a O
meaning O
beyond O
that O
of O
each O
part O
in O
isolation O
. O
Representing O
both O
of O
these O
types O
of O
phrases O
is O
critical O
for O
language O
understanding O
, O
but O
it O
is O
an O
open O
question O
whether O
modern O
language O
models O
( O
LMs O
) O
learn O
to O
do O
so O
; O
in O
this O
work O
we O
examine O
this O
question O
. O
We O
first O
formulate O
a O
problem O
of O
predicting O
the O
LM O
- O
internal O
representations O
of O
longer O
phrases O
given O
those O
of O
their O
constituents O
. O
We O
find O
that O
the O
representation O
of O
a O
parent O
phrase O
can O
be O
predicted O
with O
some O
accuracy O
given O
an O
affine O
transformation O
of O
its O
children O
. O
While O
we O
would O
expect O
the O
predictive O
accuracy B-MetricName
to O
correlate O
with O
human O
judgments O
of O
semantic O
compositionality B-TaskName
, O
we O
find O
this O
is O
largely O
not O
the O
case O
, O
indicating O
that O
LMs O
may O
not O
accurately O
distinguish O
between O
compositional O
and O
non O
- O
compositional O
phrases O
. O
We O
perform O
a O
variety O
of O
analyses O
, O
shedding O
light O
on O
when O
different O
varieties O
of O
LMs O
do O
and O
do O
not O
generate O
compositional O
representations O
, O
and O
discuss O
implications O
for O
future O
modeling O
work O
. O
1 O

Introduction O

Compositionality B-TaskName
is O
argued O
to O
be O
a O
hallmark O
of O
linguistic O
generalization O
( O
Szabó O
, O
2020 O
) O
. O
However O
, O
some O
phrases O
are O
non O
- O
compositional O
, O
and O
can O
not O
be O
reconstructed O
from O
individual O
constituents O
( O
Dankers O
et O
al O
. O
, O
2022a O
) O
. O
Intuitively O
, O
a O
phrase O
like O
" O
I O
own O
cats O
and O
dogs O
" O
is O
locally O
compositional O
, O
whereas O
" O
It O
's O
raining O
cats O
and O
dogs O
" O
is O
not O
. O
Therefore O
, O
any O
representation O
of O
language O
must O
be O
easily O
composable O
, O
but O
it O
must O
also O
correctly O
handle O
cases O
that O
deviate O
from O
compositional O
rules O
. O

Both O
lack O
( O
Hupkes O
et O
al O
. O
, O
2020 O
; O
Lake O
and O
Baroni O
, O
2017 O
) O
and O
excess O
( O
Dankers O
et O
al O
. O
, O
2022b O
) O
of O
compositionality B-TaskName
have O
been O
cited O
as O
common O
sources O
of O
1 O
Code O
and O
data O
available O
at O
https O
: O
/ O
/ O
github.com O
/ O
nightingal3 O
/ O
lm O
- O
compositionality B-TaskName
errors O
in O
NLP O
models O
, O
indicating O
that O
models O
may O
handle O
phrase O
composition O
in O
an O
unexpected O
way O
. O

In O
general O
form O
, O
the O
compositionality O
principle O
is O
simply O
" O
the O
meaning O
of O
an O
expression O
is O
a O
function O
of O
the O
meanings O
of O
its O
parts O
and O
of O
the O
way O
they O
are O
syntactically O
combined O
" O
( O
Pelletier O
, O
1994 O
) O
. O
However O
, O
this O
definition O
is O
underspecified O
( O
Partee O
, O
1984 O
) O
. O
Recent O
efforts O
to O
evaluate O
the O
compositional O
abilities O
of O
neural O
networks O
have O
resulted O
in O
several O
testable O
definitions O
of O
compositionality B-TaskName
( O
Hupkes O
et O
al O
. O
, O
2020 O
) O
. O

Previous O
work O
on O
compositionality B-TaskName
in O
natural O
language O
focuses O
largely O
on O
the O
definition O
of O
substitutivity O
, O
by O
focusing O
on O
changes O
to O
the O
constituents O
of O
a O
complex O
phrase O
and O
how O
they O
change O
its O
representation O
( O
Dankers O
et O
al O
. O
, O
2022a O
; O
Garcia O
et O
al O
. O
, O
2021 O
; O
Yu O
and O
Ettinger O
, O
2020 O
) O
. O
The O
definition O
we O
examine O
is O
localism O
: O
whether O
or O
not O
the O
representation O
of O
a O
complex O
phrase O
is O
derivable O
only O
from O
its O
local O
structure O
and O
the O
representations O
of O
its O
immediate O
" O
children O
" O
( O
Hupkes O
et O
al O
. O
, O
2020 O
) O
. O
A O
similar O
concept O
has O
been O
proposed O
separately O
to O
measure O
the O
compositionality B-TaskName
of O
learned O
representations O
, O
which O
we O
use O
in O
this O
work O
( O
Andreas O
, O
2019 O
) O
. O
We O
focus O
on O
localism O
because O
it O
is O
a O
more O
direct O
definition O
and O
does O
not O
rely O
on O
the O
collection O
of O
contrastive O
pairs O
of O
phrases O
. O
This O
allows O
us O
to O
examine O
a O
wider O
range O
of O
phrases O
of O
different O
types O
and O
lengths O
. O

In O
this O
paper O
, O
we O
ask O
whether O
reasonable O
compo O
- O
sitional O
probes O
can O
predict O
an O
LM O
's O
representation O
of O
a O
phrase O
from O
its O
children O
in O
a O
syntax O
tree O
, O
and O
if O
so O
, O
which O
kinds O
of O
phrase O
are O
more O
or O
less O
compositional O
. O
We O
also O
ask O
whether O
this O
corresponds O
to O
human O
judgements O
of O
compositionality B-TaskName
. O

We O
first O
establish O
a O
method O
to O
examine O
local O
compositionality B-TaskName
on O
phrases O
through O
probes O
that O
try O
to O
predict O
the O
representation O
of O
a O
parent O
given O
its O
children O
( O
section O
2 O
) O
. O
We O
create O
two O
Englishlanguage O
datasets O
upon O
which O
to O
experiment O
: O
a O
large O
- O
scale O
dataset O
of O
823 O
K O
phrases O
mined O
from O
the O
Penn B-DatasetName
Treebank I-DatasetName
, O
and O
a O
new O
dataset O
of O
idioms O
and O
paired O
non O
- O
idiomatic O
phrases O
for O
which O
we O
elicit O
human O
compositionality B-TaskName
judgements O
, O
which O
we O
call O
the O
Compositionality B-DatasetName
of I-DatasetName
Human I-DatasetName
- I-DatasetName
annotated I-DatasetName
Idiomatic I-DatasetName
Phrases I-DatasetName
dataset I-DatasetName
( O
CHIP B-DatasetName
) O
( O
section O
3 O
) O
. O

For O
multiple O
models O
and O
phrase O
types O
, O
we O
find O
that O
phrase O
embeddings O
across O
models O
and O
representation O
types O
have O
a O
fairly O
predictable O
affine O
compositional O
structure O
based O
on O
embeddings O
of O
their O
constituents O
( O
section O
4 O
) O
. O
We O
find O
that O
there O
are O
significant O
differences O
in O
compositionality B-TaskName
across O
phrase O
types O
, O
and O
analyze O
these O
trends O
in O
detail O
, O
contributing O
to O
understanding O
how O
LMs O
represent O
phrases O
( O
section O
5 O
) O
. O
Interestingly O
, O
we O
find O
that O
human O
judgments O
do O
not O
generally O
align O
well O
with O
the O
compositionality B-TaskName
level O
of O
model O
representations O
( O
section O
6 O
) O
. O
This O
implies O
there O
is O
still O
work O
to O
be O
done O
at O
the O
language O
modelling O
level O
to O
capture O
a O
proper O
level O
of O
compositionality B-TaskName
in O
representations O
. O

Methods O
and O
Experimental O
Details O

Tree B-MetricName
Reconstruction I-MetricName
Error I-MetricName

We O
follow O
Andreas O
( O
2019 O
) O
in O
defining O
deviance O
from O
compositionality B-TaskName
as O
tree B-MetricName
reconstruction I-MetricName
error I-MetricName
. O
Consider O
a O
phrase O
x O
= O
[ O
a O
] O
[ O
b O
] O
, O
where O
a O
and O
b O
can O
be O
any O
length O
> O
0 O
. O
Assume O
we O
always O
have O
some O
way O
of O
knowing O
how O
x O
should O
be O
divided O
into O
a O
and O
b. O
Assume O
we O
also O
have O
some O
way O
of O
producing O
representations O
for O
x O
, O
a O
, O
and O
b O
, O
which O
we O
represent O
as O
a O
function O
r. O
Given O
representations O
r O
( O
x O
) O
, O
r O
( O
a O
) O
and O
r O
( O
b O
) O
, O
we O
wish O
to O
find O
the O
function O
which O
most O
closely O
approximates O
how O
r O
( O
x O
) O
is O
constructed O
from O
r O
( O
a O
) O
and O
r O
( O
b O
) O
. O

f O
= O
arg O
min O
f O
∈F O
1 O
|X O
| O
x∈X O
δ O
x O
, O
ab O
( O
1 O
) O

δ O
x O
, O
ab O
= O
d O
( O
r O
( O
x O
) O
, O
f O
( O
r O
( O
a O
) O
, O
r O
( O
b O
) O
) O
( O
2 O
) O

Where O
X O
is O
the O
set O
of O
possible O
phrases O
in O
the O
language O
that O
can O
be O
decomposed O
into O
two O
parts O
, O
F O
is O
the O
set O
of O
functions O
under O
consideration O
, O
and O
d O
is O
a O
distance O
function O
. O
An O
example O
scenario O
is O
depicted O
in O
Figure O
1 O
. O

For O
d O
, O
we O
use O
cosine O
distance O
as O
this O
is O
the O
most O
common O
function O
used O
to O
compare O
semantic O
vectors O
. O
The O
division O
of O
x O
into O
a O
and O
b O
is O
specified O
by O
syntactic O
structure O
( O
Chomsky O
, O
1959 O
) O
. O
Namely O
, O
we O
use O
a O
phrase O
's O
annotated O
constituency O
structure O
and O
convert O
its O
constituency O
tree O
to O
a O
binary O
tree O
with O
the O
right O
- O
factored O
Chomsky O
Normal O
Form O
conversion O
included O
in O
NLTK O
( O
Bird O
and O
Loper O
, O
2004 O
) O
. O

Language O
Models O

We O
study O
representations O
produced O
by O
a O
variety O
of O
widely O
used O
language O
models O
, O
specifically O
the O
base- O
( O
uncased O
) O
variants O
of O
Transformer O
- O
based O
models O
: O
BERT B-MethodName
, O
RoBERTa B-MethodName
, O
DeBERTa B-MethodName
, O
and O
GPT-2 B-MethodName
( O
He O
et O
al O
. O
, O
2021 O
; O
Liu O
et O
al O
. O
, O
2019 O
; O
Devlin O
et O
al O
. O
, O
2019 O
; O
Radford O
et O
al O
. O
, O
2019 O
) O
. O

Representation O
extraction O

Let O
[ O
x O
0 O
, O
... O
, O
x O
N O
] O
be O
a O
sequence O
of O
N O
+ O
1 O
input O
tokens O
, O
where O
x O
0 O
is O
the O
[ O
CLS O
] O
token O
if O
applicable O
, O
and O

x O
N O
is O
the O
end O
token O
if O
applicable O
. O
Let O
[ O
h O
( O
i O
) O
0 O
, O
... O
, O
h O
( O
i O
) O
N O
] O

be O
the O
embeddings O
of O
the O
input O
tokens O
after O
the O
i O
- O
th O
layer O
. O

For O
models O
with O
the O
[ O
CLS O
] O
beginning O
of O
sequence O
token O
( O
BERT B-MethodName
, O
RoBERTa B-MethodName
, O
and O
DeBERTa B-MethodName
) O
, O
we O
extracted O
the O
embedding O
of O
the O
[ O
CLS O
] O
token O
from O
the O
last O
layer O
, O
which O
we O
refer O
to O
as O
the O
CLS O
representation O
. O
For O
GPT-2 B-MethodName
, O
we O
extracted O
the O
last O
token O
, O
which O
serves O
a O
similar O
purpose O
. O
This O
corresponds O
to O
h O
( O
12 O
) O
0 O
and O
h O
( O
12 O
) O
N O
respectively O
. O
Alternately O
, O
we O
also O
averaged O
all O
embeddings O
from O
the O
last O
layer O
, O
including O
special O
tokens O
. O
We O
refer O
to O
this O
as O
the O
AVG O
representation O
. O

1 O
N O
+ O
1 O
N O
+1 O
i=0 O
h O
( O
12 O
) O
i O
( O
3 O
) O

Approximating O
a O
Composition O
Function O

To O
use O
this O
definition O
, O
we O
need O
a O
composition O
functionf O
. O
We O
examine O
choices O
detailed O
in O
this O
section O
. O

For O
parameterized O
probes O
, O
we O
follow O
the O
probing O
literature O
in O
training O
several O
probes O
to O
predict O
a O
property O
of O
the O
phrase O
given O
a O
representation O
of O
the O
phrase O
. O
However O
, O
in O
this O
case O
, O
we O
are O
not O
predicting O
a O
categorical O
attribute O
such O
as O
part O
of O
speech O
. O
Instead O
, O
the O
probes O
that O
we O
use O
aim O
to O
predict O
the O
parent O
representation O
r O
( O
x O
) O
based O
on O
the O
child O
representations O
r O
( O
a O
) O
and O
r O
( O
b O
) O
. O
We O
call O
this O
an O
approximative O
probe O
to O
distinguish O
it O
from O
the O
usual O
use O
of O
the O
word O
probe O
. O

Arithmetic O
Probes O

In O
the O
simplest O
probes O
, O
the O
phrase O
representation O
r O
( O
x O
) O
is O
computed O
by O
a O
single O
arithmetic O
operation O
on O
r O
( O
a O
) O
and O
r O
( O
b O
) O
. O
We O
consider O
three O
arithmetic O
probes O
: O
2 O
ADD O
( O
r O
( O
a O
) O
, O
r O
( O
b O
) O
) O
= O
r O
( O
a O
) O
+ O
r O
( O
b O
) O

( O
4 O
) O
W1 O
( O
r O
( O
a O
) O
, O
r O
( O
b O
) O
) O
= O
r O
( O
a O
) O

( O
5 O
) O
W2 O
( O
r O
( O
a O
) O
, O
r O
( O
b O
) O
= O
r O
( O
b O
) O

( O
6 O
) O

Learned O
Probes O

We O
consider O
three O
types O
of O
learned O
probes O
. O
The O
linear O
probe O
expresses O
r O
( O
x O
) O
as O
a O
linear O
combination O
of O
r O
( O
a O
) O
and O
r O
( O
b O
) O
. O
The O
affine O
probe O
adds O
a O
bias O
term O
. O
The O
MLP O
probe O
is O
a O
simple O
feedforward O
neural O
network O
with O
3 O
layers O
, O
using O
the O
ReLU O
activation O
. O

LIN O
( O
r O
( O
a O
) O
, O
r O
( O
b O
) O
) O
= O
α O
1 O
r O
( O
a O
) O
+ O
α O
2 O
r O
( O
b O
) O
( O
7 O
) O
AFF O
( O
r O
( O
a O
) O
, O
r O
( O
b O
) O
) O
= O
α O
1 O
r O
( O
a O
) O
+ O
α O
2 O
r O
( O
b O
) O
+ O
β O
( O
8 O
) O
MLP O
( O
r O
( O
a O
) O
, O
r O
( O
b O
) O
) O
= O
W O
3 O
h O
2 O
( O
9 O
) O

Where O

h O
1 O
= O
σ O
( O
W O
1 O
[ O
r O
( O
a O
) O
; O
r O
( O
b O
) O
] O
) O
h O
2 O
= O
σ O
( O
W O
2 O
h O
1 O
) O
, O

W O
1 O
is O
( O
300 O
× O
2 O
) O
, O
W O
2 O
is O
( O
768 O
× O
300 O
) O
, O
and O
W O
3 O
is O
( O
1 O
× O
768 O
) O
. O
We O
do O
not O
claim O
that O
this O
is O
the O
best O
MLP O
possible O
, O
but O
use O
it O
as O
a O
simple O
architecture O
to O
contrast O
with O
the O
linear O
models O
. O

3 O
Data O
and O
Compositionality B-TaskName
Judgments O

Treebank O

To O
collect O
a O
large O
set O
of O
phrases O
with O
syntactic O
structure O
annotations O
, O
we O
collected O
all O
unique O
subphrases O
( O
≥ O
2 O
words O
) O
from O
WSJ O
and O
Brown O
sections O
of O
the O
Penn B-DatasetName
Treebank I-DatasetName
( O
v3 O
) O
( O
Marcus O
et O
al O
. O
, O
1993 O
) O
. O
3 O
The O
final O
dataset O
consists O
of O
823 O
K O
phrases O
after O
excluding O
null O
values O
and O
duplicates O
. O
We O
collected O
the O
length O
of O
the O
left O
child O
in O
words O
, O
the O
length O
of O
the O
right O
child O
in O
words O
, O
and O
the O
tree O
's O
production O
rule O
, O
which O
we O
refer O
to O
as O
tree O
type O
. O
There O
were O
50260 O
tree O
types O
in O
total O
, O
but O
many O
of O
these O
are O
unique O
. O
Examples O
and O
phrase O
length O
distribution O
can O
be O
found O
in O
Appendix O
A O
, O
and O
Appendix O
B O
. O

English O
Idioms O
and O
Matched O
Phrase O
Set O

Previous O
datasets O
center O
around O
notable O
bigrams O
, O
some O
of O
which O
are O
compositional O
and O
some O
of O
which O
are O
non O
- O
compositional O
( O
Ramisch O
et O
al O
. O
, O
2016b O
; O
Reddy O
et O
al O
. O
, O
2011 O
) O
. O
However O
, O
there O
is O
a O
positive O
correlation O
between O
bigram O
frequency O
and O
human O
compositionality B-TaskName
scores O
in O
these O
datasets O
, O
which O
means O
that O
it O
is O
unclear O
whether O
models O
are O
capturing O
compositionality B-TaskName
or O
merely O
frequency O
effects O
if O
they O
correlate O
well O
with O
the O
human O
scores O
. O

Because O
models O
are O
likely O
more O
sensitive O
to O
surface O
features O
of O
language O
than O
humans O
, O
we O
gathered O
a O
more O
controlled O
set O
of O
phrases O
to O
compare O
with O
human O
judgments O
. O

Since O
non O
- O
compositional O
phrases O
are O
somewhat O
rare O
, O
we O
began O
with O
a O
set O
of O
seed O
idioms O
and O
bigrams O
from O
previous O
studies O
( O
Jhamtani O
et O
al O
. O
, O
2021 O
; O
Ramisch O
et O
al O
. O
, O
2016b O
; O
Reddy O
et O
al O
. O
, O
2011 O
) O
. O
We O
used O
idioms O
because O
they O
are O
a O
common O
source O
of O
noncompositional O
phrases O
. O
Duplicates O
after O
lemmatization O
were O
removed O
. O

For O
each O
idiom O
, O
we O
used O
Google O
Syntactic O
NGrams O
to O
find O
three O
phrases O
with O
an O
identical O
part O
of O
speech O
and O
dependency O
structure O
to O
that O
idiom O
, O
and O
frequency O
that O
was O
as O
close O
as O
possible O
relative O
to O
others O
in O
Syntactic O
Ngrams O
( O
Goldberg O
and O
Orwant O
, O
2013 O
) O
. O
4 O
For O
example O
, O
the O
idiom O
" O
sail O
under O
false O
colors O
" O
was O
matched O
with O
" O
distribute O
among O
poor O
parishioners O
" O
. O
More O
examples O
can O
be O
found O
in O
Table O
1 O
. O
An O
author O
of O
this O
paper O
inspected O
the O
idioms O
and O
removed O
those O
that O
were O
syntactically O
analyzed O
incorrectly O
or O
offensive O
. O

Approximating O
a O
Composition O
Function O
4.1 O
Methods O

To O
approximate O
the O
composition O
functions O
of O
models O
, O
we O
extract O
the O
CLS O
and O
AVG O
representations O
from O
each O
model O
on O
the O
Treebank B-DatasetName
dataset O
. O
We O
used O
10 B-MetricName
- I-MetricName
fold I-MetricName
cross I-MetricName
- I-MetricName
validation I-MetricName
and O
trained O
the O
learned O
probes O
on O
the O
90 O
% O
training O
set O
in O
each O
fold O
. O
The O
Note O
that O
the O
frequency O
is O
based O
on O
the O
most O
common O
dependency O
and O
constituency O
pattern O
found O
in O
Syntactic O
NGrams O
. O
Humans O
were O
asked O
to O
rate O
each O
phrase O
for O
its O
compositionality O
. O

remaining O
10 O
% O
were O
divided O
into O
a O
test O
set O
( O
5 O
% O
) O
and O
dev O
set O
( O
5 O
% O
) O
. O
5 O
To O
fairly O
compare O
probes O
, O
we O
used O
minimum O
description O
length O
probing O
( O
Voita O
and O
Titov O
, O
2020 O
) O
.This O
approximates O
the O
length O
of O
the O
online O
code O
needed O
to O
transmit O
both O
the O
model O
and O
data O
, O
which O
is O
related O
to O
the O
area B-MetricName
under I-MetricName
the I-MetricName
learning I-MetricName
curve I-MetricName
. O
Specifically O
, O
we O
recorded O
average B-MetricName
cosine I-MetricName
similarity I-MetricName
of O
the O
predicted O
vector O
and O
actual O
vector O
on O
the O
test O
set O
while O
varying O
the O
size O
of O
the O
training O
set O
from O
0.005 B-MetricValue
% I-MetricValue
to O
100 B-MetricValue
% I-MetricValue
of O
the O
original O
. O
6 O
We O
compare O
the O
AUC B-MetricName
of O
each O
probe O
under O
these O
conditions O
to O
select O
the O
most O
parsimonious O
approximation O
for O
each O
model O
. O

Results O

We O
find O
that O
affine O
probes O
are O
best O
able O
to O
capture O
the O
composition O
of O
phrase O
embeddings O
from O
their O
left O
and O
right O
subphrases O
. O
A O
depiction O
of O
probe O
performance O
at O
approximating O
representations O
across O
models O
and O
representation O
types O
is O
in O
Figure O
2 O
. O
However O
, O
we O
note O
that O
scores O
for O
most O
models O
are O
very O
high O
, O
due O
to O
the O
anisotropy O
phenomenon O
. O
This O
describes O
the O
tendency O
for O
most O
embeddings O
from O
pretrained O
language O
models O
to O
be O
clustered O
in O
a O
narrow O
cone O
, O
rather O
than O
distributed O
evenly O
in O
all O
directions O
( O
Li O
et O
al O
. O
, O
2020 O
; O
Ethayarajh O
, O
2019 O
) O
. O
We O
note O
that O
it O
is O
true O
for O
both O
word O
and O
phrase O
embeddings O
. O

Since O
we O
are O
comparing O
the O
probes O
to O
each O
other O
relative O
to O
the O
same O
anisotropic O
vectors O
, O
this O
is O
not O
necessarily O
a O
problem O
. O
However O
, O
in O
order O
to O
com O
- O
pare O
each O
probe O
's O
performance O
compared O
to O
chance O
, O
we O
correct O
for O
anisotropy O
using O
a O
control O
task O
. O
This O
task O
is O
using O
the O
trained O
probe O
to O
predict O
a O
random O
phrase O
embedding O
from O
the O
set O
of O
treebank O
phrase O
embeddings O
for O
that O
model O
, O
and O
recording O
the O
distance O
between O
the O
compositional O
probe O
's O
prediction O
and O
the O
random O
embedding O
. O
This O
allows O
us O
to O
calculate O
an O
error B-MetricName
ratio I-MetricName
dist B-MetricName
probe I-MetricName
dist B-MetricName
control I-MetricName
, O
where O
dist B-MetricName
probe I-MetricName
represents O
the O
original O
average O
distance O
from O
the O
true O
representation O
, O
and O
dist B-MetricName
control I-MetricName
is O
the O
average O
distance O
on O
the O
control O
task O
. O
This O
quantifies O
how O
much O
the O
probe O
improves O
over O
a O
random O
baseline O
that O
takes O
anisotropy O
into O
account O
, O
where O
a O
smaller O
value O
is O
better O
. O
These O
results O
can O
be O
found O
in O
Appendix O
E. O
The O
results O
without O
anisotropy O
correction O
can O
be O
found O
in O
Appendix O
G. O
In O
most O
cases O
, O
the O
affine O
probe O
still O
performs O
the O
best O
, O
so O
we O
continue O
to O
use O
it O
for O
consistency O
on O
all O
the O
model O
and O
representation O
types O
. O

We O
also O
compare O
the O
AUC B-MetricName
of O
training O
curves O
for O
each O
probe O
and O
find O
that O
the O
affine O
probe O
remains O
the O
best O
in O
most O
cases O
, O
except O
RoBERTa B-MethodName
CLS O
and O
DeBERTa B-MethodName
CLS O
. O
Training O
curves O
are O
depicted O
in O
Appendix O
C. O
AUC B-MetricName
values O
are O
listed O
in O
Appendix O
H. O
Interestingly O
, O
there O
was O
a O
trend O
of O
the O
right O
child O
being O
weighted O
more O
heavily O
than O
the O
left O
child O
, O
and O
each O
model O
/ O
representation O
type O
combination O
had O
its O
own O
characteristic O
ratio O
of O
the O
left O
child O
to O
the O
right O
child O
. O
For O
instance O
, O
in O
BERT B-MethodName
, O
the O
weight B-MetricName
on O
the O
left O
child O
was O
12 B-MetricValue
, O
whereas O
it O
was O
20 B-MetricValue
for O
the O
right O
child O
. O

For O
example O
, O
the O
approximation O
for O
the O
phrase O
" O
green O
eggs O
and O
ham O
" O
with O
BERT B-MethodName
[ O
CLS O
] O
embeddings O
would O
be O
: O
r O
CLS O
( O
" O
green O
eggs O
and O
ham O
" O
) O
= O
12r O
CLS O
( O
" O
green O
eggs O
" O
) O
+ O
20r O
CLS O
( O
" O
and O
ham O
" O
) O
+ O
β O
. O
5 O
Examining O
Compositionality B-TaskName
across O
Phrase O
Types O

Methods O

Intuitively O
, O
we O
expect O
the O
phrases O
whose O
representations O
are O
close O
to O
their O
predicted O
representation O
to O
be O
more O
compositional O
. O
We O
call O
similarity O
to O
the O
expected O
representation O
, O
sim O
( O
r O
( O
x O
) O
, O
f O
( O
r O
( O
a O
) O
, O
r O
( O
b O
) O
) O
) O
, O
the O
compositionality B-MetricName
score I-MetricName
of O
a O
phrase O
. O
We O
record O
the O
mean B-MetricName
reconstruction I-MetricName
error I-MetricName
for O
each O
tree O
type O
and O
report O
the O
results O
. O
In O
addition O
to O
comparing O
tree O
types O
to O
each O
other O
, O
we O
also O
examine O
the O
treatment O
of O
named O
entities O
in O
subsubsection O
5.2.1 O
. O
We O
examine O
the O
relationship O
between O
length O
of O
a O
phrase O
in O
words O
and O
its O
compositionality B-MetricName
score I-MetricName
in O
subsubsection O
5.2.2 O
. O

Results O

There O
is O
a O
significant O
difference O
between O
the O
mean O
compositionality B-MetricName
score I-MetricName
of O
phrase O
types O
. O
Particularly O
, O
the O
AVG O
representation O
assigns O
a O
lower O
compositionality B-MetricName
score I-MetricName
to O
NP O
→ O
NNP O
NNP O
phrases O
, O
which O
is O
expected O
since O
this O
phrase O
type O
often O
corresponds O
to O
named O
entities O
. O
By O
contrast O
, O
the O
CLS O
representation O
assigns O
a O
low O
compositionality B-MetricName
score I-MetricName
to O
NP O
→ O
DT O
NN O
, O
which O
is O
unexpected O
given O
that O
such O
phrases O
are O
generally O
seen O
as O
compositional O
. O
The O
reconstruction B-MetricName
error I-MetricName
for O
the O
most O
common O
phrase O
types O
is O
shown O
in O
Figure O
5 O
. O

Because O
different O
phrase O
types O
may O
be O
treated O
differently O
by O
the O
model O
, O
we O
examine O
the O
relative O
compositionality B-TaskName
of O
phrases O
within O
each O
phrase O
type O
. O
Examples O
of O
the O
most O
and O
least O
compositional O
phrases O
from O
several O
phrase O
types O
are O
shown O
in O
Table O
2 O
for O
RoBERTa B-MethodName
CLS O
. O
Patterns O
vary O
for O
model O
and O
representation O
types O
, O
but O
long O
phrases O
are O
generally O
represented O
more O
compositionally O
. O

Named O
Entities O

We O
used O
SpaCy O
to O
tag O
and O
examine O
named O
entities O
( O
Honnibal O
and O
Montani O
, O
2017 O
) O
, O
as O
they O
are O
expected O
to O
be O
less O
compositional O
. O
We O
find O
that O
named O
entities O
indeed O
have O
a O
lower O
compositionality B-MetricName
score I-MetricName
in O
all O
cases O
except O
RoBERTa B-MethodName
CLS O
, O
indicating O
that O
they O
are O
correctly O
represented O
as O
less O
compositional O
. O
A O
representative O
example O
is O
shown O
in O
Figure O
3 O
. O
Full O
results O
can O
be O
found O
in O
Appendix O
J. O
We O
break O
down O
the O
compositionality B-MetricName
scores I-MetricName
of O
named O
entities O
by O
type O
and O
find O
surprising O
variation O
within O
categories O
of O
named O
entities O
. O
For O
numerical O
examples O
, O
this O
often O
depends O
on O
the O
unit O
used O
. O
For O
example O
, O
in O
RoBERTa B-MethodName
AVG O
representations O
, O
numbers O
with O
" O
million O
" O
and O
" O
billion O
" O
are O
grouped O
together O
as O
compositional O
, O
whereas O
numbers O
with O
quantifiers O
( O
" O
about O
" O
, O
" O
more O
than O
" O
, O
" O
some O
" O
) O
are O
grouped O
together O
as O
not O
compositional O
. O
The O
compositionality B-MetricName
score I-MetricName
distributions O
for O
types O
of O
named O
entities O
are O
presented O
in O
Figure O
4 O
. O

Examining O
Compositionality B-TaskName
and O
Phrase B-MetricName
Length I-MetricName

There O
is O
no O
consistent O
relationship O
between O
phrase B-MetricName
length I-MetricName
and O
compositionality B-MetricName
score I-MetricName
across O
models O
and O
representation O
types O
. O
However O
, O
CLS O
and O
AVG O
representations O
show O
divergent O
trends O
. O
There O
is O
a O
strong O
positive O
correlation B-MetricName
between O
phrase B-MetricName
length I-MetricName
and O
compositionality B-MetricName
score I-MetricName
in O
the O
AVG O
representations O
, O
while O
no O
consistent O
trend O
exists O
for O
the O
CLS O
representations O
. O
This O
indicates O
that O
longer O
phrases O
are O
better O
approximated O
as O
an O
affine O
transformation O
of O
their O
subphrase O
representations O
. O
This O
trend O
is O
summarized O
in O
Appendix O
D. O
All O
correlations O
are O
highly O
significant O
. O

Phrase O
type O

Most O
compositional O

Least O
compositional O
PP O
→ O
IN O
NP O
( O
" O
of O
" O
, O
" O
two O
perilous O
day O
spent O
among O
the O
planters O
of O
Attakapas O
, O
. O
. O
. O
) O
( O
" O
of O
" O
, O
" O
September O
" O
) O
( O
" O
of O
" O
, O
" O
the O
cloth O
bandoleers O
that O
marked O
the O
upper O
part O
of O
his O
body O
. O
. O
. O
) O
( O
" O
like O
" O
, O
" O
the O
Standard O
& O
Poor O
's O
500 O
" O
) O
S O
→ O
NP O
- O
SBJ O
VP O
( O
" O
him O
" O
, O
" O
to O
suggest O
it O
's O
the O
difference O
between O
the O
' O
breakup O
' O
value O
. O
. O
. O
) O
( O
" O
other O
things O
" O
, O
" O
being O
more O
equal O
" O
) O
( O
" O
it O
" O
, O
" O
was O
doing O
a O
brisk O
business O
in O
computer O
power O
- O
surge O
protectors O
. O
. O
. O
" O
) O
( O
" O
less O
" O
, O
" O
is O
more O
" O
) O
NP O
→ O
NNP O
NNP O
( O
" O
M. O
" O
, O
" O
Bluthenzweig O
" O
) O
( O
" O
Edward O
" O
, O
" O
Thompson O
" O
) O
( O
" O
Dr. O
" O
, O
" O
Volgelstein O
" O
) O
( O
" O
Alexander O
" O
, O
" O
Hamilton O
" O
) O
6 O
Comparing O
Compositionality B-TaskName
Judgments O
of O
Humans O
and O
Models O
6.1 O
Methods O

Human O
Annotation O

Human O
annotators O
assigned O
labels O
to O
each O
phrase O
in O
the O
matched O
dataset O
from O
subsection O
3.2 O
: O
1 B-MetricValue
for O
not O
compositional O
, O
2 B-MetricValue
for O
somewhat O
compositional O
, O
and O
3 B-MetricValue
for O
fully O
compositional O
. O
They O
could O
also O
decline O
to O
answer O
if O
they O
felt O
that O
the O
phrase O
did O
n't O
make O
sense O
on O
its O
own O
. O
Furthermore O
, O
they O
were O
asked O
how O
much O
each O
subphrase O
( O
left O
and O
right O
) O
contributed O
to O
the O
final O
meaning O
, O
from O
1 B-MetricValue
for O
not O
at O
all O
, O
to O
3 B-MetricValue
for O
a O
great O
deal O
. O
The O
Likert B-MetricName
scale I-MetricName
of O
1 B-MetricValue
- O
3 B-MetricValue
was O
chosen O
based O
on O
analysis O
of O
previous O
compositionality B-TaskName
annotation O
tasks O
, O
which O
found O
that O
extreme O
values O
of O
compositionality B-TaskName
were O
the O
most O
reliable O
( O
Ramisch O
et O
al O
. O
, O
2016a O
) O
. O
Initially O
, O
six O
English O
- O
speaking O
graduate O
students O
were O
recruited O
. O
The O
six O
initial O
annotators O
all O
annotated O
the O
first O
101 O
examples O
and O
the O
subset O
of O
three O
annotators O
with O
the O
highest O
agreement O
who O
agreed O
to O
continue O
( O
Krippendorff B-MetricName
α B-MetricName
= O
0.5750 B-MetricValue
) O
were O
recruited O
for O
the O
full O
study O
, O
annotating O
1001 O
examples O
. O
For O
the O
full O
study O
, O
the O
agreement O
was O
higher O
( O
α B-MetricName
= O
0.6633 B-MetricValue
) O
. O
We O
took O
the O
mean O
of O
compositionality B-TaskName
judgments O
to O
be O
the O
final O
score O
for O
phrases O
. O
The O
instructions O
shown O
to O
annotators O
are O
in O
Appendix O
F. O
Examples O
judgments O
from O
an O
annotator O
can O
be O
found O
in O
Table O
3 O
. O
3 O
: O
Example O
judgments O
of O
one O
annotator O
on O
the O
pilot O
set O
. O
Annotators O
were O
asked O
to O
rate O
each O
phrase O
from O
1 B-MetricValue
to O
3 B-MetricValue
, O
where O
1 B-MetricValue
meant O
not O
compositional O
and O
3 B-MetricValue
meant O
fully O
compositional O
. O
They O
were O
also O
asked O
how O
much O
each O
subphrase O
contributed O
to O
the O
meaning O
. O

Model O
Comparison O

To O
compare O
human B-MetricName
judgments I-MetricName
to O
model O
compositionality B-MetricName
scores I-MetricName
, O
we O
use O
the O
best O
trained O
approxi O
- O
mative O
probe O
for O
each O
model O
and O
representation O
type O
to O
predict O
a O
vector O
for O
the O
full O
phrase O
based O
on O
its O
left O
and O
right O
subphrases O
( O
taking O
the O
probe O
trained O
on O
the O
first O
fold O
) O
. O
We O
use O
cosine B-MetricName
similarity I-MetricName
to O
the O
expected O
representation O
as O
the O
measure O
of O
how O
compositional O
a O
phrase O
is O
for O
a O
model O
and O
representation O
type O
. O

We O
take O
the O
Spearman B-MetricName
correlation I-MetricName
between O
model O
compositionality B-MetricName
scores I-MetricName
and O
human B-MetricName
compositionality I-MetricName
judgments I-MetricName
and O
observe O
differences O
between O
human B-MetricName
judgments I-MetricName
and O
compositionality B-MetricName
scores I-MetricName
from O
model O
representations O
. O

Results O

Correlation B-MetricName
with O
human B-MetricName
judgments I-MetricName

There O
is O
a O
weak O
correlation O
between O
model O
and O
human O
compositionality B-MetricName
scores I-MetricName
. O
The O
most O
promising O
trend O
is O
found O
in O
RoBERTa B-MethodName
, O
where O
both O
CLS O
and O
AVG O
representations O
have O
a O
significant O
positive O
correlation O
with O
human O
judgments O
. O
Results O
are O
in O
Table O
4 O
, O
with O
corrected O
p O
- O
values O
( O
Holm O
, O
1979 O

Subphrase O
Contribution O
Test O

Annotators O
indicated O
to O
what O
extent O
they O
believed O
each O
part O
of O
the O
phrase O
contributed O
to O
the O
final O
meaning O
. O
We O
examined O
examples O
in O
which O
annotators O
rated O
one O
part O
of O
the O
phrase O
, O
for O
example O
a O
, O
as O
contributing O
more O
to O
the O
final O
meaning O
, O
and O
checked O
how O
often O
d O
cos O
( O
r O
( O
x O
) O
, O
r O
( O
a O
) O
) O
> O
d O
cos O
( O
r O
( O
x O
) O
, O
r O
( O
b O
) O
) O
. O
Models O
do O
surprisingly O
poorly O
at O
this O
test O
, O
with O
most O
performing O
below O
chance O
. O
Results O
are O
presented O
in O
Table O
5 O
. O
An O
error O
analysis O
on O
RoBERTa B-MethodName
AVG O
indicated O
that O
in O
many O
cases O
, O
errors O
were O
due O
to O
idiomaticity O
failures O
. O
For O
example O
, O
" O
noble O
gas O
" O
is O
a O
type O
of O
gas O
that O
was O
rated O
as O
being O
more O
similar O
to O
" O
gas O
" O
by O
humans O
, O
but O
" O
noble O
" O
by O
RoBERTa B-MethodName
. O
7 O

7 O
Similar O
errors O
were O
made O
for O
phrases O
such O
as O
" O
grandfather O
clock O
" O
, O
" O
as O
right O
as O
rain O
" O
, O
" O
ballpark O
estimate O
" O
. O
A O
" O
grandfather O

Idiomaticity O
Test O

Because O
idioms O
were O
matched O
with O
non O
- O
idiomatic O
expressions O
, O
we O
tested O
for O
correctly O
identifying O
the O
idioms O
. O
We O
limited O
the O
analysis O
to O
pairs O
where O
the O
idiomatic O
expression O
was O
rated O
as O
less O
compositional O
than O
the O
matched O
expression O
. O
Results O
are O
shown O
in O
Table O
5 O
. O
Results O
are O
better O
than O
the O
subphrase O
contribution O
test O
, O
but O
models O
do O
not O
achieve O
good O
results O
, O
the O
best O
performing O
representation O
being O
RoBERTa B-MethodName
CLS O
. O

Correlations B-MetricName
with O
Other O
Factors O

We O
examine O
correlations O
of O
model O
and O
human O
compositionality B-MetricName
scores I-MetricName
with O
the O
frequency O
and O
length B-MetricName
of O
the O
phrase O
in O
words O
. O
As O
noted O
before O
, O
there O
is O
a O
strong O
correlation B-MetricName
between O
length B-MetricName
and O
compositionality B-MetricName
score I-MetricName
in O
models O
but O
not O
in O
human O
results O
. O
Results O
are O
in O
Appendix O
K. O
A O
comparison O
of O
phrases O
rated O
as O
most O
and O
least O
compositional O
by O
humans O
, O
as O
well O
as O
RoBERTa B-MethodName
, O
is O
presented O
in O
Table O
6 O
. O

7 O
Related O
work O

Background O
on O
Compositionality B-TaskName

Compositionality B-TaskName
has O
been O
debated O
in O
the O
philosophy O
of O
language O
, O
with O
opposing O
views O
( O
Herbelot O
, O
2020 O
) O
: O
the O
bottom O
- O
up O
view O
that O
the O
meaning O
of O
a O
larger O
phrase O
is O
a O
function O
of O
the O
meaning O
of O
its O
parts O
( O
Cresswell O
, O
1973 O
) O
, O
and O
the O
top O
- O
down O
view O
clock O
" O
is O
a O
type O
of O
clock O
, O
" O
as O
right O
as O
rain O
" O
indicates O
that O
something O
is O
alright O
, O
and O
a O
" O
ballpark O
estimate O
" O
is O
a O
rough O
estimate O
. O

Model O
& O
representation O

Most O
compositional O

Least O
compositional O
Human O
" O
population O
growth O
" O
" O
gravy O
train O
" O
" O
few O
weeks O
away O
" O
" O
shrinking O
violet O
" O
" O
railroad O
monopoly O
" O

" O
revolving O
door O
syndrome O
" O

RoBERTaCLS B-MethodName
" O
two O
small O
sticks O
" O
" O
worse O
than O
none O
" O
" O
dark O
glass O
bottle O
" O
" O
cases O
apart O
" O
" O
annual O
music O
festival O
" O
" O
arch O
'd O
eyebrow O
" O

RoBERTaAVG B-MethodName
" O
look O
with O
open O
eyes O
" O
" O
advertisement O
revenue O
" O
" O
be O
of O
equal O
importance O
" O

" O
taking O
it O
upon O
oneself O
" O
" O
come O
after O
breakfast O
" O
" O
all O
paces O
" O
that O
smaller O
parts O
only O
have O
meaning O
as O
a O
function O
of O
the O
larger O
phrase O
( O
Fodor O
and O
LePore O
, O
1992 O
) O
. O
It O
is O
likely O
that O
there O
is O
a O
blend O
of O
bottom O
- O
up O
and O
top O
- O
down O
processing O
corresponding O
to O
compositional O
and O
non O
- O
compositional O
phrases O
respectively O
( O
Dankers O
et O
al O
. O
, O
2022a O
) O
. O
Hupkes O
et O
al O
. O
have O
proposed O
several O
compositionality O
tests O
based O
on O
previous O
interpretations O
: O
( O
Hupkes O
et O
al O
. O
, O
2020 O
) O
. O
We O
focus O
on O
localism O
, O
corresponding O
to O
the O
bottom O
- O
up O
view O
. O

Other O
Definitions O
of O
Compositionality B-TaskName

Other O
works O
do O
other O
tests O
for O
compositionality B-TaskName
, O
notably O
substitutivity O
( O
Hupkes O
et O
al O
. O
, O
2020 O
) O
. O
Evidence O
suggests O
that O
models O
may O
be O
unable O
to O
modulate O
the O
bottom O
- O
up O
and O
top O
- O
down O
processing O
of O
phrases O
( O
Dankers O
et O
al O
. O
, O
2022b O
, O
a O
) O
. O
Substitutivity O
effects O
appear O
to O
not O
be O
represented O
well O
( O
Garcia O
et O
al O
. O
, O
2021 O
; O
Yu O
and O
Ettinger O
, O
2020 O
) O
. O
This O
indicates O
that O
phrases O
are O
not O
being O
composed O
as O
expected O
and O
motivates O
our O
study O
of O
how O
local O
composition O
is O
carried O
out O
in O
these O
models O
, O
and O
which O
types O
of O
phrase O
are O
processed O
top O
- O
down O
and O
bottom O
- O
up O
. O

Studies O
of O
Localism O

Previous O
studies O
of O
local O
composition O
focus O
on O
bigrams O
, O
particularly O
adjective O
- O
noun O
and O
noun O
- O
noun O
bigrams O
( O
Nandakumar O
et O
al O
. O
, O
2019 O
; O
Cordeiro O
et O
al O
. O
, O
2019 O
; O
Salehi O
et O
al O
. O
, O
2015 O
; O
Reddy O
et O
al O
. O
, O
2011 O
; O
Mitchell O
and O
Lapata O
, O
2010 O
) O
. O
However O
, O
many O
of O
these O
studies O
assume O
an O
additive O
composition O
function O
or O
only O
fit O
a O
composition O
function O
on O
the O
bi O
- O
grams O
in O
their O
datasets O
. O

A O
study O
finds O
some O
evidence O
for O
successful O
local O
composition O
in O
the O
case O
of O
mathematical O
expressions O
, O
but O
used O
a O
constrained O
test O
set O
on O
a O
domain O
that O
is O
expected O
to O
be O
perfectly O
locally O
compositional O
( O
Russin O
et O
al O
. O
, O
2021 O
) O
. O

Approximating O
LM O
Representations O

There O
has O
been O
recent O
interest O
in O
understanding O
the O
compositionality B-TaskName
of O
continuous O
representations O
generated O
by O
neural O
models O
( O
Smolensky O
et O
al O
. O
, O
2022 O
) O
. O
LM O
representations O
have O
been O
approximated O
as O
the O
output O
of O
explicitly O
compositional O
networks O
based O
on O
tensor O
products O
( O
McCoy O
et O
al O
. O
, O
, O
2019Soulos O
et O
al O
. O
, O
2020 O
) O
. O
These O
are O
typically O
evaluated O
based O
on O
compositional O
domains O
, O
such O
as O
the O
SCAN B-DatasetName
dataset O
( O
Lake O
and O
Baroni O
, O
2017 O
) O
. O

Previous O
work O
on O
the O
geometry O
of O
word O
embeddings O
within O
a O
sentence O
shows O
that O
language O
models O
can O
encode O
hierarchical O
structure O
( O
Coenen O
et O
al O
. O
, O
2019 O
; O
Manning O
et O
al O
. O
, O
2020 O
; O
Jawahar O
et O
al O
. O
, O
2019 O
) O
. O
However O
, O
it O
is O
an O
open O
question O
as O
to O
why O
LMs O
do O
not O
tend O
to O
generalize O
well O
compositionally O
( O
Lake O
and O
Baroni O
, O
2017 O
; O
Keysers O
et O
al O
. O
, O
2020 O
) O
. O

Conclusion O

We O
analyze O
the O
compositionality B-TaskName
of O
representations O
from O
several O
language O
models O
and O
find O
that O
there O
is O
an O
effective O
affine O
approximation O
in O
terms O
of O
a O
phrase O
's O
syntactic O
children O
for O
many O
phrases O
. O
Although O
LM O
representations O
may O
be O
surprisingly O
predictable O
, O
we O
find O
that O
human O
compositionality B-TaskName
judgments O
do O
not O
align O
well O
with O
how O
LM O
representations O
are O
structured O
. O

In O
this O
work O
, O
we O
study O
the O
representations O
produced O
after O
extensive O
training O
. O
However O
, O
the O
consistency O
of O
several O
trends O
we O
observed O
suggests O
that O
there O
may O
be O
theoretical O
reasons O
why O
LM O
representations O
are O
structured O
in O
certain O
ways O
. O
Future O
work O
could O
investigate O
the O
evolution O
of O
compositionality B-TaskName
through O
training O
, O
or O
motivate O
methods O
that O
would O
allow O
LMs O
to O
achieve O
improved O
compositional O
generalization O
while O
representing O
non O
- O
compositionality O
. O

Limitations O

One O
limitation O
of O
this O
work O
is O
that O
it O
was O
conducted O
on O
a O
relatively O
small O
set O
of O
language O
models O
trained O
on O
English O
, O
and O
the O
diversity O
of O
patterns O
within O
even O
this O
set O
of O
language O
models O
and O
representation O
types O
is O
great O
. O
However O
, O
we O
note O
that O
the O
experiments O
can O
be O
easily O
repeated O
for O
any O
language O
that O
has O
a O
treebank B-DatasetName
or O
good O
- O
quality O
syntactic O
parsers O
. O
A O
related O
limitation O
is O
that O
these O
analyses O
are O
dependent O
on O
what O
we O
take O
to O
be O
the O
" O
child O
" O
constituents O
of O
a O
parent O
phrase O
. O
It O
may O
be O
harder O
to O
examine O
compositionality O
for O
languages O
that O
differ O
substantially O
from O
English O
, O
or O
that O
can O
not O
be O
easily O
parsed O
using O
existing O
tools O
. O

Although O
we O
try O
to O
carefully O
catalog O
behaviour O
observed O
on O
natural O
language O
phrases O
, O
it O
is O
likely O
that O
smaller O
- O
scale O
experiments O
providing O
a O
more O
mechanistic O
understanding O
of O
model O
behaviour O
would O
be O
easier O
to O
parse O
for O
readers O
. O
Although O
this O
would O
be O
ideal O
, O
we O
leave O
this O
for O
future O
work O
, O
as O
our O
main O
goal O
was O
to O
examine O
how O
language O
models O
represent O
phrases O
considered O
to O
be O
compositional O
and O
non O
- O
compositional O
in O
natural O
language O
. O

Another O
limitation O
is O
that O
although O
we O
diagnose O
a O
problem O
in O
language O
models O
, O
we O
do O
not O
provide O
a O
clear O
avenue O
to O
fix O
it O
. O
Further O
work O
could O
be O
done O
to O
understand O
what O
data O
distributions O
or O
training O
methods O
encourage O
model O
representations O
to O
be O
more O
aligned O
with O
human O
judgments O
. O
Additionally O
, O
although O
compositionality B-TaskName
is O
linguistically O
important O
, O
more O
effort O
could O
be O
put O
towards O
understanding O
the O
downstream O
tasks O
for O
which O
it O
is O
more O
important O
. O
For O
instance O
, O
there O
could O
be O
clear O
issues O
in O
machine O
translation O
if O
non O
- O
compositional O
phrases O
are O
not O
represented O
properly O
, O
but O
these O
phrases O
may O
not O
be O
important O
in O
other O
areas O
such O
as O
instruction O
following O
or O
code O
generation O
. O

Ethics O
Statement O
Potential O
Risks O
and O
Impacts O

Although O
we O
aim O
to O
document O
compositionality B-TaskName
effects O
in O
English O
, O
we O
acknowledge O
that O
this O
perpetuates O
the O
problem O
of O
English O
being O
the O
dominant O
language O
in O
NLP O
research O
. O
It O
is O
possible O
that O
conclusions O
here O
do O
not O
hold O
for O
other O
languages O
, O
and O
further O
work O
is O
needed O
to O
understand O
whether O
these O
conclusions O
transfer O
. O

Additionally O
, O
although O
we O
tried O
to O
filter O
out O
offensive O
idioms O
from O
CHIP B-DatasetName
, O
this O
was O
based O
on O
one O
person O
's O
best O
judgment O
, O
and O
it O
is O
possible O
that O
some O
of O
the O
terms O
in O
the O
dataset O
may O
be O
offensive O
to O
some O
people O
. O
Overall O
, O
phrases O
in O
the O
dataset O
tend O
to O
be O
benign O
, O
but O
some O
idioms O
are O
meant O
to O
have O
a O
perjorative O
meaning O
. O

Computational O
Infrastructure O
and O
Computing O
Budget O

To O
run O
our O
computational O
experiments O
, O
we O
made O
use O
of O
a O
shared O
compute O
cluster O
. O
We O
used O
approximately O
100 O
GPU O
hours O
to O
run O
experiments O
, O
mainly O
due O
to O
running O
results O
for O
different O
language O
models O
and O
representation O
types O
. O
We O
did O
not O
have O
any O
computational O
budget O
besides O
that O
already O
used O
to O
maintain O
the O
cluster O
. O

A O
Treebank B-DatasetName
dataset O
tree O
types O

Due O
to O
space O
constraints O
, O
we O
only O
show O
the O
top O
20 O
tree O
types O
. O
This O
can O
be O
found O
in O
Table O
7 O
. O

B O
Treebank B-DatasetName
dataset O
phrase O
lengths O

Figure O
6 O
: O
Length O
distribution O
of O
phrases O
mined O
from O
the O
treebank B-DatasetName
, O
in O
number O
of O
words O
. O
The O
modal O
length O
was O
3 O
words O
, O
followed O
closely O
by O
2 O
words O
. O
Few O
phrases O
contained O
more O
than O
50 O
words O
. O

C O
Probe O
learning O
curves O

Learning O
curves O
of O
the O
approximative O
probes O
( O
across O
10 O
folds O
) O
are O
shown O
in O
Figure O
7 O
. O

D O
Length O
Correlation O

The O
correlations O
of O
the O
phrase O
length O
( O
in O
words O
) O
and O
compositionality O
scores O
in O
Treebank B-DatasetName
are O
shown O
in O
Table O
8 O
. O

E O
Error B-MetricName
ratio I-MetricName
of O
probes O

F O
Annotation O
setup O
and O
instructions O

Annotators O
were O
recruited O
from O
a O
population O
of O
graduate O
students O
. O
Initially O
, O
6 O
annotators O
completed O
the O
pilot O
experiment O
, O
which O
consisted O
of O
101 O
examples O
. O
The O
subset O
of O
three O
annotators O
with O
highest O
agreement O
was O
asked O
if O
they O
would O
like O
to O
complete O
the O
full O
study O
. O
One O
annotator O
in O
the O
highestagreement O
group O
could O
not O
continue O
to O
the O
full O
study O
, O
so O
this O
annotator O
was O
excluded O
, O
and O
the O
next O
group O
with O
highest O
agreement O
was O
chosen O
. O
The O
agreement O
values O
in O
subsubsection O
6.1.1 O
are O
for O
the O
final O
group O
of O
annotators O
chosen O
. O

The O
experiment O
was O
implemented O
on O
the O
Qualtrics O
platform O
, O
and O
participants O
were O
first O
presented O
with O
a O
consent O
form O
, O
linking O
to O
more O
background O
information O
on O
the O
study O
, O
and O
informing O
them O
that O
their O
participation O
was O
entirely O
voluntary O
. O
After O
agreeing O
to O
the O
terms O
, O
participants O
were O
shown O
some O
examples O
and O
went O
through O
3 O
practice O
questions O
. O
The O
example O
given O
are O
shown O
in O
Figure O
8 O
, O
and O
the O
annotation O
interface O
is O
shown O
in O
Figure O
9 O

G O
Compositionality B-MetricName
scores I-MetricName
without O
anisotropy B-MetricName
correction I-MetricName

The O
raw O
compositionality B-MetricName
scores I-MetricName
can O
be O
found O
in O
Table O
10 O
. O

I O
Mean B-MetricName
deviation I-MetricName
of O
phrase O
types O
by O
tree O
type O

The O
mean B-MetricName
deviation I-MetricName
of O
the O
most O
common O
tree O
types O
can O
be O
found O
in O
Figure O
11 O
. O

J O
Further O
named O
entity O
results O

Named O
entity O
results O
can O
be O
found O
in O
Figure O
12 O
and O
Figure O
13 O
. O

Acknowledgments O

Thank O
you O
to O
Amanda O
Bertsch O
, O
Ting O
- O
Rui O
Chiang O
, O
Varun O
Gangal O
, O
Perez O
Ogayo O
, O
and O
Zora O
Wang O
for O
participating O
in O
compositionality O
annotations O
. O
This O
work O
was O
supported O
in O
part O
by O
a O
CMU O
Presidential O
Fellowship O
to O
the O
first O
author O
, O
and O
the O
Tang O
Family O
AI O
Innovation O
Fund O
. O

Static O
Embeddings O
as O
Efficient O
Knowledge O
Bases O
? O

Recent O
research O
investigates O
factual O
knowledge O
stored O
in O
large O
pretrained B-MethodName
language I-MethodName
models I-MethodName
( O
PLMs B-MethodName
) O
. O
Instead O
of O
structural O
knowledge B-TaskName
base I-TaskName
( O
KB B-TaskName
) O
queries B-TaskName
, O
masked O
sentences O
such O
as O
" O
Paris O
is O
the O
capital O
of O
[ O
MASK O
] O
" O
are O
used O
as O
probes O
. O
The O
good O
performance O
on O
this O
analysis O
task O
has O
been O
interpreted O
as O
PLMs B-MethodName
becoming O
potential O
repositories O
of O
factual O
knowledge O
. O
In O
experiments O
across O
ten O
linguistically O
diverse O
languages O
, O
we O
study O
knowledge O
contained O
in O
static O
embeddings O
. O
We O
show O
that O
, O
when O
restricting O
the O
output O
space O
to O
a O
candidate O
set O
, O
simple O
nearest O
neighbor O
matching O
using O
static O
embeddings O
performs O
better O
than O
PLMs B-MethodName
. O
E.g. O
, O
static O
embeddings O
perform O
1.6 B-MetricValue
% I-MetricValue
points O
better O
than O
BERT B-MethodName
while O
just O
using O
0.3 B-MetricValue
% I-MetricValue
of O
energy O
for O
training O
. O
One O
important O
factor O
in O
their O
good O
comparative O
performance O
is O
that O
static O
embeddings O
are O
standardly O
learned O
for O
a O
large O
vocabulary O
. O
In O
contrast O
, O
BERT B-MethodName
exploits O
its O
more O
sophisticated O
, O
but O
expensive O
ability O
to O
compose O
meaningful O
representations O
from O
a O
much O
smaller O
subword O
vocabulary O
. O

Introduction O

Pretrained O
language O
models O
( O
PLMs O
) O
( O
Peters O
et O
al O
. O
, O
2018 O
; O
Howard O
and O
Ruder O
, O
2018 O
; O
Devlin O
et O
al O
. O
, O
2019 O
) O
can O
be O
finetuned O
to O
a O
variety O
of O
natural O
language O
processing O
( O
NLP O
) O
tasks O
and O
then O
generally O
yield O
high O
performance O
. O
Increasingly O
, O
these O
models O
and O
their O
generative O
variants O
( O
e.g. O
, O
GPT O
, O
Brown O
et O
al O
. O
, O
2020 O
) O
are O
used O
to O
solve O
tasks O
by O
simple O
text O
generation O
, O
without O
any O
finetuning O
. O
This O
motivated O
research O
on O
how O
much O
knowledge O
is O
contained O
in O
PLMs O
: O
Petroni O
et O
al O
. O
( O
2019 O
) O
used O
models O
pretrained O
with O
a O
masked O
language O
objective O
to O
answer O
clozestyle O
templates O
such O
as O
: O

( O
Ex1 O
) O
Paris O
is O
the O
capital O
of O
[ O
MASK O
] O
. O

Using O
this O
methodology O
, O
Petroni O
et O
al O
. O
( O
2019 O
) O
showed O
that O
PLMs O
capture O
some O
knowledge O
implicitly O
. O
This O
has O
been O
interpreted O
as O
suggesting O
* O
Equal O
contribution O
-random O
order O
. O

Model O

Vocabulary O
BERT B-MethodName
and O
mBERT B-MethodName
use O
their O
subword O
vocabularies O
. O

For O
fastText B-MethodName
, O
we O
use O
BERT B-MethodName
/ O
mBERT B-MethodName
's O
vocabularies O
and O
newly O
trained O
wordpiece O
vocabularies O
on O
Wikipedia O
. O

that O
PLMs O
are O
promising O
as O
repositories O
of O
factual O
knowledge O
. O
In O
this O
paper O
, O
we O
present O
evidence O
that O
simple O
static O
embeddings O
like O
fastText O
perform O
as O
well O
as O
PLMs O
in O
the O
context O
of O
answering O
knowledge O
base O
( O
KB O
) O
queries O
. O
Answering O
KB B-TaskName
queries I-TaskName
can O
be O
decomposed O
into O
two O
subproblems O
, O
typing B-TaskName
and O
ranking B-TaskName
. O
Typing B-TaskName
refers O
to O
the O
problem O
of O
predicting O
the O
correct O
type O
of O
the O
answer O
entity O
; O
e.g. O
, O
" O
country O
" O
is O
the O
correct O
type O
for O
[ O
MASK O
] O
in O
( O
Ex1 O
) O
, O
a O
task O
that O
PLMs O
seem O
to O
be O
good O
at O
. O
Ranking B-TaskName
consists O
of O
finding O
the O
entity O
of O
the O
correct O
type O
that O
is O
the O
best O
fit O
( O
" O
France O
" O
in O
( O
Ex1 O
) O
) O
. O
By O
restricting O
the O
output O
space O
to O
the O
correct O
type O
we O
disentangle O
the O
two O
subproblems O
and O
only O
evaluate O
ranking O
. O
We O
do O
this O
for O
three O
reasons O
. O
( O
i O
) O
Ranking B-TaskName
is O
the O
knowledgeintensive O
step O
and O
thus O
the O
key O
research O
question O
. O

( O
ii O
) O
Typed B-TaskName
querying I-TaskName
reduces O
PLMs O
' O
dependency O
on O
the O
template O
. O
( O
iii O
) O
It O
allows O
a O
direct O
comparison O
between O
static O
word O
embeddings O
and O
PLMs O
. O
Prior O
work O
has O
adopted O
a O
similar O
approach O
( O
Xiong O
et O
al O
. O
, O
2020 O
; O
. O

For O
a O
PLM O
like O
BERT B-MethodName
, O
ranking B-TaskName
amounts O
to O
finding O
the O
entity O
whose O
embedding O
is O
most O
similar O
to O
the O
output O
embedding O
for O
[ O
MASK O
] O
. O
For O
static O
embeddings O
, O
we O
rank B-TaskName
entities O
( O
e.g. O
, O
entities O
of O
type O
country O
) O
with O
respect O
to O
similarity O
to O
the O
query O
entity O
( O
e.g. O
, O
" O
Paris O
" O
in O
( O
Ex1 O
) O
) O
. O
In O
experiments O
across O
ten O
linguistically O
diverse O
languages O
, O
we O
show O
that O
this O
simple O
nearest O
neighbor O
matching O
with O
fastText B-MethodName
embeddings O
performs O
comparably O
to O
or O
even O
better O
than O
BERT B-MethodName
. O
For O
example O
for O
English O
, O
fastText B-MethodName
embeddings O
perform O
1.6 B-MetricValue
% I-MetricValue
points O
better O
than O
BERT O
( O
41.2 B-MetricValue
% I-MetricValue
vs. O
39.6 B-MetricValue
% I-MetricValue
, O
see O
Table O
1 O
, O
column O
" O
LAMA B-DatasetName
" O
) O
. O
This O
suggests O
that O
BERT B-MethodName
's O
core O
mechanism O
for O
answering O
factual O
queries O
is O
not O
more O
effective O
than O
simple O
nearest O
neighbor O
matching O
using O
fastText B-MethodName
embeddings O
. O

We O
believe O
this O
means O
that O
claims O
that O
PLMs O
are O
KBs O
have O
to O
be O
treated O
with O
caution O
. O
Advantages O
of O
BERT B-MethodName
are O
that O
it O
composes O
meaningful O
representations O
from O
a O
small O
subword O
vocabulary O
and O
handles O
typing O
implicitly O
( O
Petroni O
et O
al O
. O
, O
2019 O
) O
. O
In O
contrast O
, O
answering O
queries O
without O
restricting O
the O
answer O
space O
to O
a O
list O
of O
candidates O
is O
hard O
to O
achieve O
with O
static O
word O
embeddings O
. O
On O
the O
other O
hand O
, O
static O
embeddings O
are O
cheap O
to O
obtain O
, O
even O
for O
large O
vocabulary O
sizes O
. O
This O
has O
important O
implications O
for O
green O
NLP O
. O
PLMs O
require O
tremendous O
computational O
resources O
, O
whereas O
static O
embeddings O
have O
only O
0.3 O
% O
of O
the O
carbon O
footprint O
of O
BERT O
( O
see O
Table O
4 O
) O
. O
This O
argues O
for O
proponents O
of O
resourcehungry O
deep O
learning O
models O
to O
try O
harder O
to O
find O
cheap O
" O
green O
" O
baselines O
or O
to O
combine O
the O
best O
of O
both O
worlds O
( O
cf O
. O
Poerner O
et O
al O
. O
, O
2020 O
) O
. O

In O
summary O
, O
our O
contributions O
are O
: O

i O
) O

We O
propose O
an O
experimental O
setup O
that O
allows O
a O
direct O
comparison O
between O
PLMs O
and O
static O
word O
embeddings O
. O
We O
find O
that O
static O
word O
embeddings O
show O
performance O
similar O
to O
BERT B-MethodName
on O
the O
modified O
LAMA B-DatasetName
analysis O
task O
across O
ten O
languages O
. O

ii O
) O
We O
provide O
evidence O
that O
there O
is O
a O
trade O
- O
off O
between O
composing O
meaningful O
representations O
from O
subwords O
and O
increasing O
the O
vocabulary O
size O
. O
Storing O
information O
through O
composition O
in O
a O
network O
seems O
to O
be O
more O
expensive O
and O
challenging O
than O
simply O
increasing O
the O
number O
of O
atomic O
representations O
. O

iii O
) O
Our O
findings O
may O
point O
to O
a O
general O
problem O
: O
baselines O
that O
are O
simpler O
and O
" O
greener O
" O
are O
not O
given O
enough O
attention O
in O
deep O
learning O
. O

Code O
and O
embeddings O
are O
available O
online O
. O

Data O

We O
follow O
the O
LAMA B-DatasetName
setup O
introduced O
by O
Petroni O
et O
al O
. O
( O
2019 O
) O
. O
More O
specifically O
, O
we O
use O
data O
from O
TREx B-DatasetName
( O
Elsahar O
et O
al O
. O
, O
2018 O
) O
. O
TREx B-DatasetName
consists O
of O
triples O
of O
the O
form O
( O
object O
, O
relation O
, O
subject O
) O
. O
The O
underlying O
idea O
of O
LAMA O
is O
to O
query O
knowledge O
from O
PLMs O
using O
templates O
without O
any O
finetuning O
: O
the O
triple O
( O
Paris O
, O
capital O
- O
of O
, O
France O
) O
is O
queried O
with O
the O
template O
" O
Paris O
is O
the O
capital O
of O
[ O
MASK O
] O
. O
" O
TREx B-DatasetName
covers O
41 O
relations O
. O
Templates O
for O
each O
relation O
were O
manually O
created O
by O
Petroni O
et O
al O
. O
( O
2019 O
) O
. O
LAMA B-DatasetName
has O
been O
found O
to O
contain O
many O
" O
easy O
- O
toguess O
" O
triples O
; O
e.g. O
, O
it O
is O
easy O
to O
guess O
that O
a O
person O
with O
an O
Italian O
sounding O
name O
is O
Italian O
. O
LAMA B-DatasetName
- I-DatasetName
UHN I-DatasetName
is O
a O
subset O
of O
triples O
that O
are O
" O
hard O
- O
to O
- O
guess O
" O
created O
by O
Poerner O
et O
al O
. O
( O
2020 O
) O
. O

Beyond O
English O
, O
we O
run O
experiments O
on O
nine O
additional O
languages O
using O
mLAMA B-DatasetName
, O
a O
multilingual O
version O
of O
TREx B-DatasetName
. O
For O
an O
overview O
of O
languages O
and O
language O
families O
see O
Table O
2 O
. O
For O
training O
static O
embeddings O
, O
we O
use O
Wikipedia O
dumps O
from O
October O
2020 O
. O

Methods O

We O
describe O
our O
proposed O
setup O
, O
which O
allows O
to O
compare O
PLMs O
with O
static O
embeddings O
. O

PLMs O

We O
use O
the O
following O
two O
PLMs O
: O
( O
i O
) O
BERT B-MethodName
for O
English O
( O
BERT B-MethodName
- O
base B-HyperparameterName
- I-HyperparameterName
cased I-HyperparameterName
, O
Devlin O
et O
al O
. O
( O
2019 O
) O
) O
, O
( O
ii O
) O
mBERT B-MethodName
for O
all O
ten O
languages O
( O
the O
multilingual O
version O
BERT B-MethodName
- O
base B-HyperparameterName
- I-HyperparameterName
multilingual I-HyperparameterName
- I-HyperparameterName
cased I-HyperparameterName
) O
. O
Petroni O
et O
al O
. O
( O
2019 O
) O
use O
templates O
like O
" O
Paris O
is O
the O
capital O
of O
[ O
MASK O
] O
" O
and O
give O
arg O
max O
w∈V O
p O
( O
w|t O
) O
as O
answer O
where O
V O
is O
the O
vocabulary O
of O
the O
PLM O
and O
p O
( O
w|t O
) O
is O
the O
probability O
that O
word O
w O
gets O
predicted O
in O
the O
template O
t O
. O

We O
follow O
the O
same O
setup O
as O
( O
Kassner O
et O
al O
. O
, O
120k O
27.9 O
25.2 O
31.0 O
24.2 O
28.3 O
22.4 O
28.2 O
28.0 O
33.2 O
250k O
30.1 O
30.3 O
34.2 O
28.8 O
32.8 O
24.9 O
30.5 O
31.6 O
35.6 O
500k O
31.7 O
32.5 O
36.6 O
30.9 O
33.7 O
27.0 O
31.5 O
31.8 O
36.1 O
1000k O
31.3 O
33.6 O
36.5 O
31.8 O
33.9 O
27.2 O
29.8 O
30.5 O
36.6 O
Table O
3 O
: O
p1 O
( O
Strubell O
et O
al O
. O
, O
2019 O
) O
. O
We O
use O
our O
server O
's O
peak O
power O
consumption O
. O
See O
appendix O
for O
details O
. O

2021 O
) O
and O
use O
typed B-TaskName
querying I-TaskName
: O
for O
each O
relation O
, O
we O
create O
a O
candidate O
set O
C O
and O
then O
predict O
arg O
max O
c∈C O
p O
( O
c|t O
) O
. O
For O
most O
templates O
, O
there O
is O
only O
one O
valid O
entity O
type O
, O
e.g. O
, O
country O
for O
( O
Ex1 O
) O
. O
We O
choose O
as O
C O
the O
set O
of O
objects O
across O
all O
triples O
for O
a O
single O
relation O
. O
The O
candidate O
set O
could O
also O
be O
obtained O
from O
an O
entity O
typing O
system O
( O
e.g. O
, O
Yaghoobzadeh O
et O
al O
. O
, O
2018 O
) O
, O
but O
this O
is O
beyond O
the O
scope O
of O
this O
paper O
. O
Variants O
of O
typed O
prediction O
have O
been O
used O
before O
( O
Xiong O
et O
al O
. O
, O
2020 O
) O
. O
We O
accommodate O
multi O
- O
token O
objects O
, O
i.e. O
, O
objects O
that O
are O
not O
contained O
in O
the O
vocabulary O
, O
by O
including O
multiple O
[ O
MASK O
] O
tokens O
in O
the O
templates O
. O
We O
then O
compute O
an O
object O
's O
score O
as O
the O
average O
of O
the O
log O
probabilities O
for O
its O
individual O
tokens O
. O
Note O
that O
we O
do O
not O
perform O
any O
finetuning O
. O

Vocabulary O

The O
vocabulary O
V O
of O
the O
wordpiece O
tokenizer O
is O
of O
central O
importance O
for O
static O
embeddings O
as O
well O
as O
PLMs O
. O
BERT O
models O
come O
with O
fixed O
vocabularies O
. O
It O
would O
be O
prohibitive O
to O
retrain O
the O
models O
with O
a O
new O
vocabulary O
. O
It O
would O
also O
be O
too O
expensive O
to O
increase O
the O
vocabulary O
by O
a O
large O
factor O
: O
the O
embedding O
matrix O
is O
responsible O
for O
the O
majority O
of O
the O
memory O
consumption O
of O
these O
models O
. O

In O
contrast O
, O
increasing O
the O
vocabulary O
size O
is O
cheap O
for O
static O
embeddings O
. O
We O
thus O
experiment O
with O
different O
vocabulary O
sizes O
for O
static O
embeddings O
. O
To O
this O
end O
, O
we O
train O
new O
vocabularies O
for O
each O
language O
on O
Wikipedia O
using O
the O
wordpiece O
tokenizer O
( O
Schuster O
and O
Nakajima O
, O
2012 O
) O
. O

Static O
Embeddings O

Using O
either O
newly O
trained O
vocabularies O
or O
existing O
BERT B-MethodName
vocabularies O
, O
we O
tokenize O
Wikipedia O
. O
We O
then O
train O
fastText B-MethodName
embeddings O
( O
Bojanowski O
et O
al O
. O
, O
2017 O
) O
with O
default O
parameters O
( O
http O
: O
/ O
/ O
fasttext.cc O
) O
. O

We O
consider O
the O
same O
candidate O
set O
C O
as O
for O
PLMs O
. O

Let O
c O
∈ O
C O
be O
a O
candidate O
that O
gets O
split O
into O
tokens O
t O
1 O
, O
. O
. O
. O
, O
t O
k O
by O
the O
wordpiece O
tokenizer O
. O
We O
then O
assign O
to O
c O
the O
embedding O
vector O

e O
c O
= O
1 O
k O
k O
i=1 O
e O
t O
i O

where O
e O
t O
i O
is O
the O
fastText B-MethodName
vector O
for O
token O
t O
i O
. O
We O
compute O
the O
representations O
for O
a O
query O
q O
analogously O
. O
For O
a O
query O
q O
( O
the O
subject O
of O
a O
triple O
) O
, O
we O
then O
compute O
the O
prediction O
as O
: O

arg O
max O
c∈C O
cosine O
- O
sim O
( O
ē O
q O
, O
ē O
c O
) O
, O

i.e. O
, O
we O
perform O
simple O
nearest O
neighbor O
matching O
. O

Note O
that O
the O
static O
embedding O
method O
does O
not O
get O
any O
signal O
about O
the O
relation O
. O
The O
method O
's O
only O
input O
is O
the O
subject O
of O
a O
triple O
, O
and O
we O
leave O
incorporating O
a O
relation O
vector O
to O
future O
work O
. O

Evaluation O
Metric O

We O
compute O
precision O
at O
one O
for O
each O
relation O
, O
i.e. O
, O
1 O
/ O
|T O
| O
t∈T O
1 O
{ O
t O
object O
= O
t O
object O
} O
where O
T O
is O
the O
set O
of O
all O
triples O
andt O
object O
the O
object O
predicted O
using O
contextualized O
/ O
static O
embeddings O
. O
Note O
that O
T O
is O
different O
for O
each O
language O
. O
Our O
final O
measure O
( O
p1 O
) O
is O
then O
the O
precision O
at O
one O
( O
macro- O
) O
averaged O
over O
relations O
. O
As O
a O
consistency O
check O
we O
provide O
an O
Oracle O
baseline O
: O
it O
always O
predicts O
the O
most O
frequent O
object O
across O
triples O
based O
on O
the O
gold O
candidate O
sets O
. O

BERT B-MethodName
vs. O
fastText B-MethodName

Results O
for O
English O
are O
in O
Table O
1 O
. O
The O
table O
shows O
that O
when O
increasing O
the O
vocabulary O
size O
, O
static O
embeddings O
and O
BERT B-MethodName
exhibit O
similar O
performance O
on O
LAMA B-DatasetName
. O
The O
Oracle O
baseline O
is O
mostly O
outperformed O
. O
Only O
for O
small O
vocabulary O
sizes O
, O
fast B-MethodName
- I-MethodName
Text I-MethodName
is O
worse O
. O
Performance O
of O
fastText B-MethodName
increases O
with O
larger O
vocabulary O
sizes O
and O
with O
a O
vocabulary O
size O
of O
1000k O
we O
observe O
a O
1.6 O
% O
absolute O
performance O
increase O
of O
fastText B-MethodName
embeddings O
compared O
to O
BERT B-MethodName
( O
41.2 O
% O
vs. O
39.6 O
% O
) O
. O
The O
performance O
gap O
between O
fastText B-MethodName
and O
BERT B-MethodName
increases O
to O
2.7 O
% O
points O
on O
LAMA B-DatasetName
- I-DatasetName
UHN I-DatasetName
, O
indicating O
that O
fastText B-MethodName
is O
less O
vulnerable O
to O
misleading O
clues O
about O
the O
subject O
. O

Only O
providing O
results O
on O
English O
can O
be O
prone O
to O
unexpected O
biases O
. O
Thus O
, O
we O
verify O
our O
results O
for O
nine O
additional O
languages O
. O
Results O
are O
shown O
in O
Table O
3 O
and O
the O
conclusions O
are O
similar O
: O
for O
large O
enough O
vocabularies O
, O
static O
embeddings O
consistently O
have O
better O
performance O
. O
For O
languages O
outside O
the O
Indo O
- O
European O
family O
, O
the O
performance O
gap O
between O
mBERT B-MethodName
and O
fastText B-MethodName
is O
much O
larger O
( O
e.g. O
, O
31.7 O
vs. O
17.2 O
for O
Arabic O
) O
and O
mBERT B-MethodName
is O
sometimes O
worse O
than O
the O
Oracle O
. O

Our O
fastText B-MethodName
method O
is O
quite O
primitive O
: O
it O
is O
a O
type O
- O
restricted O
search O
for O
entities O
similar O
to O
what O
is O
most O
prominent O
in O
the O
context O
( O
whose O
central O
element O
is O
the O
query O
entity O
, O
e.g. O
, O
" O
Paris O
" O
in O
( O
Ex1 O
) O
) O
. O
The O
fact O
that O
fastText O
outperforms O
BERT B-MethodName
raises O
the O
question O
: O
Does O
BERT B-MethodName
simply O
use O
associations O
between O
entities O
( O
like O
fastText O
) O
or O
has O
it O
captured O
factual O
knowledge O
beyond O
this O
? O

BERT B-MethodName
vs O
fastText B-MethodName
: O
Diversity O
of O
Predictions O

The O
entropy O
of O
the O
distribution O
of O
predicted O
objects O
is O
6.5 O
for O
BERT B-MethodName
vs. O
7.3 O
for O
fastText B-MethodName
. O
So O
BERT B-MethodName
's O
predictions O
are O
less O
diverse O
. O
Of O
151 O
possible O
objects O
on O
average O
, O
BERT O
predicts O
( O
on O
average O
) O
85 O
, O
fast O
- O
Text O
119 O
. O
For O
a O
given O
relation O
, O
BERT B-MethodName
's O
prediction O
tend O
to O
be O
dominated O
by O
one O
object O
, O
which O
is O
often O
the O
most O
frequent O
correct O
object O
-possibly O
because O
these O
objects O
are O
frequent O
in O
Wikipedia O
/ O
Wikidata O
. O
When O
filtering O
out O
triples O
whose O
correct O
answer O
is O
the O
most O
frequent O
object O
, O
BERT B-MethodName
's O
performance O
drops O
to O
35.7 O
whereas O
fastText B-MethodName
's O
increases O
to O
42.5 O
. O
See O
Table O
7 O
in O
the O
appendix O
for O
full O
results O
on O
diversity O
. O
We O
leave O
investigating O
why O
BERT B-MethodName
has O
these O
narrower O
object O
preferences O
for O
future O
work O
. O

Contextualization O
in O
BERT B-MethodName

BERT B-MethodName
's O
attention O
mechanism O
should O
be O
able O
to O
handle O
long O
subjects O
-in O
contrast O
to O
fastText O
, O
for O
which O
we O
use O
simple O
averaging O
. O
Figure O
1 O
shows O
that O
fast B-MethodName
- I-MethodName
Text I-MethodName
's O
performance O
indeed O
drops O
when O
the O
query O
gets O
tokenized O
into O
multiple O
tokens O
. O
In O
contrast O
, O
BERT B-MethodName
's O
performance O
remains O
stable O
. O
We O
conclude O
that O
token O
averaging O
harms O
fastText O
's O
performance O
and O
that O
the O
attention O
mechanism O
in O
BERT B-MethodName
composes O
meaningful O
representations O
from O
subwords O
. O
We O
try O
to O
induce O
static O
embeddings O
from O
BERT B-MethodName
by O
feeding O
object O
and O
subject O
surface O
forms O
to O
BERT B-MethodName
without O
any O
context O
and O
then O
averaging O
the O
hidden O
representations O
for O
each O
layer O
. O
Figure O
2 O
analyzes O
whether O
a O
nearest O
neighbor O
matching O
over O
this O
static O
embedding O
space O
extracted O
from O
BERT B-MethodName
's O
representations O
is O
effective O
in O
extracting O
knowledge O
from O
it O
. O
We O
find O
that O
performance O
on O
LAMA B-DatasetName
is O
significantly O
lower O
across O
all O
hidden O
layers O
with O
the O
first O
two O
layers O
performing O
best O
. O
That O
simple O
averaging O
does O
not O
work O
as O
well O
as O
contextualization O
indicates O
that O
BERT B-MethodName
is O
great O
at O
composing O
meaningful O
representations O
through O
attention O
. O
In O
future O
work O
, O
it O
would O
be O
interesting O
to O
extract O
better O
static O
representations O
from O
BERT B-MethodName
, O
for O
example O
by O
extracting O
the O
representations O
of O
entities O
in O
real O
sentences O
. O
. O
" O
and O
a O
candidate O
set O
. O
The O
solid O
lines O
reflect O
performance O
of O
nearest O
neighbor O
matching O
with O
cosine O
similarity O
when O
inducing O
a O
static O
embedding O
space O
from O
the O
representations O
at O
these O
layers O
. O
This O
shows O
that O
extracting O
high O
quality O
static O
embeddings O
is O
not O
trivial O
, O
and O
BERT B-MethodName
's O
contextualization O
is O
essential O
for O
getting O
good O
performance O
. O

Resource O
Consumption O

carbon O
emissions O
compared O
to O
BERT B-MethodName
. O
In O
a O
recent O
study O
, O
Zhang O
et O
al O
. O
( O
2020 O
) O
showed O
that O
capturing O
factual O
knowledge O
inside O
PLMs O
is O
an O
especially O
resource O
hungry O
task O
. O
These O
big O
differences O
demonstrate O
that O
fastText O
, O
in O
addition O
to O
performing O
better O
than O
BERT B-MethodName
, O
is O
the O
environmentally O
better O
model O
to O
" O
encode O
knowledge O
" O
of O
Wikipedia O
in O
an O
unsupervised O
fashion O
. O
This O
calls O
into O
question O
the O
use O
of O
large O
PLMs O
as O
knowledge O
bases O
, O
particularly O
in O
light O
of O
the O
recent O
surge O
of O
knowledge O
augmented O
LMs O
, O
e.g. O
, O
Guu O
et O
al O
. O
, O
2020 O
) O
. O

5 O
Related O
Work O
Petroni O
et O
al O
. O
( O
2019 O
) O
first O
asked O
: O
can O
PLMs O
function O
as O
KBs O
? O
Subsequent O
analysis O
focused O
on O
different O
aspects O
, O
such O
as O
negation O
Ettinger O
, O
2020 O
) O
, O
paraphrases O
( O
Elazar O
et O
al O
. O
, O
2021 O
) O
, O
easy O
to O
guess O
names O
( O
Poerner O
et O
al O
. O
, O
2020 O
) O
, O
finding O
alternatives O
to O
a O
cloze O
- O
style O
approach O
( O
Bouraoui O
et O
al O
. O
, O
2020 O
; O
Heinzerling O
and O
Inui O
, O
2020 O
; O
Jiang O
et O
al O
. O
, O
2020 O
) O
or O
analyzing O
different O
model O
sizes O
( O
Roberts O
et O
al O
. O
, O
2020 O
) O
. O

There O
is O
a O
recent O
surge O
of O
work O
that O
tries O
to O
improve O
PLMs O
' O
ability O
to O
harvest O
factual O
knowledge O
: O
In O
contrast O
, O
we O
provide O
evidence O
that O
BERT B-MethodName
's O
ability O
to O
answer O
factual O
queries O
is O
not O
more O
effective O
than O
capturing O
" O
knowledge O
" O
with O
simple O
traditional O
static O
embeddings O
. O
This O
suggests O
that O
learning O
associations O
between O
entities O
and O
typerestricted O
similarity O
search O
over O
these O
associations O
may O
be O
at O
the O
core O
of O
BERT B-MethodName
's O
ability O
to O
answer O
cloze O
- O
style O
KB B-TaskName
queries I-TaskName
, O
a O
new O
insight O
into O
BERT B-MethodName
's O
working O
mechanism O
. O

Conclusion O

We O
have O
shown O
that O
, O
when O
restricting O
cloze O
- O
style O
questions O
to O
a O
candidate O
set O
, O
static O
word O
embeddings O
outperform O
BERT B-MethodName
. O
To O
explain O
this O
puzzling O
superiority O
of O
a O
much O
simpler O
model O
, O
we O
put O
forward O
a O
new O
characterization O
of O
factual O
knowledge O
learned O
by O
BERT B-MethodName
: O
BERT B-MethodName
seems O
to O
be O
able O
to O
complete O
cloze O
- O
style O
queries O
based O
on O
similarity O
assessments O
on O
a O
type O
- O
restricted O
vocabulary O
much O
like O
a O
nearest O
neighbor O
search O
for O
static O
embeddings O
. O

However O
, O
BERT B-MethodName
may O
still O
be O
the O
better O
model O
for O
the O
task O
: O
we O
assume O
perfect O
typing O
( O
for O
BERT B-MethodName
and O
fastText B-MethodName
) O
and O
only O
evaluate O
ranking O
. O
Typing O
is O
much O
harder O
with O
static O
embeddings O
and O
BERT B-MethodName
has O
been O
shown O
to O
perform O
well O
at O
guessing O
the O
expected O
entity O
type O
based O
on O
a O
template O
. O
BERT B-MethodName
also O
works O
well O
with O
small O
vocabularies O
, O
storing O
most O
of O
its O
" O
knowledge O
" O
in O
the O
parameterization O
of O
subword O
composition O
. O
Our O
results O
suggest O
that O
increasing O
the O
vocabulary O
size O
and O
computing O
more O
atomic O
entity O
representations O
with O
fastText B-MethodName
is O
a O
cheap O
and O
environmentally O
friendly O
method O
of O
storing O
knowledge O
. O
In O
contrast O
, O
learning O
high O
quality O
composition O
of O
smaller O
units O
requires O
many O
more O
resources O
. O

fastText B-MethodName
is O
a O
simple O
cheap O
baseline O
that O
outperforms O
BERT B-MethodName
on O
LAMA B-DatasetName
, O
but O
was O
not O
considered O
in O
the O
original O
research O
. O
This O
may O
be O
an O
example O
of O
a O
general O
problem O
: O
" O
green O
" O
baselines O
are O
often O
ignored O
, O
but O
should O
be O
considered O
when O
evaluating O
resource O
- O
hungry O
deep O
learning O
models O
. O
A O
promising O
way O
forward O
would O
be O
to O
combine O
the O
best O
of O
both O
worlds O
, O
e.g. O
, O
by O
building O
on O
work O
that O
incorporates O
large O
vocabularies O
into O
PLMs O
after O
pretraining O
. O

A O
Resource O
Consumption O

We O
follow O
Strubell O
et O
al O
. O
( O
2019 O
) O
for O
our O
computation O
. O
The O
measured O
peak O
energy O
consumption O
of O
our O
CPU O
- O
server O
was O
618W. O
Considering O
the O
power O
usage O
effectiveness O
the O
required O
kWh O
are O
given O
by O
p O
t O
= O
1.58 O
• O
t O
• O
618 O
/ O
1000 O
. O
Training O
the O
English O
fast O
- O
Text O
on O
Wikipedia O
took O
around O
5 O
hours O
. O
Training O
all O
languages O
took O
20 O
hours O
. O
The O
estimated O
CO O
2 O
e O
can O
then O
be O
computed O
by O
CO O
2 O
e O
= O
0.954 O
• O
p O
t O

B O
Reproducibility O
Information O

For O
computation O
we O
use O
a O
CPU O
server O
with O
96 O
CPU O
cores O
( O
Intel O
( O
R O
) O
Xeon O
( O
R O
) O
Platinum O
8160 O
) O
and O
1024 O
GB O
RAM O
. O
For O
BERT B-MethodName
and O
mBERT B-MethodName
inference O
we O
use O
a O
single O
GeForce O
GTX O
1080Ti O
GPU O
. O

Getting O
the O
object O
predictions O
for O
BERT B-MethodName
and O
fast B-MethodName
- I-MethodName
Text I-MethodName
is O
fast O
and O
takes O
a O
negligible O
amount O
of O
time O
. O
Training O
fastText B-MethodName
embeddings O
takes O
between O
1 O
to O
5 O
hours O
depending O
on O
Wikipedia O
size O
. O

BERT B-MethodName
has O
around O
110 B-HyperparameterValue
M I-HyperparameterValue
parameters B-HyperparameterName
, O
mBERT B-HyperparameterName
around O
178M. B-HyperparameterValue
The O
fastText B-MethodName
embeddings O
have O
O O
( O
nd O
) O
parameters O
where O
n O
is O
the O
vocabulary O
size O
and O
d O
is O
the O
embedding B-HyperparameterName
dimension I-HyperparameterName
. O
We O
use O
d O
= O
300 B-HyperparameterValue
. O
Thus O
, O
for O
most O
vocabulary O
sizes O
, O
fastText B-MethodName
has O
significantly O
more O
parameters O
than O
the O
BERT B-MethodName
models O
. O
But O
overall O
they O
are O
cheaper O
to O
train O
. O

We O
did O
not O
perform O
any O
hyperparameter O
tuning O
. O
Table O
6 O
gives O
an O
overview O
on O
third O
party O
software O
. O
Table O
5 O
gives O
an O
overview O
on O
the O
number O
of O
triples O
in O
the O
dataset O
. O
Note O
that O
no O
training O
set O
is O
required O
, O
as O
all O
methods O
are O
completely O
unsupervised O
. O
Table O
7 O
: O
Analysis O
of O
the O
diversity O
of O
predictions O
. O
p1 O
- O
mf O
is O
the O
p1 O
when O
excluding O
triples O
whose O
correct O
answer O
is O
the O
most O
frequent O
object O
. O
entropy O
is O
the O
entropy O
of O
the O
distribution O
of O
predicted O
objects O
. O
# O
pred O
. O
denotes O
the O
average O
number O
of O
distinct O
objects O
predicted O
by O
the O
model O
across O
relations O
. O
The O
average O
number O
of O
unique O
objects O
in O
the O
candidate O
set O
across O
relations O
is O
151 O
. O
fastText B-MethodName
has O
more O
diverse O
predictions O
, O
as O
the O
entropy O
is O
higher O
and O
the O
set O
of O
predicted O
objects O
is O
on O
average O
much O
larger O
. O

C O
Examples O

D O
Additional O
Results O

In O
this O
section O
we O
show O
additional O
results O
. O
Table O
8 O
shows O
the O
same O
as O
Table O
1 O
but O
with O
precision O
at O
five O
. O
Analogously O
Table O
9 O
. O
Table O
10 O
shows O
the O
same O
as O
Table O
3 O
but O
for O
LAMA B-DatasetName
- I-DatasetName
UHN I-DatasetName
. O
The O
trends O
and O
key O
insights O
are O
unchanged O
. O
Table O
7 O
analyses O
the O
diversity O
of O
predictions O
by O
the O
different O
models O
. O

MINER B-MethodName
: O
Improving O
Out B-TaskName
- I-TaskName
of I-TaskName
- I-TaskName
Vocabulary I-TaskName
Named I-TaskName
Entity I-TaskName
Recognition I-TaskName
from O
an O
Information O
Theoretic O
Perspective O

NER B-TaskName
model O
has O
achieved O
promising O
performance O
on O
standard O
NER B-TaskName
benchmarks O
. O
However O
, O
recent O
studies O
show O
that O
previous O
approaches O
may O
over O
- O
rely O
on O
entity O
mention O
information O
, O
resulting O
in O
poor O
performance O
on O
out B-TaskName
- I-TaskName
of I-TaskName
- I-TaskName
vocabulary I-TaskName
( I-TaskName
OOV I-TaskName
) I-TaskName
entity I-TaskName
recognition I-TaskName
. O
In O
this O
work O
, O
we O
propose O
MINER B-MethodName
, O
a O
novel O
NER B-TaskName
learning O
framework O
, O
to O
remedy O
this O
issue O
from O
an O
information O
- O
theoretic O
perspective O
. O
The O
proposed O
approach O
contains O
two O
mutual O
information O
- O
based O
training O
objectives O
: O
i O
) O
generalizing O
information O
maximization O
, O
which O
enhances O
representation O
via O
deep O
understanding O
of O
context O
and O
entity O
surface O
forms O
; O
ii O
) O
superfluous O
information O
minimization O
, O
which O
discourages O
representation O
from O
rote O
memorizing O
entity O
names O
or O
exploiting O
biased O
cues O
in O
data O
. O
Experiments O
on O
various O
settings O
and O
datasets O
demonstrate O
that O
it O
achieves O
better O
performance O
in O
predicting O
OOV O
entities O
. O

Introduction O

Named B-TaskName
Entity I-TaskName
Recognition I-TaskName
( O
NER B-TaskName
) O
aims O
to O
identify O
and O
classify O
entity O
mentions O
from O
unstructured O
text O
, O
e.g. O
, O
extracting O
location O
mention O
" O
Berlin O
" O
from O
the O
sentence O
" O
Berlin O
is O
wonderful O
in O
the O
winter O
" O
. O
NER B-TaskName
is O
a O
key O
component O
in O
information O
retrieval O
( O
Tan O
et O
al O
. O
, O
2021 O
) O
, O
question O
answering O
( O
Min O
et O
al O
. O
, O
2021 O
) O
, O
dialog O
systems O
, O
etc O
. O
Traditional O
NER B-TaskName
models O
are O
feature O
- O
engineering O
and O
machine O
learning O
based O
( O
Zhou O
and O
Su O
, O
2002 O
; O
Takeuchi O
and O
Collier O
, O
2002 O
; O
Agerri O
and O
Rigau O
, O
2016 O
) O
. O
Benefiting O
from O
the O
development O
of O
deep O
learning O
, O
neuralnetwork O
- O
based O
NER B-TaskName
models O
have O
achieved O
stateof O
- O
the O
- O
art O
results O
on O
several O
public O
benchmarks O
( O
Lample O
et O
al O
. O
, O
2016 O
; O
Peters O
et O
al O
. O
, O
2018 O
; O
Devlin O
et O
al O
. O
, O
2018 O
; O
Yamada O
et O
al O
. O
, O
2020 O
; O
Yan O
et O
al O
. O
, O
2021 O
) O
. O

Recent O
studies O
( O
Lin O
et O
al O
. O
, O
2020 O
; O
Agarwal O
et O
al O
. O
, O
2021 O
) O
show O
that O
, O
context O
does O
influence O
predictions O
Table O
1 O
: O
The O
comparison O
between O
the O
in O
- O
dictionary O
and O
out O
- O
of O
- O
dictionary O
parts O
of O
the O
CoNLL B-DatasetName
2003 I-DatasetName
baseline O
( O
Lin O
et O
al O
. O
, O
2020 O
) O
, O
which O
was O
tested O
on O
Bert O
- O
CRF O
. O
It O
is O
obvious O
that O
the O
performance O
gap O
between O
InDict O
and O
OutDict O
is O
significantly O
large O
. O

of O
NER B-TaskName
models O
, O
but O
the O
main O
factor O
driving O
high O
performance O
is O
learning O
the O
named O
tokens O
themselves O
. O
Consequently O
, O
NER B-TaskName
models O
underperform O
when O
predicting O
entities O
that O
have O
not O
been O
seen O
during O
training O
( O
Fu O
et O
al O
. O
, O
2020 O
; O
Lin O
et O
al O
. O
, O
2020 O
) O
, O
which O
is O
referred O
to O
as O
an O
Out O
- O
of O
- O
Vocabulary O
( O
OOV O
) O
problem O
. O

There O
are O
three O
classical O
strategies O
to O
alleviate O
the O
OOV O
problem O
: O
external O
knowledge O
, O
OOV O
word O
embedding O
, O
and O
contextualized O
embedding O
. O
The O
first O
one O
is O
to O
introduce O
additional O
features O
, O
e.g. O
, O
entity O
lexicons O
( O
Zhang O
and O
Yang O
, O
2018 O
) O
, O
part O
- O
ofspeech O
tags O
, O
which O
alleviates O
the O
model O
's O
dependence O
on O
word O
embeddings O
. O
However O
, O
the O
external O
knowledge O
is O
not O
always O
easy O
to O
obtain O
. O
The O
second O
strategy O
is O
to O
get O
a O
better O
OOV O
word O
embedding O
( O
Peng O
et O
al O
. O
, O
2019 O
; O
Fukuda O
et O
al O
. O
, O
2020 O
) O
. O
The O
strategy O
is O
learning O
a O
static O
OOV O
embedding O
representation O
, O
but O
not O
directly O
utilizing O
the O
context O
. O
Last O
one O
is O
fine O
- O
tune O
pre O
- O
trained O
models O
, O
e.g. O
, O
ELMo O
( O
Peters O
et O
al O
. O
, O
2018 O
) O
, O
BERT O
( O
Devlin O
et O
al O
. O
, O
2018 O
) O
, O
which O
provide O
contextualized O
word O
representations O
. O
Unfortunately O
, O
Agarwal O
et O
al O
. O
( O
2021 O
) O
shows O
that O
the O
higher O
performance O
of O
pretrained O
models O
could O
be O
the O
results O
of O
learning O
the O
subword O
structure O
better O
. O

How O
do O
we O
make O
the O
model O
focus O
on O
contextual O
information O
to O
tackle O
the O
OOV O
problem O
? O
Motivated O
by O
the O
information O
bottleneck O
principle O
( O
Tishby O
et O
al O
. O
, O
2000 O
) O
, O
we O
propose O
a O
novel O
learning O
framework O
-Mutual B-MethodName
Information I-MethodName
based I-MethodName
Named I-MethodName
Entity I-MethodName
Recognition I-MethodName
( O
MINER B-MethodName
) O
. O
The O
proposed O
method O
provides O
an O
information O
- O
theoretic O
perspective O
to O
the O
OOV O
problem O
by O
training O
an O
encoder O
to O
minimize O
task O
- O
irrelevant O
nuisances O
while O
keeping O
predictive O
information O
. O
Specifically O
, O
MINER B-MethodName
contains O
two O
mutual O
information O
based O
learning O
objectives O
: O
i O
) O
generalizing O
information O
maximization O
, O
which O
aims O
to O
maximize O
the O
mutual O
information O
between O
representations O
and O
well O
- O
generalizing O
features O
, O
i.e. O
, O
context O
and O
entity O
surface O
forms O
; O
ii O
) O
superfluous O
information O
minimization O
, O
which O
prevents O
the O
model O
from O
rote O
memorizing O
the O
entity O
names O
or O
exploiting O
biased O
cues O
via O
eliminating O
entity O
name O
information O
. O
Our O
codes O
1 O
are O
publicly O
available O
. O

Our O
main O
contributions O
are O
summarized O
as O
follows O
: O

1 O
. O
We O
propose O
a O
novel O
learning O
framework O
, O
i.e. O
, O
MINER B-MethodName
, O
from O
an O
information O
theory O
perspective O
, O
aiming O
to O
improve O
the O
robustness O
of O
entity O
changes O
by O
eliminating O
entity O
- O
specific O
and O
maximizing O
wellgeneralizing O
information O
. O

2 O
. O
We O
show O
its O
effectiveness O
on O
several O
settings O
and O
benchmarks O
, O
and O
suggest O
that O
MINER B-MethodName
is O
a O
reliable O
approach O
to O
better O
OOV B-TaskName
entity I-TaskName
recognition I-TaskName
. O

Background O

In O
this O
section O
, O
we O
highlight O
the O
information O
bottleneck O
principle O
. O
Subsequently O
, O
the O
analysis O
of O
possible O
issues O
was O
provided O
when O
applying O
it O
to O
OOV B-TaskName
entity I-TaskName
recognition I-TaskName
. O
Furthermore O
, O
we O
review O
related O
techniques O
in O
deriving O
our O
framework O
. O

Information O
Bottleneck O
( O
IB O
) O
principle O
originated O
in O
information O
theory O
, O
and O
provides O
a O
theoretical O
framework O
for O
analyzing O
deep O
neural O
networks O
. O
It O
formulates O
the O
goal O
of O
representation O
learning O
as O
an O
information O
trade O
- O
off O
between O
predictive O
power O
and O
representation O
compression O
. O
Given O
the O
input O
dataset O
( O
X O
, O
Y O
) O
, O
it O
seeks O
to O
learn O
the O
internal O
representation O
Z O
of O
some O
intermediate O
layers O
by O
: O

L O
IB O
= O
−I O
( O
Z O
; O
Y O
) O
+ O
β O
* O
I O
( O
Z O
; O
X O
) O
, O

where O
I O
represents O
the O
mutual O
information O
( O
MI O
) O
, O
a O
measure O
of O
the O
mutual O
dependence O
between O
the O
two O
variables O
. O
The O
trade O
- O
off O
between O
the O
two O
MI O
terms O
1 O
https O
: O
/ O
/ O
github.com O
/ O
BeyonderXX O
/ O
MINER B-MethodName
is O
controlled O
by O
the O
Lagrange O
multiplier O
β O
. O
A O
low O
loss O
indicates O
that O
representation O
Z O
does O
not O
keep O
too O
much O
information O
from O
X O
while O
still O
retaining O
enough O
information O
to O
predict O
Y. O
Section O
5 O
suggests O
that O
directly O
applying O
IB O
to O
NER B-TaskName
can O
not O
bring O
obvious O
improvement O
. O
We O
argue O
that O
IB O
can O
not O
guarantee O
well O
- O
generalizing O
representation O
. O

On O
the O
one O
hand O
, O
it O
has O
been O
shown O
that O
it O
is O
challenging O
to O
find O
a O
trade O
- O
off O
between O
high O
compression O
and O
high O
predictive O
power O
( O
Tishby O
et O
al O
. O
, O
2000 O
; O
Wang O
et O
al O
. O
, O
2019 O
; O
Piran O
et O
al O
. O
, O
2020 O
) O
. O
When O
compressing O
task O
- O
irrelevant O
nuisances O
, O
however O
, O
useful O
information O
will O
inevitably O
be O
left O
out O
. O
On O
the O
other O
hand O
, O
it O
is O
unclear O
for O
the O
IB O
principle O
which O
parts O
of O
features O
are O
well O
- O
generalizing O
and O
which O
are O
not O
, O
as O
we O
usually O
train O
a O
classifier O
to O
solely O
maximize O
accuracy O
. O
Consequently O
, O
neural O
networks O
tend O
to O
use O
any O
accessible O
signal O
to O
do O
so O
( O
Ilyas O
et O
al O
. O
, O
2019 O
) O
, O
which O
is O
referred O
to O
as O
a O
shortcut O
learning O
problem O
( O
Geirhos O
et O
al O
. O
, O
2020 O
) O
. O

For O
training O
sets O
with O
limited O
size O
, O
it O
may O
be O
easier O
for O
neural O
networks O
to O
memorize O
entity O
names O
rather O
than O
to O
classify O
them O
by O
context O
and O
common O
entity O
features O
( O
Agarwal O
et O
al O
. O
, O
2021 O
) O
. O
In O
Section O
4 O
, O
we O
demonstrate O
how O
we O
extend O
IB O
to O
the O
NER B-TaskName
task O
and O
address O
these O
issues O
. O

Model O
Architecture O

In O
recent O
years O
, O
NER B-TaskName
systems O
have O
undergone O
a O
paradigm O
shift O
from O
sequence O
labeling O
, O
which O
formulates O
NER B-TaskName
as O
a O
token O
- O
level O
tagging O
task O
( O
Chiu O
and O
Nichols O
, O
2016 O
; O
Akbik O
et O
al O
. O
, O
2018 O
; O
Yan O
et O
al O
. O
, O
2019 O
) O
, O
to O
span O
prediction O
( O
SpanNER B-MethodName
) O
, O
which O
regards O
NER B-TaskName
as O
a O
span O
- O
level O
classification O
task O
( O
Mengge O
et O
al O
. O
, O
2020 O
; O
Yamada O
et O
al O
. O
, O
2020 O
; O
Fu O
et O
al O
. O
, O
2021 O
) O
. O
We O
choose O
SpanNER B-MethodName
as O
base O
architecture O
for O
two O
reasons O
: O
1 O
) O
SpanNER B-MethodName
can O
yield O
the O
whole O
span O
representation O
, O
which O
can O
be O
directly O
used O
for O
optimize O
information O
. O
2 O
) O
Compared O
with O
sequence O
labeling O
, O
SpanNER B-MethodName
does O
better O
in O
sentences O
with O
more O
OOV O
words O
( O
Fu O
et O
al O
. O
, O
2021 O
) O
. O

Overall O
, O
SpanNER B-MethodName
consists O
of O
three O
major O
modules O
: O
token O
representation O
layer O
, O
span O
representation O
layer O
, O
and O
span O
classification O
layer O
. O
Besides O
, O
our O
method O
inserts O
a O
bottleneck O
layer O
to O
the O
architecture O
for O
information O
optimization O
. O

Token O
Representation O
Layer O

Let O
X O
= O
{ O
x O
1 O
, O
x O
2 O
, O
• O
• O
• O
, O
x O
n O
} O
represents O
the O
input O
sentence O
, O
thus O
, O
the O
token O
representation O
h O
i O
is O
as O
follows O
: O

u O
1 O
, O
• O
• O
• O
, O
u O
n O
= O
Embedding O
( O
x O
1 O
, O
• O
• O
• O
, O
x O
n O
) O
( O
1 O
) O
h O
1 O
, O
• O
• O
• O
, O
h O
n O
= O
Encoder O
( O
u O
1 O
, O
• O
• O
• O
, O
u O
n O
) O
( O
2 O
) O

where O
Embedding O
( O
) O
is O
the O
non O
- O
contextualized O
word O
embeddings O
, O
e.g. O
, O
Glove O
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
or O
contextualized O
word O
embeddings O
, O
e.g. O
, O
ELMo O
( O
Peters O
et O
al O
. O
, O
2018 O
) O
, O
BERT O
( O
Devlin O
et O
al O
. O
, O
2018 O
) O
. O
Encoder O
( O
) O
can O
be O
any O
network O
structures O
with O
context O
encoding O
function O
, O
e.g. O
, O
LSTM O
( O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
) O
, O
CNN O
( O
LeCun O
et O
al O
. O
, O
1995 O
) O
, O
transformer O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
, O
and O
so O
on O
. O

Span O
Representation O
Layer O

For O
all O
possible O
spans O
S O
= O
{ O
s O
1 O
, O
s O
2 O
, O
• O
• O
• O
, O
s O
m O
} O
of O
sentence O
X O
, O
we O
re O
- O
assign O
a O
label O
y O
∈ O
Y O
for O
each O
span O
. O
Take O
" O
Berlin O
is O
wonderful O
" O
as O
an O
example O
, O
its O
possible O
spans O
and O
labels O
are O

{ O
( O
1 O
, O
1 O
) O
, O
( O
1 O
, O
2 O
) O
, O
( O
1 O
, O
3 O
) O
, O
( O
2 O
, O
2 O
) O
, O
( O
2 O
, O
3 O
) O
, O
( O
3 O
, O
3 O
) O
} O
and O
{ O
LOC O
, O
O O
, O
O O
, O
O O
, O
O O
, O
O O
} O
, O
respectively O
. O

Given O
the O
start O
index O
b O
i O
and O
end O
index O
e O
i O
, O
the O
representation O
of O
span O
s O
i O
can O
be O
calculated O
by O
two O
parts O
: O
boundary O
embedding O
and O
span O
length O
embedding O
. O

Boundary O
embedding O
: O
This O
part O
is O
calculated O
by O
concatenating O
the O
start O
and O
end O
tokens O
' O
representation O

t O
b O
i O
= O
[ O
h O
b O
i O
; O
h O
e O
i O
] O
. O

Span O
length O
embedding O
: O

In O
order O
to O
introduce O
the O
length O
feature O
, O
we O
additionally O
provide O
the O
length O
embedding O
t O
l O
i O
, O
which O
can O
be O
obtained O
by O
a O
learnable O
look O
- O
up O
table O
. O

Finally O
, O
the O
span O
representation O
can O
be O
obtained O
as O
: O

t O
i O
= O
[ O
t O
b O
i O
; O
t O
l O
i O
] O
. O

Information O
Bottleneck O
Layer O

In O
order O
to O
optimize O
the O
information O
in O
the O
span O
representation O
, O
our O
method O
additionally O
adds O
an O
information O
bottleneck O
layer O
of O
the O
form O
: O

p O
( O
z|t O
) O
= O
N O
z O
| O
f O
µ O
e O
( O
t O
) O
, O
f O
Σ O
e O
( O
t O
) O
( O
3 O
) O

where O
f O
e O
is O
an O
MLP O
which O
outputs O
both O
the O
Kdimensional O
mean O
µ O
of O
z O
as O
well O
as O
the O
K O
* O
K O
covariance O
matrix O
Σ. O
Then O
we O
can O
use O
the O
reparameterization O
trick O
( O
( O
Kingma O
and O
Welling O
, O
2013 O
) O
) O
to O
get O
the O
compressed O
representation O
z O
i O
. O

Span O
Classification O
Layer O

Once O
the O
information O
bottleneck O
layer O
is O
finished O
, O
z O
i O
is O
fed O
into O
the O
classifier O
to O
obtain O
the O
probability O
of O
its O
label O
y O
i O
. O
Based O
on O
the O
probability O
, O
the O
basic O
loss O
function O
can O
be O
calculated O
as O
follows O
: O

L O
base O
= O
− O
score O
( O
z O
i O
, O
y O
i O
) O
y O
′ O
∈Y O
score O
( O
z O
i O
, O
y O
′ O
) O
, O
( O
4 O
) O

where O
score O
( O
) O
is O
a O
function O
that O
measures O
the O
compatibility O
between O
a O
specified O
label O
and O
a O
span O
representation O
: O

score O
( O
z O
i O
, O
y O
k O
) O
= O
exp O
( O
z O
T O
i O
y O
k O
) O
, O
( O
5 O
) O

where O
y O
k O
is O
a O
learnable O
representation O
of O
class O
k. O
Heuristic O
Decoding O
A O
heuristic O
decoding O
solution O
for O
the O
flat O
NER B-TaskName
is O
provided O
to O
avoid O
the O
prediction O
of O
over O
- O
lapped O
spans O
. O
For O
those O
overlapped O
spans O
, O
we O
keep O
the O
span O
with O
the O
highest O
prediction O
probability O
and O
drop O
the O
others O
. O

It O
's O
worth O
noting O
that O
our O
method O
is O
flexible O
and O
can O
be O
used O
with O
any O
other O
NER B-TaskName
model O
based O
on O
span O
classification O
. O
In O
next O
section O
, O
we O
will O
introduce O
two O
additional O
objectives O
to O
tackle O
the O
OOV O
problem O
of O
NER B-TaskName
. O

MI O
- O
based O
objectives O

Motivated O
by O
IB O
( O
Tishby O
et O
al O
. O
, O
2000 O
; O
Federici O
et O
al O
. O
, O
2020 O
) O
, O
we O
can O
subdivide O
I O
( O
X O
; O
Z O
) O
into O
two O
components O
by O
using O
the O
chain O
rule O
of O
mutual O
information O
( O
MI O
) O
: O

I O
( O
X O
; O
Z O
) O
= O
I O
( O
Y O
; O
Z O
) O
predictive O
+ O
I O
( O
X O
; O
Z|Y O
) O
superf O
luous O
, O
( O
6 O
) O

The O
first O
term O
determines O
how O
much O
information O
about O
Y O
is O
accessible O
from O
Z. O
While O
the O
second O
term O
, O
conditional O
mutual O
information O
term O
I O
( O
X O
; O
Z|Y O
) O
, O
denotes O
the O
information O
in O
Z O
that O
is O
not O
predictive O
of O
Y O
. O

For O
NER B-TaskName
, O
which O
parts O
of O
the O
information O
retrieved O
from O
input O
are O
useful O
and O
which O
are O
redundant O
? O

From O
human O
intuition O
, O
text O
context O
should O
be O
the O
main O
predictive O
information O
for O
NER B-TaskName
. O
For O
example O
, O
" O
The O
CEO O
of O
X O
resigned O
" O
, O
the O
type O
of O
X O
in O
each O
of O
these O
contexts O
should O
always O
be O
" O
ORG O
" O
. O
Besides O
, O
entity O
mentions O
also O
provide O
much O
information O
for O
entity O
recognition O
. O
For O
example O
, O
nearly O
all O
person O
names O
capitalize O
the O
first O
letter O
and O
follow O
the O
" O
firstName O
lastName O
" O
or O
" O
lastName O

! O
! O
" O
Encoder O
Encoder O
Shared O
IB O
Shared O
! O
~ O
( O
! O
| O
! O
) O
" O
~ O
( O
" O
| O
" O
) O
" O
min O
# O
$ O
Classifier O
Shared O
IB O
Classifier O
max O
( O
! O
; O
" O
) O

Figure O
1 O
: O
Visualization O
of O
MINER B-MethodName
, O
where O
x O
1 O
and O
x O
2 O
share O
the O
same O
context O
and O
entity O
labels O
, O
while O
their O
entity O
words O
are O
different O
. O
z O
1 O
and O
z O
2 O
are O
compressed O
entity O
representations O
sampled O
by O
p O
( O
z O
1 O
|x O
1 O
) O
and O
p O
( O
z O
2 O
|x O
2 O
) O
, O
respectively O
, O
which O
are O
implemented O
by O
information O
bottleneck O
( O
IB O
) O
layer O
. O
Our O
method O
add O
two O
additional O
learning O
objectives O
to O
basic O
architecture O
. O
The O
first O
one O
is O
to O
maximize O
the O
mutual O
information O
, O
i.e. O
, O
I O
( O
z O
1 O
; O
z O
2 O
) O
, O
to O
enhance O
context O
information O
and O
entity O
surface O
form O
information O
of O
z O
1 O
and O
z O
2 O
. O
The O
second O
objective O
is O
to O
minimize O
the O
Jensen O
- O
Shannon O
divergence O
, O
representing O
an O
upper O
bound O
of O
I O
( O
x O
1 O
; O
z O
1 O
|x O
2 O
) O
, O
aiming O
to O
eliminate O
task O
- O
irrelevant O
nuisances O
. O

firstName O
" O
patterns O
. O
However O
, O
entity O
name O
is O
not O
a O
well O
- O
generalizing O
features O
. O
By O
simply O
memorizing O
the O
fact O
which O
span O
is O
an O
entity O
, O
it O
may O
be O
possible O
for O
it O
to O
fit O
the O
training O
set O
, O
but O
it O
is O
impossible O
to O
predict O
entities O
that O
have O
never O
been O
seen O
before O
. O

We O
convert O
the O
targets O
of O
Eq O
. O
( O
6 O
) O
into O
a O
form O
that O
is O
easier O
to O
solve O
via O
a O
contrastive O
strategy O
. O
Specifically O
, O
consider O
x O
1 O
and O
x O
2 O
are O
two O
contrastive O
samples O
of O
similar O
context O
, O
and O
contains O
different O
entity O
mentions O
of O
the O
same O
entity O
category O
, O
i.e. O
, O
s O
1 O
and O
s O
2 O
, O
respectively O
. O
Assuming O
both O
x O
1 O
and O
x O
2 O
are O
both O
sufficient O
for O
inferring O
label O
y. O
The O
mutual O
information O
between O
x O
1 O
and O
z O
1 O
can O
be O
factorized O
to O
two O
parts O
. O

I O
( O
x O
1 O
; O
z O
1 O
) O
= O
I O
( O
z O
1 O
; O
x O
2 O
) O
consistent O
+ O
I O
( O
x O
1 O
; O
z O
1 O
|x O
2 O
) O
specif O
ic O
, O
( O
7 O
) O

where O
z O
1 O
and O
z O
2 O
are O
span O
representations O
of O
s O
1 O
and O
s O
2 O
, O
respectively O
, O
I O
( O
z O
1 O
; O
x O
2 O
) O
denotes O
the O
information O
that O
is O
n't O
entity O
- O
specific O
. O
And O
I O
( O
x O
1 O
; O
z O
1 O
|x O
2 O
) O
represents O
the O
information O
in O
z O
1 O
which O
is O
unique O
to O
x O
1 O
but O
is O
not O
predictable O
by O
sentence O
x O
2 O
, O
i.e. O
, O
entityspecific O
information O
. O

Thus O
any O
representation O
z O
containing O
all O
information O
shared O
from O
both O
sentences O
would O
also O
contain O
the O
necessary O
label O
information O
, O
and O
sentencespecific O
information O
is O
superfluous O
. O
So O
Eq O
. O
( O
6 O
) O
can O
be O
approximated O
by O
Eq O
. O
( O
7 O
) O
by O
: O

maximize O
I O
( O
z O
1 O
; O
y O
) O
∼ O
I O
( O
z O
1 O
; O
x O
2 O
) O
, O
( O
8 O

) O

minimize O
I O
( O
x O
1 O
; O
z O
1 O
|y O
) O
∼ O
I O
( O
x O
1 O
; O
z O
1 O
|x O
2 O
) O
, O
( O
9 O

) O

The O
target O
of O
Eq O
. O
( O
8 O
) O
is O
defined O
as O
generaliz O
- O
ing O
information O
maximization O
. O
We O
proved O
that O
I O
( O
z O
1 O
; O
z O
2 O
) O
is O
a O
lower O
bound O
of O
I O
( O
z O
1 O
; O
x O
2 O
) O
( O
proof O
could O
be O
found O
in O
appendix O
7 O
) O
. O
InfoNCE O
( O
Oord O
et O
al O
. O
, O
2018 O
) O
was O
used O
as O
a O
lower O
bound O
on O
MI O
and O
can O
be O
used O
to O
approximate O
I O
( O
z O
1 O
; O
z O
2 O
) O
. O
Subsequently O
, O
it O
can O
be O
optimized O
by O
: O

Lgi O
= O
−Ep O
gw O
( O
z1 O
, O
z2 O
) O
− O
E O
p O
′ O
log O
z O
′ O
exp O
gw O
( O
z1 O
, O
z O
′ O
) O
, O
( O
10 O
) O

where O
g O
w O
( O
• O
, O
• O
) O
is O
a O
compatible O
score O
function O
approximated O
by O
a O
neural O
network O
, O
z O
2 O
are O
the O
positive O
entity O
representations O
from O
the O
joint O
distribution O
p O
of O
original O
sample O
and O
corresponding O
generated O
sample O
, O
z O
′ O
are O
the O
negative O
entity O
representations O
drawn O
from O
the O
joint O
distribution O
of O
the O
original O
sample O
and O
other O
samples O
. O

The O
target O
of O
Eq O
. O
( O
9 O
) O
is O
defined O
as O
superfluous O
information O
minimization O
. O
To O
restrict O
this O
term O
, O
we O
can O
minimize O
an O
upper O
bound O
of O
I O
( O
x O
1 O
; O
z O
1 O
|x O
2 O
) O
( O
proofs O
could O
be O
found O
in O
appendix O
7 O
) O
as O
follows O
: O

L O
si O
= O
E O
x O
1 O
, O
x O
2 O
E O
z O
1 O
, O
z O
2 O
[ O
D O
JS O
[ O
p O
z O
1 O
||p O
z O
2 O
] O
] O
, O
( O
11 O
) O

where O
D O
JS O
means O
Jensen O
- O
Shannon O
divergence O
, O
p O
z O
1 O
and O
p O
z O
2 O
represent O
p O
( O
z O
1 O
|x O
1 O
) O
and O
p O
( O
z O
2 O
|x O
2 O
) O
, O
respectively O
. O
In O
practice O
, O
Eq O
. O
( O
11 O
) O
encourage O
z O
to O
be O
invariant O
to O
entity O
changes O
. O
The O
resulting O
Mutual B-MethodName
Information I-MethodName
based I-MethodName
Named I-MethodName
Entity I-MethodName
Recognition I-MethodName
model O
is O
visualized O
in O
Figure O
1 O
. O

Contrastive O
sample O
generation O

It O
is O
difficult O
to O
obtain O
samples O
with O
similar O
contexts O
but O
different O
entity O
words O
. O
We O
generate O
contrastive O
samples O
by O
the O
mention O
replacement O
mechanism O
( O
Dai O
and O
Adel O
, O
2020 O
) O
. O
For O
each O
mention O
in O
the O
sentence O
, O
we O
replace O
it O
by O
another O
mention O
from O
the O
original O
training O
set O
, O
which O
has O
the O
same O
entity O
type O
. O
The O
corresponding O
span O
label O
can O
be O
changed O
accordingly O
. O
For O
example O
, O
" O
LOC O
" O
mention O
" O
Berlin O
" O
in O
sentence O
" O
Berlin O
is O
wonderful O
in O
the O
winter O
" O
is O
replaced O
by O
" O
Iceland O
" O
. O

Training O

Combine O
Eq O
. O
( O
4 O
) O
, O
( O
10 O
) O
, O
and O
( O
11 O
) O
, O
we O
can O
get O
the O
following O
objective O
function O
, O
which O
try O
to O
minimize O
: O

L O
= O
L O
base O
+ O
γ O
* O
L O
gi O
+ O
β O
* O
L O
si O
, O
( O
12 O
) O

where O
γ O
and O
β O
are O
the O
weights O
of O
the O
generalizing O
information O
loss O
and O
superfluous O
information O
loss O
, O
respectively O
. O

Experiment O

In O
this O
section O
, O
we O
verify O
the O
performance O
of O
the O
proposed O
method O
on O
five O
OOV O
datasets O
, O
and O
compared O
it O
with O
other O
methods O
. O
In O
addition O
, O
We O
tested O
the O
universality O
of O
the O
proposed O
method O
in O
various O
pre O
- O
trained O
models O
. O

Datasets O
and O
Metrics O

Datasets O
We O
performed O
experiments O
on O
: O

1 O
. O
WNUT2017 B-DatasetName
( O
Derczynski O
et O
al O
. O
, O
2017 O
) O
, O
a O
dataset O
focus O
on O
unusual O
, O
previous O
- O
unseen O
entities O
in O
training O
data O
, O
and O
is O
collected O
from O
social O
media O
. O

2 O
. O
TwitterNER B-DatasetName
, O
an O
English O
NER B-TaskName
dataset O
created O
from O
Tweets O
. O

3 O
. O
BioNER B-DatasetName
( O
Kim O
et O
al O
. O
, O
2004 O
) O
, O
the O
JNLPBA O
2004 O
Bio O
- O
NER O
dataset O
focus O
on O
technical O
terms O
in O
the O
biology O
domain O
. O

4 O
. O
Conll03 B-DatasetName
- I-DatasetName
Typos I-DatasetName
, O
which O
is O
generated O
from O
Conll2003 O
( O
Sang O
and O
De O
Meulder O
, O
2003 O
) O
. O
The O
entities O
in O
the O
test O
set O
are O
replaced O
by O
typos O
version O
( O
character O
modify O
, O
insert O
, O
and O
delete O
operation O
) O
. O

5 O
. O
Conll03 B-DatasetName
- I-DatasetName
OOV I-DatasetName
, O
which O
is O
generated O
from O
Conll2003 O
( O
Sang O
and O
De O
Meulder O
, O
2003 O
) O
. O
The O
entities O
in O
the O
test O
set O
are O
replaced O
by O
another O
out O
- O
of O
- O
vocabulary O
entity O
in O
test O
set O
. O

Table O
2 O
reports O
the O
statistic O
results O
of O
the O
OOV O
problem O
on O
the O
test O
sets O
of O
each O
dataset O
. O
As O
shown O
in O
the O
table O
, O
the O
test O
set O
of O
these O
datasets O
comprises O
a O
substantial O
amount O
of O
OOV O
entities O
. O

Metrics O
We O
measured O
the O
entity O
- O
level O
micro O
average O
F1 B-MetricName
score O
on O
the O
test O
set O
to O
compare O
the O
results O
of O
different O
models O
. O
2020 O
) O
share O
the O
same O
intuition O
as O
us O
, O
enriching O
word O
representations O
with O
context O
. O
However O
, O
the O
work O
is O
neither O
open O
source O
nor O
reported O
on O
the O
same O
dataset O
, O
so O
this O
method O
can O
not O
be O
compared O
with O
MINER B-MethodName
. O
We O
compare O
our O
method O
with O
baselines O
as O
follows O
: O
• O
Vanilla B-MethodName
information I-MethodName
bottleneck I-MethodName
( O
VaniIB B-MethodName
) O
, O
a O
method O
employs O
the O
original O
information O
bottleneck O
constraint O
to O
the O
SpanNER B-MethodName
, O
which O
is O
optimized O
based O
on O
Alemi O
et O
al O
. O
( O
2016 O
) O
. O
Compared O
with O
our O
method O
, O
it O
directly O
compresses O
all O
the O
information O
from O
the O
input O
. O
• O
Li O
et O
al O
. O
( O
2021 O
) O
( O
MIN B-MethodName
) O
, O
which O
utilizes O
both O
segment O
- O
level O
information O
and O
word O
- O
level O
dependencies O
, O
and O
incorporates O
an O
interaction O
mechanism O
to O
support O
information O
sharing O
between O
boundary O
detection O
and O
type O
prediction O
, O
enhancing O
the O
performance O
for O
the O
NER B-TaskName
task O
. O

Baseline O
methods O

Li O
et O
al O
. O
( O

• O
Fukuda O
et O
al O
. O
( O
2020 O
) O
( O
CoFEE B-MethodName
) O
, O
which O
refer O
to O
pre O
- O
trained O
word O
embeddings O
for O
known O
words O
with O
similar O
surfaces O
to O
target O
OOV O
words O
. O

• O
Nie O
et O
al O
. O
( O
2020 O
) O
( O
SA B-MethodName
- I-MethodName
NER I-MethodName
) O
, O
which O
utilize O
semantic O
enhancement O
methods O
to O
reduce O
the O
negative O
impact O
of O
data O
sparsity O
problems O
. O
Specifically O
, O
the O
method O
obtains O
the O
augmented O
semantic O
information O
from O
a O
largescale O
corpus O
, O
and O
proposes O
an O
attentive O
semantic O
augmentation O
module O
and O
a O
gate O
module O
to O
encode O
and O
aggregate O
such O
information O
, O
respectively O
. O

To O
verify O
the O
universality O
of O
our O
method O
, O
we O
measured O
its O
performance O
on O
various O
pre O
- O
trained O
models O
, O
i.e. O
, O
Bert B-MethodName
( O
Devlin O
et O
al O
. O
, O
2018 O
) O
, O
Roberta B-MethodName
( O
Liu O
et O
al O
. O
, O
2019 O
) O
, O
Albert B-MethodName
( O
Lan O
et O
al O
. O
, O
2019 O
) O
. O

Implementation O
Details O

Bert B-MethodName
- O
large O
released O
by O
Devlin O
et O
al O
. O
( O
2018 O
) O
is O
selected O
as O
our O
base O
encoder O
. O
The O
learning B-HyperparameterName
rate I-HyperparameterName
is O
set O
to O
5e-5 B-HyperparameterValue
, O
and O
the O
dropout B-HyperparameterName
is O
set O
to O
0.2 B-HyperparameterValue
. O
The O
output B-HyperparameterName
dim I-HyperparameterName
of O
the O
information O
bottleneck O
layer O
is O
50 B-HyperparameterValue
. O
In O
order O
to O
make O
a O
trade O
- O
off O
for O
the O
performance O
and O
efficiency O
, O
on O
the O
one O
hand O
, O
we O
truncate O
the O
part O
of O
the O
sentence O
whose O
tokens O
exceeds O
128 O
. O
On O
the O
other O
hand O
, O
we O
count O
the O
length O
distribution O
of O
entity O
length O
in O
different O
datasets O
, O
and O
finally O
choose O
4 B-HyperparameterValue
as O
the O
maximum B-HyperparameterName
enumerated I-HyperparameterName
entity I-HyperparameterName
length I-HyperparameterName
. O
The O
values O
of O
β B-HyperparameterName
and O
γ B-HyperparameterName
differ O
for O
different O
datasets O
. O
Empirically O
, O
1e-5 B-HyperparameterValue
for O
β B-HyperparameterName
and O
0.01 B-HyperparameterValue
for O
γ B-HyperparameterName
can O
get O
promised O
results O
. O
The O
model O
is O
trained O
in O
an O
NVIDIA O
GeForce O
RTX O
2080Ti O
GPU O
. O
Checkpoints O
with O
top-3 O
performance O
are O
finally O
evaluated O
on O
the O
test O
set O
to O
report O
averaged O
results O
. O

Main O
Results O

We O
demonstrate O
the O
effectiveness O
of O
MINER B-MethodName
against O
other O
state O
- O
of O
- O
the O
- O
art O
models O
. O
As O
shown O
in O
table O
3 O
, O
we O
conducted O
the O
following O
comparison O
and O
analysis O
: O

1 O
) O
Our O
baseline O
model O
, O
i.e. O
, O
SpanNER B-MethodName
, O
does O
an O
excellent O
job O
of O
predicting O
OOV O
entities O
. O
Compared O
with O
sequence O
labeling O
, O
the O
span O
classification O
could O
model O
the O
relation O
of O
entity O
tokens O
directly O
; O
2 O
) O
The O
performance O
of O
SpanNER B-MethodName
is O
further O
boosted O
with O
our O
proposed O
approach O
, O
which O
proved O
the O
effectiveness O
of O
our O
method O
. O
As O
shown O
in O
table O
3 O
, O
MINER B-MethodName
almost O
outperforms O
all O
other O
SOTA O
methods O
without O
any O
external O
resource O
; O
3 O
) O
Compared O
with O
Typos O
data O
transformation O
, O
it O
is O
more O
difficult O
for O
models O
to O
predict O
OOV O
words O
. O
The O
results O
are O
obtained O
by O
testing O
MINER B-MethodName
( O
Bert B-MethodName
large O
) O
on O
TwitterNER B-DatasetName
. O
We O
fix O
β B-HyperparameterName
= O
1e03 B-HyperparameterValue
, O
and O
the O
orange O
line O
is O
f1 B-MetricName
score O
when O
γ B-HyperparameterName
= O
0 B-HyperparameterValue
. O
The O
results O
are O
obtained O
by O
testing O
MINER B-MethodName
( O
Bert B-MethodName
large O
) O
on O
TwitterNER B-DatasetName
. O
We O
fix O
γ B-HyperparameterName
= O
1e04 B-HyperparameterValue
, O
and O
the O
orange O
line O
is O
f1 B-MetricName
score O
when O
β B-HyperparameterName
= O
0 B-HyperparameterValue
. O

To O
pre O
- O
trained O
model O
, O
typos O
word O
may O
not O
appear O
in O
training O
set O
, O
but O
they O
share O
most O
subwords O
with O
the O
original O
token O
. O
Moreover O
, O
the O
subword O
of O
OOV O
entity O
may O
be O
rare O
; O
4 O
) O
It O
seems O
that O
the O
traditional O
information O
bottleneck O
will O
not O
significantly O
improve O
the O
OOV B-TaskName
prediction I-TaskName
ability O
of O
the O
model O
. O
We O
argue O
that O
the O
traditional O
information O
bottlenecks O
will O
indiscriminately O
compress O
the O
information O
in O
the O
representation O
, O
leading O
to O
underfitting O
; O
5 O
) O
Our O
model O
has O
significantly O
improved O
the O
performance O
of O
the O
model O
on O
the O
entity O
perturbed O
methods O
of O
typos O
and O
OOV O
, O
demonstrating O
that O
MI O
improve O
the O
robustness O
substantially O
in O
the O
face O
of O
noise O
; O
6 O
) O
It O
is O
clear O
that O
our O
proposed O
method O
is O
universal O
and O
can O
further O
improve O
OOV B-TaskName
prediction I-TaskName
performance O
for O
different O
embedding O
models O
, O
as O
we O
get O
improvements O
on O
Bert B-MethodName
, O
Roberta B-MethodName
, O
and O
Albert B-MethodName
stably O
. O

Ablation O
Study O

We O
also O
perform O
ablation O
studies O
to O
validate O
the O
effectiveness O
of O
each O
part O
in O
MINER B-MethodName
. O
demonstrates O
the O
results O
of O
different O
settings O
for O
the O
proposed O
training O
strategy O
equipped O
with O
BERT B-MethodName
. O

After O
only O
adding O
the O
L O
gi O
loss O
to O
enhance O
context O
and O
entity O
surface O
form O
information O
, O
we O
find O
that O
the O
results O
are O
better O
than O
the O
original O
PLMs B-MethodName
. O
A O
similar O
phenomenon O
occurs O
in O
L O
si O
, O
too O
. O
It O
reflects O
that O
both O
L O
gi O
and O
L O
si O
are O
beneficial O
to O
improve O
the O
generalizing O
ability O
on O
OOV B-TaskName
entities I-TaskName
recognition I-TaskName
. O
Moreover O
, O
the O
results O
on O
the O
three O
datasets O
are O
significantly O
improved O
by O
adding O
both O
L O
gi O
and O
L O
si O
learning O
objectives O
. O
It O
means O
L O
gi O
and O
L O
si O
can O
boost O
each O
over O
, O
which O
proves O
that O
our O
method O
enhances O
representation O
via O
deep O
understanding O
of O
context O
and O
entity O
surface O
forms O
and O
discourages O
representation O
from O
rote O
memorizing O
entity O
names O
or O
exploiting O
biased O
cues O
in O
data O
. O

Sensitivity O
Analysis O
of O
β B-HyperparameterName
and O
γ B-HyperparameterName

To O
show O
the O
different O
influence O
of O
our O
proposed O
training O
objectives O
L O
gi O
and O
L O
si O
, O
we O
conduct O
sensitivity O
analysis O
of O
the O
coefficient O
β B-HyperparameterName
and O
γ B-HyperparameterName
. O
Figure O
2 O
shows O
the O
performance O
change O
under O
different O
settings O
of O
the O
two O
coefficients O
. O
The O
yellow O
line O
denotes O
ablation O
results O
without O
the O
corresponding O
loss O
functions O
( O
with O
β=0 B-HyperparameterName
or O
γ=0 B-HyperparameterName
) O
. O
From O
Figure O
2 O
we O
can O
observe O
that O
the O
performance O
is O
significantly O
enhanced O
with O
a O
small O
rate O
of O
β B-HyperparameterName
or O
γ B-HyperparameterName
, O
where O
the O
best O
performance O
is O
achieved O
when O
β=1e-3 B-HyperparameterName
and O
γ=1e-4 B-HyperparameterName
, O
respectively O
. O
It O
probes O
the O
effectiveness O
of O
our O
proposed O
training O
objectives O
that O
enhances O
representation O
via O
deep O
understanding O
of O
context O
and O
entity O
surface O
forms O
and O
discourages O
representation O
from O
rote O
memorizing O
entity O
names O
or O
exploiting O
biased O
cues O
in O
data O
. O
As O
the O
coefficient O
rate O
increases O
continuously O
, O
the O
performance O
shows O
a O
declining O
trend O
, O
which O
means O
the O
over O
- O
constraint O
of O
L O
gi O
or O
L O
si O
will O
hurt O
the O
generalizing O
ability O
of O
predicting O
the O
OOV O
entities O
. O

Interpretable O
Analysis O

The O
above O
experiments O
show O
the O
promising O
performance O
of O
MINER B-MethodName
on O
predicting O
the O
unseen O
entities O
. O
To O
further O
investigate O
which O
part O
of O
the O
sentence O
MINER B-MethodName
focuses O
on O
, O
we O
visualize O
the O
attention O
weights O
over O
entities O
and O
contexts O
. O
We O
demonstrate O
an O
example O
in O
Figure O
4 O
, O
where O
is O
selected O
from O
TwitterNER B-DatasetName
. O
The O
attention O
score O
is O
calculated O
by O
averaging O
the O
attention O
weight O
of O
the O
0th O
layer O
of O
BERT B-MethodName
. O
Take O
the O
attention O
weights O
of O
the O
entity O
" O
State O
Street O
" O
as O
an O
example O
, O
it O
is O
obvious O
that O
baseline O
model O
, O
i.e. O
, O
SpanNER B-MethodName
, O
focus O
on O
entity O
words O
themselves O
. O
While O
the O
scores O
of O
our O
model O
are O
more O
average O
, O
it O
means O
that O
our O
method O
concerns O
more O
context O
information O
. O

6 O
Related O
Work O

External O
Knowledge O

This O
group O
of O
methods O
makes O
it O
easier O
to O
predict O
OOV O
entities O
using O
external O
knowledge O
. O
Zhang O
and O
Yang O
( O
2018 O
) O
utilize O
a O
dictionary O
to O
list O
numerous O
entity O
mentions O
. O
It O
is O
possible O
to O
get O
stronger O
" O
lookup O
" O
models O
by O
integrating O
dictionary O
information O
, O
but O
there O
is O
no O
guarantee O
that O
entities O
outside O
the O
training O
set O
and O
vocabulary O
will O
be O
correctly O
identified O
. O
To O
diminish O
the O
model O
's O
dependency O
on O
OOV O
embedding O
, O
introduce O
partof O
- O
speech O
tags O
. O
External O
resources O
are O
not O
always O
available O
, O
which O
is O
a O
limitation O
of O
this O
strategy O
. O

OOV O
word O
Embedding O

The O
OOV O
problem O
can O
be O
alleviated O
by O
improving O
the O
OOV O
word O
embedding O
. O
The O
character O
ngram O
of O
each O
word O
is O
used O
by O
Bojanowski O
et O
al O
. O
( O
2017 O
) O
to O
represent O
the O
OOV O
word O
embedding O
. O
Pinter O
et O
al O
. O
( O
2017 O
) O
captures O
morphological O
features O
using O
character O
- O
level O
RNN O
. O
Another O
technique O
is O
to O
first O
match O
the O
OOV O
words O
with O
the O
words O
that O
have O
been O
seen O
in O
training O
, O
then O
replace O
the O
OOV O
words O
' O
embedding O
with O
the O
seen O
words O
' O
embedding O
. O
Peng O
et O
al O
. O
( O
2019 O
) O
trains O
a O
student O
network O
to O
predict O
the O
closest O
word O
representation O
to O
the O
OOV O
term O
. O
Fukuda O
et O
al O
. O
( O
2020 O
) O
referring O
to O
pre O
- O
trained O
word O
embeddings O
for O
known O
words O
with O
similar O
surfaces O
to O
target O
OOV O
words O
. O
This O
kind O
of O
method O
is O
learning O
a O
static O
OOV O
embedding O
representation O
, O
and O
does O
not O
directly O
utilize O
the O
context O
. O

Contextualized O
Embedding O

Contextual O
information O
is O
used O
to O
enhance O
the O
representation O
of O
OOV O
words O
in O
this O
strategy O
. O
( O
Hu O
et O
al O
. O
, O
2019 O
) O
formulate O
the O
OOV O
problem O
as O
a O
Kshot O
regression O
problem O
and O
learns O
to O
predict O
the O
OOV O
embedding O
by O
aggregating O
only O
K O
contexts O
and O
morphological O
features O
. O
Pre O
- O
trained O
models O
contextualized O
word O
embeddings O
via O
pretraining O
on O
large O
background O
corpora O
. O
Furthermore O
, O
contextualized O
word O
embeddings O
can O
be O
provided O
by O
the O
pre O
- O
trained O
models O
, O
which O
are O
pre O
- O
trained O
on O
large O
background O
corpora O
( O
Peters O
et O
al O
. O
, O
2018 O
; O
Devlin O
et O
al O
. O
, O
2018 O
; O
Liu O
et O
al O
. O
, O
2019 O
) O
. O
Yan O
et O
al O
. O
( O
2021 O
) O
shows O
that O
BERT B-MethodName
is O
not O
always O
better O
at O
capturing O
context O
as O
compared O
to O
Gloe O
- O
based O
BiLSTM O
- O
CRFs O
. O

Their O
higher O
performance O
could O
be O
the O
result O
of O
learning O
the O
subword O
structure O
better O
. O

Conclusion O

Based O
on O
the O
recent O
studies O
of O
NER B-TaskName
, O
we O
analyze O
how O
to O
improve O
the O
OOV B-TaskName
entity I-TaskName
recognition I-TaskName
. O
In O
this O
work O
, O
we O
propose O
a O
novel O
and O
flexible O
learning O
framework O
-MINER B-MethodName
, O
to O
tackle O
OOV B-TaskName
entities I-TaskName
recognition I-TaskName
issue O
from O
an O
information O
- O
theoretic O
perspective O
. O
On O
the O
one O
hand O
, O
this O
method O
can O
enhance O
the O
context O
information O
of O
the O
output O
of O
the O
encoder O
. O
On O
the O
other O
hand O
, O
it O
can O
safely O
eliminate O
task O
- O
irrelevant O
nuisances O
and O
prevents O
the O
model O
from O
rote O
memorizing O
the O
entities O
. O
Specifically O
, O
the O
proposed O
approach O
contains O
two O
mutual O
information O
based O
training O
objectives O
: O
generalizing O
information O
maximization O
, O
and O
superfluous O
information O
minimization O
. O
Experiments O
on O
various O
datasets O
demonstrate O
that O
MINER B-MethodName
achieves O
much O
better O
performance O
in O
predicting O
out O
- O
of O
- O
vocabulary O
entities O
. O

Acknowledgements O

The O
authors O
would O
like O
to O
thank O
the O
anonymous O
reviewers O
for O
their O
helpful O
comments O
, O
Ting O
Wu O
and O
Yiding O
Tan O
for O
their O
early O
contribution O
. O
This O
work O
was O
partially O
funded O
by O
China O
National O
Key O
RD O
Program O
( O
No O
. O
2018YFB1005104 O
) O
, O
National O
Natural O
Science O
Foundation O
of O
China O
( O
No O
. O
62076069 O
, O
61976056 O
) O
. O
This O
research O
was O
sponsored O
by O
Hikvision O
Cooperation O
Fund O
, O
Beijing O
Academy O
of O
Artificial O
Intelligence O
( O
BAAI O
) O
, O
and O
CAAI O
- O
Huawei O
MindSpore O
Open O
Fund O
. O

A O
Appendix O

This O
section O
provides O
the O
proof O
of O
generalizing O
information O
maximization O
, O
i.e. O
, O
Eq O
. O
( O
8 O
) O
. O
Consider O
x O
1 O
and O
x O
2 O
are O
two O
contrastive O
samples O
of O
similar O
context O
, O
and O
contains O
different O
entity O
mentions O
of O
the O
same O
entity O
category O
, O
i.e. O
, O
s O
1 O
and O
s O
2 O
, O
respectively O
. O

B O
Appendix O

This O
section O
provides O
the O
proof O
of O
superfluous O
information O
minimization O
, O
i.e. O
Eq O
. O
( O
9 O
) O
. O

RASAT B-MethodName
: O
Integrating O
Relational O
Structures O
into O
Pretrained O
Seq2Seq O
Model O
for O
Text B-TaskName
- I-TaskName
to I-TaskName
- I-TaskName
SQL I-TaskName

Relational O
structures O
such O
as O
schema O
linking O
and O
schema O
encoding O
have O
been O
validated O
as O
a O
key O
component O
to O
qualitatively O
translating O
natural O
language O
into O
SQL O
queries O
. O
However O
, O
introducing O
these O
structural O
relations O
comes O
with O
prices O
: O
they O
often O
result O
in O
a O
specialized O
model O
structure O
, O
which O
largely O
prohibits O
using O
large O
pretrained O
models O
in O
text B-TaskName
- I-TaskName
to I-TaskName
- I-TaskName
SQL I-TaskName
. O
To O
address O
this O
problem O
, O
we O
propose O
RASAT B-MethodName
: O
a O
Transformer O
seq2seq O
architecture O
augmented O
with O
relation O
- O
aware O
self O
- O
attention O
that O
could O
leverage O
a O
variety O
of O
relational O
structures O
while O
inheriting O
the O
pretrained O
parameters O
from O
the O
T5 B-MethodName
model O
effectively O
. O
Our O
model O
can O
incorporate O
almost O
all O
types O
of O
existing O
relations O
in O
the O
literature O
, O
and O
in O
addition O
, O
we O
propose O
introducing O
co O
- O
reference O
relations O
for O
the O
multi O
- O
turn O
scenario O
. O
Experimental O
results O
on O
three O
widely O
used O
text B-TaskName
- I-TaskName
to I-TaskName
- I-TaskName
SQL I-TaskName
datasets O
, O
covering O
both O
singleturn O
and O
multi O
- O
turn O
scenarios O
, O
have O
shown O
that O
RASAT B-MethodName
could O
achieve O
state O
- O
of O
- O
the O
- O
art O
results O
across O
all O
three O
benchmarks O
( O
75.5 B-MetricValue
% I-MetricValue
EX B-MetricName
on O
Spider B-DatasetName
, O
52.6 B-MetricValue
% I-MetricValue
IEX B-MetricName
on O
SParC B-DatasetName
, O
and O
37.4 B-MetricValue
% I-MetricValue
IEX B-MetricName
on O

Introduction O

Text B-TaskName
- I-TaskName
to I-TaskName
- I-TaskName
SQL I-TaskName
is O
the O
task O
that O
aims O
at O
translating O
natural O
language O
questions O
into O
SQL O
queries O
. O
Since O
it O
could O
significantly O
break O
down O
barriers O
for O
nonexpert O
users O
to O
interact O
with O
databases O
, O
it O
is O
among O
the O
most O
important O
semantic O
parsing O
tasks O
that O
are O
of O
practical O
importance O
( O
Kamath O
and O
Das O
, O
2018 O
; O
Deng O
et O
al O
. O
, O
2021 O
) O
. O

Various O
types O
of O
relations O
have O
been O
introduced O
for O
this O
task O
since O
Zhong O
et O
al O
. O
( O
2017 O
) O
collected O
the O
first O
large O
- O
scale O
text B-TaskName
- I-TaskName
to I-TaskName
- I-TaskName
SQL I-TaskName
dataset O
, O
which O
has O
resulted O
in O
significant O
boosts O
in O
the O
performance O
through O
recent O
years O
. O
For O
example O
, O
Bogin O
et O
al O
. O
( O
2019b O
) O
introduced O
schema O
encoding O
to O
represent O
the O
schema O
structure O
of O
the O
database O
, O
and O
the O
resulting O
augmented O
LSTM O
encoder O
- O
decoder O
architecture O
was O
able O
to O
generalize O
better O
towards O
unseen O
database O
schema O
. O
Lin O
et O
al O
. O
( O
2020a O
) O
introduced O
relations O
between O
the O
entity O
mentioned O
in O
the O
question O
and O
the O
matched O
entries O
in O
the O
database O
to O
utilize O
database O
content O
effectively O
. O
Their O
BERT O
- O
based O
encoder O
is O
followed O
by O
an O
LSTM O
- O
based O
pointer O
network O
as O
the O
decoder O
, O
which O
generalizes O
better O
between O
natural O
language O
variations O
and O
captures O
corresponding O
schema O
columns O
more O
precisely O
. O
RAT B-MethodName
- I-MethodName
SQL I-MethodName
( O
Wang O
et O
al O
. O
, O
2020a O
) O
introduced O
schema O
linking O
, O
which O
aligns O
mentions O
of O
entity O
names O
in O
the O
question O
to O
the O
corresponding O
schema O
columns O
or O
tables O
. O
Their O
augmented O
Transformer O
encoder O
is O
coupled O
with O
a O
specific O
tree O
- O
decoder O
. O
SADGA O
( O
Cai O
et O
al O
. O
, O
2021 O
) O
introduced O
the O
dependency O
structure O
of O
the O
natural O
language O
question O
and O
designed O
a O
graph O
neural O
network O
- O
based O
encoder O
with O
a O
tree O
- O
decoder O
. O
On O
the O
other O
hand O
, O
a O
tree O
- O
decoder O
that O
can O
generate O
grammatically O
correct O
SQL O
queries O
is O
usually O
needed O
to O
better O
decode O
the O
encoder O
output O
, O
among O
which O
Yin O
and O
Neubig O
( O
2017 O
) O
is O
one O
of O
the O
most O
widely O
used O
. O

Although O
integrating O
various O
relational O
structures O
as O
well O
as O
using O
a O
tree O
- O
decoder O
have O
been O
shown O
to O
be O
vital O
to O
generating O
qualitative O
SQL O
queries O
and O
generalizing O
better O
towards O
unseen O
database O
schema O
, O
the O
dev O
of O
various O
specifically O
designed O
model O
architectures O
significantly O
deviate O
from O
the O
general O
sequential O
form O
, O
which O
has O
made O
it O
hard O
if O
one O
considers O
leveraging O
large O
pre O
- O
trained O
models O
for O
this O
task O
. O
Existing O
methods O
either O
use O
BERT B-MethodName
output O
as O
the O
input O
embedding O
of O
the O
specifically O
designed O
model O
( O
Cao O
et O
al O
. O
, O
2021 O
; O
Choi O
et O
al O
. O
, O
2021 O
; O
Wang O
et O
al O
. O
, O
2020a O
; O
Guo O
et O
al O
. O
, O
2019 O
) O
, O
or O
stack O
a O
specific O
decoder O
on O
top O
of O
BERT B-MethodName
( O
Lin O
et O
al O
. O
, O
2020a O
) O
. O

In O
another O
thread O
, O
pretrained O
seq2seq O
models O
just O
have O
unveiled O
their O
powerful O
potential O
for O
this O
task O
. O
Recent O
attempts O
by O
Shaw O
et O
al O
. O
( O
2021 O
) O
show O
that O
directly O
fine O
- O
tuning O
a O
T5 B-MethodName
model O
( O
Raffel O
et O
al O
. O
, O
2020 O
) O
on O
this O
task O
without O
presenting O
any O
relational O
structures O
could O
achieve O
satisfying O
results O
. O
Moreover O
, O
PICARD B-MethodName
( O
Scholak O
et O
al O
. O
, O
2021 O
) O
presents O
a O
way O
to O
prune O
invalid O
beam O
search O
results O
during O
inference O
time O
, O
thus O
drastically O
improving O
the O
grammatical O
correctness O
of O
the O
SQL O
queries O
generated O
by O
the O
autoregressive O
decoder O
that O
comes O
with O
T5 B-MethodName
. O

In O
this O
work O
, O
different O
from O
the O
more O
common O
approach O
of O
fine O
- O
tuning O
the O
original O
pretrained O
model O
or O
using O
prompt O
tuning O
, O
we O
propose O
to O
augment O
the O
self O
- O
attention O
modules O
in O
the O
encoder O
and O
introduce O
new O
parameters O
to O
the O
model O
while O
still O
being O
able O
to O
leverage O
the O
pre O
- O
trained O
weights O
. O
We O
call O
the O
proposed O
model O
RASAT B-MethodName
2 O
. O
Our O
model O
can O
incorporate O
almost O
all O
existing O
types O
of O
relations O
in O
the O
literature O
, O
including O
schema O
encoding O
, O
schema O
linking O
, O
syntactic O
dependency O
of O
the O
question O
, O
etc O
. O
, O
into O
a O
unified O
relation O
representation O
. O
In O
addition O
to O
that O
, O
we O
also O
introduce O
coreference O
relations O
to O
our O
model O
for O
multi O
- O
turn O
text B-TaskName
- I-TaskName
to I-TaskName
- I-TaskName
SQL I-TaskName
tasks O
. O
Experimental O
results O
show O
that O
RASAT B-MethodName
could O
effectively O
leverage O
the O
advantage O
of O
T5 B-MethodName
. O
It O
achieves O
the O
stateof O
- O
art O
performance O
in O
question B-MetricName
execution I-MetricName
accuracy I-MetricName
( O
EX B-MetricName
/ O
IEX B-MetricName
) O
on O
both O
multi O
- O
turn O
( O
SParC B-DatasetName
and O
CoSQL B-DatasetName
) O
and O
single O
- O
turn O
( O
Spider B-DatasetName
) O
text B-TaskName
- I-TaskName
to I-TaskName
- I-TaskName
SQL I-TaskName
benchmarks O
. O
On O
SParC B-DatasetName
, O
RASAT B-MethodName
surpasses O
all O
previous O
methods O
in O
interaction B-MetricName
execution I-MetricName
accuracy I-MetricName
( O
IEX B-MetricName
) O
and O
improves O
state O
- O
of O
- O
the O
- O
art O
performance O
from O
21.6 B-MetricValue
% I-MetricValue
to O
52.6 B-MetricValue
% I-MetricValue
, O
31 O
% O
absolute O
improvements O
. O
On O
CoSQL B-DatasetName
, O
we O
improve O
state O
- O
of O
- O
the O
- O
art O
IEX B-MetricName
performance O
from O
8.4 B-MetricValue
% I-MetricValue
to O
37.4 B-MetricValue
% I-MetricValue
, O
achieving O
29 O
% O
absolute O
improvements O
. O
Moreover O
, O
on O
Spider B-DatasetName
, O
we O
improve O
state O
- O
ofthe O
- O
art O
execution O
accuracy O
from O
75.1 B-MetricValue
% I-MetricValue
to O
75.5 B-MetricValue
% I-MetricValue
, O
achieving O
0.4 O
% O
absolute O
improvements O
. O

Related O
Work O

Early O
works O
usually O
exploit O
a O
sketch O
- O
based O
slotfilling O
method O
that O
uses O
different O
modules O
to O
predict O
the O
corresponding O
part O
of O
SQL O
. O
These O
methods O
decompose O
the O
SQL O
generation O
task O
into O
several O
independent O
sketches O
and O
use O
different O
classifiers O
to O
predict O
corresponding O
part O
, O
such O
as O
SQLNet O
( O
Xu O
et O
al O
. O
, O
2017 O
) O
, O
SQLOVA O
( O
Hwang O
et O
al O
. O
, O
2019 O
) O
, O
X O
- O
SQL O
( O
He O
et O
al O
. O
, O
2019 O
) O
, O
RYANSQL O
( O
Choi O
et O
al O
. O
, O
2021 O
) O
, O
et.al O
, O
. O
However O
, O
most O
of O
these O
methods O
only O
2 O
RASAT B-MethodName
: O
Relation B-MethodName
- I-MethodName
Aware I-MethodName
Self I-MethodName
- I-MethodName
Attention I-MethodName
- I-MethodName
augmented I-MethodName
T5 I-MethodName
handle O
simple O
queries O
while O
failing O
to O
generate O
correct O
SQL O
in O
a O
complex O
setting O
such O
as O
on O
Spider B-DatasetName
. O

Faced O
with O
the O
multi O
- O
table O
and O
complex O
SQL O
setting O
, O
using O
graph O
structures O
to O
encode O
various O
complex O
relationships O
is O
a O
major O
trend O
in O
the O
text B-TaskName
- I-TaskName
to I-TaskName
- I-TaskName
SQL I-TaskName
task I-TaskName
. O
For O
example O
, O
Global O
- O
GNN O
( O
Bogin O
et O
al O
. O
, O
2019a O
) O
represents O
the O
complex O
database O
schema O
as O
a O
graph O
, O
RAT B-MethodName
- I-MethodName
SQL I-MethodName
( O
Wang O
et O
al O
. O
, O
2020a O
) O
introduces O
schema O
encoding O
and O
linking O
and O
assigns O
every O
two O
input O
items O
a O
relation O
, O
LGESQL B-MethodName
( O
Cao O
et O
al O
. O
, O
2021 O
) O
further O
distinguishes O
local O
and O
non O
- O
local O
relations O
by O
exploiting O
a O
line O
graph O
enhanced O
hidden O
module O
, O
SADGA O
( O
Cai O
et O
al O
. O
, O
2021 O
) O
uses O
contextual O
structure O
and O
dependency O
structure O
to O
encode O
question O
- O
graph O
while O
database O
schema O
relations O
are O
used O
in O
schema O
graph O
, O
S B-MethodName
2 I-MethodName
SQL I-MethodName
( O
Hui O
et O
al O
. O
, O
2022 O
) O
adds O
syntactic O
dependency O
information O
in O
relational O
graph O
attention O
network O
( O
RGAT O
) O
( O
Wang O
et O
al O
. O
, O
2020b O
) O
. O

For O
the O
conversational O
context O
- O
dependent O
textto B-TaskName
- I-TaskName
SQL I-TaskName
task O
that O
includes O
multiple O
turns O
of O
interactions O
, O
such O
as O
SParC B-DatasetName
and O
CoSQL B-DatasetName
, O
the O
key O
challenge O
is O
how O
to O
take O
advantage O
of O
historical O
interaction O
context O
. O
Edit O
- O
SQL O
edits O
the O
last O
turn O
's O
predicted O
SQL O
to O
generate O
the O
newly O
predicted O
SQL O
at O
the O
token O
level O
. O
IGSQL O
( O
Cai O
and O
Wan O
, O
2020 O
) O
uses O
cross O
- O
turn O
and O
intra O
- O
turn O
schema O
graph O
layers O
to O
model O
database O
schema O
items O
in O
a O
conversational O
scenario O
. O
Tree O
- O
SQL O
( O
Wang O
et O
al O
. O
, O
2021b O
) O
uses O
a O
tree O
- O
structured O
intermediate O
representation O
and O
assigns O
a O
probability O
to O
reuse O
subtree O
of O
historical O
Tree O
- O
SQLs O
. O
IST O
- O
SQL O
( O
Wang O
et O
al O
. O
, O
2021a O
) O
proposes O
an O
interaction O
state O
tracking O
method O
to O
predict O
the O
SQL O
query O
. O
RAT O
- O
SQL O
- O
TC O
adds O
two O
auxiliary O
training O
tasks O
to O
explicitly O
model O
the O
semantic O
changes O
in O
both O
turn O
grain O
and O
conversation O
grain O
. O
R O
2 O
SQL O
( O
Hui O
et O
al O
. O
, O
2021 O
) O
and O
HIE O
- O
SQL O
( O
Zheng O
et O
al O
. O
, O
2022 O
) O
introduce O
a O
dynamic O
schema O
- O
linking O
graph O
by O
adding O
the O
current O
utterance O
, O
interaction O
history O
utterances O
, O
database O
schema O
, O
and O
the O
last O
predicted O
SQL O
query O
. O

Recently O
, O
Shaw O
et O
al O
. O
( O
2021 O
) O
showed O
that O
finetuning O
a O
pre O
- O
trained O
T5 B-MethodName
- O
3B O
model O
could O
yield O
results O
competitive O
to O
the O
then O
- O
state O
- O
of O
- O
the O
- O
art O
. O
Based O
on O
this O
discovery O
, O
Scholak O
et O
al O
. O
( O
2021 O
) O
proposed O
to O
constrain O
the O
autoregressive O
decoder O
through O
incremental O
parsing O
during O
inference O
time O
, O
effectively O
filtering O
out O
grammatically O
incorrect O
sequences O
on O
the O
fly O
during O
beam O
search O
, O
which O
significantly O
improved O
the O
qualities O
of O
the O
generated O
SQL O
. O

Preliminaries O

Task O
Formulation O

Given O
a O
natural O
language O
question O
Q O
and O
database O
schema O
S O
= O
< O
T O
, O
C O
> O
, O
our O
goal O
is O
to O
predict O
the O
SQL O
query O
Y. O
Here O
Q O
= O
{ O
q O
i O
} O
|Q| O
i=1 O
is O
a O
sequence O
of O
natural O
language O
tokens O
, O
and O
the O
schema O
S O
consists O
of O
a O
series O
of O
tables O
T O
= O
{ O
t O
i O
} O

|T O
| O
i=1 O
with O
their O
corresponding O
columns O
C O
= O
{ O
C O
i O
} O
|T O
| O
i=1 O
. O
The O
content O
of O
database O
S O
is O
noted O
as O
V. O
For O
each O
table O
t O
i O
, O
the O
columns O
in O
this O
table O
is O
denoted O
as O
C O
i O
= O
{ O
c O
ij O
} O
|C O
i O
| O
j=1 O
. O
For O
each O
table O
t O
i O
, O
the O
table O
name O
contains O
|t O
i O
| O
tokens O
t O
i O
= O
t O
i,1 O
, O
• O
• O
• O
, O
t O
i O
, O
|t O
i O
| O

, O
and O
the O
same O
holds O
for O
column O
names O
. O
In O
this O
work O
, O
we O
present O
the O
predicted O
SQL O
query O
as O
a O
sequence O
of O
tokens O
, O

Y O
= O
{ O
y O
i O
} O
|Y| O
i=1 O
. O

In O
the O
multi O
- O
turn O
setting O
, O
our O
notations O
adapt O
correspondingly O
. O
i.e. O
, O
Q O
= O
{ O
Q O
i O
} O
|Q| O
i=1 O
denotes O
a O
sequence O
of O
questions O
in O
the O
interaction O
, O
with O
Q O
i O
denoting O
each O
question O
. O
Also O
, O
the O
target O
to O
be O
predicted O
is O
a O
sequence O
of O
SQL O
queries O
, O
Y O
= O
{ O
Y O
i O
} O
|Y| O
i=1 O
, O
with O
each O
Y O
i O
denoting O
the O
corresponding O
SQL O
query O
for O
the O
i O
- O
th O
question O
Q O
i O
. O
Generally O
, O
for O
each O
question O
, O
there O
is O
one O
corresponding O
SQL O
query O
, O
such O
that O
|Q| O
= O
|Y| O
. O
While O
predicting O
Y O
i O
, O
only O
the O
questions O
in O
the O
interaction O
history O
are O
available O
, O
i.e. O
, O
{ O
Q O
1 O
, O
• O
• O
• O
, O
Q O
i O
} O
. O

Relation O
- O
aware O
Self O
- O
Attention O

Relation O
- O
aware O
self O
- O
attention O
( O
Shaw O
et O
al O
. O
, O
2018 O
) O
augments O
the O
vanilla O
self O
- O
attention O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
by O
introducing O
relation O
embeddings O
into O
the O
key O
and O
value O
entries O
. O
Assume O
the O
input O
to O
the O
self O
attention O
is O
a O
sequence O
of O
n O
embeddings O
X O
= O
{ O
x O
i O
} O
n O
i=1 O
where O
x O
i O
∈ O
R O
dx O
, O
then O
it O
calculates O
its O
output O
z O
as O
( O
|| O
means O
concatenate O
operation O
) O
: O

α O
( O
h O
) O
ij O
= O
softmax O
 O
 O
 O
x O
i O
W O
( O
h O
) O
Q O
x O
j O
W O
( O
h O
) O
K O
+ O
r O
K O
ij O
⊤ O
dz O
/ O
H O
 O
 O
 O
zi O
= O
H O
h=1 O
n O
j=1 O
α O
( O
h O
) O
ij O
x O
j O
W O
( O
h O
) O
V O
+ O
r O
V O
ij O
( O
1 O
) O

where O
H B-HyperparameterName
is O
the O
number B-HyperparameterName
of I-HyperparameterName
heads I-HyperparameterName
, O
and O

W O
( O
h O
) O
Q O
, O
W O
( O
h O
) O
K O
, O
W O
( O
h O
) O
V O

are O
learnable O
weights O
. O
The O
r O
K O
ij O
, O
r O
V O
ij O
are O
two O
different O
relation O
embeddings O
used O
to O
represent O
the O
relation O
r O
between O
the O
i O
- O
th O
and O
j O
- O
th O
token O
. O

RASAT B-MethodName

Model O
Overview O

The O
overall O
structure O
of O
our O
RASAT B-MethodName
model O
is O
shown O
in O
Figure O
1 O
. O
Architecture O
- O
wise O
it O
is O
rather O
simple O
: O
the O
T5 O
model O
is O
taken O
as O
the O
base O
model O
, O
with O
its O
self O
- O
attention O
modules O
in O
the O
encoder O
substituted O
as O
relation O
- O
aware O
self O
- O
attentions O
. O

The O
input O
to O
the O
encoder O
is O
a O
combination O
of O
question O
( O
s O
) O
Q O
, O
database O
schema O
S O
= O
< O
T O
, O
C O
> O
with O
the O
database O
name O
S O
, O
as O
well O
as O
database O
content O
mentions O
and O
necessary O
delimiters O
. O
We O
mostly O
follow O
Shaw O
et O
al O
. O
( O
2021 O
) O
and O
Scholak O
et O
al O
. O
( O
2021 O
) O
to O
serialize O
the O
inputs O
. O
Formally O
, O

X O
= O
Q|S|t O
1 O
: O
c O
11 O
[ O
v O
] O
, O
• O
• O
• O
, O
c O
1|T O
1 O
| O
|t O
2 O
: O
c O
21 O
, O
• O
• O
• O

( O
2 O
) O
where O
t O
i O
is O
the O
table O
name O
, O
c O
ij O
is O
the O
j O
- O
th O
column O
name O
of O
the O
i O
- O
th O
table O
. O
The O
v O
∈ O
V O
showing O
after O
column O
c O
11 O
is O
the O
database O
content O
belonging O
to O
the O
column O
that O
has O
n O
- O
gram O
matches O
with O
the O
tokens O
in O
the O
question O
. O
As O
for O
delimiters O
, O
we O
use O
| O
to O
note O
the O
boundaries O
between O
Q O
, O
S O
, O
and O
different O
tables O
in O
the O
schema O
. O
Within O
each O
table O
, O
we O
use O
: O
to O
separate O
between O
table O
name O
and O
its O
columns O
. O
Between O
each O
column O
, O
, O
is O
used O
as O
the O
delimiter O
. O

As O
for O
the O
multi O
- O
turn O
scenario O
, O
we O
add O
the O
questions O
in O
the O
history O
at O
the O
start O
of O
the O
sequence O
and O
truncate O
the O
trailing O
tokens O
in O
the O
front O
of O
the O
sequence O
when O
the O
sequence O
length O
reaches O
512 O
. O
i.e. O
, O

X O
= O
Q O
1 O
|Q O
2 O
| O
• O
• O
• O
|Q O
t O
|S|t O
1 O
: O
c O
11 O
[ O
v O
] O
, O
• O
• O
• O
( O
3 O
) O

where O
| O
are O
the O
corresponding O
delimiters O
. O
Next O
, O
we O
add O
various O
types O
of O
relations O
as O
triplets O
, O
linking O
between O
tokens O
in O
the O
serialized O
input O
, O
which O
naturally O
turns O
the O
input O
sequence O
into O
a O
graph O
( O
Figure O
1 O
) O
. O
We O
will O
elaborate O
on O
this O
in O
Subsection O
4.2 O
. O
Moreover O
, O
since O
almost O
all O
relation O
triplets O
, O
its O
head O
and O
tail O
correspond O
to O
either O
a O
word O
or O
a O
phrase O
, O
while O
the O
T5 O
model O
is O
at O
subword O
level O
, O
we O
also O
introduce O
relation O
propagation O
to O
map O
these O
relations O
to O
subword O
level O
, O
which O
is O
detailed O
in O
Subsection O
4.3 O
. O

To O
fine O
- O
tune O
this O
model O
, O
we O
inherit O
all O
the O
parameters O
from O
T5 O
and O
randomly O
initialize O
the O
extra O
relation O
embeddings O
introduced O
by O
relation O
- O
aware O
self O
- O
attention O
. O
The O
overall O
increase O
of O
parameters O
is O
less O
than O
0.01 O
% O
( O
c.f O
. O
Appendix O
A O
) O
. O

Interaction O
Graph O

Equipped O
with O
relation O
- O
aware O
self O
- O
attention O
, O
we O
can O
incorporate O
various O
types O
of O
relations O
into O
the O

INPUT O
: O
q O
1,1 O
q O
1,2 O
| O
… O
| O
q O
t-1,1 O
q O
t-1,2 O
| O
q O
t,1 O
q O
t,2 O
q O
t,3 O
q O
t,4 O
| O
S O
| O
t O
1 O
: O
c O
11 O
[ O
v O
1 O
] O
, O
c O

Dis1 O
D O
is O
1 O
Dis O
1 O
D O
is O
1 O
Dep O
Co O
re O
f O
Match O
H O
a O
sD O
h O
a O
sC O
hasC O
hasC O
M O
at O
ch O
F O
k O
e O
y O
k O
r O
v O
r O
Interaction O
Graph O
K O
V O
Figure O
1 O
: O

The O
overview O
of O
our O
model O
. O
Our O
model O
inherits O
the O
seq2seq O
architecture O
of O
T5 B-MethodName
, O
consisting O
of O
N O
layers O
of O
encoders O
and O
decoders O
. O
The O
self O
- O
attention O
modules O
in O
the O
encoder O
are O
substituted O
with O
relation O
- O
aware O
self O
- O
attention O
, O
introducing O
two O
additional O
relation O
embedding O
lookup O
tables O
R O
K O
and O
R O
V O
. O
We O
convert O
the O
sequential O
input O
into O
an O
interaction O
graph O
by O
introducing O
various O
types O
of O
relations O
and O
adapting O
them O
to O
the O
subword O
level O
through O
relation O
propagation O
. O
During O
the O
forward O
process O
, O
the O
relation O
- O
aware O
self O
- O
attention O
modules O
read O
out O
the O
relations O
of O
each O
token O
through O
the O
interaction O
graph O
and O
retrieve O
the O
corresponding O
relations O
embeddings O
from O
the O
lookup O
tables O
R O
K O
and O
R O
V O
. O

Type O
Head O
H O
Tail O
T O
Edge O
Label O
Description O
T5 B-MethodName
model O
, O
as O
long O
as O
the O
relation O
can O
be O
presented O
as O
a O
triplet O
, O
with O
its O
head O
and O
tail O
being O
the O
tokens O
in O
the O
input O
sequence O
X. O
Formally O
, O
we O
present O
the O
triplet O
as O

Schema O
Encoding O
T O
C O
PRIMARY O
- O
KEY O
T O
is O
the O
primary O
- O
key O
for O
H O
BELONGS O
- O
TO O
T O
is O
a O
column O
in O
H O
C O
C O
FOREIGN O
- O
KEY O
H O
is O
the O
foreign O
key O
for O
T O
Schema O
Linking O
Q O
T O
/ O
C O
EXACT O
- O
MATCH O
H O
is O
part O
of O
T O
, O

< O
H O
, O
r O
, O
T O
> O
( O
4 O
) O

where O
H O
, O
T O
are O
the O
head O
and O
tail O
items O
in O
the O
triplet O
, O
and O
r O
represents O
the O
relation O
. O
Given O
the O
input O
sequence O
X O
of O
length O
|X| O
, O
we O
assume O
that O
for O
each O
direction O
of O
a O
given O
pair O
of O
tokens O
, O
there O
only O
exists O
up O
to O
one O
relation O
. O
Thus O
, O
if O
we O
consider O
the O
tokens O
in O
X O
as O
vertices O
of O
a O
graph O
, O
it O
could O
have O
up O
to O
|X| O
2 O
directed O
edges O
, O
with O
each O
edge O
corresponding O
to O
an O
entry O
in O
the O
adjacency O
matrix O
of O
the O
graph O
. O
In O
this O
paper O
, O
we O
call O
this O
graph O
, O
containing O
tokens O
from O
the O
whole O
input O
sequence O
as O
its O
vertices O
and O
the O
incorporated O
relations O
as O
its O
edges O
, O
as O
interaction O
graph O
. O
We O
assign O
two O
relation O
embeddings O
for O
each O
type O
of O
introduced O
relation O
. O
Thus O
the O
Transformer O
encoder O
comes O
with O
two O
trainable O
lookup O
tables O
storing O
relations O
embeddings O
to O
compute O
the O
key O
and O
value O
in O
the O
self O
- O
attention O
( O
c.f O
. O
Figure O
1 O
) O
. O
Formally O
, O
we O
denote O
them O
as O
R O
K O
, O
R O
V O
∈ O
R O
µ×d O
kv O
where O
µ O
is O
the O
kinds O
of O
relations O
and O
d O
kv O
is O
the O
dimension O
of O
each O
attention O
head O
in O
the O
key O
and O
value O
states O
. O
Note O
that O
we O
share O
the O
relation O
embedding O
between O
different O
heads O
and O
layers O
but O
untie O
them O
between O
key O
and O
value O
. O
During O
forward O
computation O
, O
for O
all O
the O
layers O
, O
r O
K O
ij O
and O
r O
V O
ij O
in O
Equation O
1 O
are O
retrieved O
from O
the O
two O
trainable O
look O
- O
up O
tables O
. O

We O
reserve O
a O
set O
of O
generic O
relations O
for O
serving O
as O
mock O
relations O
for O
token O
pairs O
that O
do O
not O
have O
a O
specific O
edge O
. O
In O
total O
, O
we O
have O
used O
51 O
different O
relations O
in O
the O
model O
( O
c.f O
. O
Appendix O
D O
) O
. O
Apart O
from O
the O
mock O
generic O
relations O
, O
there O
are O
generally O
5 O
types O
of O
relations O
, O
which O
are O
: O
schema O
encoding O
, O
schema O
linking O
, O
question O
dependency O
structure O
, O
coreference O
between O
questions O
, O
and O
database O
content O
mentions O
. O
Please O
refer O
to O
Table O
1 O
for O
some O
representative O
examples O
for O
each O
type O
. O
We O
will O
describe O
each O
of O
them O
in O
the O
following O
paragraphs O
. O

Schema O
Encoding O
. O
Schema O
encoding O
relations O
refer O
to O
the O
relation O
between O
schema O
items O
, O
i.e. O
, O
H O
, O
T O
∈ O
S. O
These O
relations O
describe O
the O
structure O
information O
in O
a O
database O
schema O
. O
For O
example O
, O
PRIMARY O
- O
KEY O
indicates O
which O
column O
is O
the O
primary O
key O
of O
a O
table O
, O
BELONGS O
- O
TO O
shows O
which O
table O
a O
column O
belongs O
to O
, O
and O
FORIGN O
- O
KEY O
connects O
the O
foreign O
key O
in O
one O
table O
, O
and O
the O
primary O
key O
in O
another O
table O
. O

Schema O
Linking O
. O
Schema O
linking O
relations O
refer O
to O
the O
relations O
between O
schema O
and O
question O
items O
, O
i.e. O
, O
H O
∈ O
S O
, O
T O
∈ O
Q O
or O
vice O
versa O
. O
We O
follow O
the O
settings O
in O
RAT B-MethodName
- I-MethodName
SQL I-MethodName
( O
Wang O
et O
al O
. O
, O
2020a O
) O
, O
which O
uses O
n O
- O
gram O
matches O
to O
indicate O
question O
mentions O
of O
the O
schema O
items O
. O
Detecting O
these O
relations O
is O
shown O
to O
be O
challenging O
in O
previous O
works O
( O
Guo O
et O
al O
. O
, O
2019 O
; O
Deng O
et O
al O
. O
, O
2021 O
) O
due O
to O
the O
common O
mismatch O
between O
natural O
language O
references O
and O
their O
actual O
names O
in O
the O
schema O
. O
Thus O
, O
we O
also O
discriminate O
between O
exact O
matches O
and O
partial O
matches O
to O
suppress O
the O
noise O
caused O
by O
imperfect O
matches O
. O

Question O
Dependency O
Structure O
. O
This O
type O
of O
relation O
refers O
to O
the O
edges O
of O
a O
dependency O
tree O
of O
the O
question O
, O
i.e. O
, O
H O
, O
T O
∈ O
Q. O
Unlike O
the O
previous O
two O
relation O
types O
, O
it O
is O
less O
explored O
in O
the O
literature O
on O
text B-TaskName
- I-TaskName
to I-TaskName
- I-TaskName
SQL I-TaskName
. O
Since O
it O
reflects O
the O
grammatical O
structure O
of O
the O
question O
, O
we O
believe O
it O
should O
also O
be O
beneficial O
for O
the O
task O
. O
In O
our O
work O
, O
to O
control O
the O
total O
number O
of O
relations O
and O
avoid O
unnecessary O
overfitting O
, O
we O
do O
not O
discriminate O
between O
different O
dependency O
relations O
. O
Figure O
2 O
shows O
an O
example O
of O
dependency O
relations O
in O
one O
of O
its O
questions O
. O

Coreference O
Between O
Questions O
. O
This O
type O
of O
relation O
is O
unique O
to O
the O
multi O
- O
turn O
scenario O
. O
In O
a O
dialog O
with O
multiple O
turns O
, O
it O
is O
important O
for O
the O
model O
to O
figure O
out O
the O
referent O
of O
the O
pronouns O
correctly O
. O
Figure O
2 O
shows O
a O
typical O
case O
of O
coreference O
resolution O
. O
The O
question O
item O
" O
their O
" O
in O
Turn O
1 O
, O
" O
they O
" O
in O
Turn O
2 O
, O
and O
" O
they O
" O
in O
Turn O
3 O
all O
refer O
to O
the O
question O
item O
" O
students O
" O
in O
Turn O
1 O
. O
i.e. O
, O
1 O
and O
is O
also O
widely O
used O
in O
many O
graph O
- O
structured O
models O
( O
Wang O
et O
al O
. O
, O
2020a O
; O
Cao O
et O
al O
. O
, O
2021 O
) O
. O

Relation O
Propagation O

The O
various O
aforementioned O
types O
of O
relations O
are O
between O
types O
of O
items O
, O
with O
their O
H O
and O
T O
being O
either O
words O
or O
phrases O
. O
However O
, O
almost O
all O
pretrained O
models O
take O
input O
tokens O
at O
the O
subword O
level O
, O
resulting O
in O
a O
difference O
in O
the O
granularity O
between O
the O
relations O
and O
the O
input O
tokens O
. O
Previous O
works O
use O
an O
extra O
step O
to O
aggregate O
multiple O
subword O
tokens O
to O
obtain O
a O
single O
embedding O
for O
each O
item O
in O
the O
interaction O
graph O
, O
such O
as O
mean O
pooling O
, O
attentive O
pooling O
, O
or O
with O
BiLSTMs O
( O
Wang O
et O
al O
. O
, O
2020a O
; O
Cao O
et O
al O
. O
, O
2021 O
) O
. O
However O
, O
these O
aggregation O
methods O
are O
detrimental O
to O
inheriting O
the O
pre O
- O
trained O
knowledge O
in O
the O
pretrained O
models O
. O

In O
this O
work O
, O
we O
adopt O
the O
other O
way O
: O
we O
propagate O
the O
relations O
into O
the O
subword O
level O
by O
cre- O

Experiments O

In O
this O
section O
, O
we O
will O
show O
our O
model O
's O
performance O
on O
three O
common O
text B-TaskName
- I-TaskName
to I-TaskName
- I-TaskName
SQL I-TaskName
datasets O
: O
Spider B-DatasetName
( O
Yu O
et O
al O
. O
, O
2018 O
) O
, O
SParC B-DatasetName
( O
Yu O
et O
al O
. O
, O
2019b O
) O
and O
CoSQL B-DatasetName
( O
Yu O
et O
al O
. O
, O
2019a O
) O
. O
Besides O
, O
we O
experiment O
on O
a O
more O
realistic O
setting O
of O
the O
Spider B-DatasetName
dataset O
: O
Spider B-DatasetName
- I-DatasetName
Realistic I-DatasetName
( O
Deng O
et O
al O
. O
, O
2021 O
) O
to O
test O
the O
gen O
- O
eralizability O
of O
our O
model O
. O
The O
statistics O
of O
these O
datasets O
are O
shown O
in O
Table O
3 O
. O
We O
also O
present O
a O
set O
of O
ablation O
studies O
to O
show O
the O
effect O
of O
our O
method O
on O
different O
sized O
models O
, O
as O
well O
as O
the O
relative O
contribution O
of O
different O
relations O
. O
In O
addition O
, O
we O
put O
2 O
case O
studies O
in O
Appendix O
C O
. O

Experiment O
Setup O

Datasets O
Spider B-DatasetName
is O
a O
large O
- O
scale O
, O
multi O
- O
domain O
, O
and O
cross O
- O
database O
benchmark O
. O
SparC B-DatasetName
and O
CoSQL B-DatasetName
are O
multi O
- O
turn O
versions O
of O
Spider O
on O
which O
the O
dialogue O
state O
tracking O
is O
required O
. O
All O
test O
data O
is O
hidden O
to O
ensure O
fairness O
, O
and O
we O
submit O
our O
model O
to O
the O
organizer O
of O
the O
challenge O
for O
evaluation O
. O
) O
, O
Bridge B-MethodName
( O
Lin O
et O
al O
. O
, O
2020b O
, O
GAZP B-MethodName
( O
Zhong O
et O
al O
. O
, O
2020 O
) O
, O
NatSQL B-MethodName
( O
Gan O
et O
al O
. O
, O
2021 O
) O
, O
SmBoP B-MethodName
( O
Rubin O
and O
Berant O
, O
2021 O
) O
, O
LGESQL B-MethodName
( O
Cao O
et O
al O
. O
, O
2021 O
) O
, O
S B-MethodName
2 I-MethodName
SQL I-MethodName
( O
Hui O
et O
al O
. O
, O
2022 O
) O
, O
T5 B-MethodName
and O
PICARD B-MethodName
( O
Scholak O
et O
al O
. O
, O
2021 O
) O
. O

coreference O
links O
. O
In O
total O
, O
51 O
types O
of O
relations O
are O
used O
( O
c.f O
. O
Appendix O
D O
for O
a O
detailed O
list O
) O
. O
For O
dependency O
parsing O
, O
stanza O
( O
Qi O
et O
al O
. O
, O
2020 O
) O
is O
used O
. O

The O
batch B-HyperparameterName
size I-HyperparameterName
we O
used O
is O
2048 B-HyperparameterValue
. O
We O
use O
Adafactor O
( O
Shazeer O
and O
Stern O
, O
2018 O
) O
as O
optimizer O
and O
the O
learning B-HyperparameterName
rate I-HyperparameterName
is O
1e-4 B-HyperparameterValue
. O
We O
set O
" O
parse O
with O
guards O
" O
mode O
for O
PICARD B-MethodName
and O
beam B-HyperparameterName
size I-HyperparameterName
is O
set O
to O
8 B-HyperparameterValue
. O
The O
max B-HyperparameterName
tokens I-HyperparameterName
to O
check O
for O
PICARD B-MethodName
is O
2 B-HyperparameterValue
. O
Experiments O
are O
run O
on O
NVIDIA O
A100 O
- O
SXM4 O
- O
80 O
GB O
GPUs O
. O

Results O
on O
SParC B-DatasetName

The O
results O
on O
SParC B-DatasetName
are O
shown O
in O
achieves O
state O
- O
of O
- O
the O
- O
art O
results O
on O
all O
four O
evaluation O
metrics O
. O

Compared O
with O
the O
previous O
state O
- O
of O
- O
the O
- O
art O
RAT O
- O
SQL O
- O
TC O
+ O
GAP O
, O
RASAT O
+ O
PICARD O
brings O
the O
QEM B-MetricName
from O
65.7 B-MetricValue
% I-MetricValue
to O
67.7 B-MetricValue
% I-MetricValue
and O
IEM B-MetricName
from O
43.2 B-MetricValue
% I-MetricValue
to O
45.2 B-MetricValue
% I-MetricValue
on O
the O
test O
set O
. O
In O
addition O
, O
our O
model O
can O
produce O
executable O
SQLs O
( O
with O
values O
) O
, O
whereas O
many O
of O
the O
models O
listed O
in O
the O
table O
do O
not O
provide O
value O
predictions O
. O

Among O
the O
models O
that O
can O
predict O
with O
values O
, O
the O
fine O
- O
tuned O
T5 B-MethodName
- O
3B O
model O
from O
UNIFIEDSKG O
( O
Xie O
et O
al O
. O
, O
2022 O
) O
is O
currently O
the O
state O
- O
of O
- O
the O
- O
art O
. O
We O
did O
comparison O
of O
QEX B-MetricName
/ O
IEX B-MetricName
on O
the O
dev O
set O
since O
they O
did O
not O
report O
their O
performance O
on O
the O
test O
set O
. O
RASAT B-MethodName
+ O
PICARD B-MethodName
surpasses O
all O
previous O
methods O
and O
improves O
the O
state O
- O
of O
- O
art O
QEX B-MetricName
and O
IEX B-MetricName
from O
67.3 B-MetricValue
% I-MetricValue
and O
46.4 B-MetricValue
% I-MetricValue
to O
73.3 B-MetricValue
% I-MetricValue
and O
54.0 B-MetricValue
% I-MetricValue
, O
with O
6 O
% O
and O
7.6 O
% O
absolute O
improvements O
, O
respectively O
. O

Furthermore O
, O
on O
the O
official O
leaderboard O
of O
SParc B-DatasetName
which O
reports O
over O
test O
set O
, O
our O
proposed O
RASAT B-MethodName
+ O
PICARD B-MethodName
brings O
the O
IEX B-MetricName
from O
21.6 B-MetricValue
% I-MetricValue
to O
52.6 B-MetricValue
% I-MetricValue
, O
achieving O
31 O
% O
absolute O
improvements O
. O

Results O
on O
CoSQL B-DatasetName

Compared O
with O
SParC B-DatasetName
, O
CoSQL B-DatasetName
is O
labeled O
in O
a O
Wizard O
- O
of O
- O
Oz O
fashion O
, O
forming O
a O
more O
realistic O
and O
challenging O
testbed O
. O
Nevertheless O
, O
our O
proposed O
model O
could O
still O
achieve O
state O
- O
of O
- O
the O
- O
art O
results O
( O
Table O
4 O
) O
on O
all O
four O
evaluation O
metrics O
. O

By O
comparing O
to O
the O
previous O
state O
- O
of O
- O
the O
- O
art O
HIE O
- O
SQL O
+ O
GraPPa O
( O
Zheng O
et O
al O
. O
, O
2022 O
) O
and O
T5 B-MethodName
- O
3B+PICARD B-MethodName
( O
Scholak O
et O
al O
. O
, O
2021 O
) O
, O
RASAT B-MethodName
+ O
PI B-MethodName
- I-MethodName
CARD I-MethodName
brings O
the O
QEM B-MetricName
from O
54.6 B-MetricValue
% I-MetricValue
to O
55.7 B-MetricValue
% I-MetricValue
and O
IEM B-MetricName
from O
24.6 B-MetricValue
% I-MetricValue
to O
26.5 B-MetricValue
% I-MetricValue
on O
the O
test O
set O
. O

For O
the O
same O
reason O
as O
on O
SParC B-DatasetName
, O
we O
mainly O
compare O
QEX B-MetricName
/ O
IEX B-MetricName
performance O
on O
the O
dev O
set O
, O
and O
RASAT B-MethodName
+ O
PICARD B-MethodName
surpasses O
all O
models O
that O
can O
predict O
executable O
SQLs O
( O
with O
values O
) O
. O
Especially O
for O
IEX B-MetricName
, O
our O
model O
surpasses O
the O
previous O
state O
- O
of O
- O
the O
- O
art O
from O
26.2 B-MetricValue
% I-MetricValue
to O
39.6 B-MetricValue
% I-MetricValue
, O
with O
13.4 O
% O
absolute O
improvement O
. O
Moreover O
, O
on O
the O
official O
leaderboard O
of O
CoSQL B-DatasetName
which O
reports O
over O
test O
set O
, O
RASAT B-MethodName
+ O
PICARD B-MethodName
brings O
the O
IEX B-MetricName
from O
8.4 B-MetricValue
% I-MetricValue
to O
37.4 B-MetricValue
% I-MetricValue
, O
with O
29 O
% O
absolute O
improvements O
. O

Results O
on O
Spider B-DatasetName
and O
Spider B-DatasetName
- I-DatasetName
Realistic I-DatasetName

The O
results O
on O
the O
Spider B-DatasetName
is O
provided O
in O
Table O
5 O
. O
Our O
proposed O
RASAT B-MethodName
model O
achieves O
state O
- O
of O
- O
theart O
performance O
in O
EX B-MetricName
and O
competitive O
results O
in O
EM B-MetricName
. O
On O
the O
dev O
set O
, O
compared O
with O
T5 B-MethodName
- I-MethodName
3B I-MethodName
, O
which O
also O
does O
not O
use O
the O
PICARD B-MethodName
during O
beam O
search O
, O
our O
model O
's O
EX B-MetricName
increases O
from O
74.4 B-MetricValue
% I-MetricValue
to O
76.6 B-MetricValue
% I-MetricValue
, O
achieving O
2.2 O
% O
absolute O
improvement O
. O
When O
augmented O
with O
PICARD B-MethodName
, O
RASAT+PICARD B-MethodName
brings O
the O
EX B-MetricName
even O
higher O
to O
80.5 B-MetricValue
% I-MetricValue
, O
with O
1.2 O
% O
absolute O
improvement O
compared O
to O
T5 B-MethodName
- I-MethodName
3B I-MethodName
+ O
PICARD B-MethodName
. O
Furthermore O
, O
on O
the O
official O
leaderboard O
of O
Spider B-MethodName
, O
our O
proposed O
RASAT B-MethodName
+ O
PICARD B-MethodName
brings O
the O
EX B-MetricName
from O
75.1 B-MetricValue
% I-MetricValue
to O
75.5 B-MetricValue
% I-MetricValue
, O
achieving O
new O
state O
- O
of O
- O
the O
- O
art O
. O

Furthermore O
, O
we O
also O
evaluate O
our O
model O
on O
a O
more O
challenging O
Spider B-DatasetName
variant O
, O
Spider B-DatasetName
- I-DatasetName
Realistic I-DatasetName
( O
Deng O
et O
al O
. O
, O
2021 O
) O
. O
It O
is O
a O
evaluation O
dataset O
that O
has O
modified O
the O
user O
questions O
by O
removing O
or O
paraphrasing O
explicit O
mentions O
of O
column O
names O
to O
present O
a O
realistic O
and O
challenging O
setting O
. O
Our O
model O
also O
achieves O
a O
new O
state O
- O
of O
- O
the O
- O
art O
performance O
( O
Table O
6 O
) O
, O
which O
suggests O
strong O
ability O
of O
our O
model O
to O
generalize O
to O
unseen O
data O
. O

Ablation O
Study O

In O
this O
subsection O
, O
we O
conduct O
a O
set O
of O
ablation O
studies O
to O
examine O
various O
aspects O
of O
the O
proposed O
model O
. O
Due O
to O
the O
limited O
availability O
of O
the O
test O
sets O
, O
all O
numbers O
in O
this O
subsection O
are O
reported O
on O
the O
dev O
set O
. O

Effect O
on O
SQL O
difficulty O
. O
One O
might O
conjecture O
that O
the O
introduced O
relations O
are O
only O
effective O
for O
more O
difficult O
, O
longer O
SQL O
query O
predictions O
, O
while O
for O
predicting O
short O
SQL O
queries O
, O
the O
original O
T5 B-MethodName
model O
could O
handle O
equally O
well O
. O
Thus O
, O
we O
evaluate O
our O
model O
according O
to O
the O
difficulty O
of O
the O
examples O
, O
where O
the O
question O
/ O
SQL O
pairs O
in O
the O
dev O
set O
are O
categorized O
into O
four O
subsets O
, O
i.e. O
, O
easy O
, O
medium O
, O
hard O
, O
and O
extra O
hard O
, O
according O
to O
their O
level O
of O
difficulty O
. O
In O
Table O
7 O
we O
provide O
a O
comparison O
between O
T5 B-MethodName
- I-MethodName
3B I-MethodName
+ O
PICARD B-MethodName
( O
Scholak O
et O
al O
. O
, O
2021 O
) O
and O
RASAT B-MethodName
+ O
PICARD B-MethodName
on O
the O
EX B-MetricName
metric O
on O
the O
four O
subsets O
. O
RASAT B-MethodName
+ O
PICARD B-MethodName
surpasses O
T5 B-MethodName
- I-MethodName
3B I-MethodName
+ O
PICARD B-MethodName
across O
all O
subsets O
, O
validating O
the O
effectiveness O
of O
the O
introduced O
relational O
structures O
for O
all O
SQL O
sequences O
. O
.0 O
( O
+0.5 O
) O
45.5 O
( O
-0.2 O
) O
69.9 O
( O
+0.7 O
) O
50.7 O
( O
+0.3 O
) O
w O
/ O
o O
Cf O
65.0 O
( O
+0.5 O
) O
45.0 O
( O
-0.7 O
) O
69.4 O
( O
+0.2 O
) O
50.0 O
( O
-0.4 O
) O
w O
/ O
o O
Db O
64.1 O
( O
-0.4 O
) O
45.3 O
( O
-0.4 O
) O
67.9 O
( O
-1.3 O
) O
48.5 O
( O
-1.9 O
) O
w O
/ O
o O
SL O
64.5 O
45.5 O
( O
-0.2 O
) O
68.8 O
( O
-0.4 O
) O
49.4 O
( O
-1.0 O
) O
w O
/ O
o O
SE O
63.9 O
( O
-0.6 O
) O
44.6 O
( O
-1.1 O
) O
68.6 O
( O
-0.6 O
) O
48.9 O
( O
-1.5 O
) O
Table O
10 O
: O
Ablation O
study O
on O
the O
relative O
contribution O
of O
different O
relation O
types O
. O
Experiment O
are O
conducted O
using O
RASAT B-MethodName
( O
-3B O
) O
on O
the O
SParC B-DatasetName
dataset O
. O
" O
Dp O
" O
is O
short O
for O
dependency O
relation O
, O
" O
Cf O
" O
for O
coreference O
relation O
, O
" O
SL O
" O
for O
schema O
linking O
relation O
, O
" O
SE O
" O
for O
schema O
encoding O
relation O
and O
" O
Db O
" O
means O
database O
content O
. O

Model O
Size O
Impact O
. O
To O
test O
the O
effectiveness O
of O
the O
introduced O
relational O
structures O
on O
pretrained O
models O
with O
different O
sizes O
, O
we O
implant O
RASAT B-MethodName
into O
four O
T5 B-MethodName
models O
of O
different O
sizes O
( O
T5 B-MethodName
- I-MethodName
small I-MethodName
, O
T5 B-MethodName
- I-MethodName
base I-MethodName
, O
T5 B-MethodName
- I-MethodName
large I-MethodName
, O
T5 B-MethodName
- I-MethodName
3B I-MethodName
) O
and O
test O
it O
on O
Spider B-DatasetName
( O
Table O
8 O
) O
. O
Interestingly O
, O
for O
smaller O
pretrained O
models O
, O
our O
RASAT B-MethodName
model O
could O
bring O
even O
larger O
performance O
gaps O
between O
its O
T5 B-MethodName
- I-MethodName
3B I-MethodName
counterpart O
. O
This O
suggests O
that O
the O
larger O
T5 B-MethodName
model O
might O
have O
learned O
some O
of O
the O
relational O
structures O
implicitly O
. O
We O
believe O
this O
is O
consistent O
with O
the O
findings O
on O
other O
fine O
- O
tuning O
tasks O
, O
where O
larger O
pretrained O
models O
are O
more O
capable O
of O
capturing O
the O
abundant O
implicit O
dependencies O
in O
the O
raw O
text O
. O

Relation O
Types O
. O
We O
conducted O
additional O
experiments O
to O
analyze O
the O
relative O
contribution O
of O
different O
relation O
types O
. O
The O
experimental O
results O
on O
Spider B-DatasetName
is O
shown O
in O
Table O
9 O
while O
result O
on O
SParC B-DatasetName
is O
shown O
in O
Table O
10 O
( O
since O
CoSQL B-DatasetName
has O
similar O
conversational O
modality O
with O
SParC B-DatasetName
, O
the O
experiments O
are O
only O
conducted O
on O
SParC B-DatasetName
) O
. O
We O
find O
that O
both O
T5 B-MethodName
and O
RASAT B-MethodName
models O
can O
benefit O
from O
leveraging O
database O
content O
. O
Another O
important O
finding O
is O
that O
the O
performance O
has O
increased O
obviously O
by O
adding O
dependency O
relationship O
to O
RASAT B-MethodName
( I-MethodName
-small I-MethodName
) I-MethodName
on O
Spider B-DatasetName
. O
As O
for O
SParC B-DatasetName
, O
the O
database O
content O
plays O
a O
more O
important O
role O
by O
looking O
at O
EX B-MetricName
results O
; O
from O
what O
we O
can O
see O
, O
IEX B-MetricName
will O
decrease O
by O
1.9 B-MetricValue
% I-MetricValue
after O
removing O
database O
content O
from O
the O
input O
. O

Conclusion O

In O
this O
work O
, O
we O
propose O
RASAT B-MethodName
, O
a O
Relation B-MethodName
- I-MethodName
Aware I-MethodName
Self I-MethodName
- I-MethodName
Attention I-MethodName
- I-MethodName
augmented I-MethodName
T5 I-MethodName
model O
for O
the O
textto B-TaskName
- I-TaskName
SQL I-TaskName
generation O
. O
Compared O
with O
previous O
work O
, O
RASAT B-MethodName
can O
introduce O
various O
structural O
relations O
into O
the O
sequential O
T5 B-MethodName
model O
. O
Different O
from O
the O
more O
common O
approach O
of O
fine O
- O
tuning O
the O
origi O
- O
nal O
model O
or O
using O
prompt O
tuning O
, O
we O
propose O
to O
augment O
the O
self O
- O
attention O
modules O
in O
the O
encoder O
and O
introduce O
new O
parameters O
to O
the O
model O
while O
still O
being O
able O
to O
leverage O
the O
pre O
- O
trained O
weights O
. O
RASAT B-MethodName
had O
achieved O
state O
- O
of O
- O
the O
- O
art O
performances O
, O
especially O
on O
execution O
accuracy O
, O
in O
the O
three O
most O
common O
text B-MethodName
- I-MethodName
to I-MethodName
- I-MethodName
SQL I-MethodName
benchmarks O
. O

Limitation O

Our O
method O
consumes O
plenty O
of O
computational O
resources O
since O
we O
leverage O
the O
large O
T5 B-MethodName
- I-MethodName
3B I-MethodName
model O
. O
We O
train O
our O
models O
on O
8 O
A100 O
GPUs O
( O
80 O
G O
) O
for O
around O
2 O
days O
. O
Our O
model O
truncates O
the O
source O
sequences O
to O
512 B-HyperparameterValue
, O
this O
may O
lead O
to O
information O
loss O
when O
a O
sample O
has O
long O
input O
. O
We O
find O
that O
about O
3 O
% O
of O
training O
data O
in O
CoSQL O
will O
be O
affected O
. O
We O
only O
work O
with O
English O
since O
it O
has O
richer O
analytical O
tools O
and O
resources O
than O
other O
language O
. O

A O
Model O
Size O

Compared O
with O
the O
original O
T5 B-MethodName
model O
, O
only O
two O
embedding O
matrices O
are O
added O
to O
the O
encoder O
in O
our O
model O
, O
with O
2 O
× O
µ O
× O
d O
kv O
parameters O
. O
These O
embedding O
matrices O
are O
shared O
in O
each O
encoder O
layer O
and O
each O
head O
. O
Here O
µ B-HyperparameterName
= O
51 B-HyperparameterValue
is O
the O
total B-HyperparameterName
number I-HyperparameterName
of I-HyperparameterName
relations I-HyperparameterName
and O
d B-HyperparameterName
kv I-HyperparameterName
is O
the O
dimension B-HyperparameterName
of I-HyperparameterName
the I-HyperparameterName
key I-HyperparameterName
/ I-HyperparameterName
value I-HyperparameterName
states I-HyperparameterName
in O
self O
- O
attention O
( O
64 B-HyperparameterValue
in O
T5 B-MethodName
- I-MethodName
small I-MethodName
/ I-MethodName
base I-MethodName
/ I-MethodName
large I-MethodName
and O
128 B-HyperparameterValue
in O
T5 B-MethodName
- I-MethodName
3B I-MethodName
) O
. O
The O
overall O
increase O
of O
parameters O
is O
less O
than O
0.01 O
% O
. O

Approach O

B O
Output O
Comparation O
between O
T5 B-MethodName
and O
Tree O
- O
based O
Decoder O
Model O

Here O
we O
show O
the O
output O
difference O
between O
T5 B-MethodName
and O
most O
AST O
- O
based O
models O
. O
As O
it O
shown O
in O
Table O
12 O
, O
most O
of O
these O
models O
which O
exploited O
ASTtree O
- O
based O
decoder O
( O
such O
as O
RAT B-MethodName
- I-MethodName
SQL I-MethodName
( O
Wang O
et O
al O
. O
, O
2020a O
) O
, O
LGESQL B-MethodName
( O
Cao O
et O
al O
. O
, O
2021 O
) O
) O
usually O
use O
a O
place O
holder O
( O
i.e. O
" O
value O
" O
) O
to O
represent O
the O
real O
value O
( O
" O
France O
" O
in O
this O
example O
) O
. O
These O
outputs O
can O
not O
be O
executed O
in O
a O
real O
database O
and O
they O
fail O
to O
evaluate O
in O
the O
EXecution B-MetricName
Accuracy I-MetricName
( O
EX B-MetricName
/ O
QEX B-MetricName
/ O
IEX B-MetricName
) O
metric O
. O

C O
Case O
Study O

In O

D O
Relations O
Used O
in O
Experiment O

Table O
14 O
shows O
all O
relations O
used O
in O
our O
experiment O
while O
most O
of O
these O
are O
consistant O
with O
RAT B-MethodName
- I-MethodName
SQL I-MethodName
( O
Wang O
et O
al O
. O
, O
2020a O
) O
and O
LGESQL B-MethodName
( O
Cao O
et O
al O
. O
, O
2021 O
) O
. O
There O
are O
total O
51 O
kinds O
relation O
used O
. O

Acknowledgement O

This O
work O
was O
sponsored O
by O
the O
National O
Natural O
Science O
Foundation O
of O
China O
( O
NSFC O
) O
grant O
( O
No O
. O
62106143 O
) O
, O
and O
Shanghai O
Pujiang O
Program O
( O
No O
. O
21PJ1405700 O
) O
. O
We O
would O
like O
to O
thank O
Tao O
Yu O
, O
Hongjin O
Su O
, O
and O
Yusen O
Zhang O
for O
running O
evaluations O
on O
our O
submitted O
models O
. O

DYPLOC B-MethodName
: O
Dynamic B-MethodName
Planning I-MethodName
of I-MethodName
Content I-MethodName
Using O
Mixed O
Language O
Models O
for O
Text B-TaskName
Generation I-TaskName

We O
study O
the O
task O
of O
long B-TaskName
- I-TaskName
form I-TaskName
opinion I-TaskName
text I-TaskName
generation I-TaskName
, O
which O
faces O
at O
least O
two O
distinct O
challenges O
. O
First O
, O
existing O
neural O
generation O
models O
fall O
short O
of O
coherence O
, O
thus O
requiring O
efficient O
content O
planning O
. O
Second O
, O
diverse O
types O
of O
information O
are O
needed O
to O
guide O
the O
generator O
to O
cover O
both O
subjective O
and O
objective O
content O
. O
To O
this O
end O
, O
we O
propose O
DY B-MethodName
- I-MethodName
PLOC I-MethodName
, O
a O
generation O
framework O
that O
conducts O
dynamic B-MethodName
planning I-MethodName
of I-MethodName
content I-MethodName
while O
generating O
the O
output O
based O
on O
a O
novel O
design O
of O
mixed O
language O
models O
. O
To O
enrich O
the O
generation O
with O
diverse O
content O
, O
we O
further O
propose O
to O
use O
large O
pre O
- O
trained O
models O
to O
predict O
relevant O
concepts O
and O
to O
generate O
claims O
. O
We O
experiment O
with O
two O
challenging O
tasks O
on O
newly O
collected O
datasets O
: O
( O
1 O
) O
argument O
generation O
with O
Reddit B-DatasetName
ChangeMyView I-DatasetName
, O
and O
( O
2 O
) O
writing O
articles O
using O
New B-DatasetName
York I-DatasetName
Times I-DatasetName
' I-DatasetName
Opinion I-DatasetName
section I-DatasetName
. O
Automatic O
evaluation O
shows O
that O
our O
model O
significantly O
outperforms O
competitive O
comparisons O
. O
Human O
judges O
further O
confirm O
that O
our O
generations O
are O
more O
coherent O
with O
richer O
content O
. O

Introduction O

Opinion O
articles O
serve O
as O
an O
important O
media O
to O
convey O
the O
authors O
' O
values O
, O
beliefs O
, O
and O
stances O
on O
important O
societal O
issues O
. O
Automatically O
generating B-TaskName
long I-TaskName
- I-TaskName
form I-TaskName
opinion I-TaskName
articles O
has O
the O
potential O
of O
facilitating O
various O
tasks O
, O
such O
as O
essay O
writing O
and O
speech O
drafting O
, O
and O
it O
is O
the O
focus O
of O
this O
work O
. O
Though O
opinion B-TaskName
generation I-TaskName
has O
been O
investigated O
for O
constructing O
arguments O
( O
Hua O
and O
Wang O
, O
2018 O
) O
, O
writing O
reviews O
( O
Ni O
and O
McAuley O
, O
2018 O
) O
, O
and O
producing O
emotional O
dialogue O
responses O
, O
those O
outputs O
are O
relatively O
short O
. O
While O
impressive O
progress O
in O
generation O
has O
been O
achieved O
by O
using O
large O
pre O
- O
trained O
Transformers O
( O
Radford O
et O
al O
. O
, O
2019 O
; O
Lewis O
et O
al O
. O
, O
2020a O
) O
, O
directly O
adopting O
United_States O
, O
Intelligence O
knowledge O
, O
attack O
America O
was O
never O
prepared O
and O
had O
a O
bad O
intelligence O
system O
. O

President_of_the_U.S. O
, O
Bill_Clinton O
, O
9 O
/ O
11_attacks O
make O
, O
happen O
, O
mistake O
, O
administration O
George_W._Bush O
, O
9 O
/ O
11_attacks O
, O
Iraq O
existence O

1 O
) O
- O
> O
2 O
) O
- O
> O
3 O
) O
- O
> O

Figure O
1 O
: O
Sample O
counter O
- O
argument O
on O
Reddit B-DatasetName
Change I-DatasetName
- I-DatasetName
MyView I-DatasetName
. O
Our O
generator O
considers O
an O
input O
containing O
( O
1 O
) O
a O
title O
and O
( O
2 O
) O
an O
unordered O
set O
of O
content O
items O
. O
Each O
content O
item O
consists O
of O
elements O
of O
an O
entity O
set O
[ O
ENT O
] O
, O
a O
concept O
set O
[ O
CON O
] O
, O
and O
an O
optional O
onesentence O
claim O
[ O
CLAIM O
] O
. O
Each O
output O
token O
is O
generated O
by O
conditioning O
on O
all O
content O
items O
, O
and O
the O
best O
aligned O
ones O
( O
learned O
by O
our O
model O
) O
are O
highlighted O
in O
corresponding O
colors O
. O
We O
also O
underline O
words O
that O
reflect O
the O
input O
concepts O
and O
entities O
. O
them O
for O
long O
- O
form O
opinion O
text O
generation O
poses O
distinct O
challenges O
. O

First O
, O
large O
models O
still O
fall O
short O
of O
producing O
coherent O
text O
due O
to O
the O
lack O
of O
efficient O
content O
control O
and O
planning O
( O
Ko O
and O
Li O
, O
2020 O
; O
Wu O
et O
al O
. O
, O
2020 O
; O
Tan O
et O
al O
. O
, O
2021 O
) O
. O
A O
common O
solution O
is O
to O
use O
concatenated O
phrases O
or O
semantic O
representations O
to O
guide O
the O
generation O
process O
( O
Yao O
et O
al O
. O
, O
2019 O
; O
Harkous O
et O
al O
. O
, O
2020 O
; O
Ribeiro O
et O
al O
. O
, O
2020 O
; O
Goldfarb O
- O
Tarrant O
et O
al O
. O
, O
2020 O
) O
, O
where O
content O
planning O
, O
including O
both O
content O
selection O
and O
ordering O
, O
is O
expected O
to O
be O
learned O
by O
attention O
mechanisms O
. O
However O
, O
attentions O
have O
only O
achieved O
limited O
improvements O
. O
Recent O
work O
also O
explores O
training O
a O
separate O
planning O
module O
to O
produce O
sorted O
content O
, O
which O
is O
then O
fed O
into O
a O
generator O
( O
Fan O
et O
al O
. O
, O
2019 O
; O
Hua O
and O
Wang O
, O
2020 O
; O
Goldfarb O
- O
Tarrant O
et O
al O
. O
, O
2020 O
) O
. O
Nonetheless O
, O
this O
strategy O
results O
in O
a O
disconnection O
between O
planning O
and O
realization O
, O
and O
the O
output O
is O
not O
guaranteed O
to O
respect O
the O
planning O
results O
( O
Castro O
Ferreira O
et O
al O
. O
, O
2019 O
; O
Prabhumoye O
et O
al O
. O
, O
2020 O
) O
. O

The O
second O
challenge O
for O
opinion O
generation O
resides O
in O
the O
diversity O
of O
information O
that O
is O
needed O
to O
produce O
an O
output O
with O
consistent O
stances O
and O
supported O
by O
pertinent O
facts O
. O
Though O
large O
models O
memorize O
significant O
amounts O
of O
knowledge O
, O
they O
can O
not O
retrieve O
and O
operate O
with O
them O
precisely O
( O
Lewis O
et O
al O
. O
, O
2020b O
) O
. O
Due O
to O
the O
argumentative O
nature O
of O
opinion O
text O
, O
simply O
including O
knowledge O
bases O
( O
Guan O
et O
al O
. O
, O
2020 O
; O
Zhou O
et O
al O
. O
, O
2020 O
) O
is O
insufficient O
to O
uphold O
the O
desired O
quality O
, O
as O
it O
requires O
the O
combination O
of O
subjective O
claims O
and O
objective O
evidence O
as O
supports O
. O

To O
this O
end O
, O
we O
propose O
a O
novel O
generation O
framework O
, O
DYPLOC B-MethodName
( O
dynamic B-MethodName
planning I-MethodName
of I-MethodName
content I-MethodName
) O
, O
to O
conduct O
content O
selection O
and O
ordering O
as O
text O
is O
produced O
. O
1 O
Concretely O
, O
given O
a O
set O
of O
unordered O
content O
items O
, O
as O
displayed O
in O
Figure O
1 O
, O
we O
design O
mixed O
language O
models O
, O
with O
each O
implemented O
as O
a O
sequence O
- O
to O
- O
sequence O
model O
to O
encode O
one O
item O
and O
the O
input O
statement O
. O
At O
each O
decoding O
step O
, O
our O
system O
selects O
which O
items O
to O
reflect O
, O
and O
predicts O
a O
word O
based O
on O
probabilities O
marginalized O
over O
all O
language O
models O
. O
Crucially O
, O
our O
end O
- O
to O
- O
end O
trained O
framework O
( O
1 O
) O
enables O
the O
generator O
to O
access O
multiple O
content O
items O
at O
all O
times O
and O
select O
content O
based O
on O
what O
has O
been O
generated O
so O
far O
, O
( O
2 O
) O
can O
be O
directly O
built O
on O
large O
pre O
- O
trained O
Transformers O
, O
e.g. O
, O
BART B-MethodName
( O
Lewis O
et O
al O
. O
, O
2020a O
) O
, O
with O
planning O
and O
generation O
modules O
jointly O
trained O
, O
and O
( O
3 O
) O
outputs O
learned O
content O
selection O
scores O
to O
provide O
an O
interface O
for O
system O
decision O
interpretation O
. O

Furthermore O
, O
to O
ensure O
that O
our O
framework O
can O
be O
applied O
to O
a O
broad O
range O
of O
generation O
tasks O
, O
we O
design O
content O
items O
to O
cover O
three O
critical O
elements O
: O
entities O
and O
concepts O
that O
are O
central O
to O
many O
generation O
applications O
, O
and O
claims O
that O
are O
building O
blocks O
for O
opinion O
text O
. O
We O
show O
an O
example O
for O
counter O
- O
argument O
generation O
in O
Figure O
1 O
. O
Importantly O
, O
we O
employ O
BART B-MethodName
to O
predict O
additional O
relevant O
concepts O
, O
derived O
from O
Concept O
- O
Net O
( O
Speer O
et O
al O
. O
, O
2017 O
) O
, O
and O
generate O
claims O
, O
as O
central O
propositions O
, O
to O
enrich O
the O
generated O
text O
with O
both O
objective O
and O
subjective O
content O
. O

For O
experiments O
, O
we O
collect O
two O
datasets O
: O
( O
1 O
) O
posts O
from O
Reddit B-DatasetName
ChangeMyView I-DatasetName
for O
argument O
generation O
, O
and O
( O
2 O
) O
articles O
from O
the O
New B-DatasetName
York I-DatasetName
Times I-DatasetName
Opinion I-DatasetName
section I-DatasetName
( O
Sandhaus O
, O
2008 O
) O
for O
opinion O
article O
writing O
. O
Our O
proposed O
framework O
outperforms O
competitive O
comparisons O
, O
such O
as O
finetuning O
BART B-MethodName
with O
the O
same O
content O
items O
, O
based O
on O
automatic O
metrics O
of O
BLEU B-MetricName
, O
ROUGE B-MetricName
, O
and O
ME B-MetricName
- I-MetricName
TEOR I-MetricName
. O
Human O
assessment O
further O
confirms O
that O
our O
system O
outputs O
have O
richer O
content O
and O
are O
more O
coherent O
in O
both O
tasks O
. O

Our O
main O
contributions O
are O
summarized O
as O
below O
: O

• O
We O
present O
a O
dynamic O
content O
planning O
generation O
framework O
, O
which O
is O
directly O
built O
on O
top O
of O
BART B-MethodName
. O
Our O
design O
of O
mixed O
language O
models O
overcomes O
the O
lack O
of O
control O
by O
existing O
models O
that O
use O
implicit O
planning O
with O
attentions O
or O
hard O
copying O
. O

• O
We O
propose O
content O
plan O
augmentation O
by O
automatically O
generating O
relevant O
concepts O
and O
claims O
. O

• O
We O
construct O
two O
opinion O
text O
generation O
datasets O
with O
content O
plans O
that O
capture O
prominent O
entities O
and O
concepts O
. O

Related O
Work O

Neural B-TaskName
Generation I-TaskName
with O
Planning O
. O
Text O
planning O
is O
seen O
as O
a O
crucial O
step O
to O
guide O
the O
generation O
of O
high O
- O
quality O
, O
well O
- O
organized O
natural O
language O
text O
( O
McKeown O
, O
1992 O
; O
Reiter O
and O
Dale O
, O
2000 O
) O
. O
Incorporating O
planning O
modules O
to O
neural O
text O
generator O
has O
attracted O
significant O
research O
interests O
( O
Shen O
et O
al O
. O
, O
2019 O
; O
Moryossef O
et O
al O
. O
, O
2019 O
; O
Puduppully O
et O
al O
. O
, O
2019 O
) O
, O
which O
proves O
to O
be O
especially O
beneficial O
for O
long O
- O
form O
output O
( O
Fan O
et O
al O
. O
, O
2019 O
; O
Hua O
and O
Wang O
, O
2019 O
) O
. O
More O
recently O
, O
large O
pre O
- O
trained O
Transformers O
have O
established O
new O
state O
- O
of O
- O
the O
- O
arts O
for O
a O
wide O
range O
of O
text O
generation O
tasks O
( O
Lewis O
et O
al O
. O
, O
2020a O
; O
Roller O
et O
al O
. O
, O
2020 O
; O
Kale O
and O
Rastogi O
, O
2020 O
) O
. O
But O
it O
is O
non O
- O
trivial O
to O
integrate O
planning O
modules O
into O
them O
. O
Existing O
approaches O
resort O
to O
decoupling O
planning O
and O
decoding O
stages O
( O
Hua O
and O
Wang O
, O
2020 O
; O
Kedzie O
and O
McKeown O
, O
2020 O
) O
, O
which O
inevitably O
increases O
system O
complexities O
and O
potentially O
introduces O
cascading O
errors O
. O

We O
take O
inspiration O
from O
the O
retrieval O
- O
augmented O
generation O
framework O
( O
Lewis O
et O
al O
. O
, O
2020b O
) O
, O
which O
is O
designed O
to O
incorporate O
relevant O
documents O
for O
6410 O
[ O
Left O
] O
For O
each O
input O
content O
item O
( O
a O
title O
t O
, O
an O
entity O
set O
E O
i O
, O
and O
a O
core O
concept O
set O
C O
i O
) O
, O
we O
first O
expand O
it O
with O
more O
relevant O
concepts O
, O
i.e. O
, O
C O
+ O
i O
. O
For O
sentences O
to O
be O
realized O
as O
claims O
, O
we O
employ O
a O
separate O
generator O
to O
produce O
one O
draft O
claim O
, O
m O
i O
. O
[ O
Right O
] O
The O
augmented O
content O
items O
, O
denoted O
as O
{ O
x O
i O
} O
, O
are O
encoded O
in O
parallel O
. O
At O
each O
decoding O
step O
, O
a O
plan O
scoring O
network O
estimates O
a O
distribution O
d O
( O
x O
i O
|y O
< O
t O
) O
for O
all O
content O
items O
and O
decides O
on O
relevant O
content O
. O
A O
word O
is O
predicted O
based O
on O
probabilities O
marginalized O
over O
all O
content O
item O
- O
conditioned O
language O
models O
, O
i.e. O
, O
p O
( O
y O
t O
|y O
< O
t O
, O
x O
i O
) O
for O
the O
i O
- O
th O
model O
. O
question O
answering O
. O
Our O
adaptation O
uses O
a O
trainable O
plan O
scoring O
module O
to O
reflect O
content O
selection O
and O
ordering O
, O
which O
is O
more O
suitable O
for O
long O
text O
generation O
and O
offers O
better O
interpretability O
. O
Concurrent O
work O
by O
Zhang O
et O
al O
. O
( O
2021 O
) O
presents O
a O
mixtureof O
- O
expert O
decoder O
to O
tackle O
knowledge O
- O
grounded O
generation O
. O
However O
, O
their O
score O
distribution O
for O
language O
models O
is O
fixed O
across O
all O
decoding O
steps O
, O
whereas O
ours O
is O
updated O
as O
generation O
progresses O
and O
can O
better O
reflect O
the O
dynamic O
nature O
of O
content O
planning O
. O

Controllable O
Text O
Generation O
. O
Another O
related O
line O
of O
research O
investigates O
the O
controllability O
of O
generation O
models O
( O
Wiseman O
et O
al O
. O
, O
2017 O
) O
, O
including O
conditioning O
over O
keywords O
( O
Keskar O
et O
al O
. O
, O
2019 O
; O
Hua O
and O
Wang O
, O
2020 O
; O
, O
syntactic O
structures O
( O
Casas O
et O
al O
. O
, O
2020 O
; O
Goyal O
and O
Durrett O
, O
2020 O
) O
, O
or O
semantic O
representations O
( O
Wen O
et O
al O
. O
, O
2015 O
; O
Elder O
et O
al O
. O
, O
2018 O
) O
. O
Our O
work O
differs O
from O
all O
previous O
methods O
as O
we O
combine O
different O
types O
of O
content O
, O
covering O
both O
objective O
and O
subjective O
information O
, O
and O
attain O
fine O
- O
grained O
sentence O
- O
level O
control O
using O
a O
novel O
design O
of O
mixed O
conditional O
language O
models O
. O
Opinion B-TaskName
Text I-TaskName
Generation I-TaskName
. O
Our O
model O
tackles O
opinion O
articles O
, O
which O
differs O
from O
traditional O
text O
generation O
systems O
that O
mostly O
concern O
fact O
- O
based O
generations O
( O
Gardent O
et O
al O
. O
, O
2017 O
; O
Novikova O
et O
al O
. O
, O
2017 O
; O
Puduppully O
et O
al O
. O
, O
2019 O
) O
. O
An O
extensive O
body O
of O
work O
has O
studied O
summarizing O
( O
Wang O
and O
Ling O
, O
2016 O
; O
Suhara O
et O
al O
. O
, O
2020 O
; O
Bražinskas O
et O
al O
. O
, O
2020 O
) O
or O
generating O
( O
Ni O
and O
McAuley O
, O
2018 O
; O
reviews O
and O
building O
dialogue O
systems O
enhanced O
with O
emotions O
( O
Li O
et O
al O
. O
, O
2016 O
; O
. O
More O
recently O
, O
developments O
are O
made O
in O
generating O
argumentative O
text O
( O
El O
Baff O
et O
al O
. O
, O
2019 O
; O
Hidey O
and O
McKeown O
, O
2019 O
) O
, O
which O
primarily O
focus O
on O
constructing O
single O
sentence O
claims O
on O
a O
limited O
number O
of O
topics O
. O
In O
comparison O
, O
our O
model O
can O
handle O
substantially O
longer O
output O
with O
improved O
quality O
. O

3 O
Model O
Task O
Formulation O
. O
Our O
opinion O
text O
generation O
framework O
takes O
as O
input O
a O
set O
of O
content O
items O
. O
Each O
content O
item O
consists O
of O
a O
title O
t O
, O
a O
set O
of O
entities O
E O
i O
2 O
, O
such O
as O
{ O
United O
States O
, O
9 O
/ O
11 O
attacks O
} O
, O
and O
a O
set O
of O
core O
concepts O
C O
i O
, O
such O
as O
{ O
attack O
, O
knowledge O
} O
, O
that O
are O
often O
abstract O
notions O
. O
Our O
model O
first O
expands O
C O
i O
by O
predicting O
additional O
relevant O
concepts O
C O
+ O
i O
and O
optionally O
generates O
a O
pertinent O
claim O
m O
i O
, O
and O
then O
outputs O
the O
final O
text O
with O
multiple O
sentences O
as O
y O
= O
{ O
y O
t O
} O
, O
to O
faithfully O
reflect O
the O
content O
items O
with O
a O
coherent O
structure O
. O
An O
overview O
of O
our O
system O
is O
illustrated O
in O
Figure O
2 O
. O

Below O
we O
first O
describe O
the O
content O
item O
augmentation O
methods O
( O
§ O
3.1 O
) O
, O
followed O
by O
our O
generator O
with O
mixed O
language O
models O
that O
condition O
on O
expanded O
content O
items O
( O
§ O
3.2 O
) O
. O

Content O
Item O
Augmentation O

Concept O
Expansion O
. O
With O
limited O
number O
of O
entities O
and O
concepts O
as O
input O
, O
generation O
systems O
are O
often O
incapable O
of O
producing O
long O
text O
with O
rich O
content O
, O
resulting O
in O
hallucination O
( O
Wiseman O
et O
al O
. O
, O
2017 O
; O
Tian O
et O
al O
. O
, O
2019 O
) O
. O
Therefore O
, O
from O
the O
often O
- O
abstract O
core O
concepts O
, O
we O
aim O
to O
predict O
more O
specific O
concepts O
that O
are O
also O
relevant O
to O
the O
given O
title O
. O
For O
instance O
, O
as O
displayed O
in O
Figure O
1 O
, O
for O
core O
concepts O
{ O
make O
, O
happen O
} O
and O
entities O
{ O
Bill O
Clinton O
, O
9 O
/ O
11 O
attacks O
} O
, O
we O
grow O
the O
input O
with O
more O
concrete O
concepts O
of O
{ O
mistake O
, O
administration O
} O
. O

We O
thus O
consider O
a O
concept O
expansion O
module O
g O
( O
• O
) O
, O
which O
predicts O
additional O
relevant O
concepts O
, O
denoted O
as O
C O
+ O
i O
, O
by O
conditioning O
on O
the O
original O
content O
item O
: O

C O
+ O
i O
= O
g O
( O
t O
, O
E O
i O
, O
C O
i O
) O
( O
1 O
) O

While O
g O
( O
• O
) O
can O
be O
any O
conditional O
predictor O
, O
our O
experiment O
shows O
that O
fine O
- O
tuned O
BART B-MethodName
model O
performs O
best O
on O
our O
tasks O
, O
where O
it O
generates O
C O
+ O
i O
word O
- O
by O
- O
word O
by O
consuming O
the O
content O
item O
. O
3 O
Training O
data O
construction O
is O
described O
in O
§ O
4.2 O
. O

Claim O
Generation O
. O
As O
discussed O
in O
§ O
1 O
, O
opinion O
text O
generation O
should O
be O
controlled O
with O
consistent O
propositions O
, O
which O
can O
not O
be O
effectively O
expressed O
by O
disconnected O
concepts O
. O
Therefore O
, O
we O
argue O
that O
natural O
languages O
are O
more O
suitable O
for O
delivering O
central O
claims O
, O
since O
they O
better O
encode O
stylistic O
languages O
, O
e.g. O
, O
persuasion O
strategies O
. O

Concretely O
, O
we O
fine O
- O
tune O
another O
BART B-MethodName
model O
by O
taking O
in O
the O
title O
t O
and O
the O
entities O
E O
i O
, O
which O
then O
produces O
a O
claim O
with O
nucleus O
sampling O
for O
decoding O
( O
Holtzman O
et O
al O
. O
, O
2020 O
) O
. O
In O
this O
work O
, O
we O
assume O
the O
subset O
of O
content O
items O
that O
can O
be O
used O
to O
generate O
claims O
is O
known O
. O
Possible O
future O
work O
includes O
predicting O
such O
subsets O
and O
filtering O
claims O
with O
quality O
measurement O
. O

Content O
Realization O
via O
Mixed O
Conditioning O

After O
obtaining O
the O
augmented O
content O
items O
, O
we O
leverage O
the O
BART B-MethodName
model O
to O
encode O
each O
of O
them O
as O
a O
sequence O
, O
as O
illustrated O
in O
Figure O
2 O
. O
Segmenter O
< O
s O
> O
is O
added O
to O
indicate O
the O
change O
of O
elements O
in O
a O
content O
item O
. O
Our O
encoders O
run O
over O
all O
items O
{ O
x O
i O
} O
in O
parallel O
, O
from O
which O
we O
extract O
content O
item O
representations O
{ O
h O
i O
} O
, O
based O
on O
the O
last O
layer O
's O
hidden O
states O
of O
the O
first O
token O
. O

The O
standard O
sequence O
- O
to O
- O
sequence O
( O
seq2seq O
) O
framework O
models O
output O
probabilities O
by O
taking O
a O
single O
sequence O
as O
input O
. O
It O
is O
challenging O
to O
extend O
seq2seq O
to O
consider O
multiple O
sequences O
simultaneously O
, O
and O
conduct O
content O
planning O
concurrently O
. O
Therefore O
, O
we O
introduce O
a O
plan O
scoring O
network O
, O
d O
( O
x O
i O
|y O
< O
t O
) O
, O
which O
learns O
to O
dynamically O
select O
and O
order O
content O
based O
on O
what O
has O
been O
produced O
previously O
while O
generating O
the O
outputs O
. O
As O
outlined O
in O
Figure O
2 O
, O
our O
generator O
is O
informed O
of O
all O
content O
items O
during O
generation O
. O
At O
each O
decoding O
step O
t O
, O
the O
probabilities O
of O
output O
words O
are O
estimated O
as O
a O
weighted O
sum O
of O
all O
content O
item O
- O
conditioned O
language O
models O
as O
follows O
: O

p O
( O
y O
t O
|y O
< O
t O
) O
= O
i O
d O
( O
x O
i O
|y O
< O
t O
) O
p O
( O
y O
t O
|y O
< O
t O
, O
x O
i O
) O
( O
2 O
) O
d O
( O
x O
i O
|y O
< O
t O
) O
= O
softmax O
i O
( O
e O
it O
) O
( O
3 O
) O

where O
p O
( O
y O
t O
|y O
< O
t O
, O
x O
i O
) O
corresponds O
to O
the O
i O
- O
th O
language O
model O
with O
x O
i O
as O
the O
input O
. O
Crucially O
, O
d O
( O
x O
i O
|y O
< O
t O
) O
determines O
the O
importance O
of O
x O
i O
when O
generating O
token O
y O
t O
and O
thus O
achieves O
the O
effect O
of O
content O
planning O
. O
We O
design O
a O
two O
- O
layer O
feedforward O
network O
to O
estimate O
e O
it O
: O

e O
it O
= O
W O
o O
tanh O
( O
W O
d O
[ O
h O
i O
; O
s O
t O
] O
) O
( O
4 O
) O

where O
h O
i O
denotes O
the O
representation O
of O
content O
item O
x O
i O
, O
s O
t O
is O
the O
decoder O
state O
, O
and O
W O
o O
and O
W O
d O
are O
learnable O
parameters O
. O
Although O
mixed O
language O
models O
have O
been O
used O
by O
Lewis O
et O
al O
. O
( O
2020b O
) O
to O
include O
retrieved O
documents O
for O
question O
answering O
, O
their O
relevance O
scores O
are O
given O
by O
external O
retrieval O
models O
, O
whereas O
our O
plan O
scorer O
d O
( O
x O
i O
|y O
< O
t O
) O
is O
learned O
together O
with O
the O
generator O
. O

Training O
and O
Decoding O
. O
Our O
model O
is O
end O
- O
to O
- O
end O
trained O
with O
both O
the O
standard O
cross B-MetricName
- I-MetricName
entropy I-MetricName
loss I-MetricName
L B-MetricName
gen I-MetricName
over O
the O
tokens O
in O
the O
target O
generations O
and O
a O
separate O
loss O
L B-MetricName
plan I-MetricName
for O
learning O
d O
( O
x O
i O
|y O
< O
t O
) O
: O

L B-MetricName
( O
θ O
) O
= O
L B-MetricName
gen I-MetricName
( O
θ O
) O
+ O
L B-MetricName
plan I-MetricName
( O
θ O
) O
( O
5 O
) O

To O
create O
labels O
for O
L B-MetricName
plan I-MetricName
, O
we O
leverage O
the O
correspondence O
between O
content O
items O
and O
target O
tokens O
, O
i.e. O
, O
d O
( O
x O
i O
|y O
< O
t O
) O
is O
optimized O
to O
approach O
1 O
if O
y O
i O
is O
in O
the O
sentence O
that O
derives O
x O
i O
, O
otherwise O
0 O
. O
4 O
Details O
about O
training O
data O
construction O
is O
in O
§ O
4.2 O
. O

At O
each O
decoding O
step O
, O
the O
individual O
language O
models O
, O
p O
( O
y O
t O
|y O
< O
t O
, O
x O
i O
) O
, O
and O
the O
distribution O
scores O
, O
d O
( O
x O
i O
|y O
< O
t O
) O
, O
are O
first O
calculated O
in O
parallel O
. O
We O
then O
decode O
each O
token O
greedily O
based O
on O
the O
mixed O
language O
models O
in O
an O
autoregressive O
way O
. O

Experiment O
Setups O

We O
experiment O
with O
the O
tasks O
of O
argument O
generation O
and O
opinion O
article O
writing O
( O
§ O
4.1 O
) O
. O
Both O
tasks O
require O
generating O
multi O
- O
sentence O
output O
, O
and O
contain O
a O
substantial O
amount O
of O
opinions O
and O
factual O
content O
. O
We O
describe O
the O
construction O
of O
initial O
content O
items O
and O
the O
training O
data O
for O
generating O
expanded O
concepts O
and O
claims O
in O
§ O
4.2 O
. O
We O
present O
models O
for O
comparison O
in O
§ O
4.3 O
. O
Finally O
, O
we O
provide O
implementation O
details O
in O
§ O
4.4 O
. O

Tasks O
and O
Datasets O

Argument B-TaskName
Generation I-TaskName
. O
We O
collect O
arguments O
from O
Reddit B-DatasetName
ChangeMyView I-DatasetName
5 O
( O
CMV B-DatasetName
) O
community O
, O
an O
online O
forum O
that O
features O
argumentative O
discussions O
. O
Each O
thread O
begins O
with O
an O
original O
post O
( O
OP O
) O
stating O
an O
opinion O
towards O
a O
controversial O
topic O
, O
e.g. O
, O
" O
The O
U.S. O
is O
too O
big O
for O
one O
government O
" O
. O
High O
- O
quality O
replies O
that O
counter O
- O
argue O
with O
the O
OP O
and O
are O
labeled O
with O
community O
endorsement O
are O
collected O
in O
our O
prior O
work O
( O
Hua O
and O
Wang O
, O
2020 O
) O
, O
covering O
content O
posted O
from O
2013 O
to O
2018 O
. O
In O
this O
work O
, O
we O
extend O
the O
data O
collection O
to O
2019 O
. O
Our O
goal O
is O
to O
generate O
the O
entire O
reply O
( O
i.e. O
, O
the O
target O
) O
given O
the O
OP O
title O
. O
Statistics O
about O
the O
CMV B-DatasetName
dataset O
are O
listed O
in O
Table O
1 O
. O
We O
reserve O
the O
most O
recent O
1 O
, O
000 O
samples O
for O
test O
and O
another O
1 O
, O
000 O
for O
validation O
. O

Opinion B-TaskName
Article I-TaskName
Writing I-TaskName
. O
Our O
second O
task O
is O
to O
generate O
opinion O
articles O
, O
as O
collected O
from O
the O
New B-DatasetName
York I-DatasetName
Times I-DatasetName
( O
NYT B-DatasetName
) O
corpus O
( O
Sandhaus O
, O
2008 O
) O
. O
We O
retain O
articles O
whose O
taxonomy O
labels O
include O
Top O
/ O
Opinion O
. O
To O
ensure O
that O
articles O
can O
be O
processed O
by O
our O
computing O
resource O
, O
we O
only O
keep O
the O
ones O
with O
at O
most O
20 O
sentences O
, O
representing O
60 O
% O
of O
all O
opinion O
articles O
. O
As O
shown O
in O
Table O
1 O
, O
NYT B-DatasetName
outputs O
tend O
to O
be O
significantly O
longer O
and O
contain O
less O
claims O
than O
CMV B-DatasetName
. O
Similarly O
, O
we O
keep O
1 O
, O
000 O
examples O
each O
for O
test O
and O
validation O
sets O
. O

Content O
Item O
Construction O

From O
target O
references O
, O
we O
describe O
how O
to O
automatically O
construct O
the O
input O
content O
items O
consisting O
of O
entities O
and O
core O
concepts O
, O
and O
how O
to O
collect O
training O
data O
to O
fine O
- O
tune O
BART B-MethodName
to O
predict O
more O
specific O
concepts O
and O
additional O
claims O
. O
Prior O
work O
has O
demonstrated O
the O
benefits O
of O
incorporating O
knowledge O
bases O
for O
text O
generation O
( O
Clark O
et O
al O
. O
, O
2018 O
; O
Puduppully O
et O
al O
. O
, O
2019 O
; O
Guan O
et O
al O
. O
, O
2020 O
thus O
consider O
two O
sources O
of O
knowledge O
: O
( O
1 O
) O
entities O
from O
Wikipedia O
, O
which O
are O
useful O
for O
modeling O
events O
and O
opinion O
targets O
, O
and O
( O
2 O
) O
concept O
words O
from O
ConceptNet B-MethodName
( O
Speer O
et O
al O
. O
, O
2017 O
) O
, O
that O
cover O
more O
related O
details O
. O
Note O
that O
our O
setup O
is O
generally O
applicable O
to O
other O
text O
generation O
tasks O
, O
as O
these O
input O
items O
can O
be O
obtained O
through O
standard O
NLP O
pipelines O
, O
as O
described O
below O
. O

Entity B-TaskName
Linking I-TaskName
. O
We O
first O
segment O
a O
reference O
into O
sentences O
. O
The O
ones O
with O
fewer O
than O
5 O
tokens O
are O
discarded O
for O
content O
item O
construction O
. O
For O
the O
rest O
, O
we O
extract O
entity O
mentions O
using O
Stanford O
CoreNLP O
( O
Manning O
et O
al O
. O
, O
2014 O
) O
, O
and O
further O
include O
nominal O
noun O
phrases O
. O
For O
entity O
linking O
, O
we O
adopt O
CrossWiki O
( O
Spitkovsky O
and O
Chang O
, O
2012 O
) O
, O
which O
can O
process O
our O
large O
- O
scale O
data O
within O
a O
reasonable O
amount O
of O
time O
. O
CrossWiki O
maps O
a O
mention O
to O
a O
list O
of O
frequently O
linked O
Wikipedia O
entries O
. O
We O
further O
manually O
verify O
and O
correct O
the O
linking O
results O
for O
the O
top O
500 O
most O
frequent O
mentions O
. O

Concept B-TaskName
Extraction I-TaskName
. O
To O
identify O
concepts O
in O
a O
reference O
, O
we O
match O
the O
lemmatized O
unigrams O
and O
their O
part O
- O
of O
- O
speech O
( O
POS O
) O
tags O
against O
all O
Concept O
- O
Net O
entries O
. O
To O
create O
a O
reasonably O
challenging O
task O
, O
we O
only O
keep O
a O
subset O
of O
the O
matches O
for O
inclusion O
in O
the O
core O
concept O
set O
( O
i.e. O
, O
C O
i O
) O
, O
with O
the O
rest O
used O
as O
C O
+ O
i O
, O
to O
be O
generated O
by O
our O
concept O
expansion O
model O
. O
Furthermore O
, O
we O
conjecture O
that O
an O
opinion O
article O
author O
tends O
to O
start O
with O
highlevel O
topics O
that O
cover O
more O
abstract O
topical O
words O
. O
We O
thus O
leverage O
a O
lexicon O
( O
Brysbaert O
et O
al O
. O
, O
2014 O
) O
with O
concreteness O
scores O
, O
ranging O
from O
0 O
( O
abstract O
) O
to O
5 O
( O
concrete O
) O
, O
for O
over O
40k O
English O
words O
. O
We O
keep O
concepts O
that O
are O
verbs O
or O
have O
a O
concreteness O
score O
lower O
than O
3.0 O
. O
Word O
coverage O
of O
references O
by O
using O
core O
concepts O
and O
additionally O
with O
aug O
- O
mented O
concepts O
are O
13.2 O
% O
and O
16.9 O
% O
on O
CMV B-DatasetName
respectively O
, O
and O
similarly O
on O
NYT B-DatasetName
( O
Table O
1 O
) O
. O
Finally O
, O
we O
train O
a O
concept O
generator O
with O
BART B-MethodName
to O
produce O
C O
+ O
i O
, O
conditional O
on O
C O
i O
, O
the O
title O
, O
and O
the O
entities O
. O

Claim B-TaskName
Detection I-TaskName
and I-TaskName
Generation I-TaskName
. O
Claims O
are O
indispensable O
for O
opinion O
articles O
. O
As O
described O
in O
§ O
3.1 O
, O
we O
aim O
to O
enrich O
content O
items O
with O
claims O
targeting O
the O
given O
entities O
within O
the O
title O
's O
context O
. O
To O
this O
end O
, O
we O
first O
train O
a O
claim O
detector O
by O
finetuning O
a O
BERT B-MethodName
base I-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
sequence O
classifier O
with O
a O
dataset O
consisting O
of O
sentences O
of O
claims O
and O
facts O
. O
Concretely O
, O
we O
collect O
54 O
, O
802 O
claim O
sentences O
from O
Kialo O
6 O
, O
a O
repository O
for O
debate O
arguments O
. O
We O
then O
sample O
50 O
, O
000 O
sentences O
from O
Wikipedia O
, O
which O
are O
treated O
as O
facts O
. O
This O
classifier O
is O
applied O
on O
a O
reference O
, O
and O
sentences O
that O
are O
labeled O
as O
claims O
become O
the O
target O
for O
our O
claim O
generator O
. O

We O
then O
learn O
a O
claim O
generator O
using O
BART B-MethodName
, O
which O
takes O
in O
the O
title O
and O
the O
entities O
, O
and O
outputs O
the O
claim O
. O
We O
augment O
our O
training O
data O
with O
replies O
collected O
from O
30 O
active O
subreddits O
related O
to O
political O
discussions O
, O
with O
details O
in O
Appendix O
A. O
In O
total O
, O
80 O
, O
566 O
sentences O
, O
which O
contain O
at O
least O
one O
entity O
and O
are O
labeled O
by O
our O
classifier O
as O
claims O
, O
are O
kept O
to O
train O
the O
generator O
. O

Baselines O
and O
Comparisons O

We O
compare O
with O
three O
baselines O
: O
( O
1 O
) O
RETRIEVAL B-MethodName
first O
calculates O
the O
TF O
- O
IDF O
weighted O
bag O
- O
of O
- O
words O
vectors O
for O
each O
content O
item O
, O
which O
is O
then O
used O
to O
query O
the O
training O
set O
sentences O
. O
The O
one O
with O
the O
highest O
cosine B-MetricName
similarity I-MetricName
is O
picked O
for O
each O
query O
, O
which O
are O
then O
ordered O
by O
a O
trained O
Pointer O
- O
Network O
( O
Vinyals O
et O
al O
. O
, O
2015 O
) O
as O
described O
in O
Gong O
et O
al O
. O
( O
2016 O
) O
. O
( O
2 O
) O
SENTPLANNER B-MethodName
( O
Hua O
and O
Wang O
, O
2019 O
) O
is O
an O
LSTM O
- O
based O
seq2seq O
model O
with O
a O
separate O
sentence O
planning O
decoder O
, O
where O
the O
planner O
selects O
keyphrases O
by O
using O
attentions O
and O
the O
generator O
reflects O
the O
selections O
. O
We O
treat O
our O
entities O
and O
concepts O
as O
keyphrases O
to O
feed O
to O
this O
model O
. O

( O
3 O
) O
SEQ2SEQ B-MethodName
is O
a O
fine O
- O
tuned O
BART B-MethodName
model O
, O
whose O
input O
is O
the O
original O
content O
items O
without O
augmentation O
, O
thus O
does O
not O
have O
access O
to O
the O
predicted O
concepts O
and O
claims O
. O

Additionally O
, O
we O
consider O
a O
strong O
comparison O
SEQ2SEQFULL B-MethodName
, O
by O
fine O
- O
tuning O
BART B-MethodName
with O
the O
same O
augmented O
content O
items O
as O
inputs O
as O
in O
our O
model O
. O
The O
difference O
is O
that O
the O
content O
items O
are O
concatenated O
before O
being O
used O
as O
input O
. O

Reproducibility O

We O
implement O
all O
models O
using O
the O
Huggingface O
Transformers O
library O
( O
Wolf O
et O
al O
. O
, O
2020 O
) O
with O
Py O
- O
Torch O
( O
Paszke O
et O
al O
. O
, O
2019 O
) O
. O
We O
use O
the O
base O
model O
for O
BART B-MethodName
, O
which O
has O
768 B-HyperparameterValue
dimensional B-HyperparameterName
states I-HyperparameterName
and O
6 B-HyperparameterValue
layers B-HyperparameterName
for O
both O
encoder O
and O
decoder O
( O
140 O
M O
parameters O
in O
total O
) O
. O
Our O
newly O
added O
plan O
scoring O
network O
only O
contains O
1.2 O
M O
parameters O
, O
less O
than O
1 O
% O
of O
the O
pre O
- O
trained O
model O
. O
Our O
generation O
model O
is O
optimized O
using O
Adam O
( O
Kingma O
and O
Ba O
, O
2014 O
) O
, O
with O
a O
batch B-HyperparameterName
size I-HyperparameterName
of O
3 B-HyperparameterValue
. O
To O
improve O
efficiency O
, O
we O
adopt O
the O
mixed O
- O
precision O
( O
FP16 O
) O
to O
train O
each O
model O
, O
using O
one O
NVIDIA O
Titan O
RTX O
GPU O
card O
with O
24 O
GB O
memory O
. O
The O
number B-HyperparameterName
of I-HyperparameterName
content I-HyperparameterName
items I-HyperparameterName
is O
limited O
to O
10 B-HyperparameterValue
per O
sample O
, O
and O
the O
numbers B-HyperparameterName
of I-HyperparameterName
entities I-HyperparameterName
and O
concepts B-HyperparameterName
per O
content O
item O
are O
capped O
at O
20 B-HyperparameterValue
, O
respectively O
. O
We O
also O
truncate O
the O
target O
output O
to O
at O
most O
200 B-HyperparameterValue
tokens B-HyperparameterName
during O
training O
. O
Early O
stopping O
is O
applied O
over O
validation O
loss O
. O
Our O
model O
converges O
after O
being O
trained O
for O
38 O
hours O
( O
19 O
epochs O
) O
on O
CMV B-DatasetName
, O
and O
45 O
hours O
( O
15 O
epochs O
) O
on O
NYT B-DatasetName
. O
The O
best O
validation O
perplexity B-MetricName
reaches O
about O
6.1 B-MetricValue
after O
model O
convergence O
on O
both O
datasets O
. O

Results O

Automatic O
Evaluation O

Here O
we O
report O
results O
on O
test O
sets O
with O
standard O
automatic O
metrics O
: O
BLEU B-MetricName
( O
Papineni O
et O
al O
. O
, O
2002 O
) O
measures O
the O
n B-MetricName
- I-MetricName
gram I-MetricName
precision I-MetricName
( O
here O
we O
consider O
up O
to O
bigrams O
) O
; O
ROUGE B-MetricName
( O
Lin O
, O
2004 O
) O
, O
calculated O
based O
on O
n B-MetricName
- I-MetricName
gram I-MetricName
recall I-MetricName
; O
and O
METEOR B-MetricName
( O
Denkowski O
and O
Lavie O
, O
2014 O
) O
, O
which O
also O
accounts O
for O
synonyms O
. O
In O
Table O
2 O
, O
we O
first O
present O
the O
results O
when O
goldstandard O
concept O
expansion O
is O
used O
. O

Our O
proposed O
DYPLOC B-MethodName
model O
achieves O
significantly O
higher O
performance O
across O
all O
metrics O
on O
both O
datasets O
. O
In O
particular O
, O
the O
substantial O
lead O
over O
SEQ2SEQFULL B-MethodName
, O
which O
has O
access O
to O
the O
same O
content O
items O
as O
ours O
, O
indicates O
that O
dynamic O
content O
planning O
with O
mixed O
language O
models O
produces O
superior O
generations O
. O
Among O
comparison O
models O
, O
the O
gap O
between O
SEQ2SEQFULL B-MethodName
and O
SEQ2SEQ B-MethodName
shows O
the O
effectiveness O
of O
content O
item O
augmentation O
. O
We O
also O
observe O
a O
significant O
drop O
for O
baselines O
without O
using O
large O
models O
, O
highlighting O
the O
importance O
of O
pre O
- O
training O
. O
Ablation O
Study O
. O
To O
verify O
the O
effect O
of O
each O
element O
in O
content O
items O
, O
we O
further O
train O
ablated O
models O
by O
removing O
concepts O
, O
claims O
, O
or O
entities O
. O
The O
results O
are O
also O
displayed O
in O
Table O
2 O
. O
In O
general O
, O
scores O
decrease O
when O
using O
only O
partial O
content O
items O
, O
among O
which O
removing O
all O
concepts O
lead O
to O
the O
biggest O
performance O
drop O
, O
suggesting O
that O
entities O
and O
claims O
alone O
are O
insufficient O
to O
produce O
informative O
outputs O
. O

Effect O
of O
Hard O
Selection O
of O
Content O
Items O
. O

To O
test O
the O
necessity O
of O
using O
weighted O
- O
sum O
marginalization O
( O
Eq O
. O
2 O
) O
, O
we O
experiment O
with O
two O
comparisons O
with O
hard O
selections O
, O
i.e. O
, O
either O
randomly O
choosing O
a O
content O
item O
, O
or O
using O
the O
one O
with O
the O
highest O
predicted O
plan O
score O
( O
greedy O
selection O
) O
. O
For O
both O
cases O
, O
we O
set O
the O
selected O
content O
item O
's O
plan O
score O
as O
1.0 O
, O
with O
the O
rest O
of O
the O
candidates O
having O
a O
score O
of O
0.0 O
, O
to O
ensure O
the O
probabilities O
summed O
up O
to O
1.0 O
. O
As O
can O
be O
seen O
from O
the O
bottom O
two O
rows O
of O
Table O
2 O
, O
not O
surprisingly O
, O
random O
selection O
performs O
much O
worse O
. O
We O
observe O
that O
its O
generations O
lack O
coherence O
and O
fluency O
, O
implying O
the O
effectiveness O
of O
our O
learnable O
content O
planner O
. O

On O
the O
other O
hand O
, O
using O
greedily O
selected O
content O
items O
obtains O
comparable O
results O
with O
DYPLOC B-MethodName
, O
where O
a O
weighted O
sum O
of O
content O
items O
is O
considered O
. O
Indeed O
, O
we O
find O
that O
DYPLOC B-MethodName
's O
plan O
scores O
are O
often O
sharp O
where O
one O
content O
item O
has O
much O
higher O
weight O
than O
others O
, O
and O
in O
these O
scenarios O
, O
it O
is O
almost O
equivalent O
to O
the O
greedy O
selection O
setup O
. O

Results O
with O
Generated O
Concepts O
. O
Table O
3 O
lists O
generation O
results O
with O
our O
system O
generated O
concepts O
as O
expansion O
. O
While O
all O
systems O
yield O
worse O
results O
compared O
to O
using O
gold O
- O
standard O
concepts O
, O
our O
DYPLOC B-MethodName
still O
outperforms O
other O
models O
by O
substantial O
margins O
, O
showing O
its O
robustness O
when O
input O
concepts O
are O
noisy O
. O
Yet O
it O
also O
suggests O
the O
importance O
of O
having O
more O
accurate O
and O
comprehensive O
concept O
expansion O
, O
which O
should O
be O
explored O
in O
the O
future O
work O
. O

Human O
Evaluation O

We O
hire O
three O
proficient O
English O
speakers O
to O
evaluate O
four O
key O
aspects O
of O
the O
generated O
outputs O
: O
( O
1 O
) O
grammaticality B-MetricName
; O

( O
2 O
) O
coherence B-MetricName
, O
measuring O
if O
the O
text O
is O
logical O
and O
cohesive O
; O
( O
3 O
) O
relevance B-MetricName
, O
gauging O
topic O
relatedness O
to O
the O
input O
title O
; O
and O
( O
4 O
) O
content B-MetricName
richness I-MetricName
, O
assessing O
the O
specificity O
and O
whether O
there O
is O
enough O
details O
in O
the O
outputs O
. O
Each O
aspect O
is O
rated O
on O
a O
scale O
of O
1 B-MetricValue
( O
worst O
) O
to O
5 B-MetricValue
( O
best O
) O
. O
In O
addi- O
and O
respect O
to O
the O
office O
of O
the O
Presidency O
in O
the O
eyes O
of O
the O
American O
people O
and O
the O
world O
at O
large O
. O
We O
owe O
your O
readers O
an O
apology O
and O
a O
correction O
of O
your O
editorial O
's O
glaring O
omission O
, O
which O
created O
an O
erroneous O
impression O
. O
SEQ2SEQFULL B-MethodName
" O
The O
Picture O
of O
Bill O
Clinton O
" O
( O
editorial O
, O
Dec. O
5 O
) O
states O
that O
President O
Clinton O
" O
has O
done O
more O
harm O
to O
our O
respect O
for O
the O
Presidency O
than O
Presidents O
Reagan O
, O
Ford O
, O
Carter O
and O
Bush O
. O
" O
This O
demonstrates O
that O
President O
Reagan O
's O
failure O
to O
do O
more O
damage O
to O
our O
honor O
than O
President O
Bush O
's O
failure O
in O
office O
. O
You O
omitted O
from O
your O
list O
President O
Clinton O
's O
achievements O
that O
brought O
honor O
and O
dignity O
to O
the O
eyes O
of O
the O
American O
people O
and O
to O
the O
world O
at O
large O
. O
[ O
... O
] O
tion O
, O
judges O
also O
rank O
the O
system O
outputs O
by O
their O
overall O
preferences O
. O
Detailed O
evaluation O
guideline O
is O
attached O
in O
Appendix O
C. O
We O
randomly O
select O
50 O
samples O
from O
the O
test O
sets O
for O
both O
tasks O
, O
and O
present O
outputs O
by O
SEQ2SEQ B-MethodName
, O
SEQ2SEQFULL B-MethodName
, O
and O
DYPLOC B-MethodName
in O
random O
orders O
. O
Table O
4 O
shows O
that O
DYPLOC B-MethodName
receives O
higher O
scores O
across O
all O
aspects O
and O
tasks O
. O
In O
particular O
, O
the O
considerable O
differences O
in O
coherence O
and O
content O
richness O
indicate O
that O
our O
framework O
yields O
better O
content O
organization O
as O
well O
as O
retains O
more O
useful O
information O
. O
Overall O
, O
our O
system O
outputs O
are O
ranked O
best O
for O
44.7 B-MetricValue
% I-MetricValue
and O
45.9 B-MetricValue
% I-MetricValue
of O
the O
time O
in O
two O
tasks O
, O
significantly O
more O
than O
the O
comparisons O
. O

Analysis O
on O
Argumentative O
Quality O
. O
In O
the O
ablation O
study O
, O
we O
find O
that O
our O
full O
model O
's O
performance O
is O
similar O
to O
the O
version O
without O
having O
claims O
as O
input O
. O
We O
suspect O
this O
is O
because O
claims O
are O
often O
paraphrased O
or O
even O
not O
directly O
used O
when O
delivering O
an O
argument O
, O
which O
can O
not O
be O
captured O
by O
the O
automatic O
metrics O
. O
To O
better O
understand O
how O
claims O
are O
used O
for O
generation O
, O
we O
randomly O
select O
50 O
examples O
by O
DYPLOC B-MethodName
and O
its O
variant O
without O
claims O
, O
and O
ask O
the O
same O
human O
judges O
to O
decide O
whether O
there O
is O
a O
clear O
central O
argument O
conveyed O
by O
each O
generated O
argument O
on O
CMV B-DatasetName
. O

We O
observe O
that O
66.7 B-MetricValue
% I-MetricValue
of O
the O
outputs O
by O
our O
full O
model O
are O
recognized O
as O
successfully O
delivering O
arguments O
with O
consistent O
stances O
, O
whereas O
only O
61.3 B-MetricValue
% I-MetricValue
are O
true O
for O
the O
model O
variant O
without O
claims O
. O
This O
gap O
confirms O
that O
claim O
drafts O
can O
indeed O
promote O
the O
argumentative O
quality O
as O
perceived O
by O
human O
readers O
. O

Further O
Discussions O

Evaluation O
results O
on O
generation O
quality O
have O
shown O
the O
effectiveness O
of O
our O
mixed O
language O
models O
. O
In O
this O
section O
, O
we O
aim O
to O
further O
understand O
the O
behavior O
of O
the O
plan O
scoring O
network O
, O
d O
( O
x|y O
< O
t O
) O
, O
such O
as O
how O
it O
affects O
the O
usage O
of O
content O
items O
for O
generation O
. O
Specifically O
, O
we O
adopt O
the O
following O
procedure O
to O
construct O
alignment O
between O
each O
sentence O
in O
the O
output O
and O
content O
items O
: O
for O
each O
token O
y O
t O
, O
we O
establish O
a O
mapping O
y O
t O
→ O
x O
i O
if O
x O
i O
is O
the O
most O
important O
item O
for O
producing O
y O
t O
, O
i.e. O
, O
x O
i O
= O
argmax O
x O
d O
( O
x|y O
< O
t O
) O
, O
and O
d O
( O
x O
i O
|y O
< O
t O
) O
> O
0.5 O
. O
If O
all O
tokens O
in O
an O
entire O
sentence O
are O
mapped O
to O
the O
same O
x O
i O
, O
we O
consider O
this O
sentence O
is O
aligned O
to O
that O
content O
item O
. O
Based O
on O
this O
rule O
, O
we O
show O
sample O
output O
and O
corresponding O
alignments O
in O
Table O
5 O
. O
For O
the O
rest O
of O
this O
section O
, O
we O
conduct O
analyses O
based O
on O
this O
alignment O
result O
. O
We O
first O
examine O
whether O
the O
model O
learns O
to O
utilize O
enough O
content O
items O
, O
i.e. O
, O
high O
coverage O
. O
Then O
we O
provide O
insights O
on O
whether O
the O
generation O
faithfully O
reflects O
the O
argumentative O
claims O
using O
entailment O
relation O
labeling O
by O
human O
inspection O
. O

How O
many O
content O
items O
are O
used O
by O
the O
output O
? O
Human O
judges O
have O
rated O
our O
model O
output O
to O
contain O
more O
relevant O
information O
( O
Table O
4 O
) O
. O
We O
believe O
this O
can O
be O
attributed O
to O
the O
enhanced O
capacity O
to O
access O
and O
reflect O
the O
input O
data O
with O
dynamic O
content O
planning O
, O
as O
a O
result O
of O
mixed O
language O
models O
. O
To O
verify O
this O
hypothesis O
, O
we O
calculate O
the O
percentage O
of O
content O
items O
that O
are O
aligned O
to O
at O
least O
one O
output O
sentence O
. O
Figure O
3 O
shows O
that O
, O
using O
our O
system O
, O
the O
coverage O
reaches O
87.25 B-MetricValue
% I-MetricValue
on O
CMV B-DatasetName
and O
83.89 B-MetricValue
% I-MetricValue
for O
NYT B-DatasetName
. O
If O
we O
replace O
the O
generated O
concepts O
with O
gold O
- O
standard O
concepts O
( O
as O
extracted O
from O
references O
) O
instead O
, O
the O
coverage O
exceeds O
90 B-MetricValue
% I-MetricValue
on O
both O
tasks O
. O
These O
observations O
indicate O
that O
our O
model O
can O
indeed O
adequately O
utilize O
the O
input O
data O
, O
with O
more O
accurate O
concepts O
further O
encouraging O
higher O
coverage O
. O

How O
are O
claim O
content O
items O
realized O
? O
Claims O
are O
the O
central O
elements O
for O
opinion O
text O
construction O
. O
As O
mentioned O
in O
§ O
4.2 O
, O
a O
subset O
of O
the O
content O
items O
are O
supplied O
with O
claim O
sentences O
. O
In O
order O
to O
examine O
whether O
they O
are O
realized O
as O
claim O
sentences O
in O
the O
outputs O
, O
we O
leverage O
the O
fine O
- O
tuned O
BERT B-MethodName
classifier O
( O
§ O
4.2 O
) O
to O
label O
all O
output O
sentences O
. O
90.96 B-MetricValue
% I-MetricValue
of O
the O
sentences O
that O
are O
aligned O
to O
a O
claim O
element O
in O
the O
input O
are O
also O
labeled O
as O
claim O
on O
CMV B-DatasetName
. O
The O
percentage O
is O
only O
69.41 B-MetricValue
% I-MetricValue
for O
NYT B-DatasetName
, O
though O
, O
likely O
because O
the O
NYT B-DatasetName
opinion O
articles O
still O
contain O
more O
objective O
information O
. O

Furthermore O
, O
we O
conduct O
a O
human O
evaluation O
study O
to O
assess O
the O
semantic O
relations O
between O
claim O
input O
and O
its O
aligned O
generated O
sentence O
. O
We O
randomly O
sample O
50 O
outputs O
from O
test O
sets O
, O
and O
ask O
four O
human O
judges O
to O
read O
each O
. O
For O
each O
sample O
, O
we O
highlight O
one O
output O
sentence O
that O
is O
aligned O
to O
a O
content O
item O
with O
claim O
element O
. O
The O
judges O
determine O
a O
three O
- O
way O
( O
ENTAIL B-MetricName
, O
NEUTRAL B-MetricName
, O
CONTRA B-MetricName
- I-MetricName
DICTORY I-MetricName
) O
entailment O
relation O
between O
the O
input O
claim O
( O
premise O
) O
and O
the O
output O
( O
hypothesis O
) O
. O
Results O
show O
that O
ENTAIL B-MetricName
accounts O
for O
49.06 B-MetricValue
% I-MetricValue
of O
all O
instances O
, O
while O
only O
3.77 B-MetricValue
% I-MetricValue
are O
deemed O
CONTRA B-MetricName
- I-MetricName
DICTORY I-MetricName
. O
Upon O
inspection O
, O
the O
contradictory O
pairs O
are O
usually O
disagreements O
with O
regard O
to O
implicit O
sentiments O
, O
e.g. O
, O
" O
Journalist O
is O
the O
most O
responsible O
for O
the O
problem O
" O
vs. O
" O
Media O
coverage O
is O
a O
good O
thing O
. O
" O
. O
This O
suggests O
that O
while O
our O
conditional O
language O
model O
achieves O
reasonable O
semantic O
control O
in O
most O
cases O
, O
it O
is O
still O
not O
guaranteed O
to O
capture O
more O
nuanced O
semantics O
encoded O
in O
opinions O
and O
arguments O
. O
Future O
work O
includes O
designing O
representations O
that O
can O
better O
model O
stances O
in O
opinions O
as O
well O
as O
argumentative O
structures O
. O

Conclusion O

We O
present O
a O
novel O
text O
generation O
framework O
that O
enables O
dynamic O
content O
planning O
based O
on O
mixed O
conditional O
language O
models O
. O
We O
further O
employ O
large O
models O
to O
augment O
system O
inputs O
with O
diverse O
content O
that O
covers O
both O
objective O
and O
subjective O
information O
. O
The O
experiments O
on O
two O
distinct O
opinion O
text O
generation O
tasks O
show O
that O
our O
proposed O
model O
compares O
favorably O
against O
strong O
comparisons O
based O
on O
fine O
- O
tuned O
BART B-MethodName
models O
with O
the O
same O
input O
. O
Human O
evaluation O
further O
confirms O
that O
our O
model O
generations O
have O
richer O
information O
and O
better O
content O
organization O
. O

A O
Training O
Data O
Construction O
for O
Claim B-MethodName
Generator I-MethodName

We O
describe O
the O
claim O
generation O
model O
in O
§ O
4.2 O
for O
content O
item O
enrichment O
. O
Since O
both O
our O
CMV B-DatasetName
and O
NYT B-DatasetName
data O
focus O
on O
the O
politics O
domain O
, O
we O
leverage O
a O
collection O
of O
Reddit O
posts O
from O
politics O
related O
subreddits O
. O
The O
full O
list O
of O
subreddits O
are O
shown O
in O
Table O
6 O
. O
In O
total O
, O
we O
collect O
1.6 O
million O
posts O
, O
which O
are O
split O
into O
sentences O
, O
among O
which O
we O
only O
keep O
the O
ones O
classified O
as O
claim O
by O
the O
BERT B-MethodName
base O
classifier O
and O
have O
at O
least O
one O
named O
entity O
. O

B O
Additional O
Automatic O
Evaluation O
Results O

In O
§ O
5.1 O
, O
we O
report O
results O
by O
automatic O
metrics O
using O
system O
predicted O
concepts O
in O
Table O
3 O
. O
Here O
we O
additionally O
show O
the O
results O
evaluated O
by O
ROUGE-2 B-MetricName
and O
average O
output O
lengths O
in O
Table O
7 O
. O

C O
Human O
Evaluation O
Guideline O

We O
include O
the O
detailed O
human O
evaluation O
guidelines O
in O
Figure O
4 O
. O
Note O
that O
we O
collect O
53 O
samples O
for O
annotation O
for O
each O
domain O
. O
The O
first O
three O
are O
for O
calibration O
only O
and O
not O
be O
included O
in O
the O
final O
results O
. O

D O
Additional O
Sample O
Outputs O

Additional O
example O
content O
items O
and O
generations O
are O
demonstrated O
in O
Table O
8 O
and O
Table O
9 O
. O

In O
the O
following O
studies O
, O
you O
will O
evaluate O
the O
system O
outputs O
of O
three O
text O
generation O
models O
on O
two O
different O
domains O
. O
For O
each O
domain O
, O
there O
will O
be O
53 O
examples O
presented O
, O
each O
starting O
with O
a O
statement O
, O
followed O
by O
three O
system O
generations O
. O
Please O
first O
read O
the O
statement O
and O
then O
the O
system O
outputs O
. O
At O
the O
end O
of O
each O
output O
, O
please O
provide O
your O
judgment O
on O
the O
quality O
of O
the O
following O
aspects O
, O
based O
on O
a O
scale O
of O
1 B-MetricValue
( O
worst O
) O
to O
5 B-MetricValue
( O
best O
) O
: O

• O
Grammaticality B-MetricName
: O
whether O
the O
text O
reads O
fluently O
and O
has O
no O
grammar O
error O
-1 O
. O
Major O
grammatical O
errors O
that O
significantly O
impact O
comprehension O
of O
text O
. O
E.g. O
, O
" O
I O
'm O
not O
a O
quick O
skimming O
, O
but O
i O
m O
quickly O
making O
a O
comment O
. O
" O
. O
-3 B-MetricValue
. O
Minor O
grammatical O
errors O
that O
do O
not O
significantly O
impact O
comprehension O
of O
text O
. O
E.g. O
, O
" O
I O
have O
car O
that O
works O
, O
and O
I O
make O
it O
to O
work O
by O
commute O
45 O
minutes O
to O
an O
hour O
on O
my O
bike O
. O
" O
-5 B-MetricValue
. O
No O
grammatical O
issues O
. O
E.g. O
, O
" O
There O
are O
swathes O
of O
people O
whose O
function O
is O
determined O
by O
technology O
, O
and O
they O
use O
technology O
as O
a O
crutch O
. O
" O

• O
Coherence B-MetricName
: O
whether O
the O
information O
transition O
is O
natural O
and O
well O
- O
structured O
-1 O
. O
Sentences O
are O
completely O
unrelated O
. O
E.g. O
, O
" O
The O
Supreme O
Court O
created O
a O
mechanism O
for O
interpreting O
the O
Constitution O
through O
a O
modern O
lens O
. O
The O
question O
is O
, O
do O
you O
create O
jobs O
? O
Ukraine O
is O
a O
direct O
ally O
of O
the O
US O
. O
" O
-3 B-MetricValue
. O
Sentences O
are O
connected O
to O
one O
another O
but O
transitions O
seem O
disjointed O
; O
there O
does O
n't O
appear O
to O
be O
a O
strong O
progression O
of O
ideas O
. O
E.g. O
, O
" O
Muslims O
worship O
the O
figure O
of O
Allah O
. O
Christians O
worship O
the O
figures O
of O
God O
. O
Muslims O
do O
not O
worship O
the O
Jews O
. O
Muslims O
do O
n't O
worship O
the O
Christian O
figure O
of O
God O
, O
Muslims O
worship O
God O
. O
They O
worship O
the O
Jewish O
figure O
of O
the O
figure O
. O
" O
-5 B-MetricValue
. O
Sentences O
transition O
smoothly O
and O
connect O
to O
form O
a O
logical O
progression O
. O
E.g. O
, O
" O
Every O
country O
has O
to O
deal O
with O
their O
own O
geography O
. O
USA O
benefits O
from O
decent O
climate O
country O
wide O
, O
plentiful O
natural O
resources O
and O
distance O
from O
areas O
of O
war O
. O
The O
downside O
is O
that O
they O
are O
close O
to O
Mexico O
and O
Mexico O
pretty O
much O
sucks O
, O
so O
it O
's O
inhabitants O
want O
to O
get O
into O
the O
USA O
. O
Unless O
you O
believe O
that O
all O
resources O
and O
other O
benefits O
should O
be O
shared O
then O
why O
should O
the O
world O
take O
on O
the O
USA O
downfalls O
while O
not O
getting O
any O
of O
the O
plusses O
? O
" O

• O
Relevance B-MetricName
: O
whether O
the O
content O
of O
the O
text O
is O
relevant O
to O
the O
title O
Title O
: O
The O
recent O
swell O
in O
protesting O
Commencement O
speakers O
at O
colleges O
is O
a O
good O
thing O
. O

-1 B-MetricValue
. O
The O
output O
is O
generic O
or O
completely O
irrelevant O
. O
E.g. O
, O
" O
Supply O
and O
demand O
. O
The O
US O
thinks O
those O
drugs O
are O
worth O
price O
X. O
Other O
countries O
are O
only O
willing O
to O
pay O
price O
Y. O
The O
US O
develops O
more O
IP O
related O
content O
than O
other O
countries O
because O
it O
has O
a O
huge O
military O
and O
is O
able O
to O
enforce O
IP O
laws O
. O
" O
-3 B-MetricValue
. O
The O
text O
is O
tangential O
to O
the O
title O
and O
the O
input O
( O
it O
may O
share O
an O
entity O
or O
key O
concept O
in O
common O
) O
, O
though O
it O
might O
not O
be O
precisely O
on O
topic O
. O
E.g. O
, O
" O
When O
you O
enter O
a O
college O
career O
, O
you O
decide O
to O
take O
literature O
studies O
. O
You O
can O
become O
an O
engineer O
, O
history O
, O
linguistics O
, O
etc O
. O
" O
-5 B-MetricValue
. O
The O
text O
is O
highly O
relevant O
with O
the O
title O
and O
the O
input O
. O
E.g. O
, O
" O
The O
problem O
with O
protesting O
minority O
opinions O
is O
that O
you O
force O
the O
majority O
opinion O
to O
come O
out O
against O
them O
, O
and O
as O
a O
result O
you O
find O
controversial O
speakers O
turning O
their O
commencement O
speeches O
into O
bland O
speeches O
. O
Commencement O
speeches O
are O
a O
recognition O
of O
a O
person O
, O
and O
offer O
an O
affirmation O
of O
their O
worldview O
. O
" O

• O
Content B-MetricName
Richness I-MetricName
: O
whether O
the O
output O
conveys O
substantial O
content O
-1 B-MetricValue
. O
Generic O
response O
with O
no O
useful O
information O
about O
the O
topic O
. O
E.g. O
, O
" O
I O
do O
n't O
have O
time O
to O
address O
the O
point O
you O
're O
making O
. O
" O
-3 B-MetricValue
. O
With O
one O
or O
two O
key O
points O
that O
are O
useful O
as O
counter O
- O
argument O
. O
E.g. O
, O
" O
Reducing O
costs O
is O
not O
the O
goal O
of O
the O
free O
market O
. O
" O
-5 B-MetricValue
. O
With O
sufficient O
key O
information O
that O
is O
useful O
as O
counter O
- O
argument O
. O
E.g. O
, O
" O
Reducing O
costs O
is O
not O
the O
goal O
of O
the O
free O
market O
. O
Simply O
setting O
prices O
for O
medical O
procedures O
has O
been O
shown O
to O
be O
extremely O
effective O
. O
I O
will O
tacitly O
admit O
that O
your O
post O
is O
true O
for O
many O
countries O
, O
but O
the O
US O
health O
share O
is O
less O
than O
1 O
% O
of O
GDP O
. O
" O
The O
founding O
idea O
of O
the O
police O
force O
was O
literally O
to O
function O
as O
an O
alternative O
to O
preventing O
crime O
, O
and O
as O
such O
, O
as O
a O
military O
force O
. O
Basically O
, O
a O
group O
of O
people O
who O
are O
paid O
full O
time O
as O
US O
citizens O
have O
a O
responsibility O
to O
protect O
themselves O
. O
Separating O
the O
police O
from O
the O
law O
completely O
elevates O
the O
power O
of O
the O
people O
. O
A O
lot O
of O
the O
comments O
argue O
that O
guns O
are O
necessary O
, O
but O
I O
agree O
that O
the O
most O
important O
aspect O
of O
police O
consent O
is O
the O
principles O
of O
consent O
. O
SEQ2SEQFULL B-MethodName
I O
see O
a O
lot O
of O
comments O
arguing O
for O
guns O
, O
and O
I O
agree O
with O
you O
that O
the O
most O
important O
aspect O
of O
police O
is O
consent O
. O
The O
principles O
of O
the O
founding O
of O
the O
idea O
of O
the O
police O
force O
are O
literally O
the O
only O
ones O
that O
function O
in O
preventing O
crime O
. O
The O
military O
is O
basically O
an O
extension O
of O
the O
Police O
force O
. O
It O
's O
a O
group O
of O
people O
who O
pay O
full O
time O
citizens O
to O
take O
responsibility O
for O
protecting O
themselves O
. O
Separating O
them O
from O
the O
law O
completely O
elevates O
their O
power O
Title O
I O
do O
not O
believe O
democracy O
inherently O
protects O
freedom O
. O
CMV B-DatasetName
. O

DYPLOC B-MethodName

Modern O
democracies O
do O
a O
better O
job O
of O
protecting O
people O
's O
freedom O
from O
government O
. O
In O
that O
respect O
, O
modern O
democracies O
have O
grown O
freer O
over O
time O
, O
and O
the O
strides O
in O
freedom O
that O
took O
place O
in O
the O
20th O
century O
were O
mostly O
because O
of O
the O
democratic O
movement O
. O
SEQ2SEQFULL B-MethodName
Modern O
democracy O
's O
job O
is O
to O
protect O
people O
's O
freedom O
. O
The O
government O
in O
many O
respects O
has O
grown O
freer O
over O
time O
, O
and O
has O
taken O
on O
a O
different O
striding O
place O
in O
the O
20th O
century O
. O
The O
democratic O
movement O
was O
formed O
during O
the O
time O
of O
the O
time O
, O
during O
the O
2020th O
century O
, O
and O
during O
the O
21st O
century O
, O
the O
democratic O
movement O
has O
taken O
over O
. O
The O
movement O
has O
done O
a O
better O
job O
of O
protecting O
freedom O
than O
modern O
democracy O
. O

Acknowledgements O

This O
research O
is O
supported O
in O
part O
by O
National O
Science O
Foundation O
through O
Grant O
IIS-1813341 O
. O
We O
thank O
three O
anonymous O
reviewers O
for O
their O
valuable O
suggestions O
on O
various O
aspects O
of O
this O
work O
. O

Ethics O
Statement O

Large O
models O
that O
are O
pre O
- O
trained O
on O
heterogeneous O
web O
data O
are O
shown O
to O
encode O
biases O
and O
can O
be O
potentially O
harmful O
for O
marginalized O
populations O
. O
Along O
with O
the O
improved O
controllability O
, O
we O
also O
recognize O
that O
our O
system O
might O
be O
misused O
to O
create O
fabricated O
or O
offensive O
content O
. O
We O
therefore O
advocate O
cautious O
and O
responsible O
practices O
in O
realworld O
deployment O
. O

Title O

Can O
the O
Global O
Economy O
Really O
Stabilize O
Itself O
? O

Content O
Items O
( O
a O
) O
[ O
ENT O
] O
Nation O
[ O
CON O
] O
include O
, O
impose O
, O
limit O
, O
foreign O
, O
ownership O
, O
natural O
, O
cumulative O
, O
investor O
, O
treaty O
, O
media O
, O
traditional O
[ O
CLAIM O
] O
If O
the O
nation O
is O
already O
wealthy O
and O
ca O
n't O
afford O
to O
spend O
its O
money O
on O
its O
own O
people O
, O
why O
should O
it O
spend O
their O
money O
on O
other O
people O
? O
( O
b O
) O
[ O
ENT O
] O
BERMAN O
New O
York O
, O
Feb. O
13 O
, O
1999 O
The O
writer O
is O
vice O
chairman O
of O
fiduciary O
trust O
at O
Prudential O
International O
. O
SEQ2SEQFULL B-MethodName
To O
the O
Editor O
: O
Re O
" O
Funds O
Big O
and O
Fail O
, O
Fed O
Assists O
in O
Bailout O
" O
( O
front O
page O
, O
Dec. O
17 O
) O
: O
It O
is O
difficult O
to O
understand O
why O
the O
Federal O
Reserve O
Bank O
of O
New O
York O
should O
not O
organize O
a O
privately O
held O
hedge O
fund O
to O
manage O
long O
- O
term O
capital O
. O
The O
Federal O
Reserve O
has O
been O
the O
only O
central O
bank O
of O
central O
bank O
management O
and O
regulatory O
governance O
in O
the O
country O
to O
assert O
the O
efficiency O
of O
free O
- O
market O
forces O
. O
The O
Fed O
has O
consistently O
advocated O
the O
principle O
that O
the O
global O
community O
should O
not O
allow O
regulators O
to O
arrange O
the O
work O
of O
hedge O
funds O
. O
MICHAEL O
J. O
KAPLAN O
New O
York O
, O
Dec. O
" O
17 O
, O
1998 O
The O
writer O
is O
vice O
chairman O
of O
the O
fiduciary O
trust O
at O
the O
International O
Monetary O
Fund O
. O

Language B-TaskName
Grounding I-TaskName
Through O
Neuro B-TaskName
- I-TaskName
Symbolic I-TaskName
Interaction I-TaskName
in O
a O
3D O
World O

We O
propose O
PIGLeT B-MethodName
: O
a O
model O
that O
learns O
physical O
commonsense O
knowledge O
through O
interaction O
, O
and O
then O
uses O
this O
knowledge O
to O
ground O
language O
. O
We O
factorize O
PIGLeT B-MethodName
into O
a O
physical O
dynamics O
model O
, O
and O
a O
separate O
language O
model O
. O
Our O
dynamics O
model O
learns O
not O
just O
what O
objects O
are O
but O
also O
what O
they O
do O
: O
glass O
cups O
break O
when O
thrown O
, O
plastic O
ones O
do O
n't O
. O
We O
then O
use O
it O
as O
the O
interface O
to O
our O
language O
model O
, O
giving O
us O
a O
unified O
model O
of O
linguistic O
form O
and O
grounded O
meaning O
. O
PIGLeT B-MethodName
can O
read O
a O
sentence O
, O
simulate O
neurally O
what O
might O
happen O
next O
, O
and O
then O
communicate O
that O
result O
through O
a O
literal O
symbolic O
representation O
, O
or O
natural O
language O
. O
Experimental O
results O
show O
that O
our O
model O
effectively O
learns O
world O
dynamics O
, O
along O
with O
how O
to O
communicate O
them O
. O
It O
is O
able O
to O
correctly O
forecast O
" O
what O
happens O
next O
" O
given O
an O
English O
sentence O
over O
80 B-MetricValue
% I-MetricValue
of O
the O
time O
, O
outperforming O
a O
100x B-MetricValue
larger O
, O
text O
- O
to O
- O
text O
approach O
by O
over O
10 B-MetricValue
% I-MetricValue
. O
Likewise O
, O
its O
natural O
language O
summaries O
of O
physical O
interactions O
are O
also O
judged O
by O
humans O
as O
more O
accurate O
than O
LM O
alternatives O
. O
We O
present O
comprehensive O
analysis O
showing O
room O
for O
future O
work O
. O
The O
robot O
throws O
the O
vase O
onto O
the O
coffee O
table O
. O
The O
robot O
is O
holding O
a O
vase O
, O
and O
there O
is O
a O
laptop O
on O
the O
coffee O
table O
that O
is O
on O
. O
The O
laptop O
and O
the O
vase O
both O
break O
, O
with O
the O
vase O
shattering O
into O
smaller O
pieces O
, O
and O
the O
laptop O
powers O
off O
. O

Introduction O

As O
humans O
, O
our O
use O
of O
language O
is O
linked O
to O
the O
physical O
world O
. O
To O
process O
a O
sentence O
like O
" O
the O
robot O
turns O
on O
the O
stove O
, O
with O
a O
pan O
on O
it O
" O
( O
Figure O
1 O
) O
we O
might O
imagine O
a O
physical O
Pan O
object O
. O
This O
meaning O
representation O
in O
our O
heads O
can O
be O
seen O
as O
a O
part O
of O
our O
commonsense O
world O
knowledge O
, O
about O
what O
a O
Pan O
is O
and O
does O
. O
We O
might O
reasonably O
predict O
that O
the O
Pan O
will O
become O
Hot O
-and O
if O
there O
's O
an O
Egg O
on O
it O
, O
it O
would O
become O
cooked O
. O

As O
humans O
, O
we O
learn O
such O
a O
commonsense O
world O
model O
through O
interaction O
. O
Young O
children O
learn O
to O
reason O
physically O
about O
basic O
objects O
by O
manipulating O
them O
: O
observing O
the O
properties O
they O
have O
, O

Language O
Model O

PIGLeT B-MethodName
t O
t+1 O

The O
robot O
turns O
on O
the O
stove O
, O
with O
a O
pan O
on O
it O
. O

< O
heatUp O
, O
Pan O
> O

Physical O
Dynamics O
Model O

The O
pan O
is O
now O
hot O
and O
the O
egg O
becomes O
cooked O
. O

Figure O
1 O
: O
PIGLeT. B-MethodName
Through O
physical O
interaction O
in O
a O
3D O
world O
, O
we O
learn O
a O
model O
for O
what O
actions O
do O
to O
objects O
. O
We O
use O
our O
physical O
model O
as O
an O
interface O
for O
a O
language O
model O
, O
jointly O
modeling O
elements O
of O
language O
form O
and O
meaning O
. O
Given O
an O
action O
expressed O
symbolically O
or O
in O
English O
, O
PIGLeT O
can O
simulate O
what O
might O
happen O
next O
, O
expressing O
it O
symbolically O
or O
in O
English O
. O
and O
how O
they O
change O
if O
an O
action O
is O
applied O
on O
them O
( O
Smith O
and O
Gasser O
, O
2005 O
) O
. O
This O
process O
is O
hypothesized O
to O
be O
crucial O
to O
how O
children O
learn O
language O
: O
the O
names O
of O
these O
elementary O
objects O
become O
their O
first O
" O
real O
words O
" O
upon O
which O
other O
language O
is O
scaffolded O
( O
Yu O
and O
Smith O
, O
2012 O
) O
. O

In O
contrast O
, O
the O
dominant O
paradigm O
today O
is O
to O
train O
large O
language O
or O
vision O
models O
on O
static O
data O
, O
such O
as O
language O
and O
photos O
from O
the O
web O
. O
Yet O
such O
a O
setting O
is O
fundamentally O
limiting O
, O
as O
suggested O
empirically O
by O
psychologists O
' O
failed O
attempts O
to O
get O
kittens O
to O
learn O
passively O
( O
Held O
and O
Hein O
, O
1963 O
) O
. O
More O
recently O
, O
though O
large O
Transformers O
have O
made O
initial O
progress O
on O
benchmarks O
, O
they O
also O
have O
frequently O
revealed O
biases O
in O
those O
same O
datasets O
, O
suggesting O
they O
might O
not O
be O
solving O
underlying O
tasks O
( O
Zellers O
et O
al O
. O
, O
2019b O
) O
. O
This O
has O
been O
argued O
philosophically O
by O
a O
flurry O
of O
re O
- O
Figure O
2 O
: O
PIGPeN B-DatasetName
, O
a O
setting O
for O
few O
- O
shot O
language O
- O
world O
grounding O
. O
We O
collect O
data O
for O
280k O
physical O
interactions O
in O
THOR O
, O
a O
3D O
simulator O
with O
20 O
actions O
and O
125 O
object O
types O
, O
each O
with O
42 O
attributes O
( O
e.g. O
isBroken O
) O
. O
We O
annotate O
2k O
interactions O
with O
English O
sentences O
describing O
the O
initial O
world O
state O
, O
the O
action O
, O
and O
the O
action O
result O
. O
cent O
work O
arguing O
that O
no O
amount O
of O
language O
form O
could O
ever O
specify O
language O
meaning O
( O
McClelland O
et O
al O
. O
, O
2019 O
; O
Bender O
and O
Koller O
, O
2020 O
; O
Bisk O
et O
al O
. O
, O
2020 O
) O
; O
connecting O
back O
to O
the O
Symbol O
Grounding O
Problem O
of O
Harnad O
( O
1990 O
) O
. O

In O
this O
paper O
, O
we O
investigate O
an O
alternate O
strategy O
for O
learning O
physical O
commonsense O
through O
interaction O
, O
and O
then O
transferring O
that O
into O
language O
. O
We O
introduce O
a O
model O
named O
PIGLeT B-MethodName
, O
short O
for O
Physical B-MethodName
Interaction I-MethodName
as I-MethodName
Grounding I-MethodName
for I-MethodName
Language I-MethodName
Transformers I-MethodName
. O
We O
factorize O
an O
embodied O
agent O
into O
an O
explicit O
model O
of O
world O
dynamics O
, O
and O
a O
model O
of O
language O
form O
. O
We O
learn O
the O
dynamics O
model O
through O
interaction O
. O
Given O
an O
action O
heatUp O
applied O
to O
the O
Pan O
in O
Figure O
1 O
, O
the O
model O
learns O
that O
the O
Egg O
on O
the O
pan O
becomes O
Hot O
and O
Cooked O
, O
and O
that O
other O
attributes O
do O
not O
change O
. O

We O
integrate O
our O
dynamics O
model O
with O
a O
pretrained O
language O
model O
, O
giving O
us O
a O
joint O
model O
of O
linguistic O
form O
and O
meaning O
. O
The O
combined O
PIGLeT B-MethodName
can O
then O
reason O
about O
the O
physical O
dynamics O
implied O
by O
English O
sentences O
describing O
actions O
, O
predicting O
literally O
what O
might O
happen O
next O
. O
It O
can O
then O
communicate O
that O
result O
either O
symbolically O
or O
through O
natural O
language O
, O
generating O
a O
sentence O
like O
' O
The O
egg O
becomes O
hot O
and O
cooked O
. O
" O
Our O
separation O
between O
physical O
dynamics O
and O
language O
allows O
the O
model O
to O
learn O
about O
physical O
commonsense O
from O
the O
physical O
world O
itself O
, O
while O
also O
avoiding O
recurring O
problems O
of O
artifacts O
and O
biases O
that O
arise O
when O
we O
try O
to O
model O
physical O
world O
understanding O
solely O
through O
language O
. O

We O
study O
this O
through O
a O
new O
environment O
and O
evaluation O
setup O
called O
PIGPeN B-DatasetName
, O
short O
for O
Physical B-DatasetName
Interaction I-DatasetName
Grounding I-DatasetName
Paired I-DatasetName
with I-DatasetName
Natural I-DatasetName
Language I-DatasetName
. O
In O
PIGPeN B-DatasetName
, O
a O
model O
is O
given O
unlimited O
access O
to O
an O
environment O
for O
pretraining O
, O
but O
only O
500 O
examples O
with O
paired O
English O
annotations O
. O
Models O
in O
our O
setup O
must O
additionally O
generalize O
to O
novel O
' O
unseen O
' O
objects O
for O
which O
we O
intentionally O
do O
not O
provide O
paired O
language O
- O
environment O
supervision O
. O
We O
build O
this O
on O
top O
of O
the O
THOR O
environment O
( O
Kolve O
et O
al O
. O
, O
2017 O
) O
, O
a O
physics O
engine O
that O
enables O
agents O
to O
perform O
contextual O
interactions O
( O
Fig O
2 O
) O
on O
everyday O
objects O
. O

Experiments O
confirm O
that O
PIGLeT B-MethodName
performs O
well O
at O
grounding O
language O
with O
meaning O
. O
Given O
a O
sentence O
describing O
an O
action O
, O
our O
model O
predicts O
the O
resulting O
object O
states O
correctly O
over O
80 B-MetricValue
% I-MetricValue
of O
the O
time O
, O
outperforming O
even O
a O
100x B-MetricValue
larger O
model O
( O
T5 B-MethodName
- I-MethodName
11B I-MethodName
) O
by O
over O
10 B-MetricValue
% I-MetricValue
. O
Likewise O
, O
its O
generated O
natural O
language O
is O
rated O
by O
humans O
as O
being O
more O
correct O
than O
equivalently O
- O
sized O
language O
models O
. O
Last O
, O
it O
can O
generalize O
in O
a O
' O
zero O
- O
shot O
' O
way O
to O
objects O
that O
it O
has O
never O
read O
about O
before O
in O
language O
. O

In O
summary O
, O
we O
make O
three O
key O
contributions O
. O
First O
, O
we O
introduce O
PIGLeT B-MethodName
, O
a O
model O
decoupling O
physical O
and O
linguistic O
reasoning O
. O
Second O
, O
we O
introduce O
PIGPeN B-DatasetName
, O
to O
learn O
and O
evaluate O
the O
transfer O
of O
physical O
knowledge O
to O
the O
world O
of O
language O
. O
Third O
, O
we O
perform O
experiments O
and O
analysis O
suggesting O
promising O
avenues O
for O
future O
work O
. O

2 O
PIGPeN B-DatasetName
: O
A O
Resource O
for O
Neuro B-TaskName
- I-TaskName
Symbolic I-TaskName
Language I-TaskName
Grounding I-TaskName

We O
introduce O
PIGPeN B-DatasetName
as O
a O
setting O
for O
learning O
and O
evaluating O
physically O
grounded O
language O
understanding O
. O
An O
overview O
is O
shown O
in O
Figure O
2 O
. O
The O
idea O
is O
that O
an O
agent O
gets O
access O
to O
an O
interactive O
3D O
environment O
, O
where O
it O
can O
learn O
about O
the O
world O
through O
interaction O
-for O
example O
, O
that O
objects O
such O
as O
a O
Vase O
can O
become O
Broken O
if O
thrown O
. O
The O
goal O
for O
a O
model O
is O
to O
learn O
natural O
language O
meaning O
grounded O
in O
these O
interactions O
. O
Task O
definition O
. O
Through O
interaction O
, O
an O
agent O
observes O
the O
interplay O
between O
objects O
o O
2 O
O O
( O
represented O
by O
their O
attributes O
) O
and O
actions O
a O
2 O
A O
through O
the O
following O
transition O
: O

{ O
o O
1 O
, O
. O
. O
. O
, O
o O
N O
} O
| O
{ O
z O
} O
o O
, O
state O
pre O
- O
action O
⇥ O
a O
! O
{ O
o O
0 O
1 O
, O
. O
. O
. O
, O
o O
0 O
N O
} O
| O
{ O
z O
} O
o O
0 O
, O
state O
post O
- O
action O
. O
( O
1 O
) O

Actions O
change O
the O
state O
of O
a O
subset O
of O
objects O
: O
turning O
on O
a O
Faucet O
affects O
a O
nearby O
Sink O
, O
but O
it O
will O
not O
change O
a O
Mirror O
on O
the O
wall O
. O

To O
encourage O
learning O
from O
interaction O
, O
and O
not O
just O
language O
, O
an O
agent O
is O
given O
a O
small O
number O
of O
natural O
language O
annotations O
of O
transitions O
. O
We O
denote O
these O
sentences O
as O
sõ O
, O
describing O
the O
state O
pre O
- O
action O
, O
s O
a O
the O
action O
, O
and O
sõ0 O
the O
state O
postaction O
respectively O
. O
During O
evaluation O
, O
an O
agent O
will O
sometimes O
encounter O
new O
objects O
o O
that O
were O
not O
part O
of O
the O
paired O
training O
data O
. O

We O
evaluate O
the O
model O
's O
transfer O
in O
two O
ways O
: O
a. O
PIGPeN B-DatasetName
- I-DatasetName
NLU I-DatasetName
. O
A O
model O
is O
given O
object O
states O
o O
, O
and O
an O
English O
sentence O
s O
a O
describing O
an O
action O
. O
It O
must O
predict O
the O
grounded O
object O
states O
o O
0 O
that O
result O
after O
the O
action O
is O
taken O
. O
b. O
PIGPeN B-DatasetName
- I-DatasetName
NLG I-DatasetName
. O
A O
model O
is O
given O
object O
states O
o O
and O
a O
literal O
action O
a. O
It O
must O
generate O
a O
sentence O
sõ0 O
describing O
the O
state O
post O
- O
action O
. O

We O
next O
describe O
our O
environment O
, O
feature O
representation O
, O
and O
language O
annotation O
process O
. O

Environment O
: O
THOR O

We O
use O
AI2 O
- O
THOR O
as O
an O
environment O
for O
this O
task O
( O
Kolve O
et O
al O
. O
, O
2017 O
) O
. O
In O
THOR O
, O
a O
robotic O
agent O
can O
navigate O
around O
and O
perform O
rich O
contextual O
interactions O
with O
objects O
in O
a O
house O
. O
For O
instance O
, O
it O
can O
grab O
an O
Apple O
, O
slice O
it O
, O
put O
it O
in O
a O
Fridge O
, O
drop O
it O
, O
and O
so O
on O
. O
The O
state O
of O
the O
Apple O
, O
such O
as O
whether O
it O
is O
sliced O
or O
cold O
, O
changes O
accordingly O
; O
this O
is O
not O
possible O
in O
many O
other O
environments O
. O

In O
this O
work O
, O
we O
use O
the O
underlying O
THOR O
simulator O
as O
a O
proxy O
for O
grounded O
meaning O
. O
Within O
THOR O
, O
it O
can O
be O
seen O
as O
a O
' O
complete O
' O
meaning O
representation O
( O
Artzi O
et O
al O
. O
, O
2013 O
) O
, O
as O
it O
fully O
specifies O
the O
kind O
of O
grounding O
a O
model O
can O
expect O
in O
its O
perception O
within O
THOR O
. O

Objects O
. O
The O
underlying O
THOR O
representation O
of O
each O
object O
o O
is O
in O
terms O
of O
42 O
attributes O
; O
we O
provide O
a O
list O
in O
Appendix O
B. O
We O
treat O
these O
attributes O
as O
words O
specific O
to O
an O
attribute O
- O
level O
dictionary O
; O
for O
example O
, O
the O
temperature O
Hot O
is O
one O
of O
three O
possible O
values O
for O
an O
object O
's O
temperature O
; O
the O
others O
being O
Cold O
and O
RoomTemp O
. O

Actions O
. O
An O
action O
a O
in O
THOR O
is O
a O
function O
that O
takes O
up O
to O
two O
objects O
as O
arguments O
. O
Actions O
are O
highly O
contextual O
, O
affecting O
not O
only O
the O
arguments O
but O
potentially O
other O
objects O
in O
the O
scene O
( O
Figure O
2 O
) O
. O
We O
also O
treat O
action O
names O
as O
words O
in O
a O
dictionary O
. O

Filtering O
out O
background O
objects O
. O
Most O
actions O
change O
the O
state O
of O
only O
a O
few O
objects O
, O
yet O
there O
can O
be O
many O
objects O
in O
a O
scene O
. O
We O
keep O
annotation O
and O
computation O
tractable O
by O
having O
models O
predict O
( O
and O
humans O
annotate O
) O
possible O
changes O
of O
at O
most O
two O
key O
objects O
in O
the O
scene O
. O
As O
knowing O
when O
an O
object O
does O
n't O
change O
is O
also O
important O
, O
we O
include O
non O
- O
changing O
objects O
if O
fewer O
than O
two O
change O
. O

Exploration O
. O
Any O
way O
of O
exploring O
the O
environment O
is O
valid O
for O
our O
task O
, O
however O
, O
we O
found O
that O
exploring O
intentionally O
was O
needed O
to O
yield O
good O
coverage O
of O
interesting O
states O
. O
Similar O
to O
prior O
work O
for O
instruction O
following O
( O
Shridhar O
et O
al O
. O
, O
2020 O
) O
, O
we O
designed O
an O
oracle O
to O
collect O
diverse O
and O
interesting O
trajectories O
{ O
õ O
, O
a O
, O
õ O
0 O
} O
. O
Our O
oracle O
randomly O
selects O
one O
of O
ten O
high O
level O
tasks O
, O
see O
Appendix O
B O
for O
the O
list O
. O
These O
in O
turn O
require O
randomly O
choosing O
objects O
in O
the O
scene O
; O
e.g. O
a O
Vase O
and O
a O
Laptop O
in O
Figure O
2 O
. O
We O
randomize O
the O
manner O
in O
which O
the O
oracle O
performs O
the O
task O
to O
discover O
diverse O
situations O
. O

In O
total O
, O
we O
sampled O
20k O
trajectories O
. O
From O
these O
we O
extracted O
280k O
transitions O
( O
Eqn O
1 O
's O
) O
where O
at O
least O
one O
object O
changes O
state O
, O
for O
training O
. O

Annotating O
Interactions O
with O
Language O

Data O
Selection O
for O
Annotation O

We O
select O
2k O
action O
state O
- O
changes O
from O
trajectories O
held O
out O
from O
the O
training O
set O
. O
We O
select O
them O
while O
also O
balancing O
the O
distribution O
of O
action O
types O
to O
ensure O
broad O
coverage O
in O
the O
final O
dataset O
. O
We O
are O
also O
interested O
in O
a O
model O
's O
ability O
to O
generalize O
to O
new O
object O
categories O
-beyond O
what O
it O
has O
read O
about O
, O
or O
observed O
in O
a O
training O
set O
. O
We O
thus O
select O
30 O
objects O
to O
be O
" O
unseen O
, O
" O
and O
exclude O
these O
from O
paired O
environment O
- O
language O
training O
data O
. O
We O
sample O
500 O
state O
transitions O
, O
containing O
only O
" O
seen O
" O
objects O
to O
be O
the O
training O
set O
; O
we O
use O
500 O
for O
validation O
and O
1000 O
for O
testing O
. O

Natural O
Language O
Annotation O

Workers O
on O
Mechanical O
Turk O
were O
shown O
an O
environment O
in O
THOR O
before O
and O
after O
a O
given O
action O
a. O
Each O
view O
contains O
the O
THOR O
attributes O
of O
the O
two O
key O
objects O
. O
Workers O
then O
wrote O
three O
English O
sentences O
, O
corresponding O
to O
sõ O
, O
s O
a O
, O
and O
sõ0 O
respectively O
. O
Workers O
were O
instructed O
to O
write O
at O
a O
particular O
level O
of O
detail O
: O
enough O
so O
that O
a O
reader O
could O
infer O
" O
what O
happens O
next O
" O
from O
sõ O
and O
s O
a O
, O
yet O
without O
mentioning O
redundant O
attributes O
. O
We O
provide O
more O
details O
in O
Appendix O
C O
. O

Modeling O
PIGLeT B-MethodName

In O
this O
section O
, O
we O
describe O
our O
PIGLeT B-MethodName
model O
. O

First O
, O
we O
learn O
a O
neural O
physical O
dynamics O
model O

The O
robot O
is O
holding O
a O
glass O
vase O
. O

The O
robot O
throws O
the O
vase O
. O

Language O
Model O

Size O
: O
medium O
< O
l O
a O
t O
e O
x O
i O
t O
s O
h O
a O
1 O
_ O
b O
a O
s O
e O
6 O
4 O
= O
" O
T O
k O
3 O
H O
X O
n O

F O
V O
I O
r O
K O
V O
r O
y O
i O
j O
e O
/ O
Q O
E O
d O
n O
L O
V O
R O
C O
o O
= O
" O
> O
A O
A O
A O
D O
M O
H O
i O
c O
f O
V O
J O
L O
b O
9 O
N O
A O
E O
N O
6 O
6 O
B O
V O
r O
z O
S O
o O
E O
b O
F O
4 O
s O
I O
C O
X O
G O
I O
7 O
I O
I O
E O
x O
w O
o O
4 O
c O
E O
E O
U O
i O
b O
S O
V O
4 O
i O
g O
a O
b O
y O
b O
O O
K O
v O
u O
w O
Z O
t O
e O
l O
q O
e O
X O
/ O
0 O
i O
s O
c O
+ O
T O
V O
w O
Q O
r O
3 O
y O
K O
9 O
g O
k O
P O
u O
C O
E O
M O
t O
J O
q O
v O
v O
3 O
m O
u O
b O
O O
T O
F O
V O
J O
Y O
F O
8 O
c O
/ O
t O
4 O
L O
t O
n O
R O
s O
3 O
b O
+ O
3 O
u O
h O
b O
f O
v O
3 O
L O
1 O
3 O
v O
7 O
P O
/ O
4 O
N O
i O
a O
k O
j O
j O
2 O
u O
Z O
G O
G O
T O
j O
O O
w O
K O
I O
X O
G O
v O
h O
N O
O O
4 O
m O
l O
B O
C O
C O
q O
T O
e O
J O
L O
N O
3 O
i O
7 O
s O
J O
2 O
d O
I O
V O
h O
j O
9 O
2 O
c O
0 O
L O
H O
C O
r O
I O
t O
Z O
g O
I O
D O
s O
5 O
T O
o O
8 O
6 O
j O
9 O
A O
x O
5 O
l O
W O
Z O
G O
j O
u O
1 O
c O
e O
V O
W O
Z O
u O
h O
5 O
1 O
u O
n O
E O
v O
X O
k O
q O
0 O
C O
Z O
I O
G O
d O
F O
k O
j O
R O
6 O
P O
9 O
Y O
C O
c O
d O
G O
1 O
4 O
q O
1 O
I O
5 O
L O
s O
H O
a O
Q O
x O
I O
U O
b O
V O
k O
B O
O O
c O
I O
l O
1 O
m O
J O
Y O
W O
C O
+ O
A O
z O
y O
H O
H O
g O
o O
Q O
a O
F O
d O
l O
g O
t O
2 O
6 O
+ O
j O
p O
5 O
4 O
Z O
R O
x O
N O
D O
/ O
m O
g O
X O
L O
d O
m O
/ O
I O
y O
p O
Q O
d O
t O
G O
c O
9 O
1 O
T O
g O
p O
n O
b O
d O
t O
i O
D O
/ O
Z O
R O
u O
U O
b O
v O
J O
6 O
W O
A O
l O
d O
l O
A O
4 O
1 O
X O
x O
W O
a O
l O
D O
J O
y O
J O
l O
r O
M O
I O
h O
o O
L O
Q O
u O
7 O
k O
3 O
A O
P O
g O
J O
H O
y O
v O
E O
Z O
8 O
C O
A O
X O
d O
+ O
Y O
q O
0 O
q O
q O
p O
R O
O O
k O
P O
n O
S O
e O
k O
n O
F O
Q O
f O
I O
2 O
k O
x O
M O
U O
U O
8 O
H O
P O
2 O
y O
y O
h O
t O
O O
K O
i O
P O
Y O
Z O
r O
U O
p O
J O
x O
/ O
m O
d O
0 O
3 O
m O
Y O
z O
1 O
b O
6 O
X O
J O
N O
e O
S O
G O
c O
L O
N O
E O
p O
k O
x O
M O
w O
e O
Z O
v O
b O
b O
w O
O O
/ O
S O
/ O
R O
f O
j O
B O
T O
+ O
5 O
j O
g O
Q O
T O
O O
0 O
P O
M O
q O
B O
c O
o O
V O
n O
N O
d O
V O
o O
/ O
/ O
n O
J O
v O
T O
K O
z O
e O
s O
w O
D O
P O
3 O
e O
J O
O O
t O
b O
s O
g O
m O
O O
D O
3 O
r O
J O
i O
1 O
7 O
8 O
6 O
W O
X O
3 O
8 O
E O
2 O
z O
Q O
b O
v O
s O
M O
X O
v O
C O
n O
r O
G O
E O
v O
W O
K O
H O
7 O
D O
0 O
7 O
Y O
n O
3 O
G O
2 O
Q O
W O
7 O

Z O
F O
/ O
Z O
t O
+ O
B O
7 O
8 O
C O
P O
4 O
F O
V O
y O
t O
X O
I O
O O
t O
J O
u O
Y O
h O
a O
0 O
n O
w O
+ O
w O
/ O
L O
m O
Q O
/ O
3 O
< O
/ O
l O
a O
t O
e O
x O
i O
t O
> O
õ O

s O
A O
V O
3 O
W O
y O
N O
F O
o O
P O
9 O
r O
N O
x O
4 O
Z O
X O
C O
r O
X O
j O
E O
q O
w O
d O
Z O
G O
n O
p O
h O
h O
7 O
I O
C O
S O
6 O
x O
j O
v O
P O
K O
Y O
g O
l O
8 O
D O
l O
M O
c O
B O
K O
h O
B O
o O
R O
3 O
6 O
V O
f O
9 O
1 O
8 O
j O
g O
w O
4 O
2 O
R O
i O
K O
B O
z O
t O
k O
h O
X O
7 O
d O
4 O
Q O
H O
Z O
Z O
f O
d O
B O
U O
8 O
F O
b O
m O
Y O
3 O
b O
U O
v O
y O
X O
7 O
Z O
B O
5 O
S O
a O
v O
h O
l O
7 O
o O
s O
n O
K O
o O
+ O
b O
r O
Q O
p O
J O
K O
J O
M O
8 O
l O
y O
G O
M O
l O
Y O
E O
H O
I O
n O
F O
w O
E O
A O
J O
x O
F O
6 O
T O
f O
g O
M O
C O
L O
g O
L O
I O
2 O
t O
V O
U O
Z O
V O
0 O
g O
s O
z O
n O
1 O
k O
s O
8 O
B O
8 O
n O
b O
z O
J O
S O
g O
n O
A O
l O
+ O
3 O
m O
Y O
J O
p O
R O
V O
f O
2 O
m O
O O
4 O
J O
C O
U O
Z O
F O
7 O
5 O
G O
T O
9 O
t O
s O
o O
d O
r O
3 O
i O
u O
R O
G O
M O
k O
O O
4 O
X O
a O
I O
w O
Z O
u O
6 O
g O
s O
J O
c O
W O
f O
o O
v O
h O
t O
w O
j O
f O
h O
8 O
l O
9 O
K O
J O
H O
A O
G O
X O
r O
m O
c O
6 O
C O
p O
g O
v O
P O
a O
N O
/ O
p O
/ O
b O
k O
K O
v O
3 O
Y O
K O
O O
4 O
z O
j O
s O
T O
b O
a O
5 O
J O
d O
v O
g O
+ O
K O
C O

X O
P O
e O
+ O
l O
H O
1 O
9 O
0 O
D O
1 O
8 O
3 O
G O
7 O
T O
H O
H O
r O
J O
H O
7 O
C O
n O
L O
2 O
E O
t O
2 O
y O
N O
6 O
x O
I O
9 O
Z O
n O
n O
H O
n O
2 O
l O
X O
1 O
j O
3 O
6 O
M O
f O
0 O
U O
X O
0 O
M O
/ O
q O
1 O
d O
o O
1 O
2 O
m O
p O
j O
7 O
r O
C O
X O
R O
7 O
z O
9 O
k O
c O
R O
A O
o O
< O
/ O
l O
a O
t O
e O
x O
i O
t O
> O
a O
< O
l O
a O
t O
e O
x O
i O
t O
s O
h O
a O
1 O
_ O
b O
a O
s O
e O
6 O
4 O
= O
" O
e O
l O
I O
5 O
F O
e O
e O
j O
P O
p O
e O
M O
9 O
u O
U O
n O
R O
f O
3 O
I O
Q O
j O
S O
e O
v O
x O
U O
= O
" O
> O
A O

A O
A O
D O
K O
H O
i O
c O
f O
V O
J O
L O
b O
9 O
N O
A O
E O
N O
6 O
a O
Q O
o O
t O
5 O
t O
X O
D O
k O
Y O
h O
E O
h O
I O
Q O
6 O
R O
T O
Z O
H O
g O
W O
A O
E O
H O
L O
o O
g O
i O
k O
b O

Z O
S O
E O
q O
r O
x O
Z O
u O
K O
s O
s O
g O
9 O
r O
d O
l O
w O
a O
r O
P O
w O
P O
r O
n O
D O
k O
1 O
3 O
B O
D O
v O
f O
J O
L O
2 O
C O
Q O
+ O
4 O
I O
Q O
y O
0 O
m O
q O
+ O
/ O
e O
a O
5 O
s O
5 O
O O
X O
W O
n O
l O
O O
0 O
8 O
u O
t O
6 O
N O
r O
2 O
9 O
R O
s O
7 O
u O
z O
f O
j O
W O
7 O
f O
v O
3 O
L O
2 O
3 O
t O
3 O
/ O
/ O
2 O
L O
u O
K O
J O
P O
a O
k O
0 O
T O
enc O
< O
l O
a O
t O
e O
x O
i O
t O
s O
h O
a O
1 O
_ O
b O
a O
s O
e O
6 O
4 O
= O
" O
F O
A O
l O
D O
c O
C O
J O
y O
x O
B O
B O
e O
5 O
F O
r O
g O
+ O
r O
9 O
i O
t O
6 O
V O
U O
k O
m O
A O
= O
" O
> O
A O

4 O
5 O
O O
c O
/ O
C O
o O
l O
c O
U O
e O
K O
9 O
Z O
4 O
W O
h O
K O
C O
y O
T O
W O
e O
5 O
N O
P O
X O
C O
/ O
v O
J O
O O
Z O
J O
X O
z O
n O
7 O
k O
W O
Y O
l O
D O
A O
4 O
V O
V O
Y O
y O
W O
B O
A O
/ O
V O
p O
k O
D O
s O
9 O
8 O
j O
M O
T O
V O
A O
3 O
z O
s O
7 O
1 O
O O
2 O
k O
2 O
X O
k O
m O
y O
C O
r O
A O
E O
d O
0 O
c O
j O
R O
2 O
X O
6 O
0 O
P O
R O
g O
5 O
W O
R O
m O
0 O
L O
D O
V O
4 O
3 O
8 O
/ O
S O
k O
o O
c O
1 O
E O
C O
u O
p O
c O
R O
4 O
P O
K O
o O
8 O
l O
y O
C O
k O
U O
2 O
A O
/ O
Q O
g O
k O
E O
/ O
r O
J O
d O
t O
z O
5 O
P O
H O
g O
R O
k O
l O
Y O
0 O
f O
h O
W O
E O
6 O
W O
7 O
N O
8 O
R O
N O
R O
i O
/ O
6 O
C O
1 O
4 O
G O
u O
C O
J O
X O
7 O
c O
t O
y O
H O
/ O
Z O
+ O
h O
W O
P O
X O
w O
5 O
r O
Z O
c O
u O
K O
0 O
c O
p O
V O
o O
X O
G O
l O
E O
3 O
b O
J O
Y O
g O
b O
J O
S O
B O
F O
K O
1 O
r O
M O
A O
Q O
J O
I O
K O
v O
S O
Z O
y O
A O
g O
S O
S O
w O
6 O
R O
a O
V O
U O
y O
l O
W O
Z O
H O
7 O
3 O
H O
p O
J O
L O
U O
H O
L O
N O
l O
M O
Q O
l O
B O
M O
l O
L O
9 O
o O
s O
o O
f O
b O
q O
S O
3 O
s O
M O
V O
6 O
Q O
k O
x O
+ O
F O
H O
b O
N O
F O
m O
c O
9 O
O O
+ O
V O
6 O
T O
X O
k O
j O
n O
C O
z O
R O
K O
5 O
c O
1 O
O O
G O
3 O
F O
9 O
Z O
+ O
A O
2 O
G O
3 O
y O
J O
8 O
F O
y O
b O
3 O
v O
k O
Q O
C O
d O
v O
S O
0 O
H O
g O
A O
V O
B O
i O

A O
A O
D O
N O
X O
i O
c O
f O
V O
J O
L O
b O
9 O
N O
A O
E O
N O
6 O
a O
A O
s O
W O
8 O
U O
j O
g O
h O
L O
h O
Y O
R O
E O
u O
I O
Q O
O O
Q O
U O
J O
j O
h O
V O
w O
4 O
I O
I O
o O
U O
t O
N O
W O
i O
q O
N O
o O
v O
R O
k O
7 O
q O
+ O
z O
D O
m O
h O
1 O
D O
g O
m O
X O
x O
a O
7 O
j O
C O
k O
d O
/ O
C O
g O
R O
v O
i O
y O
l O
9 O
g O
n O
f O
q O
A O
E O
8 O
p O
I O
q O
/ O
n O
2 O
m O
5 O
m O
d O
n O
U O
d O
a O
K O
O O
k O
o O
j O
r O
/ O
v O
B O
J O
d O
2 O
L O
1 O
+ O
5 O
u O
n O
c O
t O
v O
H O
7 O
j O
5 O
q O
3 O
b O
v O
f O
0 O
7 O
J O
8 O
6 O
W O
K O
G O
A O
k O
r O
L O
J O
4 O
l O
n O
I O
H O
S O
h O
o O
Y O
k O
S O
Q O
F O
Z O
w O
U O
C O
1 O
6 O
m O
C O
0 O
3 O
T O
x O
s O
r O
G O
f O
v O
g O
d O
0 O
0 O
p O
p O
j O
W O
h O
U O
w O
0 O
T O
w O
3 O
M O
p O
O O
C O
k O
6 O
e O
m O
v O
X O
u O
J O
5 O
j O
R O
P O
s O
+ O
q O
4 O
n O
l O
Y O
J O
w O
Z O
J O
Q O
V O
2 O
B O
E O
X O
U O
9 O
7 O
/ O
X O
g O
Q O
r O
y O
X O
a O
B O
s O
M O
W O
9 O
F O
k O
r O
R O
9 O
P O
9 O
Y O
D O
e O
Z O
W O
V O
F O
q O
M O
C O
Q O
U O
d O
2 O
4 O
8 O
j O
A O
u O
a O
V O
B O
x O
J O
C O
g O
V O
1 O
m O
J O
Q O
O O
C O
i O
4 O
W O
P O
I O
e O
x O
h O
4 O
Z O
r O
c O
J O
N O
q O
X O
U O
M O
d O
P O
f O
T O
M O
L O
M O
o O
s O
+ O
m O
M O
o O
W O
r O
N O
/ O
R O
1 O
R O
c O
O O
7 O
f O
S O
q O
f O
d O
s O
f O
u O
w O
2 O
b O
Q O
3 O
5 O
L O
9 O
u O
4 O
p O
O O
z O
5 O
p O
J O
K O
m O
K O
K O
m O
p O
a O
5 O
0 O
o O
K O
1 O
V O
E O
N O
m O
o O
a O
E O
s O
0 O
k O
g O
i O
C O
1 O
8 O
o O
A O
L O
l O
P O
6 O
v O
k O
Z O
h O
z O
5 O
I O
J O
8 O
2 O
z O
p O
Z O
d O
K O
l O
I O
o O
v O
3 O
Q O
q O
a O
Q O
S O
X O
I O
k O
u O
k O
y O
M O
v O
5 O
l O
I O
s O
u O
y O
y O
C O
c O
v O
J O
j O
t O
w O
0 O
X O
P O
I O
m O
W O
/ O
H O
h O
M O
3 O
m O
V O
T O
3 O
b O
2 O
X O
q O
D O
Y O
e O
s O
w O
j O
b O
K O
V O
J O
r O
F O
8 O
R O
T O
d O
2 O
H O
i O
V O
+ O
C O
n O
h O
f O
D O
G O
d O
+ O
5 O
t O
A O
c O
j O
J O
4 O
u O
M O
q O
4 O
Z O
h O
r O
v O
q O
y O
r O
V O
v O
/ O
P O
T O
Z O
p O
z O
N O
6 O
/ O
D O
M O
P O
R O
7 O
M O
9 O
z O
c O
k O
m O
1 O
w O
c O
j O
A O
Y O
P O
h O
n O
E O
7 O
5 O
7 O
2 O
D O
1 O
+ O
0 O
G O
7 O
T O
H O
7 O
r O
M O
H O
7 O
B O
E O
b O
s O
m O
f O
s O
k O
L O
1 O
m O
R O
2 O
z O
E O
B O
P O
v O
E O
P O
r O
M O
v O
7 O
G O
v O
w O
L O
f O
g O
R O
/ O
A O
x O
+ O
n O
b O
s O
G O
O O
2 O
3 O
M O
X O
d O
a O
R O
4 O
P O
c O
f O
W O
w O
o O
S O
P O
A O
= O
= O
< O
/ O
l O
a O
t O
e O
x O
i O
t O
> O
T O
dec O
< O
l O
a O
t O
e O
x O
i O
t O
s O
h O
a O
1 O
_ O
b O
a O
s O
e O
6 O
4 O
= O
" O
2 O
g O
P O
2 O
+ O
x O
Y O
k O
d O
i O
+ O
q O
a O
h O
Q O
k O
M O
a O
t O
f O
m O
5 O
C O
H O
G O
x O
A O
= O
" O
> O
A O
A O
A O
D O
N O
X O
i O
c O
f O
V O
L O
N O
j O
t O
M O
w O
E O
P O
a O
G O
h O
V O
3 O
C O
X O
x O
d O
O O
i O
E O
t O
E O
h O
Y O
Q O
4 O
V O
O O
m O
C O
x O
B O
5 O
X O
w O
I O
E O
L O
Y O
p O
G O
2 O
u O
y O
s O
1 O
V O
T O
V O
x O
J O
6 O
l O
V O
O O
4 O
7 O
s O
C O
b O
R O
Y O
E O
U O
/ O
D O
F O
Y O
4 O
8 O
C O
w O
d O
u O
i O
C O
u O
v O
g O
N O
v O
m O
Q O
F O
q O
W O
k O
S O
x O
/ O
/ O
u O
Z O
/ O
P O
G O
k O
p O
h O
a O
U O
4 O
/ O
r O
4 O
T O
X O
N O
m O
9 O
e O
m O
1 O
v O
/ O
3 O
p O
4 O
4 O
+ O
a O
t O
2 O
3 O
c O
6 O
B O
3 O
f O
P O
r O
K O
4 O
M O
x O
w O
H O
X O
U O
p O
u O
L O
F O
C O
x O
K O
U O
e O
C O
A O
B O
E O
m O
8 O
K O
A O
2 O
C O
S O
i O
W O
e O
p O
7 O
O O
X O
S O
/ O
3 O
5 O
e O
z O
R O
W O
6 O
O O
K O
U O
F O
i O
W O
O O
F O
O O
S O
F O
y O
A O
Q O
H O
8 O
t O
S O
4 O
c O
z O
9 O
R O
Q O
N O
M O
0 O
c O
6 O
f O
1 O
2 O
C O
W O
E O
c O
z O
L O
K O
T O
Z O
D O
X O
9 O
b O
j O
T O
j O
X O
v O
x O
S O
q O
J O
t O
0 O
G O
9 O
A O
l O
z O
V O
y O
M O
j O
4 O
I O
d O
p O
O O
J O
5 O
p O
X O
C O
g O
r O
g O
E O
a O
4 O
f O
9 O
u O
K O
S O
R O
A O
0 O
O O
C O
S O
6 O
z O
D O
p O
L O
J O
Y O
A O
p O
9 O
B O
j O
k O
M O
P O
C O
1 O
B O
o O
R O
2 O
7 O
V O
Q O
x O
0 O
9 O
8 O
s O
w O
k O
y O
r O
T O
x O
p O
6 O
B O
o O
x O
f O
7 O
t O
4 O
U O
B O
Z O
u O
1 O
C O
p O
t O
1 O
x O
W O
b O
D O
d O
1 O
S O
/ O
J O
f O
u O
m O
F O
F O
2 O
d O
H O
I O
i O
a O
K O
s O
C O
A O
u O
+ O
T O
p O
R O
V O
M O
i O
I O
d O
L O
Q O
c O
S O
T O
Y O
R O
B O
T O
n O
L O
h O
A O
X O
A O
j O
f O
K O
0 O
R O
n O
4 O
I O
B O
T O
n O
5 O
s O
r O
S O
y O
q O
k O
i O
S O
M O
/ O
t O
D O
q O
x O
H O
G O
Q O
v O
M O
3 O
k O
B O
s O
q O
p O
4 O
P O
M O
2 O
a O
1 O
B O
a O
8 O
b O
E O
9 O
h O
k O
t O
C O
G O
k O
3 O
+ O
e O
4 O
q O
8 O
z O
a O
a O
q O
/ O
a O
6 O
M O
3 O
A O
i O
m O
D O
W O
6 O
n O
S O
L O
W O
e O
E O
a O
T O
2 O
0 O
s O
S O
v O
0 O
P O
+ O
W O
w O
T O
d O
+ O
c O
m O
9 O
L O
N O
E O
D O
a O
P O
H O
E O
J O
m O
F O
z O
B O
v O
H O
b O
N O
/ O
T O
8 O
z O
U O
a O
z O
N O
/ O
B O
2 O
G O
o O
d O
+ O
b O
/ O
u O
a O
W O
b O
I O
O O
z O
w O
1 O
7 O
/ O
a O
S O
9 O
+ O
9 O
6 O
x O
7 O
/ O
K O
L O
Z O
o O
H O
3 O
2 O
g O
D O
1 O
k O
j O
1 O
m O
f O
P O
W O
f O
H O
7 O
D O
U O
7 O
Y O
Q O
P O
G O
2 O
S O
f O
2 O
m O
X O
1 O
h O
X O
4 O
N O
v O
w O
Y O
/ O
g O
Z O
/ O
B O
r O
b O
R O
r O
s O
N O
D O
7 O

3 O
W O
E O
u O
C O
3 O
3 O
8 O
A O
P O
8 O
U O
S O
M O
g O
= O
= O
< O
/ O
l O
a O
t O
e O
x O
i O
t O
> O

MLPapply O

< O
l O
a O
t O
e O
x O
i O
t O
s O
h O
a O
1 O
_ O
b O
a O
s O
e O
6 O
4 O
= O
" O
z O
P O
F O
X O
F O
r O
p O
2 O
A O
w O
q O
1 O
e O
m O
5 O
t O
G O

L O
Q O
O O
T O
q O
O O
0 O
h O
4 O
o O
= O
" O
> O
A O
A O
A O
D O
O O
X O
i O
c O
f O
V O
J O
N O
j O
9 O
M O
w O
E O
P O
W O
G O
h O
V O
3 O
C O
V O
x O
e O
O O
C O
C O
m O
i O
Q O
k O
I O
c O
q O
m O
R O
B O
Y O
o O
8 O
r O
4 O
M O
C O
B O
F O
U O
W O
i O
u O
y O
u O
1 O
V O
T O
V O
x O
p O
6 O
l O
V O
O O
7 O
b O
s O
C O
b O
R O
E O
O O
f O
F O
r O
u O
M O
K O
R O
X O
8 O
K O
R O
G O
+ O
L O
K O
H O
8 O
B O
t O
g O
0 O
R O
a O
l O
p O
E O
s O
P O
7 O
9 O
5 O
M O
2 O
O O
P O
J O
z O
V O
S O
O O
I O
r O
j O
b O
z O
v O
B O
p O
d O
3 O
L O
V O
/ O
b O
2 O
r O
4 O
b O
X O
r O
t O
+ O
4 O
e O
a O
t O
1 O
c O
P O
v O
U O
6 O
c O
J O
y O
7 O
H O
E O
t O
t O
T O
1 O
P O
w O
a O
E O
U O
O O
f O
Z O
I O
k O
M O
R O
z O
Y O
x O
F O
U O
K O
v O
E O
s O
n O
T O
1 O
f O
+ O
s O
/ O
e O
o O
X O
V O
C O
5 O
2 O
9 O
p O
Y O
X O
C O
o O
I O
M O
v O
F O
R O
H O
A O
g O
T O
4 O
1 O
a O
9 O
w O
a O
E O
c O
7 O
K O
q O
P O
H O
n O
V O
r O
U O
b O
l O
n O
x O
M O
Y O
I O
x O
d O
V O
N O
W O
q O
1 O
4 O
0 O
6 O
8 O
s O
m O
g O
b O
J O
D O
V O
o O
s O
9 O
q O
6 O
o O
4 O
N O
g O
d O
z O
D O
W O
v O
F O
C O
Y O
E O
5 O
f O
g O
X O
D O
+ O
J O
D O
Q O
1 O
L O
s O
C O
S O
4 O
x O
C O
o O
c O
F O
A O
4 O
N O
8 O
B O
l O
k O
2 O
P O
c O
w O
B O
4 O
V O
u O
W O
K O
7 O
e O
U O
U O
U O
P O
P O
D O
O O
O O
J O
t O
r O
6 O
l O
V O
O O
0 O
Y O
v O
+ O
O O
K O
E O
E O
5 O
t O
1 O
C O
p O
V O
y O
q O
g O
q O
d O
v O
0 O
L O
c O
l O
/ O
+ O
f O
o O
F O
T O
Y O
6 O
G O
p O
c O
h O
N O
Q O
Z O
j O
z O
d O

P O
q O
A O
2 O
T O
M O
n O
3 O
d O
l O
n O
h O
V O
E O
A O
p O
W O
c O
E O
o O
W O
E O
9 O
l O
g O
/ O
u O
p O
A O
D O
v O
P O
C O
z O
d O
v O
M O
p O
f O
m O
i O
s O
/ O
M O
U O
n O
j O
l O
V O
J O
M O
l O
T O
T O
Y O
Y O
x O
q O
N O
4 O
J O
d O
E O
2 O
S O
D O
o O
w O
J O
J O
0 O
c O
Z O
f O
v O
B O
b O
j O
p O
T O
t O
B O
Y O
o O
L O
e O
V O
g O
z O
C O
S O
J O
K O
z O
t O
1 O
o O
C O
2 O
j O
H O
J O
s O
w O
r O
Q O
1 O
W O
Q O
B O
d O
Q O
4 O
s O
R O
D O
C O
Q O
L O
N O
1 O
K O
3 O
q O
a O
K O
K O
H O
n O
p O
l O
F O
h O
d O
L O
+ O
S O
B O
u O
t O
2 O
L O
9 O
f O
O O
B O
C O
m O
T O
c O
9 O
7 O
t O
l O
m O
b O
T O
V O
t O
L O
/ O
s O
s O
2 O
q O
W O
3 O
x O
Y O
u O
q O
Y O
r O
G O
q O
L O
k O
q O
4 O
D O
F O
T O
W O
P O
r O
I O
r O
a O
p O
k O
Q O
z O
p O
p O
F O
a O
v O
v O
Q O
A O
q O
G O
Y O
+ O
1 O
4 O
j O
O O
Q O
Q O
O O
1 O
v O
n O
W O
9 O
K O
K O
L O
m O
l O
m O
n O
1 O
s O
V O
e O
J O
o O
8 O
B O
p O
n O
y O
k O
1 O
V O
H O
N O
G O
z O
/ O
q O
s O
R O
m O
7 O
Y O
p O
3 O
4 O
b O
L O
v O
h O
S O
K O
+ O
t O
H O
J O
M O
s O
+ O
m O
4 O
v O
+ O
v O
d O
Z O
8 O
4 O
z O
O O
l O
c O
T O
t O
E O
r O
t O
T O
C O
Q O
m O
4 O
u O
D O
P O
w O
a O
/ O
b O
Q O
0 O
H O
v O
r O
O O
v O
a O
1 O
Q O
g O
1 O
X O
6 O
s O
U O
t O
B O
l O
w O
L O
O O
G O
t O
f O
p O
/ O
7 O
k O
x O
u O
X O
b O
z O
O O
g O
x O
D O
v O
z O
f O
J O
5 O
p O
Z O
s O
g O
+ O
M O
n O
o O
+ O
T O
p O
K O
H O
7 O

3 O
b O
H O
j O
w O
s O
t O
u O
g O
P O
X O
K O
P O
P O
C O
C O
P O
S O
E O
K O
e O
k O
w O
P O
y O
h O
h O
y O
R O
M O
a O
H O
k O
M O
/ O
l O
C O
v O
p O
J O
v O
w O
f O
f O
g O
R O
/ O
A O
z O
+ O
L O
V O
2 O
D O
X O
a O
6 O
N O
3 O
d O
J O
T O
4 O
L O
f O
f O
w O
A O
I O
h O
R O
P O
U O
< O
/ O
l O
a O
t O
e O
x O
i O
t O
> O
< O
l O
a O
t O
e O
x O
i O
t O
s O
h O
a O
1 O
_ O
b O
a O
s O
e O
6 O
4 O
= O
" O
0 O
a O
b O
H O
H O
p O
k O
g O
F O
2 O
9 O
P O
U O
V O
V O
x O
n O
/ O
Y O
J O
E O
6 O
a O
K O
E O
Y O
Y O
= O
" O
> O
A O
A O
A O
D O
Q O
X O
i O
c O
f O
V O
L O
N O
j O
t O
M O
w O
E O
P O
a O
G O
B O
Z O
b O
w O
1 O
4 O
U O
j O
l O
4 O
h O
q O
B O
e O
J O
Q O
J O
Q O
s O
S O
H O
F O
f O
A O
g O
Q O
t O
i O
k O
e O
j O
u O
S O
k O
0 O
V O
T O
d O
x O
J O
a O
t O
U O
/ O
k O
e O
3 O
A O
F O
i O
t O
P O
w O
N O
N O
w O
h O
S O
N O
P O
w O
S O
N O
w O
Q O
1 O
y O
5 O
4 O
L O
Q O
5 O
k O
J O
Z O
l O
J O
G O
s O

2 O
S O
W O
2 O
L O
Z O
1 O
P O
H O
Z O
F O
V O
b O
l O
H O
Q O
d O
q O
K O
h O
5 O
Z O
F O
X O
U O
N O
i O
W O
a O
M O
Y O
3 O
U O
8 O
q O
U O
H O
Q O
D O
X O
z O
u O
U O
Z O
0 O
D O
h O
q O
o O
9 O
a O
3 O
r O
R O
R O
E O
1 O
t O
0 O
y O
r O
D O
7 O
1 O
K O
H O
A O
V O
O O
+ O
0 O
y O
p O
o O
Z O
o O
z O
e O
t O
5 O
n O
N O
X O
L O
D O
P O
v O
b O
b O
c O
M O
G O
X O
W O
l O
k O
/ O
I O
l O
n O
2 O
2 O
V O
z O
0 O
7 O
7 O
X O
m O
G O
5 O
8 O
p O
j O
d O
s O
h O
c O
q O
U O
W O
F O
n O
J O
z O
Y O
e O
C O
X O
6 O
K O
e O
l O
8 O
b O
X O
v O
3 O
J O
s O
K O
N O
V O
i O
l O
H O
7 O
s O
U O
d O
C O
n O
g O
v O
H O
G O
d O
/ O
p O

+ O
f O
z O
P O
2 O
/ O
O O
Y O
V O
Z O
8 O
b O
G O
8 O
f O
e O
d O
4 O
N O
L O
u O
5 O
S O
t O
X O
9 O
6 O
6 O
F O
1 O
2 O
/ O
c O
v O
H O
V O
7 O
s O
H O
/ O
n O
x O
K O
h O
a O
U O
x O
x O
T O
x O
Z O
U O
+ O
y O
8 O
E O
g O
Z O
x O
L O
H O
l O
l O
m O
O O
Z O
5 O
V O
G O
E O
D O
n O
H O
0 O
3 O
z O
x O
o O
r O
W O
f O
v O
k O
d O
t O
m O
J O
L O
v O
7 O
L O
L O
C O
q O
Y O
B O
S O
s O
o O
J O
R O
s O
J O
7 O
K O
B O
g O
d O
R O
O O
g O
f O
r O
U O
g O
F O
2 O
n O
h O
d O
u O
3 O
j O
S O
Z O
S O
3 O
P O
F O
Z O
2 O
Y O
p O
v O
H O
I O
q O
S O
5 O
o O
H O
T O
T O
Y O
Y O
x O
q O
N O
4 O
J O
d O
E O
2 O
S O
D O
o O
w O
J O
J O
0 O
c O
Z O
/ O
v O
B O
b O
j O
p O
T O
t O
B O
Y O
o O
L O
e O
V O
g O
z O
C O
S O
J O
K O
z O
t O
1 O
o O
C O
2 O
j O
H O
J O
s O
w O
r O
Q O
1 O
W O
Q O
B O
d O
Q O
4 O
s O
R O
D O
C O
Q O
L O
N O
1 O
K O
3 O
q O
a O
a O
I O
D O
z O
8 O
y O
i O
Q O
m O
l O
/ O
p O
I O
1 O
W O
7 O
N O
8 O
v O
H O
A O
j O
T O
5 O
u O
c O
9 O
2 O
7 O
z O
N O
p O
q O
0 O
l O
/ O
2 O
W O
b O
1 O
L O
Z O
4 O
N O
n O
V O
M O
V O
r O
V O
F O
S O
d O
e O
B O
i O
p O
p O
H O
V O
k O
V O
t O
c O
6 O
I O
Z O
0 O
0 O
g O
t O
X O
3 O
o O
A O
V O
D O
O O
f O
a O
0 O

T O
n O
o O
I O
F O
a O
3 O
8 O
J O
e O
F O
F O
F O
z O
y O
7 O
T O
6 O
0 O
K O
v O
E O
U O
e O
C O
0 O
z O
5 O
Q O
a O
q O
j O
m O
j O
5 O
3 O
1 O
W O
I O
z O
f O
s O
Y O
7 O
8 O
N O
F O
3 O
y O
p O
l O
f O
W O
j O
k O
m O
W O
f O
z O
U O
X O
/ O
X O
m O
u O
+ O
8 O
Z O
n O
S O
u O
B O
0 O
i O
V O
2 O
p O
h O
I O
T O
c O
X O
B O
n O
6 O
J O
f O
l O
o O
a O
X O
/ O
v O
O O
v O
a O
l O
Q O
g O
1 O
X O
6 O
k O
U O
t O
B O
l O
w O
L O
O O
G O
9 O
f O
p O
/ O
7 O
k O
x O
u O
X O
b O
z O
O O
g O
x O
D O
v O
z O
f O
J O
5 O
p O
Z O
s O
g O
5 O
P O
D O
U O
f O
J O
4 O
F O
L O
9 O
9 O
M O
j O
x O
6 O
3 O
m O
3 O
Q O
H O
r O
l O
H O
7 O
p O
O O
H O
J O
C O
F O
P O
y O
R O
F O
5 O
R O
Y O
7 O
J O
m O
F O
D O
y O
i O
X O
w O
m O
X O
8 O
j O
X O
4 O
F O
v O
w O
I O
/ O
g O
Z O
/ O
F O
q O
7 O
B O
j O
v O
d O
m O
7 O
u O
k O
J O
8 O
H O
v O
P O
1 O
c O
n O
F O
v O
w O
= O
< O
/ O
l O
a O
t O
e O
x O
i O
t O
> O
ĥ O
o O
2 O
< O
l O
a O
t O
e O
x O
i O
t O
s O
h O
a O
1 O
_ O
b O
a O
s O
e O
6 O
4 O
= O
" O
l O
C O
c O
t O
r O
n O
5 O
j O
U O
I O
+ O
p O
P O
I O
t O
i O
h O
M O
G O
7 O
P O
A O
c O
L O
L O
3 O
k O
= O
" O
> O
A O
A O
A O
D O
Q O
X O
i O
c O
f O
V O
L O
N O
j O
t O
M O
w O
E O
P O
a O
G O
B O
Z O
b O
w O
1 O
4 O
U O
j O
l O
4 O
h O
q O
B O
e O
J O
Q O
p O
Q O
s O
S O
H O
F O
f O
A O
g O
Q O
t O
i O
k O
e O
j O
u O
S O
k O
0 O
V O
T O
d O
x O
J O
Y O
t O
U O
/ O
k O
e O
3 O
A O
F O
i O
t O
P O
w O
N O
N O
w O
h O
S O
N O
P O
w O
S O
N O
w O
Q O
1 O
y O
5 O
4 O
L O
Q O
5 O
k O
J O
Z O
l O
J O
G O
s O

+ O
f O
z O
P O
2 O
/ O
G O
Y O
V O
Z O
8 O
b O
G O
8 O
f O
e O
d O
4 O
N O
L O
u O
5 O
S O
t O
X O
9 O
6 O
6 O
F O
1 O
2 O
/ O
c O
v O
H O
V O
7 O
s O
H O
/ O
n O
x O
K O
h O
a O
U O
5 O
x O
Q O
x O
Z O
U O
+ O
y O
8 O
A O
g O
Z O
x O
I O
n O
l O
l O
m O
O O
Z O
5 O
V O
G O
E O
B O
n O
H O
0 O
2 O
z O
x O
o O
r O
W O
f O
v O
k O
d O
t O
m O
J O
L O
v O
7 O
L O
L O
C O
m O
Y O
B O
C O
s O
p O
x O
R O
s O
J O
5 O
K O
B O
w O
d O
R O
U O
o O
J O
1 O
i O
Q O
B O
b O
Z O
r O
k O
r O
m O
y O
Z O
1 O
S O
a O
b O
4 O
3 O
C O
y O
F O
V O
0 O
6 O
l O
h O
8 O
2 O
D O
J O
h O
0 O
M O
4 O
1 O
G O
8 O
k O
m O
g O
b O
j O
D O
s O
w O
J O
J O
0 O
c O
p O
/ O
v O
B O
b O
j O
J O
X O
t O
B O
Y O
o O
L O
e O
V O
g O
z O
H O
Q O
c O
V O
3 O
b O
m O
Q O
F O
t O
G O
O O
T O
Z O
h O
U O
h O
u O
s O
g O
C O
6 O
g O
w O
K O
m O
H O
E O
g O
S O
a O
m O
V O
v O
V O
0 O
0 O
Q O
H O
n O
p O
l O
H O
u O
d O
L O
+ O
S O
B O
u O
t O
2 O
L O
9 O
f O
O O
B O
C O
m O
z O
c O
9 O
7 O
t O
n O
m O
b O
T O
V O
t O
L O
/ O
s O
s O
2 O
r O
W O
3 O
+ O
b O
O O
a O
Y O
r O
G O
q O
L O
k O
q O
4 O
D O
5 O
T O
W O
P O
r O
I O
r O
a O
5 O
k O
R O
z O
p O
p O
F O
a O
v O
v O
Q O
A O
q O
G O
Y O
+ O
1 O
4 O
i O
W O
o O
I O
F O
a O
3 O

8 O
J O
e O
F O
F O
F O
z O
y O
7 O
T O
6 O
0 O
K O
v O
E O
U O
e O
C O
0 O
z O
x O
Q O
a O
q O
p O
L O
R O
8 O
z O
6 O
r O
k O
R O
v O
2 O
s O
d O
+ O
G O
C O
7 O
7 O
U O
y O
v O
p O
R O
y O
a O
L O
P O
Z O
q O
J O
/ O
r O
z O
X O
f O
+ O
E O
x O
p O
3 O
A O
6 O
R O
K O
b O
W O
w O
k O
J O
k O
L O
A O
7 O
9 O
E O
P O
y O
2 O
N O
r O
3 O
3 O
n O
3 O
l O
S O
o O
w O
S O
r O
9 O
y O
C O
W O
g O
C O
w O
H O
n O
j O
e O
v O
0 O
/ O
9 O
y O
Y O
X O
L O
t O
5 O
H O
Y O
a O
h O
3 O
5 O
v O
x O
5 O
p O
Z O
s O
g O
5 O
P O
D O
0 O
f O
j O
x O
K O
H O
7 O
7 O
Z O
H O
j O
0 O
v O
N O
u O
g O
P O
X O
K O
P O
3 O
C O
c O
P O
y O
Z O
g O
8 O
J O
U O
f O
k O
F O
T O
k O
m O
E O
0 O
L O
J O
J O
/ O
K O
Z O
f O
C O
F O
f O
g O
2 O
/ O
B O
j O
+ O
B O
n O
8 O
G O
v O
t O
G O
u O
x O
0 O
b O
+ O
6 O
S O
n O
g O
S O
/ O
/ O
w O
B O
Z O
4 O
R O
b O
9 O
< O
/ O
l O
a O
t O
e O
x O
i O
t O
> O
s O
a O
< O
l O
a O
t O
e O
x O
i O
t O
s O
h O
a O
1 O
_ O
b O
a O
s O
e O
6 O
4 O
= O
" O
L O
e O
R O
1 O
J O
A O
f O
P O
9 O

6 O
t O
E O
2 O
M O
T O
Q O
r O
X O
B O
V O
v O
0 O
E O
7 O
x O
E O
o O
= O
" O
> O
A O
A O
A O
D O
O O
3 O
i O
c O
f O
V O
I O
7 O
b O
x O
N O
B O
E O
N O
4 O
c O
A O
c O
L O
x O
c O
q O
C O
k O
4 O
I O
S O
F O
h O
C O
i O
s O
u O
4 O
A O
E O
Z O
Q O
Q O
U O
N O
I O
g O
g O
4 O
S O
S O
S O
b O
V O
l O
z O
6 O
/ O
F O
5 O
5 O
X O
2 O
c O
d O
u O
c O
g O
Z O
n O
U O
l O
v O
4 O
Y O
W O
S O
n O
4 O
I O
N O
R O
2 O
i O
p O
W O
d O
t O
X O
5 O
G O
z O
C O
S O
O O
t O
5 O
t O
t O
v O
Z O
n O
Y O
e O
O O
3 O
k O
p O
h O
a O
M O
0 O
/ O
b O
E O
T O
X O
d O
q O
9 O
f O
O O
X O
q O
3 O
r O
X O
4 O
+ O
o O
2 O
b O
t O
2 O
5 O
3 O
9 O
u O
8 O
c O
O O
1 O
N O
Z O
j O
n O
1 O
u O
p O
L O
G O
n O
O O
T O
i O
U O
Q O
m O
O O
f O
B O
E O
k O
8 O
L O
S O
2 O
C O
y O
i O
W O
e O
5 O
P O
O O
X O
S O
/ O
v O
J O
B O
7 O
R O
O O
G O
P O
2 O
e O
F O
i O
W O
O O
F O
B O
R O
a O
T O
A O
U O
H O
C O
t O
S O
4 O
c O
3 O
+ O
Y O
G O
z O
l O
x O
C O
x O
W O
U O
d O
/ O
X O
Y O
n O
7 O
9 O
D O
X O
Y O
8 O
7 O
3 O
b O
S O
X O
r O
i O
T O
Z O
B O
l O
k O
D O
u O
q O
y O
R O
o O
/ O
F O
+ O
t O
D O
u O
c O
G O
F O
4 O
p O
1 O
M O
Q O
l O
O O
D O
f O
I O
0 O
p O
J O
G O
H O
i O
w O
J O
L O
r O
G O
O O
h O
5 O
X O
D O
E O
v O
g O
c O
C O
h O
w O
E O
q O
E O
G O
h O
G O
/ O
l O
V O
J O
3 O
X O
y O
M O
D O
C O
T O
Z O
G O
p O
s O
O O
J O
q O
S O
F O
X O
s O
+ O
w O
o O
N O
y O
y O
+ O
K O
C O
p O
w O
K O
a O
u O
U O
3 O
b O
k O
v O
y O
X O
b O
V O
D O
R O
9 O
P O
n O
I O
C O
1 O
1 O
W O
h O
J O
q O
v O
E O
0 O
0 O
r O
m O
Z O
B O
J O
l O
m O
N O
J O
J O
s O
I O
i O
J O
7 O
k O
I O
A O
L O
g O
V O
o O
d O
a O
E O
z O
8 O
A O
C O
p O
z O
C O
8 O
V O
h O
Z O
V O
S O
R O
L O
W O
f O
G O
x O
1 O
4 O
j O
l O
I O
3 O
m O
Y O
K O
C O
+ O
V O
M O
8 O
L O
M O
2 O
a O
1 O
E O

V O
i O
A O
Q O
0 O
1 O
k O
2 O
M O
z O
6 O
E O
= O
" O
> O
A O
A O
A O
D O
K O
3 O
i O
c O
f O
V O
J O
L O
b O
9 O
N O
A O
E O
N O
6 O
a O
A O
s O
U O
8 O
m O
p O
Y O
j O
F O
4 O
s O
I O
C O
X O
G O
I O
b O
E O
C O
C O
Y O
w O
U O
9 O
c O
K O
C O
i O
l O
Z O
q O
2 O
U O
h O
J O
F O
4 O
8 O
3 O
Y O
W O
W O
U O
f O
1 O
u O
4 O
Y O
G O
i O
z O
/ O
E O
q O
5 O
w O
5 O
N O
d O
w O

A O
n O
H O
l O
f O
3 O
S O
T O
+ O
I O
A O
T O
y O
k O
i O
r O
+ O
f O
a O
b O
5 O
8 O
5 O
O O
W O
k O
j O
h O
K O
I O
5 O
/ O
b O
g O
U O
3 O
t O
m O
/ O
e O
u O
r O
1 O
z O
J O
7 O
x O
7 O
7 O
/ O
6 O
D O
3 O
c O
7 O
e O
/ O
p O
k O
z O
p O
e O
X O
Y O
5 O
0 O
Y O
a O
e O
5 O
G O
C O
Q O
y O
k O
0 O
9 O
k O
m O
Q O
x O
I O
v O
C O
I O
q O
h O
U O
4 O
n O
k O
6 O
e O
7 O
u O
w O
n O
3 O
9 O
E O
6 O
4 O
T O
R O
p O
z O
Q O
v O
c O
K O
Q O
g O
1 O
y O
I O
T O
H O
M O
h O
T O
4 O
8 O
7 O
u O
U O
A O
F O
N O
0 O
6 O
w O
6 O
r O
c O
f O
V O
+ O
6 O
N O
6 O
3 O
O O
n O
G O
v O
X O
g O
p O
0 O
S O
Z O
I O
G O
t O
B O
l O
j O
R O
y O
P O
9 O
4 O
L O
t O
4 O
c O
T O
w O
U O
q O
E O
m O
L O
s O
G O
5 O
Q O
R O
I O
X O
N O
K O
r O
A O
k O
u O
A O
S O
6 O
3 O
B O
Y O
O O
i O
y O
A O
z O
y O
D O
H O
g O
Y O
c O
a O
F O
L O
p O
R O
t O
e O
y O
8 O
j O
p O
5 O
4 O
Z O
h O
J O
l O
x O
v O
q O
j O
K O
V O
q O
y O
f O
0 O
d O
U O
o O
J O
y O
b O
q O
9 O
R O
7 O
L O
v O
p O
0 O
6 O
7 O
Y O
F O
+ O
S O
/ O
b O
o O
K O
T O
s O
9 O
a O
g O
S O
u O
i O
g O
J O
N O
V O
8 O
V O
y O
k O
o O
Z O
k O
Y O
k O
W O
Y O
4 O
g O
m O
w O
i O
I O
n O
O O
f O
c O
A O
u O
B O
W O
+ O
1 O
4 O
h O
P O
w O
Q O
I O
n O
P O
6 O
x O
W O
F O
V O
V O
K O
E O
t O
Z O
8 O
a O
r O
2 O
k O
4 O
i O
B O
5 O
m O
8 O
k O
t O
F O
F O
P O
B O
L O
9 O
u O
s O
R O
e O
n O
E O
5 O
/ O
Y O
Y O
r O
k O
l O
p O
D O
f O
l O
P O
0 O
X O
m O
b O
T O
V O
X O
7 O
X O
l O
q O
5 O
l O
s O
x O
Y O
3 O
C O
y O
R O
G O
j O
M O
j O
S O
N O
2 O
1 O
h O
Q O
/ O
R O
/ O
5 O
b O
F O
I O
z O
+ O
5 O
D O
w O
V O
a O
I O
G O
O O
f O
V O
U O
O O
w O
u O
Y O
L O
L O
u O
m O
r O
0 O
/ O
9 O
y O
E O
X O
r O
l O
5 O
H O
Y O
a O
h O
3 O
5 O
t O
k O
f O
U O
s O
2 O
w O
d O
n O
z O
X O
v O
K O
i O
F O
5 O
+ O
8 O
7 O
B O
6 O
8 O
a O
T O
Z O
o O
h O
z O
1 O
i O
j O
9 O
l O
T O
l O
r O
B O
X O
7 O
I O
C O
9 O
Y O
8 O
e O
s O
z O
z O
g O
r O
2 O
R O
f O
2 O
l O
X O
0 O
L O
v O
g O
c O
/ O
g O
l O
/ O
B O
7 O
5 O
V O
r O
s O
N O
X O
E O
P O
G O
Q O
t O
C O
f O
5 O
c O
A O
R O
g O
6 O
D O
U O
M O
= O
< O
/ O
l O
a O
t O
e O
x O
i O
t O
> O

Language O
Model O

T O
LM O
< O
l O
a O
t O
e O
x O
i O
t O
s O
h O
a O
1 O
_ O
b O
a O
s O
e O
6 O
4 O
= O
" O
b O
W O
X O
v O
2 O
O O
8 O
N O
N O
h O
4 O
H O
6 O
P O
1 O
V O
i O
A O
Q O
0 O
1 O
k O
2 O
M O
z O
6 O
E O
= O
" O
> O
A O
A O
A O
D O
K O
3 O
i O
c O
f O
V O
J O
L O
b O
9 O
N O
A O
E O
N O
6 O
a O
A O
s O
U O
8 O
m O
p O
Y O
j O
F O
4 O
s O
I O
C O
X O
G O
I O
b O
E O
C O
C O
Y O
w O
U O
9 O
c O
K O
C O
i O
l O
Z O
q O
2 O
U O
h O
J O
F O
4 O
8 O
3 O
Y O
W O
W O
U O
f O
1 O
u O
4 O
Y O
G O
i O
z O
/ O
E O
q O
5 O
w O
5 O
N O
d O
w O
A O
n O
H O
l O
f O
3 O
S O
T O
+ O
I O
A O
T O
y O
k O
i O
r O
+ O
f O
a O
b O
5 O
8 O
5 O
O O
W O
k O
j O
h O
K O
I O
5 O
/ O
b O
g O
U O
3 O
t O
m O
/ O
e O
u O
r O
1 O
z O
J O
7 O
x O
7 O
7 O
/ O
6 O
D O
3 O
c O
7 O
e O
/ O
p O
k O
z O
p O
e O
X O
Y O
5 O
0 O
Y O
a O
e O
5 O
G O
C O
Q O
y O
k O
0 O
9 O
k O
m O
Q O
x O
I O
v O
C O
I O
q O
h O
U O
4 O
n O
k O
6 O
e O
7 O
u O
w O
n O
3 O
9 O
E O
6 O
4 O
T O
R O
p O
z O
Q O
v O
c O
K O
Q O
g O
1 O
y O
I O
T O
H O
M O
h O
T O
4 O
8 O
7 O
u O
U O
A O
F O
N O
0 O
6 O
w O
6 O
r O
c O
f O
V O
+ O
6 O
N O
6 O
3 O
O O
n O
G O
v O
X O
g O
p O
0 O
S O
Z O
I O
G O
t O
B O
l O
j O
R O
y O
P O
9 O
4 O
L O
t O
4 O
c O
T O
w O
U O
q O
E O
m O
L O
s O
G O
5 O
Q O
R O
I O
X O
N O
K O
r O
A O
k O
u O
A O
S O
6 O
3 O
B O
Y O
O O
i O
y O
A O
z O
y O
D O
H O
g O
Y O
c O
a O
F O
L O
p O
R O
t O
e O
y O
8 O
j O
p O
5 O
4 O
Z O
h O
J O
l O
x O
v O
q O
j O
K O
V O
q O
y O
f O
0 O
d O
U O
o O
J O
y O
b O
q O
9 O
R O
7 O
L O
v O
p O
0 O
6 O
7 O
Y O
F O
+ O
S O
/ O
b O
o O
K O
T O
s O
9 O
a O
g O
S O
u O
i O
g O
J O
N O
V O
8 O
V O
y O
k O
o O
Z O
k O
Y O
k O
W O
Y O
4 O
g O
m O
w O
i O
I O
n O
O O
f O
c O
A O
u O
B O
W O
+ O
1 O
4 O
h O
P O
w O
Q O
I O
n O
P O
6 O
x O
W O
F O
V O
V O
K O
E O
t O
Z O
8 O
a O
r O
2 O
k O
4 O
i O
B O
5 O
m O
8 O
k O
t O
F O
F O
P O
B O
L O
9 O
u O
s O
R O
e O
n O
E O
5 O
/ O
Y O
Y O
r O
k O
l O
p O
D O
f O
l O
P O
0 O
X O
m O
b O
T O
V O
X O
7 O
X O
l O
q O
5 O
l O
s O
x O
Y O
3 O
C O
y O
R O
G O
j O
M O
j O
S O
N O
2 O
1 O
h O
Q O
/ O
R O
/ O
5 O
b O
F O
I O
z O
+ O
5 O
D O
w O
V O
a O
I O
G O
O O
f O
V O
U O
O O
w O
u O
Y O
L O
L O
u O
m O
r O
0 O
/ O
9 O
y O
E O
X O
r O
l O
5 O
H O
Y O
a O
h O
3 O
5 O
t O
k O
f O
U O
s O
2 O
w O
d O
n O
z O
X O
v O
K O
i O
F O
5 O
+ O
8 O
7 O
B O
6 O
8 O
a O
T O
Z O
o O
h O
z O
1 O
i O
j O
9 O
l O
T O
l O
r O
B O
X O
7 O
I O
C O
9 O
Y O
8 O
e O
s O
z O
z O
g O
r O
2 O
R O
f O
2 O
l O
X O
0 O
L O
v O
g O
c O
/ O
g O
l O
/ O
B O
7 O
5 O
V O
r O
s O
N O
X O
E O
P O
G O
Q O
t O
C O
f O
5 O
c O
A O
R O
g O
6 O
D O
U O
M O
= O
< O
/ O
l O
a O
t O
e O
x O
i O
t O
> O

The O
vase O
breaks O
and O
is O
no O
longer O
being O
held O
. O

Action O
Summarizer O

MLP O

< O
l O
a O
t O
e O
x O
i O
t O
s O
h O
a O
1 O
_ O
b O
a O
s O
e O
6 O
4 O
= O
" O
J O
5 O
y O
L O
7 O
s O
R O
9 O
1 O
y O
R O
U O
8 O
/ O
L O
f O
F O
S O
o O
p O
l O
T O
f O
A O

V O
l O
E O
= O
" O
> O
A O
A O
A O
D O
M O
X O
i O
c O
f O
V O
J O
L O
b O
9 O
N O
A O
E O
N O
6 O
a O
A O
s O
W O
8 O
U O
h O
A O
n O
L O
h O
Y O
R O
E O
u O
I O
Q O
2 O
Y O
A O
E O
x O
w O
p O
6 O
4 O
E O
B O
F O
k O
E O
h O
b O
K O
Y O
6 O
i O
8 O
W O
b O
i O
r O
L O
I O
P O
a O
3 O
c O
M O
C O
Z O
Z O
/ O
D O
F O
c O
4 O
8 O
m O
t O
6 O
Q O
1 O
z O
5 O
E O
2 O
w O
S O
H O
3 O
B O
C O
G O
W O
k O
1 O
3 O
3 O
7 O
z O
3 O
N O
n O
J O
C O
i O
k O
c O
x O
f O
H O
F O
X O
n O
B O
l O
/ O
+ O
q O
1 O
6 O
w O
c O
3 O
w O
p O
u O
3 O
b O
t O
+ O
5 O
2 O
z O
m O
8 O
d O
+ O
p O
M O
a O
T O
k O
O O
u O
J O
H O
G O
n O
m O
f O
g O
U O
A O
q O
N O
A O
x O
I O
k O
8 O
b O
y O
w O
C O
C O
q O
T O
e O
J O
b O
N O
3 O
6 O
z O
s O
Z O
5 O
/ O
Q O
O O
m O
H O
0 O
R O
1 O
o O
W O
O O
F O
K O
Q O
a O
z O
E O
V O
H O
M O
h O
T O
4 O
8 O
6 O
D O
l O
H O
B O
B O
V O
l O
U O
n O
7 O
/ O
r O
1 O
u O
E O
q O
P O
U O
R O
L O
U O
4 O
0 O
4 O
3 O
7 O
s O
V O
r O
i O
X O
Z O
B O
0 O
o O
A O
u O
a O
6 O
Q O
/ O
P O
g O
z O
2 O
0 O
4 O
n O
h O
p O
U O
J O
N O
X O
I O
J O
z O
w O
y O
Q O
u O
a O
F O
S O
B O
J O
c O
E O
l O
1 O
m O
F O
a O
O O
i O
y O
A O
z O
y O
H O
H O
o O
Y O
c O
a O
F O
L O
p O
R O
t O
e O
6 O
/ O
j O
h O
5 O
7 O
Z O
h O
J O
N O
j O
f O
V O
H O
U O
7 O
R O
m O
/ O
4 O
6 O
o O
Q O
D O
m O
3 O
V O
J O
n O
3 O
V O
E O
A O
z O
t O
2 O
1 O
b O
k O
f O
+ O
y O
D O
U O
u O
a O
v O
h O
p O
V O
Q O
h O
c O
l O
o O
e O
a O
b O
Q O
t O
N O
S O
R O
m O
S O
i O
1 O
T O
C O
i O
i O
b O
D O
I O
S O
S O
4 O
9 O
A O
G O
6 O
F O
7 O
z O
X O
i O
M O
7 O
D O
A O
y O
Y O
+ O
s O
V O
U O
W O
V O
k O
o O
Q O
1 O
n O
1 O
s O
v O
q O
T O
h O
I O
3 O
m O
Z O
y O
C O
8 O
V O
M O
8 O
E O
W O
b O
t O
S O
i O
d O
+ O
N O
I O

e O
w O
y O
U O
p O
r O
S O
H O
/ O
N O
T O
p O
v O
s O
5 O
l O
q O
3 O
0 O
s O
r O
t O
5 O
I O
Z O
i O
7 O
s O
l O
M O
m O
P O
m O
B O
J O
m O
7 O
t O
P O
A O
x O
+ O
t O
+ O
y O
e O
O O
I O
n O
9 O
7 O
5 O
A O
C O
2 O
T O
s O
0 O
y O
o O
F O
m O
y O
t O
Y O
1 O
F O
W O
j O
/ O
+ O
c O
m O
9 O
M O
b O
N O
6 O
z O
A O
M O
/ O
d O
4 O
k O
2 O
1 O
u O
y O
C O
0 O
6 O
f O
9 O
Z O
L O
n O
v O
f O
j O
D O
i O
+ O
7 O
R O
6 O
2 O
a O
D O
D O
t O
h O
D O
9 O
o O
g O
9 O
Y O
Q O
l O
7 O
y O
Y O
7 O
Y O
W O
9 O
Z O
n O
A O
8 O
Z O
Z O
x O
b O
6 O
y O
b O
+ O
x O
7 O
8 O
C O
O O
4 O
C O
H O
4 O
G O
v O
z O
a O
u O
w O
V O
4 O
T O
c O
5 O
+ O
1 O
J O
P O
j O
9 O
B O
9 O
K O
O O
D O
/ O
M O
= O
< O
/ O
l O
a O
t O
e O
x O
i O
t O
> O
s O
o O
< O
l O
a O
t O
e O
x O
i O
t O
s O
h O
a O
1 O
_ O
b O
a O
s O
e O
6 O
4 O
= O
" O
9 O
9 O
U O
Q O
b O
A O
s O
r O
v O
A O
Y O
0 O
2 O
F O
4 O
v O
v O
i O
N O
X O
5 O
h O
J O
s O
l O
v O
0 O
= O
" O
> O
A O
A O
A O
D O
Q O
n O
i O
c O
f O
V O
L O
N O
b O
h O
M O
x O
E O
H O
a O
X O
A O
m O
X O
5 O
S O
+ O
H O
I O
Z O
U O
X O
E O
j O
z O
h O
E O
u O
w O
W O
p O
P O
V O
b O
A O
g O
Q O
u O
i O
S O
K O
S O
t O
l O
I O
2 O
i O
W O
W O
e O
y O
s O
e O
K O
1 O
V O
/ O
Z O
s O
a O
b O
D O
2 O
D O
X O
g O
a O
r O
n O
D O
k O
J O
X O
g O
F O
b O
o O
g O
r O
B O
5 O
x O
k O
D O
9 O
2 O
E O
M O
p O
I O
1 O
n O
7 O
/ O
5 O
9 O
X O
i O
y O
U O
g O
p O
L O
c O
f O
x O
j O
K O
7 O
i O
y O
f O
f O
X O
a O
9 O
Z O
0 O
b O
4 O
c O
1 O
b O
t O
+ O
/ O
c O
7 O
e O
z O
e O
O O
7 O
a O
6 O
M O
h O
z O
7 O
X O
E O
t O
t O
T O
j O
O O
w O
K O
I O
X O
C O
P O
g O
m O
S O
e O
F O
o O
a O
h O
C O
K O
T O
e O
J O
L O
N O
X O
i O
3 O
s O
J O
2 O
d O
o O
r O
N O
D O
q O
A O
8 O
1 O
L O
H O
B O
a O
Q O
K O
z O
E O
R O
H O
M O
h O
T O
o O
8 O
7 O
j O
N O
N O
N O
y O
b O
O O
e O
F O
V O
8 O
7 O
W O
I O
5 O
e O
e O
I O
X O
c O
X O
S O
V O
3 O

X O
T O
+ O
p O
R O
p O
x O
v O
3 O
4 O
q O
V O
E O
m O
y O
B O
p O
Q O
J O
c O
1 O
c O
j O
T O
a O
D O
b O
b O
T O
s O
e O
Z O
V O
g O
Y O
q O
4 O
B O
G O
s O
H O
S O
V O
z O
S O
0 O
I O
E O
h O
w O
S O
X O
W O
Y O
V O
p O
Z O
L O
I O
H O
P O
I O
M O
e O
B O
h O
w O
o O
K O
t O
E O
O O
3 O
f O
F O
A O
d O
P O
f O
L O
M O
O O
J O
p O
o O
4 O
4 O
+ O
i O
a O
M O
l O
e O
j O
H O
B O
Q O
2 O
E O
V O
7 O
3 O
r O
M O
A O
m O
t O
p O
1 O
2 O
4 O
L O
8 O
l O
2 O
1 O
Q O
0 O
e O
R O
g O
6 O
I O
Q O
q O
K O
0 O
L O
F O
V O
4 O
U O
m O
l O
Y O
x O
I O
R O
4 O
v O
p O
R O
G O
N O
h O
k O
J O
O O
c O
e O
w O
D O
c O
C O
N O
9 O
r O
x O
K O
d O
g O
g O
J O
O O
f O
Y O
a O
t O
K O
U O
U O
k O
S O
R O
n O
9 O
s O
v O
c O
R O
x O
k O
L O
z O
N O
5 O
A O
b O
K O
q O
e O
D O
n O
b O
d O
a O
g O
t O
O O
J O
T O
e O
w O
y O
X O
p O
D O
S O
a O
/ O
F O
+ O
p O
v O
M O
1 O
m O
R O
f O
t O
e O
G O
b O
m O
W O
T O
B O
v O

c O
L O
J O
F O
p O
P O
S O
P O
I O
7 O
K O
W O
F O
X O
6 O
P O
/ O
L O
Y O
N O
v O
/ O
e O
T O
e O
l O
W O
i O
A O
t O
H O
n O
m O
U O
j O
B O
5 O
A O
e O
e O
1 O
a O
/ O
T O
/ O
3 O
I O
R O
a O
u O
X O
k O
d O
h O
q O
H O
f O
m O
2 O
R O
9 O
S O
z O
b O
B O
8 O
V O
4 O
v O
e O
d O
6 O
L O
3 O
7 O
/ O
o O
H O
r O
5 O
s O
N O
m O
i O
H O
P O
W O
A O
P O
2 O
V O
O O
W O
s O
H O
1 O
2 O
y O
N O
6 O
w O
I O
9 O
Z O
n O
n O
H O
1 O
m O
X O
9 O
h O
X O
9 O
i O
3 O
4 O
H O
v O
w O
M O
f O
g O
W O
/ O
V O
6 O
7 O
B O
V O
h O
N O
z O
n O
7 O
U O
k O
+ O
P O
M O
X O
r O
A O
Y O
Y O
J O
w O
= O
= O
< O
/ O
l O
a O
t O
e O
x O
i O
t O
> O
Figure O
3 O
: O
PIGLeT O
architecture O
. O
We O
pretrain O
a O
model O
of O
physical O
world O
dynamics O
by O
learning O
to O
transform O
objectsõ O
and O
actions O
a O
into O
new O
updated O
objectsõ O
0 O
. O
Our O
underlying O
world O
dynamics O
model O
-the O
encoder O
, O
the O
decoder O
, O
and O
the O
action O
application O
module O
, O
can O
augment O
a O
language O
model O
with O
grounded O
commonsense O
knowledge O
. O
from O
interactions O
, O
and O
second O
, O
integrate O
with O
a O
pretrained O
model O
of O
language O
form O
. O

Modeling O
Physical O
Dynamics O

We O
take O
a O
neural O
, O
auto O
- O
encoder O
style O
approach O
to O
model O
world O
dynamics O
. O
An O
object O
o O
gets O
encoded O
as O
a O
vector O
h O
o O
2 O
R O
do O
. O
The O
model O
likewise O
encodes O
an O
action O
a O
as O
a O
vector O
h O
a O
2 O
R O
da O
, O
using O
it O
to O
manipulate O
the O
hidden O
states O
of O
all O
objects O
. O
The O
model O
can O
then O
decode O
any O
object O
hidden O
representation O
back O
into O
a O
symbolic O
form O
. O

Object O
Encoder O
and O
Decoder O

We O
use O
a O
Transformer B-MethodName
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
to O
encode O
objects O
into O
vectors O
o O
2 O
R O
do O
, O
and O
then O
another O
to O
decode O
from O
this O
representation O
. O

Encoder O
. O
Objects O
o O
are O
provided O
to O
the O
encoder O
as O
a O
set O
of O
attributes O
, O
with O
categories O
c O
1 O
, O
... O
, O
c O
n O
. O
Each O
attribute O
c O
has O
its O
own O
vocabulary O
and O
embedding O
E O
c O
. O
For O
each O
object O
o O
, O
we O
first O
embed O
all O
the O
attributes O
separately O
and O
feed O
the O
result O
into O
a O
Transformer B-MethodName
encoder O
T O
enc O
. O
This O
gives O
us O
( O
with O
position O
embeddings O
omitted O
for O
clarity O
) O
: O

h O
o O
= O
T O
enc O
⇣ O
E O
1 O
( O
o O
1 O
) O
, O
. O
. O
. O
, O
E O
cn O
( O
o O
cn O
) O
⌘ O
( O
2 O
) O

Decoder O
. O
We O
can O
then O
convert O
back O
into O
the O
original O
symbolic O
representation O
through O
a O
left O
- O
to O
- O
right O
Transformer B-MethodName
decoder O
, O
which O
predicts O
attributes O
oneby O
- O
one O
from O
c O
1 O
to O
c O
n O
. O
This O
captures O
the O
inherent O
correlation O
between O
attributes O
, O
while O
making O
no O
independence O
assumptions O
, O
we O
discuss O
our O
ordering O
in O
Appendix O
A.2 O
. O
The O
probability O
of O
predicting O
the O
next O
attribute O
o O
c O
i+1 O
is O
then O
given O
by O
: O

p O
( O
o O
c O
i+1 O
|h O
o O
, O
o O
: O
c O
i O
) O
= O
T O
dec O
⇣ O
h O
o O
, O
E O
1 O
( O
o O
1 O
) O
, O
... O
, O
E O
c O
i O
( O
o O
c O
i O
) O
⌘ O
( O
3 O
) O

Modeling O
actions O
as O
functions O

We O
treat O
actions O
a O
as O
functions O
that O
transform O
the O
state O
of O
all O
objects O
in O
the O
scene O
. O
Actions O
in O
our O
environment O
take O
at O
most O
two O
arguments O
, O
so O
we O
embed O
the O
action O
a O
and O
the O
names O
of O
its O
arguments O
, O
concatenate O
them O
, O
and O
pass O
the O
result O
through O
a O
multilayer O
perceptron O
; O
yielding O
a O
vector O
representation O
h O
a O
. O
Applying O
Actions O
. O
We O
use O
the O
encoded O
action O
h O
a O
to O
transform O
all O
objects O
in O
the O
scene O
, O
obtaining O
updated O
representationsĥ O
o O
0 O
for O
each O
one O
. O
We O
take O
a O
global O
approach O
, O
jointly O
transforming O
all O
objects O
. O
This O
takes O
into O
account O
that O
interactions O
are O
contextual O
: O
turning O
on O
a O
Faucet O
might O
fill O
up O
a O
Cup O
if O
and O
only O
if O
there O
is O
one O
beneath O
it O
. O

Letting O
the O
observed O
objects O
in O
the O
interaction O
be O
o O
1 O
and O
o O
2 O
, O
with O
encodings O
h O
o O
1 O
and O
h O
o O
2 O
respectively O
, O
we O
model O
the O
transformation O
via O
the O
following O
multilayer O
perceptron O
: O

[ O
ĥ O
o O
0 O
1 O
, O
ĥ O
o O
0 O
2 O
] O
= O
MLP O
apply O
⇣ O
⇥ O
h O
a O
, O
h O
o O
1 O
, O
h O
o O
2 O
⇤ O
⌘ O
. O
( O
4 O
) O

The O
result O
can O
be O
decoded O
into O
symbolic O
form O
using O
the O
object O
decoder O
( O
Equation O
3 O
) O
. O

Loss O
function O
and O
training O

We O
train O
our O
dynamics O
model O
on O
( O
õ O
, O
a O
, O
õ O
0 O
) O
transitions O
. O
The O
model O
primarily O
learns O
by O
runningõ O
, O
a O
through O
the O
model O
, O
predicting O
the O
updated O
output O
stateĥ O
o O
0 O
, O
and O
minimizing O
the O
cross B-MetricName
- I-MetricName
entropy I-MetricName
of O
generating O
attributes O
of O
the O
real O
changed O
objectõ O
0 O
. O
We O
also O
regularize O
the O
model O
by O
encoding O
objectsõ O
, O
õ O
0 O
and O
having O
the O
model O
learn O
to O
reconstruct O
them O
. O
We O
weight O
all O
these O
cross B-MetricName
- I-MetricName
entropy I-MetricName
losses O
equally O
. O
We O
discuss O
our O
architecture O
in O
Appendix O
A.1 O
; O
it O
uses O
3 B-HyperparameterValue
- O
layer B-HyperparameterName
Transformers B-MethodName
, O
totalling O
17 B-HyperparameterValue
M I-HyperparameterValue
parameters O
. O

Language B-MethodName
Grounding I-MethodName

After O
pretraining O
our O
physical O
dynamics O
model O
, O
we O
integrate O
it O
with O
a O
Transformer B-MethodName
Language I-MethodName
Model I-MethodName
( O
LM O
) O
. O
In O
our O
framework O
, O
the O
role O
of O
the O
LM O
will O
be O
to O
both O
encode O
natural O
language O
sentences O
of O
actions O
into O
a O
hidden O
state O
approximating O
h O
a O
, O
as O
well O
as O
summarizing O
the O
result O
of O
an O
interaction O
( O
õ O
, O
a O
, O
õ O
0 O
) O
in O
natural O
language O
. O

Choice O
of O
LM O
. O
Our O
framework O
is O
compatible O
with O
any O
language O
model O
. O
However O
, O
to O
explore O
the O
impact O
of O
pretraining O
data O
on O
grounding O
later O
in O
this O
paper O
, O
we O
pretrain O
our O
own O
with O
an O
identical O
architecture O
to O
the O
smallest O
GPT2 B-MethodName
( O
Radford O
et O
al O
. O
( O
2019 O
) O
; O
117 O
M O
) O
. O
To O
handle O
both O
classification O
and O
generation O
well O
, O
we O
mask O
only O
part O
of O
the O
attention O
weights O
out O
, O
allowing O
the O
model O
to O
encode O
a O
" O
prefix O
" O
bidirectionally O
; O
it O
generates O
subsequent O
tokens O
leftto O
- O
right O
( O
Dong O
et O
al O
. O
, O
2019 O
) O
. O
We O
pretrain O
the O
model O
on O
Wikipedia O
and O
books O
; O
details O
in O
Appendix O
D O
. O

We O
next O
discuss O
architectural O
details O
of O
performing O
the O
language O
transfer O
, O
along O
with O
optimization O
. O

Transfer O
Architecture O

English O
actions O
to O
vector O
form O
. O
Given O
a O
natural O
language O
description O
s O
a O
of O
an O
action O
a O
, O
like O
" O
The O
robot O
throws O
the O
vase O
, O
" O
for O
PIGPeN B-DatasetName
- I-DatasetName
NLU I-DatasetName
, O
our O
model O
will O
learn O
to O
parse O
this O
sentence O
into O
a O
neural O
representation O
h O
a O
, O
so O
the O
dynamics O
model O
can O
simulate O
the O
result O
. O
We O
do O
this O
by O
encoding O
s O
a O
through O
our O
language O
model O
, O
T O
LM O
, O
with O
a O
learned O
linear O
transformation O
over O
the O
resulting O
( O
bidirectional O
) O
encoding O
. O
The O
resulting O
vector O
h O
sa O
can O
then O
be O
used O
by O
Equation O
4 O
. O

Summarizing O
the O
result O
of O
an O
action O
. O
For O
PIGPeN B-DatasetName
- I-DatasetName
NLG I-DatasetName
, O
our O
model O
simulates O
the O
result O
of O
an O
action O
a O
neurally O
, O
resulting O
in O
a O
predicted O
hidden O
stateĥ O
o O
for O
each O
object O
in O
the O
scene O
o. O
To O
write O
an O
English O
summary O
describing O
" O
what O
changed O
, O
" O
we O
first O
learn O
a O
lightweight O
fused O
representation O
of O
the O
transition O
, O
aggregating O
the O
initial O
and O
final O
states O
, O
along O
with O
the O
action O
, O
through O
a O
multilayer O
perceptron O
. O
For O
each O
object O
o O
i O
we O
have O
: O

h O
o O
i O
= O
MLP O
( O
[ O
h O
o O
i O
, O
ĥ O
o O
0 O
i O
, O
h O
a O
] O
) O
. O
( O
5 O
) O

We O
then O
use O
the O
sequence O
[ O
h O
o O
1 O
, O
h O
o O
2 O
] O
as O
bidirectional O
context O
for O
our O
our O
LM O
to O
decode O
from O
. O
Additionally O
, O
since O
our O
test O
set O
includes O
novel O
objects O
not O
seen O
in O
training O
, O
we O
provide O
the O
names O
of O
the O
objects O
as O
additional O
context O
for O
the O
LM O
generator O
( O
e.g. O
' O
Vase O
, O
Laptop O
' O
) O
; O
this O
allows O
a O
LM O
to O
copy O
those O
names O
over O
rather O
than O
hallucinate O
wrong O
ones O
. O
Importantly O
we O
only O
provide O
the O
surfaceform O
names O
, O
not O
underlying O
information O
about O
these O
objects O
or O
their O
usage O
as O
with O
few O
- O
shot O
scenarios O
in O
the O
recent O
GPT-3 B-MethodName
experiments O
( O
Brown O
et O
al O
. O
, O
2020 O
) O
-necessitating O
that O
PIGLeT B-MethodName
learns O
what O
these O
names O
mean O
through O
interaction O
. O

Loss O
functions O
and O
training O
. O

Modeling O
text O
generation O
allows O
us O
to O
incorporate O
a O
new O
loss O
function O
, O
that O
of O
minimizing O
the O
loglikelihood B-MetricName
of O
generating O
each O
sõ0 O
given O
previous O
words O
and O
the O
result O
of O
Equation O
5 O
: O

p O
( O
s O
post O
i+1 O
|sõ O
0 O
, O
1 O
: O
i O
) O
= O
T O
LM O
( O
h O
o O
1 O
, O
h O
o O
2 O
, O
sõ O
0 O
, O
1 O
: O
i O
) O
. O
( O
6 O
) O

We O
do O
the O
same O
for O
the O
object O
states O
sõ O
pre O
- O
action O
, O
using O
h O
o O
i O
as O
the O
corresponding O
hidden O
states O
. O

For O
PIGPeN B-DatasetName
- I-DatasetName
NLU I-DatasetName
, O
where O
no O
generation O
is O
needed O
, O
optimizing O
Equation O
5 O
is O
not O
strictly O
necessary O
. O
However O
, O
as O
we O
will O
show O
later O
, O
it O
helps O
provide O
additional O
signal O
to O
the O
model O
, O
improving O
overall O
accuracy O
by O
several O
percentage O
points O
. O

Experiments O

We O
test O
our O
model O
's O
ability O
to O
encode O
language O
into O
a O
grounded O
form O
( O
PIGPeN B-DatasetName
- I-DatasetName
NLU I-DatasetName
) O
, O
and O
decode O
that O
grounded O
form O
into O
language O
( O
PIGPeN B-DatasetName
- I-DatasetName
NLG I-DatasetName
) O
. O

PIGPeN B-DatasetName
- I-DatasetName
NLU I-DatasetName
Results O
. O

We O
first O
evaluate O
models O
by O
their O
performance O
on O
PIGPeN B-MethodName
- I-MethodName
NLU I-MethodName
: O
given O
objectsõ O
, O
and O
a O
sentence O
s O
a O
describing O
an O
action O
, O
a O
model O
must O
predict O
the O
resulting O
state O
of O
objectsõ O
0 O
. O
We O
primarily O
evaluate O
models O
by O
accuracy O
; O
scoring O
how O
many O
objects O
for O
which O
they O
got O
all O
attributes O
correct O
. O
We O
compare O
with O
the O
following O
strong O
baselines O
: O
a. O
No O
Change O
: O
this O
baseline O
copies O
the O
initial O
state O
of O
all O
objectsõ O
as O
the O
final O
stateõ O
0 O
. O
b. O
GPT3 B-MethodName
- I-MethodName
175B I-MethodName
( O
Brown O
et O
al O
. O
, O
2020 O
) O
, O
a O
very O
large O
language O
model O
for O
' O
few O
- O
shot O
' O
learning O
using O
a O
prompt O
. O
For O
GPT3 B-MethodName
, O
and O
other O
text O
- O
to O
- O
text O
models O
, O
we O
encode O
and O
decode O
the O
symbolic O
object O
states O
in O
a O
JSON O
- O
style O
dictionary O
format O
, O
discussed O
in O
Appendix O
A.4 O
. O
c. O
T5 B-MethodName
( O
Raffel O
et O
al O
. O
, O
2019 O
) O
. O
With O
this O
model O
, O
we O
use O
the O
same O
' O
text O
- O
to O
- O
text O
' O
format O
, O
however O
here O
we O
train O
it O
on O
the O
paired O
data O
from O
PIG B-MethodName
- I-MethodName
PeN. I-MethodName
We O
consider O
varying O
sizes O
of O
T5 B-MethodName
, O
from O
T5 B-MethodName
- I-MethodName
Small I-MethodName
-the O
closest O
in O
size O
to O
PIGLeT B-MethodName
, O
up O
until O
T5 B-MethodName
- I-MethodName
11B I-MethodName
, O
roughly O
100x O
the O
size O
. O
d. O
( O
Alberti O
et O
al O
. O
, O
2019 O
) O
Table O
1 O
: O
Overall O
results O
. O
Left O
: O
we O
show O
the O
model O
accuracies O
at O
predicting O
all O
attributes O
of O
an O
object O
correctly O
. O
We O
compare O
PIGLeT B-MethodName
with O
' O
text O
- O
to O
- O
text O
' O
approaches O
that O
represent O
the O
object O
states O
as O
a O
string O
, O
along O
with O
BERT B-MethodName
- O
style O
approaches O
with O
additional O
machinery O
to O
encode O
inputs O
or O
decode O
outputs O
. O
PIGLeT B-MethodName
outperforms O
a O
T5 B-MethodName
model O
100x O
its O
size O
( O
11B B-HyperparameterValue
params O
) O
and O
shows O
gains O
over O
the O
BERT B-MethodName
- O
style O
models O
that O
also O
model O
action O
dynamics O
through O
a O
language O
transformer O
. O
Right O
: O
we O
show O
several O
attribute O
- O
level O
accuracies O
, O
along O
with O
the O
number O
of O
categories O
per O
attribute O
; O
PIGLeT B-MethodName
outperforms O
baselines O
by O
over O
4 B-MetricValue
points O
for O
some O
attributes O
such O
as O
size B-MetricName
and O
distance B-MetricName
. O

2019a O
) O
, O
where O
grounded O
visual O
information O
is O
fed O
into O
a O
BERT B-MethodName
model O
as O
tokens O
; O
the O
transformer B-MethodName
performs O
the O
grounded O
reasoning O
. O
We O
adapt O
it O
for O
our O
task O
by O
using O
our O
base O
LM O
and O
feeding O
in O
object O
representations O
from O
our O
pretrained O
object O
encoder O
, O
also O
as O
tokens O
. O
Our O
object O
decoder O
predicts O
the O
object O
, O
given O
the O
LM O
's O
pooled O
hidden O
state O
. O
This O
is O
" O
pretrained O
dynamics O
, O
" O
we O
also O
consider O
a O
version O
without O
a O
randomly O
initialized O
dynamics O
model O
. O
e. O
( O
Gupta O
and O
Durrett O
, O
2019 O
) O
-style O
. O
Thiso O
paper O
proposes O
using O
Transformers B-MethodName
to O
model O
physical O
state O
, O
for O
tasks O
like O
entity O
tracking O
in O
recipes O
. O

Here O
, O
the O
authors O
propose O
decoding O
a O
physical O
state O
attribute O
( O
like O
isCooked O
) O
by O
feeding O
the O
model O
a O
label O
- O
specific O
[ O
CLS O
] O
token O
, O
and O
then O
mapping O
the O
result O
through O
a O
hidden B-HyperparameterName
layer I-HyperparameterName
. O
We O
do O
this O
and O
use O
a O
similar O
object O
encoder O
as O
our O
( O
Alberti O
et O
al O
. O
, O
2019 O
) O
-style O
baseline O
. O
We O
discuss O
hyperparameters O
in O
Appendix O
A.3 O
. O

Results O
. O
From O
the O
results O
( O
Table O
1 O
) O
, O
we O
can O
draw O
several O
patterns O
. O
Our O
model O
, O
PIGLeT B-MethodName
performs O
best O
at O
getting O
all O
attributes O
correct O
; O
doing O
so O
over O
80 B-MetricValue
% I-MetricValue
on O
both O
validation O
and O
test O
sets O
, O
even O
for O
novel O
objects O
not O
seen O
during O
training O
. O
The O
next O
closest O
model O
is O
T5 B-MethodName
- I-MethodName
11B I-MethodName
, O
which O
scores O
68 B-MetricValue
% I-MetricValue
on O
validation O
. O
Though O
when O
evaluated O
on O
objects O
' O
seen O
' O
during O
training O
it O
gets O
77 B-MetricValue
% I-MetricValue
, O
that O
number O
drops O
by O
over O
18 B-MetricValue
% I-MetricValue
for O
unseen O
objects O
. O
On O
the O
other O
hand O
, O
PIGLeT B-MethodName
has O
a O
modest O
gap O
of O
3 B-MetricValue
% I-MetricValue
. O
This O
suggests O
that O
our O
approach O
is O
particularly O
effective O
at O
connecting O
unpaired O
language O
and O
world O
representations O
. O
At O
the O
other O
extreme O
, O
GPT3 B-MethodName
does O
poorly O
in O
its O
' O
fewshot O
' O
setting O
, O
suggesting O
that O
size O
is O
no O
replacement O
for O
grounded O
supervision O
. O

PIGLeT B-MethodName
also O
outperforms O
' O
BERT B-MethodName
style O
' O
approaches O
that O
control O
for O
the O
same O
language O
model O
architecture O
, O
but O
perform O
the O
physical O
reasoning O
inside O
the O
language O
transformer O
rather O
than O
as O
a O
separate O
model O
. O
Performance O
drops O
when O
the O
physical O
decoder O
must O
be O
learned O
from O
few O
paired O
examples O
( O
as O
in O
Gupta O
and O
Durrett O
( O
2019 O
) O
) O
; O
it O
drops O
even O
further O
when O
neither O
model O
is O
given O
access O
to O
our O
pretrained O
dynamics O
model O
, O
with O
both O
baselines O
then O
underperforming O
' O
No O
Change O
. O
' O
This O
suggests O
that O
our O
approach O
of O
having O
a O
physical O
reasoning O
model O
outside O
of O
an O
LM O
is O
a O
good O
inductive O
bias O
. O

Ablation O
study O

In O
Table O
2 O
we O
present O
an O
ablation O
study O
of O
PIGLeT B-MethodName
's O
components O
. O
Of O
note O
, O
by O
using O
a O
global O
representation O
of O
objects O
in O
the O
world O
( O
Equation O
4 O
) O
, O
we O
get O
over O
6 B-MetricValue
% I-MetricValue
improvement O
over O
a O
local O
representation O
where O
objects O
are O
manipulated O
independently O
. O
We O
get O
another O
3 B-MetricValue
% I-MetricValue
boost O
by O
adding O
a O
generation O
loss O
, O
suggesting O
that O
learning O
to O
generate O
summaries O
helps O
the O
model O
better O
connect O
the O
world O
to O
language O
. O
Last O
, O
we O
benchmark O
how O
much O
headroom O
there O
is O
on O
PIGPeN B-DatasetName
- I-DatasetName
NLU I-DatasetName
by O
evaluating O
model O
performance O
on O
a O
' O
symbols O
only O
' O
version O
of O
the O
task O
, O
where O
the O
symbolic O
action O
a O
is O
given O
explicitly O
to O
our O
dynamics O
model O
. O
This O
upper O
bound O
is O
roughly O
7 B-MetricValue
% I-MetricValue
higher O
than O
PIGLeT B-MethodName
, O
suggesting O
space O
for O
future O
work O
. O

PIGPeN B-DatasetName
- I-DatasetName
NLG I-DatasetName
Results O

Next O
, O
we O
turn O
to O
PIGPeN B-DatasetName
- I-DatasetName
NLG I-DatasetName
: O
given O
objectsõ O
, O
and O
the O
literal O
next O
action O
a O
, O
a O
model O
must O
generate O
a O
sentence O
sõ0 O
describing O
what O
will O
change O
in O
the O
scene O
. O
We O
compare O
with O
the O
following O
baselines O
: O
a. O
T5 B-MethodName
. O
We O
use O
a O
T5 B-MethodName
model O
that O
is O
given O
a O
JSONstyle O
dictionary O
representation O
of O
bothõ O
and O
a O
, O
it O
is O
finetuned O
to O
generate O
summaries O
sõ0 O
. O
b. O
LM O
Baseline O
. O
We O
feed O
our O
LM O
hidden O
states O
h O
o O
from O
our O
pretrained O
encoder O
, O
along O
with O
its O
representation O
of O
a. O
The O
key O
difference O
between O
it O
and O
PIGLeT B-MethodName
is O
that O
we O
do O
not O
allow O
it O
to O
simulate O
neurally O
what O
might O
happen O
next O
-MLP O
apply O
is O
never O
used O
here O
. O
Size B-HyperparameterName
matters O
. O
Arguably O
the O
most O
important O
factor O
controlling O
the O
fluency O
of O
a O
language O
generator O
is O
its O
size O
( O
Kaplan O
et O
al O
. O
, O
2020 O
) O
. O
Since O
our O
LM O
could O
also O
be O
scaled O
up O
to O
arbitrary O
size O
, O
we O
control O
for O
size O
in O
our O
experiments O
and O
only O
consider O
models O
the O
size O
of O
GPT2 B-MethodName
- I-MethodName
base I-MethodName
( O
117 B-HyperparameterValue
M I-HyperparameterValue
) O
or O
smaller O
; O
we O
thus O
compare O
against O
T5 B-MethodName
- I-MethodName
small I-MethodName
as O
T5 B-MethodName
- I-MethodName
Base I-MethodName
has O
220 B-HyperparameterValue
M I-HyperparameterValue
parameters O
. O
We O
discuss O
optimization O
and O
sampling O
hyperparameters O
in O
Appendix O
A.3 O
. O

Evaluation O
metrics O
. O
We O
evaluate O
models O
over O
the O
validation O
and O
test O
sets O
. O
We O
consider O
three O
main O
evaluation O
metrics O
: O
BLEU B-MetricName
( O
Papineni O
et O
al O
. O
, O
2002 O
) O
with O
two O
references O
, O
the O
recently O
proposed O
BERTScore B-MetricName
( O
Zhang O
et O
al O
. O
, O
2020 O
) O
, O
and O
conduct O
a O
human O
evaluation O
. O
Humans O
rate O
both O
the O
fluency O
of O
post O
- O
action O
text O
, O
as O
well O
as O
its O
faithfulness O
to O
true O
action O
result O
, O
on O
a O
scale O
from O
1 O
to O
1 O
. O

Results O
. O
We O
show O
our O
results O
in O
next O
, O
it O
leads O
to O
more O
faithful O
generation O
as O
well O
. O

Analysis O

Qualitative O
examples O
. O

We O
show O
two O
qualitative O
examples O
in O
Figure O
4 O
, O
covering O
both O
PIGPeN B-DatasetName
- I-DatasetName
NLU I-DatasetName
as O
well O
as O
PIGPeN B-DatasetName
- I-DatasetName
NLG I-DatasetName
. O

In O
the O
first O
row O
, O
the O
robot O
empties O
a O
held O
Mug O
that O
is O
filled O
with O
water O
. O
PIGLeT B-MethodName
gets O
the O
state O
, O
and O
generates O
a O
faithful O
sentence O
summarizing O
that O
the O
mug O
becomes O
empty O
. O
T5 B-MethodName
struggles O
somewhat O
, O
emptying O
the O
water O
from O
both O
the O
Mug O
and O
the O
( O
irrelevant O
) O

Sink O
. O
It O
also O
generates O
text O
saying O
that O
the O
Sink O
becomes O
empty O
, O
instead O
of O
the O
Mug O
. O

In O
the O
second O
row O
, O
PIGLeT B-MethodName
correctly O
predicts O
the O
next O
object O
states O
, O
but O
its O
generated O
text O
is O
incomplete O
-it O
should O
also O
write O
that O
the O
mug O
becomes O
filled O
wtih O
Coffee O
. O
T5 B-MethodName
makes O
the O
same O
mistake O
in O
generation O
, O
and O
it O
also O
underpredicts O
the O
state O
changes O
, O
omitting O
all O
changes O
to O
the O
Mug O
. O

We O
suspect O
that O
T5 B-MethodName
struggles O
here O
in O
part O
because O

Mug O
is O
an O
unseen O
object O
. O
T5 B-MethodName
only O
experiences O
it O
through O
language O
- O
only O
pretraining O
, O
but O
this O
might O
not O
be O
enough O
for O
a O
fully O
grounded O
representation O
. O

Representing O
novel O
words O

The O
language O
models O
that O
perform O
best O
today O
are O
trained O
on O
massive O
datasets O
of O
text O
. O
However O
, O
this O
has O
unintended O
consequences O
( O
Bender O
et O
al O
. O
, O
2021 O
) O
and O
it O
is O
unlike O
how O
children O
learn O
language O
, O
with O
children O
learning O
novel O
words O
from O
experience O
( O
Carey O
and O
Bartlett O
, O
1978 O
) O
. O
The O
large O
scale O
of O
our O
pretraining O
datasets O
might O
allow O
models O
to O
learn O
to O
perform O
physical O
- O
commonsense O
like O
tasks O
for O
wrong O
reasons O
, O
overfitting O
to O
surface O
patterns O
rather O
than O
learning O
meaningful O
grounding O
. O
We O
investigate O
the O
extent O
of O
this O
by O
training O
a O
' O
zero O
- O
shot O
' O
version O
of O
our O
backbone O
LM O
on O
Wikipedia O
and O
books O
-the O
only O
difference O
is O
that O

2047 O

The O
sink O
is O
now O
empty O
. O
The O
coffee O
maker O
is O
now O
on O
and O
the O
mug O
is O
hot O
and O
filled O
with O
coffee O
. O
) O
, O
in O
a O
structured O
and O
explicit O
way O
. O
However O
, O
it O
often O
struggles O
at O
generating O
sentences O
for O
unseen O
objects O
like O
Mug O
that O
are O
excluded O
from O
the O
training O
set O
. O
T5 B-MethodName
struggles O
to O
predict O
these O
changes O
, O
for O
example O
, O
it O
seems O
to O
suggest O
that O
emptying O
the O
Mug O
causes O
all O
containers O
in O
the O
scene O
to O
become O
empty O
. O

Figure O
5 O
: O
PIGPeN B-DatasetName
- I-DatasetName
NLU I-DatasetName
performance O
of O
a O
zero O
- O
shot O
PIGLeT B-MethodName
, O
that O
was O
pretrained O
on O
Books O
and O
Wikipedia O
without O
reading O
any O
words O
of O
our O
' O
unseen O
' O
objects O
like O
' O
mug O
. O
' O
It O
outperforms O
a O
much O
bigger O
T5 B-MethodName
- I-MethodName
11B I-MethodName
overall O
, O
though O
is O
in O
turn O
beaten O
by O
PIGLeT B-MethodName
on O
unseen O
objects O
like O
' O
Sink O
' O
and O
' O
Microwave O
. O
' O

we O
explicitly O
exclude O
all O
mentioned O
sentences O
containing O
one O
of O
our O
" O
unseen O
" O
object O
categories O
. O
In O
this O
setting O
, O
not O
only O
must O
PIGLeT B-MethodName
learn O
to O
ground O
words O
like O
' O
mug O
, O
' O
it O
must O
do O
so O
without O
having O
seen O
the O
word O
' O
mug O
' O
during O
pretraining O
. O
This O
is O
significant O
because O
we O
count O
over O
20k O
instances O
of O
' O
Mug O
' O
words O
( O
including O
morphology O
) O
in O
our O
dataset O
. O
We O
show O
results O
in O
Figure O
5 O
. O
A O
version O
of O
PIGLeT B-MethodName
with O
the O
zero O
- O
shot O
LM O
does O
surprisingly O
well O
-achieving O
80 B-MetricValue
% I-MetricValue
accuracy B-MetricName
at O
predicting O
the O
state O
changes O
for O
" O
Mug O
" O
-despite O
never O
having O
been O
pretrained O
on O
one O
before O
. O
This O
even O
outperforms O
T5 B-MethodName
at O
the O
overall O
task O
. O
Nevertheless O
, O
PIGLeT B-MethodName
outperforms O
it O
by O
roughly O
7 B-MetricValue
% I-MetricValue
at O
unseen O
objects O
, O
with O
notable O
gains O
of O
over O
10 B-MetricValue
% I-MetricValue
on O
highly O
dynamic O
objects O
like O
Toaster O
s O
and O
Sink O
s O
. O

Related O
Work O

Grounded O
commonsense O
reasoning O
. O
In O
this O
work O
, O
we O
study O
language O
grounding O
and O
common O
- O
sense O
reasoning O
at O
the O
representation O
and O
concept O
level O
. O
The O
aim O
is O
to O
train O
models O
that O
learn O
to O
acquire O
concepts O
more O
like O
humans O
, O
rather O
than O
performing O
well O
on O
a O
downstream O
task O
that O
( O
for O
humans O
) O
requires O
commonsense O
reasoning O
. O
Thus O
, O
this O
work O
is O
somewhat O
different O
versus O
other O
3D O
embodied O
tasks O
like O
QA O
( O
Gordon O
et O
al O
. O
, O
2018 O
; O
Das O
et O
al O
. O
, O
2018 O
) O
, O
along O
with O
past O
work O
for O
measuring O
such O
grounded O
commonsense O
reasoning O
, O
like O
SWAG O
, O
HellaSWAG O
, O
and O
VCR O
( O
Zellers O
et O
al O
. O
, O
2018 O
( O
Zellers O
et O
al O
. O
, O
, O
2019b O
. O
The O
knowledge O
covered O
is O
different O
, O
as O
it O
is O
self O
- O
contained O
within O
THOR O
. O
While O
VCR O
, O
for O
instance O
, O
includes O
lots O
of O
visual O
situations O
about O
what O
people O
are O
doing O
, O
this O
paper O
focuses O
on O
learning O
the O
physical O
properties O
of O
objects O
. O

Zero O
- O
shot O
generalization O
. O
There O
has O
been O
a O
lot O
of O
past O
work O
involved O
with O
learning O
' O
zero O
- O
shot O
' O
: O
often O
learning O
about O
the O
grounded O
world O
in O
language O
, O
and O
transferring O
that O
knowledge O
to O
vision O
. O
Techniques O
for O
this O
include O
looking O
at O
word O
embeddings O
( O
Frome O
et O
al O
. O
, O
2013 O
) O
and O
dictionary O
definitions O
( O
Zellers O
and O
Choi O
, O
2017 O
) O
. O
In O
this O
work O
, O
we O
propose O
the O
inverse O
. O
This O
approach O
was O
used O
to O
learn O
better O
word O
embeddings O
or O
semantic O
tuples O
( O
Yatskar O
et O
al O
. O
, O
2016 O
) O
, O
but O
we O
consider O
learning O
a O
component O
to O
be O
plugged O
into O
a O
deep O
Transformer B-MethodName
language O
model O
. O
Past O
work O
evaluating O
these O
types O
of O
zero O
- O
shot O
generalization O
have O
also O
looked O
into O
how O
well O
models O
can O
compose O
concepts O
in O
language O
together O
( O
Lake O
and O
Baroni O
, O
2018 O
; O
Ruis O
et O
al O
. O
, O
2020 O
) O
. O
Our O
work O
considers O
elements O
of O
compositionality O
through O
grounded O
transfer O
. O
For O
example O
, O
in O
PIGPeN B-DatasetName
- I-DatasetName
NLG I-DatasetName
, O
models O
must O
generate O
sentences O
about O
the O
equivalent O
of O
dropping O
a O
' O
dax O
' O
, O
despite O
never O
having O
seen O
one O
before O
. O
However O
, O
our O
work O
is O
also O
contextual O
, O
in O
that O
the O
outcome O
of O
' O
dropping O
a O
dax O
' O
might O
depend O
on O
external O
attributes O
( O
like O
how O
high O
we O
're O
dropping O
it O
from O
) O
. O

Structured O
Models O
for O
Attributes O
and O
Objects O
. O
The O
idea O
of O
modeling O
actions O
as O
functions O
that O
transform O
objects O
has O
been O
explored O
in O
the O
computer O
vision O
space O
( O
Wang O
et O
al O
. O
, O
2016 O
) O
. O
Past O
work O
has O
also O
built O
formal O
structured O
models O
for O
connecting O
vision O
and O
language O
( O
Matuszek O
et O
al O
. O
, O
2012 O
; O
Krishnamurthy O
and O
Kollar O
, O
2013 O
) O
, O
we O
take O
a O
neural O
approach O
and O
connect O
today O
's O
best O
models O
of O
language O
form O
to O
similarly O
neural O
models O
of O
a O
simulated O
environment O
. O

Conclusion O

In O
this O
paper O
, O
we O
presented O
an O
approach O
PIGLeT B-MethodName
for O
jointly O
modeling O
language O
form O
and O
meaning O
. O
We O
presented O
a O
testbed O
PIGPeN B-DatasetName
for O
evaluating O
our O
model O
, O
which O
performs O
well O
at O
grounding O
language O
to O
the O
( O
simulated O
) O
world O
. O

Acknowledgments O

We O
thank O
the O
reviewers O
for O
their O
helpful O
feedback O
, O
and O
the O
Mechanical O
Turk O
workers O
for O
doing O
a O
great O
job O
in O
annotating O
our O
data O
. O
Thanks O
also O
to O
Zak O
Stone O
and O
the O
Google O
Cloud O
TPU O
team O
for O
help O
with O
the O
computing O
infrastructure O
. O
This O
work O
was O
supported O
by O
the O
DARPA O
CwC O
program O
through O
ARO O
( O
W911NF-15 O
- O
1 O
- O
0543 O
) O
, O
the O
DARPA O
MCS O
program O
through O
NIWC O
Pacific O
( O
N66001 O
- O
19 O
- O
2 O
- O
4031 O
) O
, O
and O
the O
Allen O
Institute O
for O
AI O
. O

Contrasting O
Human B-MethodName
- O
and O
Machine B-MethodName
- O
Generated B-TaskName
Word B-TaskName
- I-TaskName
Level I-TaskName
Adversarial B-TaskName
Examples I-TaskName
for O
Text O
Classification O

Research O
shows O
that O
natural O
language O
processing O
models O
are O
generally O
considered O
to O
be O
vulnerable O
to O
adversarial B-TaskName
attacks I-TaskName
; O
but O
recent O
work O
has O
drawn O
attention O
to O
the O
issue O
of O
validating O
these O
adversarial O
inputs O
against O
certain O
criteria O
( O
e.g. O
, O
the O
preservation O
of O
semantics O
and O
grammaticality O
) O
. O
Enforcing O
constraints O
to O
uphold O
such O
criteria O
may O
render O
attacks O
unsuccessful O
, O
raising O
the O
question O
of O
whether O
valid O
attacks O
are O
actually O
feasible O
. O
In O
this O
work O
, O
we O
investigate O
this O
through O
the O
lens O
of O
human O
language O
ability O
. O
We O
report O
on O
crowdsourcing O
studies O
in O
which O
we O
task O
humans O
with O
iteratively O
modifying O
words O
in O
an O
input O
text O
, O
while O
receiving O
immediate O
model O
feedback O
, O
with O
the O
aim O
of O
causing O
a O
sentiment O
classification O
model O
to O
misclassify O
the O
example O
. O
Our O
findings O
suggest O
that O
humans O
are O
capable O
of O
generating O
a O
substantial O
amount O
of O
adversarial O
examples O
using O
semantics O
- O
preserving O
word O
substitutions O
. O
We O
analyze O
how O
human O
- O
generated O
adversarial O
examples O
compare O
to O
the O
recently O
proposed O
TEXTFOOLER B-MethodName
, O
GENETIC B-MethodName
, O
BAE B-MethodName
and O
SEMEMEPSO B-MethodName
attack O
algorithms O
on O
the O
dimensions O
naturalness B-MetricName
, O
preservation B-MetricName
of I-MetricName
sentiment I-MetricName
, O
grammaticality B-MetricName
and O
substitution B-MetricName
rate O
. O
Our O
findings O
suggest O
that O
human O
- O
generated O
adversarial O
examples O
are O
not O
more O
able O
than O
the O
best O
algorithms O
to O
generate O
natural O
- O
reading O
, O
sentimentpreserving O
examples O
, O
though O
they O
do O
so O
by O
being O
much O
more O
computationally O
efficient O
. O

The O
vulnerability O
of O
natural O
language O
processing O
( O
NLP O
) O
models O
to O
adversarial B-TaskName
examples I-TaskName
has O
received O
widespread O
attention O
( O
Alzantot O
et O
al O
. O
, O
2018 O
; O
Iyyer O
et O
al O
. O
, O
2018 O
; O
Ren O
et O
al O
. O
, O
2019 O
) O
. O
Text O
processing O
models O
have O
been O
shown O
to O
be O
susceptible O
to O
adversarial B-TaskName
input I-TaskName
perturbations I-TaskName
across O
tasks O
, O
including O
question O
answering O
and O
text O
classification O
( O
Jia O
and O
Liang O
, O
2017 O
; O
Jin O
et O
al O
. O
, O
2019 O
) O
. O
The O
concept O
of O
adversarial B-TaskName
examples I-TaskName
originated O
in O
computer O
vision O
( O
Szegedy O
et O
al O
. O
, O
2013 O
; O
Goodfellow O
et O
al O
. O
, O
2014 O
) O
, O
and O
in O
that O
domain O
defines O
perturbations O
of O
input O
data O
to O
neural O
networks O
that O
are O
barely O
perceptible O
to O
the O
human O
viewer O
. O
Due O
to O
the O
discrete O
nature O
of O
text O
, O
however O
, O
that O
definition O
is O
less O
applicable O
in O
an O
NLP O
context O
, O
since O
every O
perturbation O
to O
the O
input O
tokens O
is O
unavoidably O
perceptible O
. O
Consequently O
, O
recent O
work O
aims O
to O
perturb O
textual O
inputs O
while O
preserving O
the O
sequence O
's O
naturalness O
and O
semantics O
( O
i.e. O
, O
rendering O
changes O
imperceptible O
on O
these O
dimensions O
) O
. O
However O
, O
as O
shown O
by O
Morris O
et O
al O
. O
( O
2020a O
) O
, O
achieving O
these O
desiderata O
is O
challenging O
because O
even O
small O
perturbations O
can O
render O
a O
text O
meaningless O
, O
grammatically O
incorrect O
or O
unnatural O
, O
and O
furthermore O
several O
proposed O
adversarial O
attacks O
fail O
routinely O
to O
achieve O
them O
. O
If O
the O
algorithms O
are O
modified O
to O
ensure O
that O
they O
do O
achieve O
the O
desiderata O
then O
their O
rate O
of O
generating O
successful O
examples O
greatly O
diminishes O
, O
suggesting O
that O
the O
reported O
success O
rates O
of O
recently O
proposed O
attacks O
might O
represent O
an O
overestimation O
of O
their O
true O
capabilities O
. O
This O
, O
in O
turn O
, O
raises O
the O
question O
of O
whether O
valid O
word B-TaskName
- I-TaskName
level I-TaskName
adversarial I-TaskName
examples I-TaskName
are O
routinely O
possible O
against O
trained O
NLP O
models O
. O

In O
this O
work O
, O
we O
aim O
to O
address O
this O
question O
by O
incorporating O
human O
judgments O
into O
the O
adversarial B-TaskName
example I-TaskName
generation I-TaskName
process O
. O
Specifically O
, O
we O
report O
on O
a O
series O
of O
data O
collection O
efforts O
in O
which O
we O
task O
humans O
to O
generate B-TaskName
adversarial I-TaskName
examples I-TaskName
from O
existing O
movie O
reviews O
, O
while O
instructed O
to O
strictly O
adhere O
to O
a O
set O
of O
validity O
constraints O
. O
In O
contrast O
to O
previous O
work O
( O
e.g. O
, O
Bartolo O
et O
al O
. O
, O
2020 O
; O
Potts O
et O
al O
. O
, O
2020 O
) O
, O
and O
in O
an O
attempt O
to O
replicate O
a O
word O
- O
level O
attack O
's O
mode O
of O
operation O
, O
human O
participants O
were O
only O
able O
to O
substitute O
individual O
words O
, O
and O
were O
not O
allowed O
to O
delete O
or O
insert O
new O
words O
into O
the O
sequence O
. O
This O
represents O
a O
blackbox O
attack O
scenario O
, O
since O
human B-MethodName
participants O
do O
not O
have O
access O
to O
information O
about O
the O
model O
's O
parameters O
or O
gradients O
. O
Participants O
worked O
in O
a O
web O
interface O
( O
Figure O
1 O
) O
that O
allowed O
them O
to O
conduct O
word O
- O
level O
substitutions O
while O
receiving O
immediate O
feedback O
from O
a O
trained O
model O
. O

After O
collecting O
the O
human B-MethodName
- O
generated B-TaskName
adversarial I-TaskName
examples I-TaskName
, O
we O
compare O
them O
to O
a O
set O
of O
automated O
adversarial O
examples O
for O
the O
same O
sequences O
using O
four O
recently O
proposed O
attacks O
: O
TEXTFOOLER B-MethodName
( O
Jin O
et O
al O
. O
, O
2019 O
) O
, O
GENETIC B-MethodName
( O
Alzantot O
et O
al O
. O
, O
2018 O
) O
, O
BAE B-MethodName
( O
Garg O
and O
Ramakrishnan O
, O
2020 O
) O
, O
and O
SE B-MethodName
- I-MethodName
MEMEPSO I-MethodName
( O
Zang O
et O
al O
. O
, O
2020 O
) O
. O
Using O
human B-MethodName
judgments O
from O
an O
independent O
set O
of O
crowdworkers O
, O
we O
assess O
for O
each O
generated O
adversarial O
example O
( O
human O
and O
automated O
) O
whether O
the O
perturbations O
changed O
the O
sequence O
's O
overall O
sentiment B-MetricName
and O
whether O
they O
remained O
natural B-MetricName
. O

We O
find O
that O
humans B-MethodName
are O
capable O
of O
generating B-TaskName
label O
- O
flipping O
word O
- O
level O
adversarial B-TaskName
examples I-TaskName
( O
i.e. O
, O
the O
classifier O
misclassifies O
the O
sequence O
after O
human B-MethodName
perturbation O
) O
in O
approximately O
50 O
% O
of O
the O
cases O
. O
However O
, O
when O
comparing O
the O
ground O
truth O
labels O
of O
perturbed O
sequences O
to O
the O
sentiment O
labels O
provided O
by O
the O
independent O
set O
of O
human O
annotators O
, O
we O
find O
that O
only O
58 O
% O
of O
the O
labelflipping O
human O
adversarial O
examples O
preserve O
their O
target O
sentiment O
after O
perturbation O
. O
This O
is O
considerably O
lower O
than O
for O
the O
best O
automated O
attacks O
, O
which O
exhibit O
a O
label O
consistency O
of O
up O
to O
93 O
% O
( O
TEXTFOOLER B-MethodName
) O
after O
perturbation O
. O
In O
terms O
of O
naturalness B-MetricName
, O
we O
find O
no O
statistically O
significant O
differences O
between O
the O
human O
and O
machine O
attacks O
for O
the O
majority O
of O
comparisons O
. O
We O
furthermore O
observe O
that O
the O
human B-MethodName
- O
generated B-TaskName
sequences O
introduce O
fewer O
grammatical O
errors O
than O
most O
attacks O
. O
These O
findings O
show O
that O
under O
similar O
constraints O
, O
machine B-MethodName
- O
generated B-TaskName
, O
word O
- O
level O
adversarial B-TaskName
examples I-TaskName
are O
comparable O
to O
human B-MethodName
- O
generated B-TaskName
ones O
with O
respect O
to O
their O
naturalness B-MetricName
and O
grammaticality B-MetricName
. O
Importantly O
, O
however O
, O
humans B-MethodName
require O
, O
on O
average O
, O
only O
10.9 O
queries O
to O
run O
against O
the O
model O
to O
generate B-TaskName
label O
- O
flipping O
adversarial B-TaskName
examples I-TaskName
, O
while O
some O
attacks O
require O
thousands O
. O
We O
believe O
that O
our O
findings O
could O
further O
push O
the O
development O
of O
reliable O
word O
- O
level O
adversarial O
attacks O
in O
NLP O
, O
and O
our O
method O
and O
data O
might O
aid O
researchers O
in O
identifying O
human B-MethodName
- O
inspired O
, O
more O
efficient O
ways O
of O
conducting O
adversarial B-TaskName
word I-TaskName
substitutions I-TaskName
against O
neural O
text O
classification O
models O
. O

The O
remainder O
of O
this O
paper O
is O
structured O
as O
follows O
. O
Section O
2 O
discusses O
previous O
work O
related O
to O
our O
research O
. O
Section O
3 O
describes O
both O
phases O
of O
our O
data O
collection O
approach O
, O
i.e. O
, O
the O
human B-MethodName
generation B-TaskName
of O
word B-TaskName
- I-TaskName
level I-TaskName
adversarial B-TaskName
examples I-TaskName
and O
the O
subsequent O
validation O
of O
human B-MethodName
- O
and O
machine B-MethodName
- O
generated O
sequences O
with O
respect O
to O
their O
preservation B-MetricName
of I-MetricName
semantics I-MetricName
and O
naturalness B-MetricName
. O
This O
is O
followed O
by O
the O
analysis O
reported O
in O
Section O
4 O
, O
and O
a O
discussion O
of O
our O
findings O
and O
future O
work O
in O
Section O
5 O
. O
Finally O
, O
we O
conclude O
our O
paper O
in O
Section O
6 O
. O

Adversarial O
attacks O
for O
NLP O
. O
Adversarial O
attacks O
have O
been O
increasingly O
applied O
to O
NLP O
, O
with O
a O
diverse O
set O
of O
attack O
types O
being O
investigated O
, O
ranging O
from O
character O
- O
level O
edits O
( O
Ebrahimi O
et O
al O
. O
, O
2018b O
, O
a O
) O
, O
word O
- O
level O
replacements O
( O
Alzantot O
et O
al O
. O
, O
2018 O
) O
, O
adding O
text O
to O
the O
input O
( O
Jia O
and O
Liang O
, O
2017 O
) O
, O
paraphrase O
- O
level O
modifications O
( O
Iyyer O
et O
al O
. O
, O
2018 O
; O
Ribeiro O
et O
al O
. O
, O
2018 O
) O
, O
to O
creating B-TaskName
adversarial I-TaskName
examples I-TaskName
from O
scratch O
( O
Bartolo O
et O
al O
. O
, O
2020 O
; O
Nie O
et O
al O
. O
, O
2019 O
) O
. O
This O
work O
focuses O
on O
word B-TaskName
- I-TaskName
level I-TaskName
attacks I-TaskName
. O

Word B-TaskName
- I-TaskName
level I-TaskName
attacks I-TaskName
. O
Our O
work O
builds O
on O
existing O
efforts O
on O
word O
- O
level O
adversarial O
attacks O
. O
Attacks O
of O
this O
type O
can O
be O
further O
distinguished O
by O
whether O
the O
adversary O
has O
access O
to O
the O
model O
parameters O
( O
i.e. O
, O
white O
- O
box O
) O
or O
is O
restricted O
to O
accessing O
only O
the O
predicted O
labels O
or O
confidence O
scores O
( O
i.e. O
, O
black O
- O
box O
) O
( O
Yuan O
et O
al O
. O
, O
2019 O
) O
. O
Word B-TaskName
- I-TaskName
level I-TaskName
attacks I-TaskName
have O
been O
explored O
for O
NLP O
tasks O
such O
as O
question O
answering O
( O
Blohm O
et O
al O
. O
, O
2018 O
; O
Welbl O
et O
al O
. O
, O
2020 O
) O
, O
natural O
language O
inference O
( O
Jin O
et O
al O
. O
, O
2019 O
) O
, O
and O
text O
classification O
( O
Papernot O
et O
al O
. O
, O
2016 O
; O
Jin O
et O
al O
. O
, O
2019 O
) O
. O
A O
range O
of O
methodologies O
has O
been O
explored O
for O
finding O
optimal O
synonym O
substitutions O
, O
including O
population O
- O
based O
gradient O
- O
free O
optimization O
via O
genetic O
algorithms O
( O
Alzantot O
et O
al O
. O
, O
2018 O
) O
, O
word O
saliency O
probability O
weighting O
( O
Ren O
et O
al O
. O
, O
2019 O
) O
, O
similarity O
and O
consistency O
filtering O
( O
Jin O
et O
al O
. O
, O
2019 O
) O
, O
sememe O
- O
based O
word O
substitution O
and O
particle O
swarm O
optimization O
- O
based O
search O
( O
Zang O
et O
al O
. O
, O
2020 O
) O
, O
and O
contextual O
perturbations O
from O
masked O
language O
models O
( O
Garg O
and O
Ramakrishnan O
, O
2020 O
) O
. O
Word O
- O
level O
perturbations O
have O
also O
been O
used O
as O
part O
of O
data O
augmentation O
strategies O
to O
certifiably O
improve O
model O
robustness O
( O
Jia O
et O
al O
. O
, O
2019 O
) O
. O

Existing O
efforts O
to O
detect O
and O
defend O
against O
word B-TaskName
- I-TaskName
level I-TaskName
adversarial I-TaskName
examples I-TaskName
resort O
to O
adversarial O
data O
augmentation O
( O
e.g. O
, O
Ren O
et O
al O
. O
, O
2019 O
; O
Jin O
et O
al O
. O
, O
2019 O
) O
as O
well O
as O
rule O
- O
based O
( O
Mozes O
et O
al O
. O
, O
2021 O
) O
and O
learning O
- O
based O
approaches O
to O
identify O
adversarially O
perturbed O
inputs O
. O

Evaluating O
word O
- O
level O
attacks O
. O
Of O
particular O
importance O
for O
this O
paper O
is O
how O
adversarial O
at O
- O
tacks O
can O
be O
evaluated O
against O
various O
dimensions O
. O
Adversarial O
attack O
performance O
has O
been O
shown O
to O
vary O
across O
evaluation O
dimensions O
including O
adversarial O
success O
rates O
, O
readability O
and O
content O
preservation O
( O
Xu O
et O
al O
. O
, O
2020 O
) O
, O
as O
well O
as O
linguistic O
constraints O
such O
as O
semantics O
, O
grammaticality B-MetricName
, O
the O
edit O
distance O
between O
original O
and O
perturbed O
text O
, O
and O
non O
- O
suspicion O
( O
Morris O
et O
al O
. O
, O
2020a O
) O
. O
However O
, O
to O
the O
best O
of O
our O
knowledge O
, O
such O
evaluation O
efforts O
are O
limited O
to O
automated O
attacks O
and O
how O
humans O
perform O
at O
creating O
word B-TaskName
- I-TaskName
level I-TaskName
adversarial I-TaskName
examples I-TaskName
across O
these O
dimensions O
remains O
unexplored O
. O

Human B-MethodName
- O
in O
- O
the O
- O
loop O
adversarial B-TaskName
examples I-TaskName
. O

When O
the O
task O
is O
unconstrained O
, O
human B-MethodName
crowdworkers O
have O
been O
shown O
to O
be O
capable O
of O
creating B-TaskName
high O
quality O
adversarial B-TaskName
examples I-TaskName
for O
a O
variety O
of O
NLP O
tasks O
such O
as O
question O
answering O
( O
Wallace O
et O
al O
. O
, O
2019 O
; O
Dua O
et O
al O
. O
, O
2019 O
; O
Bartolo O
et O
al O
. O
, O
2020 O
; O
Khashabi O
et O
al O
. O
, O
2020 O
) O
, O
natural O
language O
inference O
, O
( O
Nie O
et O
al O
. O
, O
2019 O
) O
, O
and O
sentiment O
analysis O
( O
Potts O
et O
al O
. O
, O
2020 O
) O
. O
We O
extend O
this O
line O
of O
work O
and O
investigate O
whether O
human B-MethodName
capabilities O
for O
creating B-TaskName
adversarial I-TaskName
examples I-TaskName
persist O
when O
the O
examples O
are O
constrained O
to O
arise O
from O
word O
- O
level O
perturbations O
, O
which O
have O
been O
shown O
to O
be O
highly O
effective O
( O
Alzantot O
et O
al O
. O
, O
2018 O
; O
Jin O
et O
al O
. O
, O
2019 O
) O
. O

Our O
data O
collection O
process O
has O
two O
stages O
: O
first O
, O
we O
ask O
human B-MethodName
annotators O
to O
perform O
a O
word B-TaskName
- I-TaskName
level I-TaskName
adversarial I-TaskName
attack I-TaskName
for O
given O
input O
sequences O
. O
To O
this O
end O
, O
we O
prepared O
an O
online O
interface O
that O
lets O
participants O
perturb O
input O
sequences O
on O
a O
word O
- O
level O
whilst O
receiving O
immediate O
feedback O
as O
to O
how O
their O
changes O
affected O
classifier O
confidence O
. O
Second O
, O
we O
ask O
an O
independent O
set O
of O
crowdworkers O
to O
evaluate O
the O
generated B-TaskName
adversarial I-TaskName
examples I-TaskName
. O

Stage O
one O
: O
human B-MethodName
- O
generated B-TaskName
word I-TaskName
- I-TaskName
level I-TaskName
adversarial I-TaskName
examples I-TaskName

In O
order O
to O
familiarize O
participants O
with O
the O
concept O
of O
word B-TaskName
- I-TaskName
level I-TaskName
adversarial I-TaskName
attacks I-TaskName
for O
stage O
one O
of O
the O
data O
collection O
, O
we O
lead O
them O
through O
a O
sequence O
of O
four O
subtasks O
, O
each O
building O
on O
the O
preceding O
one O
: O

1 O
. O
Participants O
are O
asked O
to O
freely O
write O
a O
movie O
review O
with O
a O
specified O
sentiment O
2 O
. O
Participants O
are O
asked O
to O
freely O
write B-TaskName
an I-TaskName
adversarial I-TaskName
example I-TaskName
3 O
. O
Participants O
are O
given O
an O
existing O
movie O
review O
and O
are O
asked O
to O
use O
word B-TaskName
- I-TaskName
level I-TaskName
adversarial I-TaskName
perturbations I-TaskName
without O
adhering O
to O
semantic O
preservation O
and O
grammatical O
correctness O
4 O
. O
Same O
as O
3 O
, O
but O
with O
the O
constraints O
to O
preserve O
semantics O
and O
grammatical O
correctness O

The O
data O
collected O
in O
tasks O
1 O
, O
2 O
and O
3 O
are O
not O
further O
analyzed O
in O
this O
paper O
, O
since O
these O
tasks O
were O
intended O
to O
help O
participants O
understand O
adversarial B-TaskName
examples I-TaskName
for O
text O
classification O
. O
After O
having O
successfully O
completed O
the O
three O
preparation O
tasks O
, O
the O
participants O
are O
considered O
fit O
to O
conduct O
task O
4 O
, O
which O
is O
the O
main O
topic O
of O
interest O
in O
this O
paper O
. O
For O
each O
subtask O
, O
we O
ask O
participants O
to O
submit O
four O
instances O
. O
For O
tasks O
3 O
and O
4 O
, O
we O
randomly O
select O
four O
test O
set O
samples O
from O
the O
IMDb O
movie O
reviews O
dataset O
( O
Maas O
et O
al O
. O
, O
2011 O
) O
for O
each O
participant O
. O
The O
reference O
classifier O
is O
a O
RoBERTa O
model O
( O
Liu O
et O
al O
. O
, O
2019 O
) O
fine O
- O
tuned O
on O
IMDb O
, O
as O
it O
has O
been O
shown O
to O
perform O
highly O
on O
this O
task O
. O
1 O
Our O
fine O
- O
tuned O
model O
achieves O
an O
accuracy O
of O
93.8 O
% O
on O
the O
IMDb O
test O
set O
. O
2 O
For O
tasks O
1 O
and O
2 O
, O
the O
participants O
were O
able O
to O
directly O
see O
the O
classifier O
prediction O
before O
they O
submitted O
their O
reviews O
through O
clicking O
a O
button O
that O
queries O
the O
current O
sequence O
against O
the O
sentiment O
classification O
model O
. O
For O
tasks O
3 O
and O
4 O
, O
we O
asked O
participants O
to O
submit O
at O
least O
15 O
iterations O
of O
word O
- O
level O
substitutions O
before O
moving O
on O
to O
the O
next O
review O
. O
3 O
After O
each O
submitted O
iteration O
the O
model O
provided O
immediate O
feedback O
as O
to O
how O
the O
change O
affected O
its O
prediction O
. O
The O
sequence O
of O
display O
of O
the O
four O
reviews O
in O
tasks O
3 O
and O
4 O
is O
based O
on O
the O
review O
length O
in O
ascending O
order O
. O

Word O
saliencies O
. O
For O
tasks O
3 O
and O
4 O
, O
the O
interface O
additionally O
displays O
the O
word O
saliencies O
( O
Li O
et O
al O
. O
, O
2016a O
, O
b O
) O
for O
each O
word O
in O
the O
movie O
review O
. O
Here O
, O
the O
word O
saliency O
is O
defined O
as O
the O
model O
's O
difference O
in O
prediction O
confidence O
before O
and O
after O
replacing O
the O
word O
with O
an O
out O
- O
of O
- O
vocabulary O
token O
. O
The O
interface O
for O
tasks O
3 O
and O
4 O
is O
shown O
in O
Figure O
1 O
. O
We O
use O
Amazon O
's O
Mechanical O
Turk O
to O
collect O
the O
data O
. O
We O
restrict O
participation O
to O
workers O
that O
have O
previously O
conducted O
more O
than O
1,000 O
successful O
Human O
Intelligence O
Tasks O
( O
HITs O
) O
, O
have O
an O
approval O
rate O
of O
above O
98 O
% O
and O
who O
are O
located O
in O
Canada O
, O
the O
US O
, O
or O
the O
UK O
. O
We O
estimate O
the O
completion O
time O
to O
be O
under O
60 O
minutes O
, O
and O
pay O
USD O
12.40 O
per O
user O
per O
HIT O
. O
In O
total O
, O
we O
collected O
responses O
from O
n O
= O
43 O
participants O
. O
For O
task O
4 O
, O
we O
had O
to O
exclude O
two O
individual O
submissions O
due O
to O
technical O
errors O
. O
The O
resulting O
sample O
consists O
of O
172 O
collected O
reviews O
for O
the O
first O
three O
tasks O
and O
170 O
reviews O
for O
task O
4 O
. O
Despite O
a O
random O
allocation O
of O
test O
set O
sequences O
to O
participants O
, O
we O
did O
not O
encounter O
duplicate O
sequences O
in O
the O
sample O
. O
5 O
Comparison O
to O
automated O
attacks O
. O
We O
compare O
the O
human B-MethodName
- O
generated B-TaskName
, O
word B-TaskName
- I-TaskName
level I-TaskName
adversarial I-TaskName
examples I-TaskName
against O
a O
set O
of O
automatically O
generated O
ones O
. O
Specifically O
, O
we O
attack O
the O
fine O
- O
tuned O
RoBERTa O
model O
as O
used O
for O
the O
data O
collection O
phase O
on O
the O
170 O
sequences O
collected O
in O
task O
4 O
. O
We O
experiment O
with O
four O
recently O
proposed O
attacks O
. O

GENETIC B-MethodName
. O
The O
GENETIC B-MethodName
attack O
( O
Alzantot O
et O
al O
. O
, O
2018 O
) O
uses O
a O
population O
- O
based O
genetic O
search O
method O
to O
generate B-TaskName
word I-TaskName
- I-TaskName
level I-TaskName
adversarial I-TaskName
examchanges I-TaskName
made O
to O
the O
input O
sequence O
. O
5 O
The O
data O
are O
available O
at O
http O
: O
/ O
/ O
github.com O
/ O
maximilianmozes O
/ O
human_adversaries O
. O
ples O
. O
Specifically O
, O
the O
attack O
iteratively O
adds O
individual O
perturbations O
to O
an O
input O
sequence O
until O
the O
model O
misclassifies O
the O
perturbed O
input O
. O

TEXTFOOLER B-MethodName
. O
TEXTFOOLER B-MethodName
( O
Jin O
et O
al O
. O
, O
2019 O
) O
is O
a O
black O
- O
box O
word O
- O
level O
adversarial O
attack O
that O
ranks O
words O
according O
to O
their O
importance O
for O
classifier O
decision O
- O
making O
, O
and O
then O
iteratively O
replaces O
the O
selected O
words O
with O
semantically O
similar O
ones O
to O
lead O
the O
model O
into O
misclassification O
. O
TEXTFOOLER B-MethodName
ensures O
that O
the O
replacement O
tokens O
have O
the O
same O
part O
- O
of O
- O
speech O
as O
the O
selected O
word O
. O
Furthermore O
, O
the O
algorithm O
utilizes O
the O
Universal O
Sentence O
Encoder O
( O
Cer O
et O
al O
. O
, O
2018 O
) O
to O
identify O
replacements O
that O
best O
preserve O
sequence O
semantics O
. O

SEMEMEPSO B-MethodName
. O
Whereas O
existing O
work O
predominantly O
relies O
on O
embedding O
spaces O
or O
thesauri O
like O

WordNet O
( O
Fellbaum O
, O
1998 O
) O
, O
Zang O
et O
al O
. O
( O
2020 O
) O
propose O
an O
attack O
using O
sememes O
( O
which O
the O
authors O
describe O
as O
minimum O
semantic O
units O
of O
language O
) O
to O
identify O
semantics O
- O
preserving O
word O
substitutions O
. O
The O
attack O
, O
referred O
to O
as O
SEMEMEPSO B-MethodName
, O
additionally O
uses O
a O
combinatorial O
optimization O
method O
based O
on O
particle O
swarm O
optimization O
. O

BAE B-MethodName
. O

In O
contrast O
to O
previous O
approaches O
, O
Garg O
and O
Ramakrishnan O
( O
2020 O
) O
propose O
BERT B-MethodName
- I-MethodName
based I-MethodName
Adversarial I-MethodName
Examples I-MethodName
( O
BAE B-MethodName
) O
, O
an O
attack O
that O
relies O
on O
a O
BERT O
masked O
language O
model O
used O
to O
both O
replace O
and O
insert O
new O
tokens O
into O
an O
existing O
sequence O
to O
generate O
an O
adversarial O
example O
. O
They O
introduce O
multiple O
variants O
of O
BAE O
and O
in O
this O
work O
, O
we O
experiment O
with O
the O
BAE B-MethodName
- I-MethodName
R I-MethodName
variant O
, O
which O
only O
replaces O
tokens O
, O
but O
does O
not O
insert O
new O
ones O
. O
This O
is O
to O
ensure O
that O
BAE B-MethodName
is O
directly O
comparable O
to O
the O
other O
attacks O
analyzed O
in O
our O
experiments O
. O

We O
generate B-TaskName
adversarial I-TaskName
examples I-TaskName
based O
on O
the O
170 O
sequences O
used O
during O
the O
data O
collection O
study O
, O
and O
use O
the O
TextAttack B-HyperparameterName
( O
Morris O
et O
al O
. O
, O
2020b O
) O
Python O
library O
with O
all O
attacks O
in O
their O
default O
configuration O
. O
For O
computational O
efficiency O
, O
for O
the O
GENETIC B-MethodName
attack O
, O
we O
use O
a O
slightly O
different O
variant O
compared O
to O
Alzantot O
et O
al O
. O
( O
2018 O
) O
. O
Specifically O
, O
we O
use O
the O
faster O
- O
alzantot O
variant O
offered O
by O
TextAttack B-HyperparameterName
, O
which O
implements O
the O
modifications O
suggested O
in O
Jia O
et O
al O
. O
( O
2019 O
) O
. O

Stage O
two O
: O
evaluating O
generated B-TaskName
adversarial I-TaskName
examples I-TaskName

To O
evaluate O
the O
adversarial B-TaskName
examples I-TaskName
generated I-TaskName
by O
algorithmic O
approaches O
and O
human O
participants O
in O
stage O
one O
, O
we O
ask O
an O
independent O
set O
of O
crowdworkers O
to O
annotate O
the O
collected O
data O
. O
Specifically O
, O
in O
a O
new O
data O
collection O
stage O
, O
participants O
read O
and O
judged O
each O
adversarial O
example O
on O
its O
sentiment B-MetricName
and O
naturalness B-MetricName
, O
both O
on O
a O
five B-MetricName
- I-MetricName
point I-MetricName
Likert I-MetricName
scale I-MetricName
. O

Here O
, O
a O
rating O
of O
1 B-MetricValue
would O
denote O
very O
negative O
sentiment O
( O
a O
very O
unnatural O
review O
) O
, O
whereas O
a O
rating O
of O
5 B-MetricValue
would O
indicate O
a O
very O
positive O
sentiment O
( O
a O
very O
natural O
review O
) O
. O
We O
use O
the O
sentiment B-MetricName
judgments I-MetricName
to O
measure O
the O
deviation O
of O
sentiment B-MetricName
resulting O
from O
introducing O
the O
perturbations O
( O
high O
deviations O
imply O
a O
larger O
shift O
in O
sentiment O
) O
, O
and O
the O
naturalness B-MetricName
judgment I-MetricName
to O
evaluate O
whether O
the O
adversarial O
substitutions O
distort O
the O
naturalness B-MetricName
of O
the O
sequence O
. O
Specifically O
, O
we O
ask O
participants O
to O
rate O
the O
172 B-HyperparameterValue
generated O
adversarial O
examples O
from O
task O
2 O
, O
the O
170 B-HyperparameterValue
unperturbed B-HyperparameterName
reviews I-HyperparameterName
used O
in O
task O
4 O
, O
and O
the O
corresponding O
human B-MethodName
- O
and O
machine B-MethodName
- O
generated B-TaskName
adversarial I-TaskName
examples I-TaskName
. O
For O
the O
examples O
in O
task O
4 O
, O
we O
select O
the O
first O
label O
- O
flipping O
iteration O
for O
a O
successful O
submission O
, O
and O
the O
iteration O
which O
exhibits O
the O
lowest O
confidence O
on O
the O
ground O
truth O
for O
unsuccessful O
submissions O
. O

We O
recruited O
participants O
via O
the O
Prolific O
Academic O
6 O
platform O
, O
and O
aimed O
to O
collect O
three O
independent O
ratings O
per O
text O
. O
We O
used O
independent O
workers O
per O
criterion O
and O
recruited O
120 B-HyperparameterValue
participants B-HyperparameterName
for O
each O
. O
Each O
participant O
was O
asked O
to O
rate O
30 B-HyperparameterValue
texts B-HyperparameterName
( O
randomly O
selected O
from O
all O
available O
sequences O
) O
and O
received O
GBP O
1.50 O
as O
compensation O
. O
On O
average O
, O
each O
text O
was O
rated O
by O
3.55 O
human O
judges O
. O

After O
collecting O
the O
human B-MethodName
judgments O
we O
analyze O
both O
the O
human B-MethodName
and O
machine B-MethodName
attacks O
' O
performance O
on O
generating B-TaskName
adversarial I-TaskName
examples I-TaskName
. O
The O
primary O
objective O
is O
to O
investigate O
the O
feasibility O
of O
wordlevel B-TaskName
adversarial I-TaskName
examples I-TaskName
that O
adhere O
to O
validity O
criteria O
as O
suggested O
in O
previous O
work O
( O
Morris O
et O
al O
. O
, O
2020a O
) O
. O
We O
use O
the O
attack B-MetricName
success I-MetricName
rate I-MetricName
( O
ASR B-MetricName
) O
as O
the O
initial O
metric O
to O
evaluate O
the O
performance O
of O
either O
attack O
mode O
( O
human B-MethodName
and O
algorithmic B-MethodName
) O
. O
The O
attack B-MetricName
success I-MetricName
rate I-MetricName
is O
defined O
as O
the O
percentage O
of O
successful O
adversarial O
examples O
( O
i.e. O
, O
those O
that O
are O
misclassified O
after O
perturbation O
) O
to O
all O
perturbed O
sequences O
. O
We O
observe O
that O
overall O
, O
workers O
were O
generally O
able O
to O
generate O
successful O
movie O
reviews O
for O
task O
1 O
( O
for O
90 B-MetricValue
% I-MetricValue
of O
the O
submitted O
sequences O
the O
model O
predicted O
the O
desired O
sentiment O
) O
and O
led O
the O
model O
into O
misclassification O
in O
task O
2 O
for O
the O
majority O
of O
the O
cases O
( O
ASR B-MetricName
80 B-MetricValue
% I-MetricValue
) O
. O
For O
task O
3 O
, O
workers O
also O
managed O
to O
flip O
the O
model O
prediction O
by O
introducing O
arbitrary O
word O
- O
level O
perturbations O
( O
ASR B-MetricName
86 B-MetricValue
% I-MetricValue
) O
. O

Crucially O
, O
when O
we O
introduced O
constraints O
in O
task O
4 O
, O
the O
ASR B-MetricName
drops O
to O
49 B-MetricValue
% I-MetricValue
, O
suggesting O
an O
increased O
difficulty O
of O
generating B-TaskName
word I-TaskName
- I-TaskName
level I-TaskName
adversarial I-TaskName
examples I-TaskName
when O
attempting O
to O
preserve O
the O
sentiment B-MetricName
and O
naturalness B-MetricName
of O
the O
text O
. O
It O
is O
worth O
mentioning O
that O
we O
conducted O
additional O
experiments O
with O
expert O
annotators O
( O
i.e. O
, O
academic O
researchers O
with O
experience O
in O
NLP O
) O
and O
found O
that O
the O
ASR B-MetricName
for O
task O
4 O
was O
even O
lower O
compared O
to O
the O
crowdworkers O
. O
As O
a O
comparison O
, O
we O
report O
the O
ASR B-MetricName
of O
all O
word B-TaskName
- I-TaskName
level I-TaskName
attacks I-TaskName
in O
Table O
1 O
, O
and O
observe O
that O
the O
HUMAN B-MethodName
ASR B-MetricName
is O
higher O
than O
the O
ones O
for O
GENETIC B-MethodName
and O
BAE B-MethodName
, O
but O
lower O
than O
TEXTFOOLER B-MethodName
and O
SE B-MethodName
- I-MethodName
MEMEPSO I-MethodName
. O

Figure O
2 O
depicts O
the O
distribution O
of O
times O
needed O
for O
the O
human B-MethodName
participants O
to O
generated B-TaskName
the I-TaskName
wordlevel I-TaskName
adversarial I-TaskName
examples I-TaskName
in O
task O
4 O
. O
We O
observe O
that O
participants O
need O
on O
average O
111.29 O
minutes O
( O
standard O
deviation O
: O
119.77 O
) O
to O
complete O
the O
task O
. O

Analysis O
of O
human B-MethodName
annotations O

Sentiment B-MetricName
. O
We O
define O
the O
final O
sentiment B-MetricName
value O
for O
each O
text O
as O
negative O
if O
its O
mean O
rating O
is O
below O
3.0 B-MetricValue
, O
and O
positive O
if O
above O
. O
7 B-MetricValue
As O
an O
initial O
test O
, O
we O
compute O
the O
correlation O
between O
the O
ground O
truth O
label O
( O
positive O
or O
negative O
) O
and O
the O
mean O
human O
sentiment O
rating O
for O
unperturbed O
samples O
for O
task O
4 O
. O
We O
obtain O
a O
Pearson B-MetricName
correlation I-MetricName
of O
r O
= O
0.89 B-MetricValue
( O
95 O
% O
CI O
= O
[ O
0.85 O
, O
0.92 O
] O
, O
p O
< O
.001 O
) O
. O
This O
demonstrates O
high O
agreement O
between O
the O
IMDb O
ground O
truth O
labels O
and O
the O
human O
annotations O
for O
both O
tasks O
. O

Next O
, O
we O
want O
to O
assess O
whether O
adversarial O
examples O
preserve O
the O
sentiment B-MetricName
of O
the O
original O
sequence O
. O
To O
test O
this O
, O
we O
compare O
the O
ground O
truth O
label O
for O
each O
text O
with O
its O
binarized O
human B-MethodName
sentiment O
label O
and O
consider O
sentiment B-MetricName
to O
have O
been O
preserved O
when O
these O
agree O
. O
Table O
2 O
shows O
the O
proportion O
of O
adversarial O
examples O
whose O
ground O
truth O
label O
matches O
the O
binarized O
human O
rating O
. O
∆ B-MetricName
S I-MetricName
and O
∆ B-MetricName
U I-MetricName
represent O
the O
mean O
( O
standard O
deviation O
) O
differences O
in O
ratings O
between O
the O
original O
and O
adversarial O
sequences O
. O
The O
higher O
the O
difference O
, O
the O
more O
do O
human B-MethodName
ratings O
between O
the O
unperturbed O
and O
perturbed O
sequences O
deviate O
from O
each O
other O
. O

All O
algorithmic O
attacks O
show O
high O
values O
( O
above O
80 B-MetricValue
% I-MetricValue
) O
for O
successful O
examples O
, O
while O
the O
HUMAN B-MethodName
attacks O
preserve O
the O
sentiment B-MetricName
less O
often O
( O
58 B-MetricValue
% I-MetricValue
) O
. O
Similarly O
, O
the O
mean O
distance O
( O
∆ B-MetricName
S I-MetricName
= O
1.15 B-MetricValue
) O
for O
the O
HUMAN B-MethodName
attack O
is O
considerably O
higher O
than O
that O
for O
the O
algorithmic O
attacks O
. O
Thus O
, O
of O
the O
humangenerated B-MethodName
adversarial B-TaskName
examples I-TaskName
, O
only O
58 B-MetricValue
% I-MetricValue
preserve O
the O
original O
sentiment B-MetricName
and O
can O
be O
considered O
for O
further O
evaluation O
. O
The O
central O
question O
now O
is O
whether O
the O
higher O
sentiment B-MetricName
- O
preservation O
rate O
of O
algorithmic O
attacks O
holds O
up O
if O
we O
submit O
the O
data O
to O
a O
naturalness B-MetricName
test O
. O

Naturalness B-MetricName
. O
Similar O
to O
sentiment B-MetricName
, O
we O
now O
compare O
the O
naturalness B-MetricName
ratings O
between O
the O
unperturbed O
and O
attacked O
sequences O
. O
The O
average O
naturalness O
rating O
per O
text O
is O
compared O
between O
unperturbed O
texts O
and O
their O
adversarial O
counterparts O
. O
The O
larger O
that O
difference O
, O
the O
more O
unnatural O
the O
adversarial O
perturbations O
have O
rendered O
the O
respective O
movie O
review O
. O
We O
only O
consider O
the O
sentimentpreserving O
adversarial O
examples O
as O
explained O
in O
Section O
4.1 O
. O

To O
test O
statistically O
, O
whether O
the O
attacks O
differed O
in O
their O
naturalness B-MetricName
deviation O
, O
we O
ran O
a O
5 B-HyperparameterValue
( O
attack O
types O
) O
by O
2 B-HyperparameterValue
( O
success O
: O
successful O
and O
unsuccessful O
) O
ANOVA B-HyperparameterName
with O
the O
naturalness O
differences O
as O
the O
dependent O
variable O
. O
That O
analysis O
yielded O
a O
significant O
main O
effect O
of O
attack O
type O
, O
F B-MetricName
( O
4 O
, O
666 O
) O
= O
7.87 B-MetricValue
, O
p O
< O
0.001 O
and O
success O
, O
F B-MetricName
( O
1 O
, O
666 O
) O
= O
18.64 B-MetricValue
, O
p O
< O
0.001 O
, O
both O
of O
which O
were O
subsumed O
in O
the O
interaction O
effect O
, O
F B-MetricName
( O
3 O
, O
666 O
) O
= O
7.29 B-MetricValue
, O
p O
< O
0.001 O
. O

To O
disentangle O
the O
interaction O
effect O
, O
we O
show O
the O
Cohen O
's O
d O
effect O
sizes O
( O
Cohen O
, O
1988 O
) O
for O
the O
attack O
type O
comparisons O
for O
successful O
and O
unsuccessful O
attacks O
. O
This O
analysis O
helps O
us O
to O
understand O
how O
the O
effect O
of O
attack O
type O
depends O
on O
the O
attack O
's O
success O
. O
The O
effect O
size O
d O
expresses O
the O
absolute O
magnitude O
of O
the O
mean O
naturalness O
difference O
per O
comparison O
and O
is O
preferred O
over O
p O
- O
values O
. O
8 O
that O
the O
rating O
differences O
are O
computed O
by O
subtracting O
the O
mean O
of O
the O
column O
attack O
from O
the O
mean O
of O
the O
row O
attack O
( O
i.e. O
, O
a O
negative O
effect O
size O
indicates O
that O
the O
mean O
naturalness B-MetricName
difference O
of O
the O
row O
attack O
is O
lower O
than O
that O
of O
the O
column O
attack O
) O
. O
* O
denotes O
statistically O
significant O
differences O
. O
99.75 O
% O
( O
p O
= O
0.05 O
/ O
20 O
) O
confidence O
intervals O
( O
CI O
) O
. O

A O
CI O
containing O
zero O
implies O
that O
the O
difference O
in O
naturalness B-MetricName
can O
not O
be O
considered O
statistically O
significant O
and O
therefore O
be O
disregarded O
. O
For O
the O
unsuccessful O
examples O
, O
the O
comparisons O
are O
missing O
for O
the O
TEXTFOOLER B-MethodName
and O
SEMEMEPSO B-MethodName
attacks O
. O
This O
is O
because O
both O
attacks O
are O
highly O
successful O
, O
such O
that O
only O
a O
single O
( O
TEXTFOOLER B-MethodName
) O
and O
none O
( O
SEMEMEPSO B-MethodName
) O
of O
the O
adversarial O
examples O
did O
not O
flip O
the O
classifier O
's O
prediction O
. O

No O
differences O
emerge O
between O
the O
mean O
naturalness B-MetricName
rating O
difference O
for O
the O
majority O
of O
comparisons O
with O
respect O
to O
the O
HUMAN B-MethodName
attack O
. O
Only O
for O
the O
unsuccessful O
adversarial O
examples O
do O
we O
see O
that O
the O
rating O
differences O
between O
HUMAN B-MethodName
and O
BAE B-MethodName
are O
significantly O
different O
. O
As O
a O
whole O
, O
this O
analysis O
suggests O
that O
in O
terms O
of O
naturalness B-MetricName
, O
the O
HUMAN B-MethodName
adversarial O
examples O
are O
not O
significantly O
different O
from O
the O
machine B-MethodName
- O
generated O
ones O
( O
see O
Table O
4 O
for O
the O
means O
) O
. O

Substitution B-MetricName
rate O
and O
number O
of O
queries B-MetricName

Next O
, O
we O
analyze O
the O
effect O
of O
the O
substitution B-MetricName
rate O
for O
each O
adversarial O
example O
on O
its O
corresponding O
naturalness O
rating O
as O
well O
as O
the O
number O
of O
model O
queries B-MetricName
required O
per O
attack O
. O
Statistical O
testing O
with O
an O
ANOVA B-HyperparameterName
showed O
that O
there O
were O
significant O
main O
effects O
of O
attack O
type O
and O
success O
as O
well O
a O
significant O
interaction O
. O
Table O
5 O
indicates O
significant O
differences O
between O
the O
comparisons O
. O
Further O
, O
we O
observe O
a O
negative O
Pearson B-MetricName
correlation I-MetricName
of O
r O
= O
−0.31 B-MetricValue
( O
95 O
% O
CI O
= O
[ O
−0.38 O
, O
−0.24 O
] O
, O
p O
< O
.001 O
) O
between O
the O
mean O
naturalness B-MetricName
ratings O
and O
the O
word O
substitution B-MetricName
rate O
, O
indicating O
that O
the O
naturalness B-MetricName
deteriorated O
with O
increasing O
substitutions O
. O
Moreover O
, O
Table O
5 O
shows O
that O
the O
automated O
attacks O
perform O
notably O
more O
model O
queries O
as O
compared O
to O
the O
HU B-MethodName
- I-MethodName
MAN I-MethodName
attack O
. O
9 O
While O
some O
attacks O
query O
a O
model O
thousands O
of O
times O
for O
a O
single O
adversarial O
example O
, O
humans B-MethodName
are O
able O
to O
find O
successful O
adversarial O
examples O
with O
an O
average O
of O
10.9 O
queries O
run O
against O
a O
model O
. O
This O
suggests O
that O
humans O
are O
considerably O
more O
efficient O
in O
generating O
valid O
word O
- O
level O
adversarial O
examples O
. O
Together O
, O
these O
findings O
raise O
the O
question O
of O
how O
automated O
attacks O
might O
be O
further O
optimized O
with O
respect O
to O
their O
computational O
efficiency O
. O

Grammaticality B-MetricName

As O
a O
last O
evaluation O
dimension O
, O
we O
look O
at O
the O
number O
of O
grammatical B-MetricName
mistakes I-MetricName
made O
between O
the O
original O
reviews O
and O
their O
adversarial O
counterparts O
. O
We O
follow O
Morris O
et O
al O
. O
( O
2020a O
) O
by O
using O
the O
LanguageTool B-HyperparameterName
10 I-HyperparameterName
grammar I-HyperparameterName
checker I-HyperparameterName
but O
exclude O
all O
errors O
related O
to O
the O
category O
CASING B-HyperparameterName
since O
all O
sequences O
have O
been O
lower O
- O
cased O
. O
We O
compare O
the O
mean O
number O
of O
grammatical B-MetricName
errors I-MetricName
made O
per O
attack O
and O
the O
percentage O
of O
unperturbed O
- O
adversarial O
sequence O
pairs O
for O
which O
the O
adversarial O
example O
has O
more O
grammatical O
errors O
than O
the O
unperturbed O
sequence O
. O
For O
the O
former O
, O
we O
conduct O
an O
ANOVA B-HyperparameterName
and O
compute O
effect O
sizes O
analogously O
to O
aforementioned O
experiments O
. O

Table O
6 O
suggests O
that O
all O
attacks O
produce O
texts O
with O
a O
higher O
number O
of O
grammatical B-MetricName
errors I-MetricName
than O
the O
unperturbed O
sequences O
. O
Among O
the O
different O
attacks O
, O
BAE B-MethodName
generates O
considerably O
more O
grammatical B-MetricName
errors I-MetricName
( O
15.0 B-MetricValue
errors O
per O
review O
) O
than O
the O
other O
attacks O
( O
between O
11.0 B-MetricValue
and O
11.7 B-MetricValue
errors O
per O
review O
) O
. O
The O
SEMEMEPSO B-MethodName
attack O
has O
the O
lowest O
rate O
( O
22.4 B-MetricValue
% I-MetricValue
) O
of O
increasing O
grammatical O
errors O
. O
For O
34.7 B-MetricValue
% I-MetricValue
of O
all O
tested O
sequences O
, O
the O
HUMAN B-MethodName
adversarial O
word O
substitutions O
yielded O
an O
increase O
in O
grammatical O
errors O
. O
The O
percentages O
of O
37.1 B-MetricValue
% I-MetricValue
for O
the O
GENETIC B-MethodName
and O
56.5 B-MetricValue
% I-MetricValue
for O
TEXTFOOLER B-MethodName
are O
comparable O
to O
the O
results O
reported O
in O
Morris O
et O
al O
. O
( O
2020a O
) O
. O

Despite O
some O
reported O
successes O
, O
recent O
work O
questions O
the O
validity O
of O
machine B-MethodName
- O
generated B-TaskName
word I-TaskName
- I-TaskName
level I-TaskName
adversarial I-TaskName
examples I-TaskName
. O
Central O
to O
that O
critical O
view O
are O
evaluation O
criteria O
on O
which O
the O
adversarial O
ex O
- O
amples O
fall O
short O
( O
Morris O
et O
al O
. O
, O
2020a O
) O
. O
The O
argument O
is O
that O
with O
these O
criteria O
as O
constraints O
, O
most O
( O
if O
not O
all O
) O
word O
- O
level O
adversarial O
examples O
are O
deemed O
invalid O
. O
In O
this O
work O
, O
we O
investigated O
how O
feasible O
such O
adversarial B-TaskName
examples I-TaskName
can O
be O
generated B-TaskName
by O
humans B-MethodName
when O
explicitly O
asked O
to O
respect O
a O
set O
of O
validity O
constraints O
. O
The O
underlying O
reasoning O
was O
that O
human B-MethodName
performance I-MethodName
might O
have O
been O
able O
to O
improve O
the O
quality O
standard O
of O
word B-TaskName
- I-TaskName
level I-TaskName
adversarial I-TaskName
examples I-TaskName
. O

Our O
findings O
suggest O
that O
with O
respect O
to O
the O
success O
rate O
as O
well O
as O
the O
preservation B-MetricName
of I-MetricName
semantics I-MetricName
and O
naturalness B-MetricName
, O
humans O
do O
not O
outperform O
stateof O
- O
the O
- O
art O
attack O
algorithms O
in O
generating B-TaskName
wordlevel I-TaskName
adversarial I-TaskName
substitutions I-TaskName
. O
But O
they O
also O
do O
not O
differ O
much O
. O
This O
finding O
speaks O
to O
the O
difficulty O
of O
the O
task O
. O
However O
, O
our O
findings O
suggest O
that O
while O
humans B-MethodName
do O
not O
outperform O
machines B-MethodName
with O
respect O
to O
the O
aforementioned O
criteria O
, O
they O
are O
able O
to O
generate B-TaskName
adversarial I-TaskName
examples I-TaskName
of O
similar O
quality O
using O
a O
fraction O
of O
the O
attack O
iterations O
required O
by O
the O
algorithms O
. O
Humans O
are O
able O
to O
generate O
label O
- O
flipping O
examples O
with O
only O
a O
handful O
of O
queries B-MetricName
, O
while O
the O
algorithmic O
attacks O
might O
need O
thousands O
of O
inference O
steps O
to O
find O
successful O
word O
substitutions O
. O
Further O
, O
humans O
do O
this O
without O
introducing O
more O
grammatical B-MetricName
errors I-MetricName
than O
the O
algorithmic O
attacks O
. O
In O
sum O
, O
this O
work O
suggests O
that O
humans B-MethodName
produce B-TaskName
adversarial I-TaskName
examples I-TaskName
comparable O
to O
state O
- O
of O
- O
the O
- O
art O
attacks O
but O
at O
a O
fraction O
of O
the O
computational O
costs O
. O
With O
a O
better O
understanding O
of O
how O
humans O
achieve O
this O
, O
future O
work O
could O
try O
to O
close O
that O
gap O
and O
develop O
more O
computationally O
efficient O
algorithmic O
adversarial O
attacks O
inspired O
by O
human O
language O
reasoning O
. O

Our O
work O
comes O
with O
various O
limitations O
. O
First O
, O
the O
broad O
distribution O
of O
human B-MethodName
naturalness B-MetricName
ratings O
of O
unperturbed O
IMDb O
test O
set O
sequences O
reflects O
the O
in- O
formal O
style O
of O
these O
texts O
. O
Future O
work O
would O
need O
to O
assess O
whether O
our O
results O
would O
differ O
in O
more O
formal O
writing O
( O
e.g. O
, O
journalistic O
or O
academic O
writing O
) O
where O
finding O
adequate O
replacements O
while O
meeting O
the O
quality O
criteria O
could O
be O
even O
harder O
. O
Second O
, O
with O
respect O
to O
the O
number O
of O
queries O
, O
a O
direct O
comparison O
between O
the O
success O
rates O
of O
human B-MethodName
and O
algorithmic B-MethodName
attacks O
might O
be O
misleading O
, O
since O
asking O
humans B-MethodName
to O
conduct O
thousands O
of O
iterations O
per O
sequence O
is O
practically O
infeasible O
. O
Future O
work O
could O
assess O
how O
algorithmic B-MethodName
attacks O
perform O
if O
constrained O
to O
the O
same O
number O
of O
iterations O
as O
humans B-MethodName
. O

Moreover O
, O
the O
notable O
difference O
in O
efficiency B-MetricName
between O
humans B-MethodName
and O
algorithms B-MethodName
needs O
to O
be O
investigated O
further O
, O
for O
example O
by O
analyzing O
human O
strategies O
in O
conducting O
word O
substitutions O
, O
which O
can O
potentially O
be O
beneficial O
for O
developing O
more O
efficient O
attack O
algorithms O
. O

Additionally O
, O
our O
findings O
support O
previous O
work O
( O
Morris O
et O
al O
. O
, O
2020a O
) O
and O
suggest O
that O
wordlevel B-TaskName
adversarial I-TaskName
attacks I-TaskName
might O
impose O
unrealistic O
constraints O
( O
even O
on O
humans O
) O
. O
This O
observation O
raises O
the O
question O
of O
whether O
an O
attention O
shift O
towards O
phrase O
- O
based O
adversarial O
examples O
is O
needed O
to O
guarantee O
the O
validity O
of O
adversarial O
examples O
in O
NLP O
. O
To O
this O
end O
, O
it O
would O
be O
interesting O
to O
expand O
our O
research O
focus O
beyond O
word O
- O
level O
attacks O
, O
for O
example O
by O
relaxing O
the O
constraint O
on O
wordlevel O
substitutions O
for O
humans B-MethodName
and O
giving O
them O
additional O
degrees O
of O
freedom O
to O
rephrase O
sequences O
in O
individual O
iterations O
. O

This O
paper O
compared O
human B-MethodName
and O
machine B-MethodName
performance O
on O
generating B-TaskName
word I-TaskName
- I-TaskName
level I-TaskName
adversarial I-TaskName
examples I-TaskName
against O
a O
text O
classification O
model O
for O
sentiment O
analysis O
. O
We O
observe O
that O
human B-MethodName
- O
generated B-TaskName
adversarial I-TaskName
examples I-TaskName
do O
not O
preserve O
a O
sequence O
's O
sentiment B-MetricName
as O
well O
as O
machine O
- O
generated O
ones O
do O
, O
but O
are O
similar O
in O
terms O
of O
their O
naturalness B-MetricName
after O
label O
- O
flipping O
perturbation O
. O
While O
these O
findings O
do O
not O
suggest O
that O
humans B-MethodName
outperform O
algorithms O
for O
this O
task O
, O
we O
find O
that O
they O
achieve O
similar O
performance O
in O
a O
much O
more O
efficient O
manner O
. O
We O
therefore O
believe O
that O
our O
work O
can O
build O
the O
foundation O
for O
future O
research O
aiming O
to O
further O
optimize O
algorithmic B-MethodName
word B-TaskName
- I-TaskName
level I-TaskName
attacks I-TaskName
by O
potentially O
adapting O
human B-MethodName
- O
inspired O
strategies O
for O
this O
task O
. O

This O
work O
uses O
publicly B-DatasetName
available I-DatasetName
data I-DatasetName
( O
Maas O
et O
al O
. O
, O
2011 O
) O
and O
data B-DatasetName
collected I-DatasetName
from I-DatasetName
human I-DatasetName
participants I-DatasetName
. O
All O
human O
participants O
provided O
informed O
consent O
and O
the O
studies O
were O
approved O
by O
the O
local O
ethics O
review O
board O
. O
No O
personal O
information O
was O
collected O
. O

DaMSTF B-MethodName
: O
Domain B-MethodName
Adversarial I-MethodName
Learning I-MethodName
Enhanced I-MethodName
Meta I-MethodName
Self I-MethodName
- I-MethodName
Training I-MethodName
for O
Domain B-TaskName
Adaptation I-TaskName

Self O
- O
training O
emerges O
as O
an O
important O
research O
line O
on O
domain B-TaskName
adaptation I-TaskName
. O
By O
taking O
the O
model O
's O
prediction O
as O
the O
pseudo O
labels O
of O
the O
unlabeled O
data O
, O
self O
- O
training O
bootstraps O
the O
model O
with O
pseudo O
instances O
in O
the O
target O
domain O
. O
However O
, O
the O
prediction O
errors O
of O
pseudo O
labels O
( O
label O
noise O
) O
challenge O
the O
performance O
of O
self O
- O
training O
. O
To O
address O
this O
problem O
, O
previous O
approaches O
only O
use O
reliable O
pseudo O
instances O
, O
i.e. O
, O
pseudo O
instances O
with O
high O
prediction O
confidence O
, O
to O
retrain O
the O
model O
. O
Although O
these O
strategies O
effectively O
reduce O
the O
label O
noise O
, O
they O
are O
prone O
to O
miss O
the O
hard O
examples O
. O
In O
this O
paper O
, O
we O
propose O
a O
new O
self O
- O
training O
framework O
for O
domain B-TaskName
adaptation I-TaskName
, O
namely O
Domain B-MethodName
adversarial I-MethodName
learning I-MethodName
enhanced I-MethodName
Self I-MethodName
- I-MethodName
Training I-MethodName
Framework I-MethodName
( O
DaMSTF B-MethodName
) O
. O
Firstly O
, O
DaMSTF O
involves O
meta O
- O
learning O
to O
estimate O
the O
importance O
of O
each O
pseudo O
instance O
, O
so O
as O
to O
simultaneously O
reduce O
the O
label O
noise O
and O
preserve O
hard O
examples O
. O
Secondly O
, O
we O
design O
a O
meta O
constructor O
for O
constructing O
the O
meta O
validation O
set O
, O
which O
guarantees O
the O
effectiveness O
of O
the O
meta O
- O
learning O
module O
by O
improving O
the O
quality O
of O
the O
meta O
validation O
set O
. O
Thirdly O
, O
we O
find O
that O
the O
meta O
- O
learning O
module O
suffers O
from O
the O
training O
guidance O
vanishment O
and O
tends O
to O
converge O
to O
an O
inferior O
optimal O
. O
To O
this O
end O
, O
we O
employ O
domain O
adversarial O
learning O
as O
a O
heuristic O
neural O
network O
initialization O
method O
, O
which O
can O
help O
the O
meta O
- O
learning O
module O
converge O
to O
a O
better O
optimal O
. O
Theoretically O
and O
experimentally O
, O
we O
demonstrate O
the O
effectiveness O
of O
the O
proposed O
DaMSTF B-MethodName
. O
On O
the O
cross O
- O
domain O
sentiment B-TaskName
classification I-TaskName
task O
, O
DaMSTF B-MethodName
improves O
the O
performance O
of O
BERT B-MethodName
with O
an O
average O
of O
nearly O
4 B-MetricValue
% I-MetricValue
. O

Introduction O

Domain B-TaskName
adaptation I-TaskName
, O
which O
aims O
to O
adapt O
the O
model O
trained O
on O
the O
source O
domain O
to O
the O
target O
domain O
, O
† O
contributed O
equally O
to O
this O
work O
* O
corresponding O
author O
attracts O
much O
attention O
in O
Natural O
Language O
Processing O
( O
NLP O
) O
applications O
( O
Du O
et O
al O
. O
, O
2020 O
; O
Chen O
et O
al O
. O
, O
2021 O
; O
Lu O
et O
al O
. O
, O
2022 O
) O
. O
Since O
domain B-TaskName
adaptation I-TaskName
involves O
labeled O
data O
from O
the O
source O
domain O
and O
unlabeled O
data O
from O
the O
target O
domain O
, O
it O
can O
be O
regarded O
as O
a O
semi O
- O
supervised O
learning O
problem O
. O
From O
this O
perspective O
, O
self O
- O
training O
, O
a O
classical O
semi O
- O
supervised O
learning O
approach O
, O
emerges O
a O
prospective O
research O
direction O
on O
domain B-TaskName
adaptation I-TaskName
( O
Zou O
et O
al O
. O
, O
2019 O
; O
Liu O
et O
al O
. O
, O
2021 O
) O
. O

Self O
- O
training O
consists O
of O
a O
series O
of O
loops O
over O
the O
pseudo O
labeling O
phase O
and O
model O
retraining O
phase O
. O
In O
the O
pseudo O
labeling O
phase O
, O
self O
- O
training O
takes O
the O
model O
's O
prediction O
as O
the O
pseudo O
labels O
for O
the O
unlabeled O
data O
from O
the O
target O
domain O
. O
Based O
on O
these O
pseudo O
- O
labeled O
instances O
, O
self O
- O
training O
retrains O
the O
current O
model O
in O
the O
model O
retraining O
phase O
. O
The O
trained O
model O
can O
be O
adapted O
to O
the O
target O
domain O
by O
repeating O
these O
two O
phases O
. O
Due O
to O
the O
prediction O
errors O
, O
there O
exists O
label O
noise O
in O
pseudo O
instances O
, O
which O
challenges O
self O
- O
training O
approaches O
( O
Zhang O
et O
al O
. O
, O
2017 O
) O
. O

Previous O
self O
- O
training O
approaches O
usually O
involve O
a O
data O
selection O
process O
to O
reduce O
the O
label O
noise O
, O
i.e. O
, O
preserving O
the O
reliable O
pseudo O
instances O
and O
discarding O
the O
remaining O
ones O
. O
In O
general O
, O
higher O
prediction O
confidence O
implies O
higher O
prediction O
correctness O
, O
so O
existing O
self O
- O
training O
approaches O
prefer O
the O
pseudo O
instances O
with O
high O
prediction O
confidence O
( O
Zou O
et O
al O
. O
, O
2019 O
; O
Shin O
et O
al O
. O
, O
2020 O
) O
. O
However O
, O
fitting O
the O
model O
on O
these O
easy O
pseudo O
instances O
can O
not O
effectively O
improve O
the O
model O
, O
as O
the O
model O
is O
already O
confident O
about O
its O
prediction O
. O
On O
the O
contrary O
, O
pseudo O
instances O
with O
low O
prediction O
confidence O
can O
provide O
more O
information O
for O
improving O
the O
model O
, O
but O
contain O
more O
label O
noise O
at O
the O
same O
time O
. O

To O
simultaneously O
reduce O
the O
label O
noise O
and O
preserve O
hard O
examples O
, O
we O
propose O
to O
involve O
in O
meta O
- O
learning O
to O
reweight O
pseudo O
instances O
. O
Within O
a O
learning O
- O
to O
- O
learn O
schema O
, O
the O
meta O
- O
learning O
mod O
- O
ule O
learns O
to O
estimate O
the O
importance O
of O
every O
pseudo O
instance O
, O
and O
then O
, O
allocates O
different O
instance O
weights O
to O
different O
pseudo O
instances O
. O
Ideally O
, O
hard O
and O
correct O
pseudo O
instances O
will O
be O
assigned O
larger O
weights O
, O
while O
easy O
or O
error O
pseudo O
instances O
will O
be O
assigned O
smaller O
weights O
. O
To O
achieve O
this O
, O
the O
process O
in O
the O
meta O
- O
learning O
module O
is O
formulated O
as O
a O
bi O
- O
level O
hyperparameters O
optimization O
problem O
( O
Franceschi O
et O
al O
. O
, O
2018 O
) O
, O
where O
instance O
weights O
are O
taken O
as O
the O
hyperparameters O
and O
determined O
by O
a O
series O
of O
meta O
- O
training O
steps O
and O
meta O
- O
validation O
steps O
. O
In O
the O
meta O
- O
training O
step O
, O
the O
model O
is O
virtually O
updated O
on O
the O
metatraining O
set O
with O
respect O
to O
the O
current O
instance O
weights O
. O
In O
the O
meta O
validation O
step O
, O
we O
validate O
the O
virtually O
updated O
model O
with O
an O
unbiased O
meta O
validation O
set O
, O
and O
optimize O
the O
instance O
weights O
with O
the O
training O
guidance O
back O
- O
propagated O
from O
the O
validation O
performance O
. O

According O
to O
the O
analysis O
in O
( O
Ren O
et O
al O
. O
, O
2018 O
) O
, O
a O
high O
- O
quality O
meta O
validation O
set O
, O
which O
is O
clean O
and O
unbiased O
to O
the O
test O
set O
, O
is O
important O
for O
the O
effectiveness O
of O
the O
meta O
- O
learning O
algorithm O
. O
To O
this O
end O
, O
we O
propose O
a O
meta O
constructor O
oriented O
to O
the O
domain O
adaptation O
scenario O
. O
At O
each O
self O
- O
training O
iteration O
, O
the O
meta O
constructor O
selects O
out O
the O
most O
reliable O
pseudo O
instances O
and O
inserts O
them O
into O
the O
meta O
validation O
set O
. O
Since O
the O
instances O
in O
the O
meta O
validation O
set O
are O
all O
from O
the O
target O
domain O
and O
vary O
along O
with O
the O
self O
- O
training O
iterations O
, O
the O
data O
distribution O
in O
the O
constructed O
meta O
validation O
set O
approximates O
the O
one O
in O
the O
target O
domain O
. O
Thus O
, O
the O
meta O
constructor O
reduces O
the O
bias O
of O
the O
meta O
validation O
set O
. O
On O
the O
other O
hand O
, O
selecting O
the O
most O
reliable O
pseudo O
instances O
can O
reduce O
the O
label O
noise O
, O
making O
the O
meta O
validation O
set O
cleaner O
. O

Another O
challenge O
for O
the O
meta O
- O
learning O
module O
is O
the O
training O
guidance O
vanishment O
, O
referring O
to O
the O
gradient O
vanishment O
on O
hyperparameters O
. O
With O
a O
theoretical O
analysis O
, O
we O
attribute O
this O
problem O
to O
the O
gradient O
vanishment O
on O
the O
meta O
validation O
set O
. O
To O
this O
end O
, O
we O
introduce O
a O
domain B-MethodName
adversarial I-MethodName
learning I-MethodName
module O
to O
perturb O
the O
model O
's O
parameters O
, O
thereby O
increasing O
the O
model O
's O
gradients O
on O
the O
meta O
validation O
set O
. O
In O
DaMSTF B-MethodName
, O
we O
also O
interpret O
the O
domain B-MethodName
adversarial I-MethodName
learning I-MethodName
module O
as O
a O
heuristic O
neural O
network O
initialization O
method O
. O
Before O
the O
model O
retraining O
phase O
, O
the O
domain B-MethodName
adversarial I-MethodName
learning I-MethodName
module O
first O
initializes O
the O
model O
's O
parameters O
by O
aligning O
the O
model O
's O
feature O
space O
. O
For O
domain B-TaskName
adaptation I-TaskName
, O
the O
global O
optimal O
refers O
to O
the O
state O
where O
the O
model O
's O
parameters O
are O
agnostic O
to O
the O
domain O
information O
but O
discriminative O
to O
the O
task O
information O
. O
Thus O
, O
the O
training O
process O
in O
the O
domain B-MethodName
adversarial I-MethodName
learning I-MethodName
module O
makes O
the O
model O
's O
parameters O
closer O
to O
the O
global O
optimal O
, O
serving O
as O
a O
heuristic O
neural O
network O
initialization O
. O

Our O
contributions O
can O
be O
summarized O
as O
follows O
: O

• O
• O
We O
propose O
a O
meta O
constructor O
to O
construct O
the O
meta O
validation O
set O
, O
which O
guarantees O
the O
effectiveness O
of O
the O
meta O
- O
learning O
module O
. O

• O
We O
theoretically O
point O
out O
the O
training O
guidance O
vanishment O
problem O
in O
the O
meta O
- O
learning O
module O
and O
propose O
to O
address O
this O
problem O
with O
a O
domain B-MethodName
adversarial I-MethodName
learning I-MethodName
module O
. O

• O
Theoretically O
, O
We O
analyze O
the O
effectiveness O
of O
the O
DaMSTF B-MethodName
in O
achieving O
domain B-TaskName
adaptation I-TaskName
. O
Experimentally O
, O
we O
validate O
the O
DaMSTF B-MethodName
on O
two O
popular O
models O
, O
i.e. O
, O
BERT B-MethodName
for O
the O
sentiment B-TaskName
analysis I-TaskName
task O
and O
BiGCN B-MethodName
for O
the O
rumor B-TaskName
detection I-TaskName
task O
, O
with O
four O
benchmark O
datasets O
. O

Problem O
Formulation O

We O
denote O
the O
set O
that O
involves O
all O
instances O
in O
the O
source O
domain O
as O
D O
S O
, O
and O
denote O
the O
set O
that O
contains O
all O
instances O
in O
the O
target O
domain O
as O
D O
T O
. O

From O
D O
S O
, O
we O
can O
obtain O
a O
labeled O
dataset O
for O
training O
, O
i.e. O
, O
D O
S O
= O
{ O
( O
x O
i O
, O
y O
i O
) O
} O
N O
i=1 O
. O
In O
text O
classification O
tasks O
, O
the O
input O
x O
i O
is O
a O
text O
from O
the O
input O
space O
X O
, O
the O
corresponding O
label O
y O
i O
is O
a O
C O
- O
dimensional O
one O
- O
hot O
label O
vector O
, O
i.e. O
, O
y O
i O
∈ O
{ O
0 O
, O
1 O
} O
C O
, O
where O
C O
is O
the O
number O
of O
classes O
. O
Based O
on O
D O
S O
, O
we O
learn O
a O
hypothesis O
, O
h O
: O
X O
→ O
{ O
0 O
, O
1 O
} O
C O
. O
Since O
D O
S O
comes O
from O
D O
S O
( O
i.e. O
, O
D O
S O
⊆ O
D O
S O
) O
, O
the O
learned O
hypothesis O
h O
usually O
performs O
well O
on O
D O
S O
. O
When O
we O
transfer O
the O
hypothesis O
h O
from O
D O
S O
to O
D O
T O
, O
h O
may O
perform O
poorly O
due O
to O
the O
domain O
shift O
. O
The O
goal O
of O
domain O
adaptation O
is O
to O
adapt O
the O
hypothesis O
h O
to O
D O
T O
. O

In O
general O
, O
unlabeled O
text O
in O
the O
target O
domain O
is O
available O
( O
Gururangan O
et O
al O
. O
, O
2020 O
) O
. O
We O
denote O
the O
unlabeled O
target O
domain O
dataset O
as O

D O
u O
T O
= O
{ O
( O
x O
m O
) O
} O
U O
m=1 O

, O
where O
x O
m O
∈ O
X O
is O
a O
text O
input O
. O
In O
some O
cases O
, O
we O
can O
even O
access O
an O
in O
- O
domain O
dataset O
, O
i.e. O
, O
a O
small O
set O
of O
labeled O
data O
in O
the O
target O
Sort O
the O
D O
p O
T O
with O
respect O
to O
H O
in O
ascending O
order O
, O
and O
denote O
the O
first O
K O
data O
as O
DE O
, O
the O
remaining O
data O
as O
D O
tr O
T O
6 O
: O

DM O
= O
D O
l O
T O
∪ O
DE O
7 O
: O
DOMAINADVERSARIAL O
( O
DS O
∪ O
D O
u O
T O
, O
θF O
, O
ϑ O
) O
8 O
: O

METALEARNING O
( O
DS O
∪ O
D O
tr O
T O
, O
θ O
, O
w O
) O
9 O
: O
end O
while O
10 O
: O
function O
METALEARNING O
( O
D O
, O
θ O
, O
w O
) O
11 O
: O

for O
training O
batch O
B O
in O
D O
do O
12 O
: O

for O
t=1 O
→ O
TM O
do O
13 O
: O

Computeθ O
( O
w O
t O
) O
via O
Eq O
. O
( O
3 O
) O
14 O
: O

Compute O
weight O
w O
t+1 O
via O
Eq O
. O
( O
6 O
) O
15 O
: O

end O
for O
16 O
: O

w O
* O
← O
w O
T O
M O
, O
update O
θ O
with O
Eq O
. O
( O
7 O
) O
17 O
: O

end O
for O
18 O
: O

return O
θ O
, O
w O
19 O
: O
end O
function O
20 O
: O
function O
DOMAINADVERSARIAL O
( O
D O
, O
θF O
, O
ϑ O
) O
21 O
: O

for O
training O
batch O
B O
in O
D O
do O
22 O
: O

for O
t=1 O
→ O
TD O
do O
23 O
: O

ϑ O
= O
ϑ O
− O
η1 O
ϑ O
LDA O
( O
θF O
, O
ϑ O
, O
B O
) O
24 O
: O

end O
for O
25 O
: O

for O
t=1 O
→ O
TG O
do O
26 O
: O

θF O
= O
θF O
+ O
η2 O
θ O
LDA O
( O
θF O
, O
ϑ O
, O
B O
) O
27 O
: O
end O
for O
28 O
: O

end O
for O
29 O
: O

return O
θ O
, O
ϑ O
30 O
: O
end O
function O
domain O
, O
which O
is O
denoted O
as O
D O
l O
T O
= O
{ O
( O
x O
j O
, O
y O
j O
) O
} O
L O
j=1 O
( O
x O
i O
∈ O
X O
and O
y O
i O
∈ O
{ O
0 O
, O
1 O
} O
C O
) O
. O
When O
D O
l O
T O
= O
∅ O
, O
the O
task O
is O
a O
case O
of O
unsupervised O
domain B-TaskName
adaptation I-TaskName
( O
Wilson O
and O
Cook O
, O
2020 O
) O
. O
Otherwise O
, O
the O
task O
is O
a O
case O
of O
semi O
- O
supervised O
domain B-TaskName
adaptation I-TaskName
( O
Saito O
et O
al O
. O
, O
2019 O
) O
. O

Methodology O

Model O
Overview O

DaMSTF B-MethodName
inherits O
the O
basic O
framework O
of O
selftraining O
, O
which O
consists O
of O
iterations O
over O
the O
" O
Pseudo O
Labeling O
" O
phase O
and O
the O
" O
Model O
Retraining O
" O
phase O
. O
To O
achieve O
domain B-TaskName
adaptation I-TaskName
, O
selftraining O
simultaneously O
optimizes O
the O
model O
's O
parameters O
and O
the O
pseudo O
labels O
with O
Eq O
. O
( O
1 O
) O
. O

min O
θ O
, O
Ŷ O
T O
Lst O
( O
θ O
, O
ŶT O
) O
= O
( O
x O
k O
, O
y O
k O
) O
∈D O
S O
E O
( O
Φ O
( O
x O
k O
; O
θ O
) O
, O
y O
k O
) O
+ O
x O
i O
∈D O
u O
T O
E O
( O
Φ O
( O
xi O
; O
θ O
) O
, O
ŷ O
( O
xi O
) O
) O
( O
1 O
) O

whereŶ O
T O
= O
[ O
ŷ O
1 O
, O
ŷ O
2 O
, O
. O
. O
. O
, O
ŷ O
|D O
u O
T O
| O
] O
T O
denotes O
the O
pseudo O
label O
set O
of O
the O
unlabeled O
target O
domain O
data O
, O
Φ O
θ O
denotes O
the O
model O
under O
the O
hypothesis O
( O
h O
) O
, O
and O
θ O
denotes O
the O
model O
's O
parameters O
. O

In O
the O
pseudo O
labeling O
phase O
, O
DaMSTF B-MethodName
predicts O
the O
unlabeled O
data O
in O
the O
target O
domain O
, O
and O
the O
predictions O
are O
taken O
as O
pseudo O
labels O
. O
Then O
, O
these O
pseudo O
instances O
are O
sent O
to O
the O
meta O
constructor O
. O
For O
the O
instances O
with O
high O
prediction O
confidence O
, O
the O
meta O
constructor O
uses O
them O
to O
expand O
the O
meta O
validation O
set O
. O
For O
the O
remaining O
ones O
, O
the O
meta O
constructor O
uses O
them O
to O
construct O
the O
meta O
- O
training O
set O
. O

In O
the O
model O
retraining O
phase O
, O
DaMSTF B-MethodName
first O
trains O
the O
model O
in O
the O
domain O
adversarial O
training O
module O
to O
align O
the O
feature O
space O
. O
Then O
, O
the O
model O
is O
trained O
in O
the O
meta O
- O
learning O
module O
. O
Afterward O
, O
DaMSTF B-MethodName
backs O
to O
the O
pseudo O
labeling O
phase O
to O
start O
another O
self O
- O
training O
iteration O
. O

Fig O
. O
1 O
shows O
the O
structure O
of O
DaMSTF B-MethodName
, O
and O
Algorithm O
1 O
presents O
the O
corresponding O
pseudo O
- O
code O
. O

Meta O
- O
Learning O
Module O

As O
described O
in O
Fig O
. O
1 O
, O
the O
meta O
- O
learning O
module O
involves O
a O
series O
of O
loops O
over O
the O
" O
Meta O
Training O
" O
step O
and O
" O
Meta O
Validation O
" O
step O
to O
optimize O
the O
hyper O
- O
parameters O
and O
the O
model O
parameters O
. O

Meta O
Training O
. O
The O
training O
batch O
in O
the O
meta O
training O
phase O
, O
i.e. O
, O
B O
= O
{ O
( O
x O
1 O
, O
y O
1 O
) O
, O
( O
x O
2 O
, O
y O
2 O
) O
, O
. O
. O
. O
} O
, O
merges O
the O
labeled O
data O
from O
the O
source O
domain O
with O
the O
pseudo O
labeled O
data O
from O
the O
target O
domain O
. O
The O
supervision O
on O
the O
pseudo O
instances O
is O
the O
pseudo O
- O
label O
, O
and O
the O
supervision O
on O
the O
labeled O
instances O
is O
the O
ground O
- O
truth O
label O
. O
We O
compute O
the O
risk O
loss O
on O
the O
training O
batch O
with O
Eq O
. O
( O
2 O
) O
: O

LT O
( O
θ O
, O
w O
t O
, O
B O
) O
= O
1 O
|B| O
x O
i O
, O
y O
i O
∈B O
σ O
( O
w O
t O
i O
) O
E O
( O
Φ O
( O
xi O
; O
θ O
) O
, O
yi O
) O
( O
2 O
) O

where O
|B| O
is O
the O
size O
of O
B O
, O
E O
is O
the O
loss O
function O
. O
Φ O
θ O
denotes O
the O
model O
under O
the O
hypothesis O
( O
h O
) O
, O
and O
θ O
denotes O
the O
model O
's O
parameters O
. O

θ O
( O
w O
t O
) O
= O
θ O
− O
η O
θ O
LT O
( O
θ O
, O
w O
t O
, O
B O
) O
( O
3 O
) O

where O
η O
is O
the O
learning O
rate O
. O
Meta O
Validation O
After O
being O
virtually O
updated O
in O
the O
meta O
training O
phase O
, O
the O
model O
is O
validated O
on O
the O
meta O
validation O
set O
D O
M O
with O
Eq O
. O
( O
4 O
) O
: O

LM O
( O
θ O
( O
w O
t O
) O
) O
= O
1 O
|DM O
| O
• O
x O
j O
, O
y O
j O
∈D O
M O
E O
( O
Φ O
( O
xj O
; O
θ O
( O
w O
t O
) O
) O
, O
yj O
) O
( O
4 O
) O

where O
E O
is O
the O
loss O
function O
, O
|D O
M O
| O
is O
the O
size O
of O
the O
meta O
validation O
set O
. O
By O
backpropagating O
the O
performance O
on O
the O
meta O
validation O
set O
, O
we O
derive O
the O
training O
guidance O
for O
updating O
the O
instance O
weights O
on O
the O
training O
batch O
as O
below O
: O

∂LM O
( O
θ O
( O
w O
) O
) O
∂w O
= O
∂LM O
( O
θ O
( O
w O
) O
) O
∂θ O
( O
w O
) O
• O
∂θ O
( O
w O
) O
∂w O
( O
5 O
) O

To O
reduce O
the O
computation O
cost O
, O
we O
use O
the O
approximation O
technique O
in O
( O
Chen O
et O
al O
. O
, O
2021 O
) O
to O
compute O
the O
training O
guidance O
( O
i.e. O
, O
∂L O
M O
( O
θ O
( O
w O
) O
) O

∂w O

) O
. O
Based O
on O
the O
computed O
training O
guidance O
, O
we O
obtain O
the O
optimal O
instance O
weights O
( O
marked O
as O
w O
* O
) O
with O
gradient O
descent O
algorithm O
, O
as O
described O
in O
Eq O
. O
( O
6 O
) O
. O
Further O
, O
we O
update O
θ O
with O
Eq O
. O
( O
7 O
) O
: O

w O
t+1 O
= O
w O
t O
− O
γ O
• O
∂L O
M O
( O
θ O
( O
w O
) O
) O
∂w O
( O
6 O
) O
θ O
t+1 O
= O
θ O
t O
− O
η O
θ O
LT O
( O
θ O
, O
w O
* O
, O
B O
) O
( O
7 O
) O

After O
the O
above O
process O
is O
completed O
on O
the O
training O
batch O
B O
, O
another O
training O
batch O
will O
be O
selected O
to O
start O
the O
meta O
- O
learning O
phase O
again O
, O
as O
shown O
in O
lines O
15 O
- O
21 O
in O
Algorithm O
1 O
. O

Meta O
Constructor O

In O
previous O
studies O
, O
the O
meta O
validation O
set O
is O
constructed O
by O
collecting O
a O
set O
of O
labeled O
data O
that O
have O
the O
same O
distribution O
as O
the O
test O
set O
( O
Ren O
et O
al O
. O
, O
2018 O
; O
Shu O
et O
al O
. O
, O
2019 O
) O
. O
However O
, O
such O
practice O
is O
not O
acceptable O
in O
domain O
adaptation O
, O
as O
we O
are O
not O
aware O
of O
the O
data O
distribution O
of O
the O
target O
domain O
during O
the O
training O
phase O
. O

To O
this O
end O
, O
we O
propose O
a O
meta O
constructor O
to O
construct O
a O
meta O
validation O
set O
that O
approximates O
the O
target O
domain O
. O
Specifically O
, O
we O
select O
the O
reliable O
instances O
from O
the O
pseudo O
- O
labeled O
data O
as O
the O
instances O
in O
the O
meta O
validation O
set O
. O
To O
evaluate O
the O
reliability O
of O
each O
of O
the O
pseudo O
instances O
, O
we O
compute O
their O
prediction O
entropy O
via O
Eq O
. O
( O
8 O
) O
: O

H O
( O
xi O
) O
= O
− O
C O
c=1 O
( O
Φ O
( O
c|xi O
; O
θ O
) O
• O
log O
( O
Φ O
( O
c|xi O
; O
θ O
) O
) O
) O
( O
8 O
) O

where O
Φ O
( O
c|x O
i O
; O
θ O
) O
is O
the O
probability O
of O
the O
instance O
x O
i O
belongs O
to O
the O
c O
th O
category O
. O

In O
general O
, O
a O
lower O
prediction O
entropy O
indicates O
a O
higher O
prediction O
correctness O
( O
Nguyen O
et O
al O
. O
, O
2020 O
) O
. O
Thus O
, O
we O
first O
sort O
the O
D O
p O
T O
( O
pseudo O
labeled O
dataset O
) O
in O
ascending O
order O
according O
to O
their O
prediction O
entropy O
. O
Then O
, O
the O
top O
- O
ranked O
K O
instances O
, O
denoted O
as O
D O
E O
, O
are O
selected O
as O
the O
validation O
instances O
, O
and O
the O
remaining O
pseudo O
samples O
, O
denoted O
as O
D O
tr O
T O
, O
are O
preserved O
in O
the O
meta O
training O
set O
. O

In O
the O
semi O
- O
supervised O
domain B-TaskName
adaptation I-TaskName
, O
we O
take O
the O
in O
- O
domain O
dataset O
to O
initialize O
the O
meta O
validation O
dataset O
and O
use O
D O
E O
to O
expand O
the O
meta O
validation O
set O
along O
with O
the O
self O
- O
training O
iterations O
. O
In O
the O
unsupervised O
domain O
adaptation O
, O
where O
the O
in O
- O
domain O
dataset O
is O
empty O
, O
we O
directly O
take O
D O
E O
as O
the O
meta O
validation O
set O
. O
The O
above O
process O
is O
detailed O
in O
lines O
2 O
- O
8 O
of O
Algorithm O
1 O
. O

Here O
, O
meta O
constructor O
is O
an O
important O
knot O
that O
combines O
meta O
- O
learning O
and O
self O
- O
training O
. O
On O
the O
one O
hand O
, O
traditional O
machine O
learning O
approaches O
can O
not O
exploit O
the O
pseudo O
instances O
with O
high O
prediction O
entropy O
, O
due O
to O
the O
inherent O
label O
noise O
. O
In O
this O
case O
, O
the O
meta O
constructor O
uses O
them O
to O
construct O
the O
meta O
training O
set O
, O
as O
the O
meta O
- O
learning O
module O
is O
tolerant O
to O
the O
label O
noise O
in O
the O
metatraining O
set O
. O
On O
the O
other O
hand O
, O
pseudo O
instances O
with O
low O
prediction O
entropy O
can O
not O
provide O
extra O
information O
for O
improving O
the O
model O
but O
contain O
less O
label O
noise O
. O
In O
this O
case O
, O
the O
meta O
constructor O
uses O
them O
to O
validate O
the O
model O
, O
i.e. O
, O
uses O
them O
to O
construct O
or O
expand O
the O
meta O
validation O
set O
, O
which O
can O
improve O
the O
quality O
of O
the O
meta O
validation O
set O
. O

Domain B-MethodName
Adversarial I-MethodName
Learning I-MethodName

As O
theoretically O
explained O
in O
§ O
4.1 O
, O
the O
training O
guidance O
would O
not O
be O
indicative O
if O
the O
model O
's O
gradient O
on O
the O
validation O
instance O
is O
negligible O
. O
The O
presence O
of O
domain B-MethodName
adversarial I-MethodName
learning I-MethodName
can O
prevent O
the O
gradient O
vanishment O
on O
the O
meta O
validation O
set O
, O
thereby O
preventing O
the O
training O
guidance O
vanishment O
. O
On O
the O
other O
hand O
, O
domain B-MethodName
adversarial I-MethodName
learning I-MethodName
can O
explicitly O
align O
the O
feature O
space O
along O
with O
the O
self O
- O
training O
iterations O
. O

To O
present O
the O
details O
in O
the O
domain B-MethodName
adversarial I-MethodName
learning I-MethodName
module O
, O
we O
divide O
the O
model O
Φ O
( O
• O
; O
θ O
) O
into O
two O
parts O
: O
the O
feature O
extraction O
layer O
Φ O
F O
( O
• O
; O
θ O
F O
) O
and O
the O
task O
- O
specific O
layer O
Φ O
c O
( O
• O
; O
θ O
c O
) O
. O
Usually O
, O
θ O
c O
is O
the O
parameters O
of O
the O
last O
layer O
in O
the O
model O
, O
whose O
output O
is O
the O
prediction O
probability O
of O
each O
category O
. O
The O
prediction O
process O
in O
the O
model O
is O
: O

Φ O
( O
xi O
; O
θ O
) O
= O
Φc O
( O
ΦF O
( O
xi O
; O
θF O
) O
; O
θc O
) O
( O
9 O
) O

Following O
Ganin O
et O
al O
. O
( O
2016 O
) O
, O
we O
introduce O
an O
extra O
domain O
discriminator O
to O
discriminate O
the O
instances O
' O
domains O
, O
i.e. O
, O
ϕ O
( O
• O
; O
ϑ O
) O
, O
where O
ϑ O
is O
the O
parameters O
. O
On O
a O
training O
batch O
B O
, O
the O
risk O
loss O
for O
domain B-MethodName
adversarial I-MethodName
learning I-MethodName
is O
: O

LDA O
( O
θF O
, O
ϑ O
, O
B O
) O
= O
1 O
|B| O
x O
i O
, O
d O
i O
∈B O
E O
( O
ϕ O
( O
ΦF O
( O
xi O
; O
θF O
) O
; O
ϑ O
) O
, O
di O
) O
( O
10 O
) O

where O
d O
i O
is O
a O
one O
- O
hot O
vector O
representing O
the O
domain O
of O
x O
i O
, O
E O
is O
the O
cross O
- O
entropy O
function O
. O
The O
specific O
training O
process O
of O
the O
proposed O
domain B-MethodName
adversarial I-MethodName
learning I-MethodName
module O
is O
depicted O
in O
Algorithm O
1 O
, O
lines O
25 O
- O
35 O
. O

Theoretical O
Analysis O

This O
section O
first O
introduces O
the O
training O
guidance O
vanishment O
problem O
and O
then O
explains O
the O
effectiveness O
of O
DaMSTF B-MethodName
in O
achieving O
domain O
adaptation O
. O
The O
proofs O
are O
detailed O
in O
Appendix O
. O
A O
and O
Appendix O
. O
B O
. O

Training O
Guidance O
Vanishment O

Theorem O
1 O
. O
Let O
w O
i O
be O
the O
weight O
of O
the O
training O
instance O
i O
, O
denoted O
as O
( O
x O
i O
, O
y O
i O
) O
, O
in O
B O
, O
the O
gradient O
of O
w O
i O
on O
L O
M O
can O
be O
represented O
by O
the O
similarity O
between O
the O
gradients O
on O
training O
instance O
i O
and O
the O
gradients O
on O
the O
meta O
validation O
set O
: O

∂LM O
( O
θ O
( O
w O
) O
) O
∂wi O
= O
− O
η O
|B| O
• O
[ O
1 O
|DM O
| O
|D O
M O
| O
j=1 O
gθ O
( O
xj O
, O
yj O
) O
T O
] O
• O
g O
θ O
( O
xi O
, O
yi O
) O

where O

1 O
|D O
M O
| O
|D O
M O
| O
j=1 O
gθ O
( O
x O
j O
, O
y O
j O
) O
T O
is O
the O
gradients O
of O
θ O
on O
D O
M O
, O
g O
i O
θ O
( O
x O
i O
, O
y O
i O
) O

is O
the O
gradients O
of O
θ O
on O
the O
training O
instance O
i O
, O
η O
is O
the O
learning O
rate O
in O
Eq O
. O
( O
3 O
) O
According O
to O
Theorem O
1 O
, O
∂L O
M O
( O
θ O
( O
w O
) O
) O
∂w O
i O
is O
not O
indicative O
for O
every O
training O
instance O
if O
the O
model O
's O
gradient O
on O
the O
meta O
validation O
set O
( O
i.e. O
, O

1 O
|D O
M O
| O
|D O
M O
| O
j=1 O
gθ O
( O
x O
j O
, O
y O
j O
) O

) O
is O
very O
small O
, O
which O
we O
named O
as O
the O
training O
guidance O
vanishment O
problem O
. O
In O
DaMSTF B-MethodName
, O
the O
meta O
- O
learning O
module O
is O
challenged O
by O
the O
training O
guidance O
vanishment O
problem O
from O
the O
following O
aspects O
. O

Firstly O
, O
the O
meta O
validation O
set O
is O
much O
smaller O
than O
the O
meta O
training O
set O
, O
so O
the O
model O
converges O
faster O
on O
the O
meta O
validation O
set O
than O
that O
on O
the O
meta O
training O
set O
. O
Considering O
the O
optimization O
on O
neural O
networks O
is O
non O
- O
convex O
, O
the O
model O
can O
converge O
to O
an O
inferior O
optimal O
if O
it O
converges O
too O
early O
on O
the O
meta O
validation O
set O
. O
In O
this O
case O
, O
the O
model O
's O
gradient O
on O
the O
meta O
validation O
set O
is O
very O
small O
, O
which O
results O
in O
the O
training O
guidance O
vanishment O
. O

Secondly O
, O
the O
instances O
in O
D O
E O
are O
the O
ones O
with O
small O
prediction O
entropy O
. O
Since O
the O
supervision O
for O
the O
pseudo O
instances O
is O
exactly O
the O
model O
's O
predictions O
, O
lower O
prediction O
entropy O
results O
in O
lower O
risk O
loss O
. O
Then O
, O
the O
gradients O
back O
- O
propagated O
from O
the O
risk O
loss O
are O
negligible O
, O
which O
also O
results O
in O
the O
training O
guidance O
vanishment O
. O

Theoretical O
Explanation O
of O
DaMSTF B-MethodName

The O
disagreement O
and O
H∆H O
- O
distance O
were O
first O
proposed O
in O
Ben O
- O
David O
et O
al O
. O
( O
2010 O
) O
and O
have O
been O
widely O
applied O
to O
analyze O
the O
effectiveness O
of O
domain O
adaptation O
approaches O
( O
Saito O
et O
al O
. O
, O
2019 O
; O
Du O
et O
al O
. O
, O
2020 O
) O
. O
For O
any O
two O
different O
hypotheses O
h O
1 O
and O
h O
2 O
, O
disagreement O
D O
( O
h O
1 O
, O
h O
2 O
) O
quantifies O
the O
discrepancy O
of O
their O
different O
predictions O
on O
a O
specific O
dataset O
D. O
When O
h O
2 O
is O
an O
ideal O
hypothesis O
that O
can O
correctly O
map O
all O
instances O
in O
D O
, O
D O
( O
h O
1 O
, O
h O
2 O
) O
also O
represents O
the O
error O
rate O
of O
the O
hypothesis O
h O
1 O
on O
dataset O
D O
, O
abbreviated O
as O
D O
( O
h O
1 O
) O
. O
H∆H O
- O
distance O
is O
a O
metric O
for O
evaluating O
the O
divergence O
of O
the O
data O
distribution O
between O
two O
datasets O
, O
which O
is O
only O
relevant O
to O
the O
input O
space O
of O
the O
datasets O
. O
Theorem O
2 O
. O
Assume O
there O
exists O
an O
ideal O
hypothesis O
, O
denoted O
as O
h O
* O
, O
which O
correctly O
maps O
all O
instances O
in O
the O
target O
domain O
to O
their O
groud O
- O
truth O
labels O
. O
In O
the O
self O
- O
training O
iteration O
t O
, O
let O
D O
l O
T O
( O
h O
t O
) O
and O
D O
E O
( O
h O
t O
) O
be O
the O
error O
rate O
of O
the O
hypothesis O
h O
t O
on O
D O
l O
T O
and O
D O
E O
, O
respectively O
. O
Then O
, O
the O
error O
rate O
of O
the O
hypothesis O
h O
t O
on O
the O
target O
domain O
is O
upper O
bounded O
by O
: O

D O
T O
( O
h O
t O
) O
≤ O
D O
l O
T O
∪D O
E O
( O
h O
t O
) O
+ O
1 O
2 O
dH∆H O
( O
DT O
, O
D O
l O
T O
∪ O
DE O
) O
+ O
ρ O
• O
D O
E O
( O
h O
* O
, O
h O
t−1 O
) O

where O

ρ O
= O
|D O
E O
| O
|D O
l O
T O
|+|D O
E O
| O
is O
a O
coefficient O
related O
to O
the O
size O
of O
D O
l O
T O
and O
D O
E O
, O
D O
l O
T O
∪D O
E O
( O
h O
t O
) O

is O
the O
error O
rate O
of O
the O
hypothesis O
h O
t O
on O
the O
union O
of O
D O
l O
T O
and O
D O
E O
. O
Theorem O
3 O
. O
Assume O
there O
exists O
three O
datasets O
, O
D O
1 O
, O
D O
2 O
, O
D O
3 O
, O
and O
let O
X O
1 O
, O
X O
2 O
, O
X O
3 O
denotes O
the O
set O
of O
input O
cases O
in O
these O
three O
datasets O
, O
i.e. O
, O

X O
1 O
= O
{ O
x O
i O
| O
( O
x O
i O
, O
y O
i O
) O
∈ O
D O
1 O
} O
, O
X O
2 O
= O
{ O
x O
i O
| O
( O
x O
i O
, O
y O
i O
) O
∈ O
D O
2 O
} O
, O
X O
3 O
= O
{ O
x O
i O
| O
( O
x O
i O
, O
y O
i O
) O
∈ O
D O
3 O
} O
. O
If O
X O
1 O
⊆ O
X O
2 O
⊆ O
X O
3 O
, O
then O
d O
H∆H O
( O
D O
2 O
, O
D O
3 O
) O
≤ O
d O
H∆H O
( O
D O
1 O
, O
D O
3 O
) O
holds O

Based O
on O
Theorem O
2 O
, O
we O
demonstrate O
the O
effectiveness O
of O
DaMSTF O
from O
the O
following O
aspects O
. O

First O
of O
all O
, O
expanding O
the O
meta O
validation O
set O
can O
decrease O
the O
second O
term O
in O
Theorem O
2 O
, O
i.e. O
, O
Last O
but O
not O
least O
, O
by O
selecting O
examples O
that O
have O
the O
lowest O
prediction O
entropy O
, O
the O
error O
rate O
on O
D O
E O
is O
much O
lower O
than O
that O
of O
the O
expected O
error O
rates O
on O
D O
p O
T O
, O
formally O
, O

D O
E O
( O
h O
* O
, O
h O
t−1 O
) O
< O
D O
p O
T O
( O
h O
* O
, O
h O
t−1 O
) O
. O

In O
other O
words O
, O
the O
data O
selection O
process O
in O
the O
meta O
constructor O
reduces O
the O
third O
term O
in O
Theorem O
2 O
, O
i.e O
. O
, O
ρ O
• O
D O
E O
( O
h O
* O
, O
h O
t−1 O
) O
. O

Experiments O

We O
provide O
the O
experiment O
settings O
in O
§ O
5.1 O
and O
compare O
DaMSTF B-MethodName
with O
previous O
domain B-TaskName
adaptation I-TaskName
approaches O
in O
§ O
5.2 O
. O
In O
§ O
5.3 O
, O
we O
analyze O
the O
effectiveness O
of O
the O
meta O
constructor O
and O
the O
domain B-MethodName
adversarial I-MethodName
learning I-MethodName
module O
with O
an O
ablation O
study O
. O
§ O
5.4 O
validate O
that O
exposing O
more O
unlabeled O
data O
to O
DaMSTF B-MethodName
can O
improve O
the O
domain B-TaskName
adaptation I-TaskName
performance O
( O
Theorem O
3 O
) O
. O
Appendix O
E O
provides O
extra O
experiments O
of O
the O
domain B-MethodName
adversarial I-MethodName
learning I-MethodName
module O
in O
preventing O
the O
training O
guidance O
vanishment O
problem O
, O
and O
the O
meta O
- O
learning O
module O
in O
highlighting O
the O
hard O
and O
correct O
pseudo O
instances O
. O

Experiment O
Settings O

Dataset O
On O
the O
rumor B-TaskName
detection I-TaskName
task O
, O
we O
conduct O
experiments O
with O
the O
public O
dataset O
TWIT B-DatasetName
- I-DatasetName
TER I-DatasetName
( O
Zubiaga O
et O
al O
. O
, O
2016 O
) O
. O
As O
the O
instances O
in O
the O
TWITTER B-DatasetName
dataset O
are O
collected O
with O
five O
topics O
, O
we O
categorized O
the O
instances O
into O
five O
domains O
. O
On O
the O
sentiment B-TaskName
classification I-TaskName
task O
, O
we O
conduct O
experiments O
withs O
the O
public O
dataset O
Amazon B-DatasetName
( O
Blitzer O
et O
al O
. O
, O
2007 O
) O
. O
We O
follow O
the O
method O
in O
( O
He O
et O
al O
. O
, O
2018 O
) O
to O
preprocess O
the O
Amazon B-DatasetName
dataset O
, O
and O
the O
resultant O
dataset O
consists O
of O
8,000 O
instances O
from O
four O
domains O
: O
books O
, O
dvd O
, O
electronics O
, O
and O
kitchen O
. O
More O
statistics O
about O
the O
TWITTER B-DatasetName
dataset O
and O
the O
Amazon B-DatasetName
dataset O
can O
be O
found O
in O
Appendix O
D O
. O

Implementation O
Details O
The O
base O
model O
on O
the O
rumor B-TaskName
detection I-TaskName
task O
is O
BiGCN B-MethodName
( O
Bian O
et O
al O
. O
, O
2020 O
) O
, O
while O
the O
base O
model O
on O
the O
sentiment B-TaskName
classification I-TaskName
task O
is O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
. O
On O
the O
benchmark O
datasets O
, O
we O
conduct O
domain B-TaskName
adaptation I-TaskName
experiments O
on O
every O
domain O
. O
When O
one O
domain O
is O
taken O
as O
the O
target O
domain O
for O
evaluation O
, O
the O
rest O
domains O
are O
merged O
as O
the O
source O
domain O
. O
More O
impelementation O
details O
are O
provided O
in O
Appendix O
C O
. O

Comparing O
Methods O
Since O
the O
DaMSTF B-MethodName
can O
be O
customized O
to O
both O
semi O
- O
supervised O
and O
unsupervised O
domain B-TaskName
adaptation I-TaskName
scenarios O
, O
the O
baselines O
contain O
both O
unsupervised O
and O
semisupervised O
domain O
adaptation O
approaches O
. O
For O
the O
unsupervised O
domain B-TaskName
adaptation I-TaskName
, O
Out B-MethodName
( O
Chen O
et O
al O
. O
, O
2021 O
) O
, O
DANN B-MethodName
( O
Ganin O
et O
al O
. O
, O
2016 O
) O
and O
CRST B-MethodName
( O
Zou O
et O
al O
. O
, O
2019 O
) O
are O
selected O
as O
the O
baselines O
, O
while O
In+Out B-MethodName
( O
Chen O
et O
al O
. O
, O
2021 O
) O
, O
MME B-MethodName
( O
Saito O
et O
al O
. O
, O
2019 O
) O
, O
BiAT B-MethodName
( O
Jiang O
et O
al O
. O
, O
2020 O
) O
, O
and O
Wind B-MethodName
( O
Chen O
et O
al O
. O
, O
2021 O
) O
are O
selected O
as O
the O
baselines O
for O
the O
semi O
- O
supervised O
domain B-TaskName
adaptation I-TaskName
. O
Out O
and O
In+Out B-MethodName
are O
two O
straightforward O
ways O
for O
realizing O
unsupervised O
and O
semi O
- O
supervised O
domain B-TaskName
adaptation I-TaskName
, O
where O
Out B-MethodName
means O
the O
base O
model O
is O
trained O
on O
the O
out O
- O
of O
- O
domain O
data O
( O
i.e. O
, O
labeled O
source O
domain O
data O
) O
and O
In+Out B-MethodName
means O
the O
base O
model O
is O
trained O
on O
both O
the O
in O
- O
domain O
and O
the O
out O
- O
of O
- O
domain O
data O
. O
The O
core O
of O
DANN B-MethodName
is O
an O
adversarial O
learning O
algorithm O
that O
takes O
the O
domain O
classification O
loss O
as O
an O
auxiliary O
loss O
. O
CRST B-MethodName
is O
also O
a O
self O
- O
training O
method O
that O
uses O
a O
label O
regularization O
technique O
to O
reduce O
the O
label O
noise O
from O
mislabeled O
data O
. O
WIND B-MethodName
is O
a O
meta O
- O
learning O
- O
based O
domain O
adaptation O
approach O
that O
optimizes O
the O
weights O
of O
different O
training O
instances O
. O
The O
difference O
between O
the O
WIND B-MethodName
and O
DaMSTF B-MethodName
lies O
in O
that O
, O
( O
i O
) O
WIND B-MethodName
only O
use O
the O
labeled O
source O
data O
to O
construct O
the O
meta O
training O
set O
, O
while O
the O
meta O
training O
set O
in O
the O
DaMSTF B-MethodName
contains O
both O
the O
labeled O
data O
from O
the O
source O
domain O
and O
the O
pseudo O
data O
from O
the O
target O
domain O
. O
( O
ii O
) O
WIND B-MethodName
does O
not O
consider O
the O
training O
guidance O
vanishment O
problem O
and O
the O
bias O
between O
the O
test O
set O
( O
i.e. O
, O
target O
domain O
) O
and O
the O
meta O
validation O
set O
. O

Results O

To O
validate O
the O
effectiveness O
of O
the O
meta O
selftraining O
, O
we O
conduct O
unsupervised O
and O
semisupervised O
domain B-TaskName
adaptation I-TaskName
experiments O
on O
two O
benchmark O
datasets O
, O
i.e. O
, O
BiGCN B-MethodName
on O
TWITTER B-DatasetName
, O
and O
BERT B-MethodName
on O
Amazon B-DatasetName
. O
Since O
the O
rumor B-TaskName
detec I-TaskName
- I-TaskName
tion I-TaskName
task O
focuses O
more O
on O
the O
' O
rumor O
' O
category O
, O
we O
evaluate O
different O
models O
by O
their O
F1 B-MetricName
score O
in O
classifying O
the O
' O
rumor O
' O
category O
. O
On O
the O
sentiment B-TaskName
classification I-TaskName
task O
, O
the O
prediction O
accuracy B-MetricName
of O
different O
classes O
is O
equally O
important O
, O
so O
we O
take O
the O
macro B-MetricName
- I-MetricName
F1 I-MetricName
score O
to O
evaluate O
different O
models O
. O
For O
semi O
- O
supervised O
domain B-TaskName
adaptation I-TaskName
, O
100 O
labeled O
instances O
in O
the O
target O
domain O
are O
taken O
as O
the O
indomain O
dataset O
. O
The O
experiment O
results O
are O
listed O
in O
Tab O
. O
1 O
, O
Tab O
. O
2 O
. O

As O
shown O
in O
Tab O
. O
1 O
, O
Tab O
. O
2 O
, O
DaMSTF B-MethodName
outperforms O
all O
baseline O
approaches O
on O
all O
benchmark O
datasets O
. O
On O
the O
rumor B-TaskName
detection I-TaskName
task O
, O
DaMSTF B-MethodName
surpasses O
the O
best O
baseline O
approaches O
( O
CRST B-MethodName
for O
unsupervised O
domain B-TaskName
adaptation I-TaskName
, O
WIND B-MethodName
for O
semisupervised O
domain B-TaskName
adaptation I-TaskName
) O
by O
nearly O
5 B-MetricValue
% I-MetricValue
on O
average O
. O
For O
the O
" O
Fer O
. O
" O
domain O
, O
where O
most O
approaches O
perform O
worse O
than O
the O
Out B-MethodName
and O
In+Out B-MethodName
, O
DaMSTF B-MethodName
still O
achieves O
an O
F1 B-MetricName
value O
of O
0.629 B-MetricValue
, O
which O
is O
40 B-MetricValue
% I-MetricValue
higher O
than O
that O
of O
the O
In+Out B-MethodName
. O
On O
the O
sentiment B-TaskName
classification I-TaskName
task O
, O
DaMSTF B-MethodName
also O
outperforms O
other O
approaches O
. O
Under O
the O
unsupervised O
domain B-TaskName
adaptation I-TaskName
scenario O
, O
DaMSTF B-MethodName
surpasses O
the O
best O
baseline O
approach O
( O
DANN B-MethodName
on O
the O
Amazon B-DatasetName
dataset O
) O
by O
nearly O
2 B-MetricValue
% I-MetricValue
on O
average O
. O
Under O
the O
semisupervised O
domain B-TaskName
adaptation I-TaskName
scenario O
, O
DaMSTF B-MethodName
surpasses O
Wind B-MethodName
, O
the O
best O
baseline O
approach O
on O
the O
Amazon B-DatasetName
dataset O
, O
by O
nearly O
3 B-MetricValue
% I-MetricValue
on O
average O
. O

Ablation O
Study O

This O
subsection O
presents O
an O
ablation O
study O
to O
understand O
the O
effectiveness O
of O
the O
DaMSTF B-MethodName
. O
As O
illustrated O
in O
§ O
3 O
and O
§ O
4.2 O
, O
DaMSTF B-MethodName
combines O
metalearning O
and O
self O
- O
training O
via O
two O
strategies O
: O
( O
i O
) O
expanding O
the O
meta O
validation O
set O
with O
a O
meta O
constructor O
; O
( O
ii O
) O
preventing O
the O
training O
guidance O
vanishment O
problem O
with O
a O
domain B-MethodName
adversarial I-MethodName
module O
. O
Thus O
, O
we O
separately O
remove O
the O
above O
strategies O
from O
the O
DaMSTF B-MethodName
, O
yielding O
three O
different O
variants O
, O
As O
shown O
in O
Tab O
. O
3 O
and O
Tab O
. O
4 O
, O
both O
strategies O
are O
indispensable O
for O
the O
effectiveness O
of O
DaMSTF B-MethodName
, O
and O
removing O
either O
strategy O
can O
result O
in O
performance O
degeneration O
. O
Removing O
the O
domain B-MethodName
adversarial I-MethodName
learning I-MethodName
module O
( O
DaMSTF B-MethodName
-w I-MethodName
/ I-MethodName
o I-MethodName
D I-MethodName
) O
leads O
to O
an O
average O
decrease O
from O
0.713 B-MetricValue
to O
0.623 B-MetricValue
on O
the O
TWITTER B-DatasetName
dataset O
and O
from O
0.942 B-MetricValue
to O
0.918 B-MetricValue
on O
the O
Amazon B-DatasetName
dataset O
. O
Without O
expanding O
the O
meta O
validation O
set O
, O
DaMSTF B-MethodName
-w I-MethodName
/ I-MethodName
o I-MethodName
E I-MethodName
performs O
worse O
than O
DaMSTF B-MethodName
on O
both O
the O
TWITTER B-DatasetName
dataset O
( O
0.630 B-MetricValue
vs. O
0.731 B-MetricValue
on O
average O
) O
and O
the O
Amazon B-DatasetName
dataset O
( O
0.931 B-MetricValue
vs. O
0.942 B-MetricValue
on O
average O
) O
. O
After O
removing O
both O
strategies O
, O
DaMSTF B-MethodName
suffers O
a O
severe O
performance O
deterioration O
on O
both O
benchmark O
datasets O
. O

Effect O
of O
the O
unlabeled O
dataset O
size O

As O
illustrated O
in O
§ O
4.2 O
, O
the O
second O
term O

d O
H∆H O
( O
D O
T O
, O
D O
l O
T O
∪ O
D O
E O
) O
is O
close O
to O
d O
H∆H O
( O
D O
T O
, O
D O
u O
T O
) O

in O
the O
whole O
training O
process O
. O
From O
this O
perspective O
, O
increasing O
the O
size B-HyperparameterName
of I-HyperparameterName
the I-HyperparameterName
unlabeled I-HyperparameterName
dataset I-HyperparameterName
can O
improve O
the O
performance O
. O
To O
validate O
this O
, O
we O
separately O
expose O
0 B-HyperparameterValue
% I-HyperparameterValue
, O
5 B-HyperparameterValue
% I-HyperparameterValue
, O
10 B-HyperparameterValue
% I-HyperparameterValue
, O
20 B-HyperparameterValue
% I-HyperparameterValue
, O
30 B-HyperparameterValue
% I-HyperparameterValue
, O
40 B-HyperparameterValue
% I-HyperparameterValue
, O
50 B-HyperparameterValue
% I-HyperparameterValue
, O
60 B-HyperparameterValue
% I-HyperparameterValue
, O
70 B-HyperparameterValue
% I-HyperparameterValue
, O
80 B-HyperparameterValue
% I-HyperparameterValue
, O
90 B-HyperparameterValue
% I-HyperparameterValue
, O
100 B-HyperparameterValue
% I-HyperparameterValue
of O
the O
unlabeled B-HyperparameterName
data I-HyperparameterName
during O
the O
training O
. O
These O
new O
unlabeled B-HyperparameterName
dataset I-HyperparameterName
are O
denote O
as O
D O
u O
T O
( O
0 O
% O
) O
, O
D O
u O
T O
( O
5 O
% O
) O
, O
. O
. O
. O
, O
D O
u O
T O
( O
100 O
% O
) O
respectively O
. O
The O
experiments O
are O
conducted O
on O
" O
Ott O
. O
" O
Domain O
of O
TWITTER B-DatasetName
and O
the O
results O
are O
presented O
in O
Fig O
. O
2 O
. O
From O
Fig O
. O
2 O
, O
we O
observe O
that O
the O
model O
performs O
poorly O
when O
using O
a O
small O
proportion O
of O
the O
unlabeled B-HyperparameterName
data I-HyperparameterName
in O
the O
training O
process O
. O
For O
example O
, O
exposing O
D O
u O
T O
( O
5 B-HyperparameterValue
% I-HyperparameterValue
) O
to O
the O
DaMSTF B-MethodName
only O
achieves O
an O
F1 B-MetricName
score O
of O
0.701 B-MetricValue
, O
which O
is O
14.2 B-MetricValue
% I-MetricValue
lower O
than O
the O
0.843 B-MetricValue
achieved O
by O
exposing O
the O
D O
u O
T O
( O
100 B-HyperparameterValue
% I-HyperparameterValue
) O
. O
From O
0 B-HyperparameterValue
% I-HyperparameterValue
to O
50 B-HyperparameterValue
% I-HyperparameterValue
, O
increasing O
the O
exposure B-HyperparameterName
ratio I-HyperparameterName
consistently O
improves O
the O
F1 B-MetricName
score O
. O
The O
improvements O
saturate O
after O
more O
than O
50 B-HyperparameterValue
% I-HyperparameterValue
of O
the O
unlabeled B-HyperparameterName
data I-HyperparameterName
are O
exposed O
, O
which O
can O
be O
explained O
by O
the O
law O
of O
large O
numbers O
in O
the O
statistic O
theory O
( O
Kraaikamp O
and O
Meester O
, O
2005 O
) O
. O
An O
exposure B-HyperparameterName
ratio I-HyperparameterName
of O
50 B-HyperparameterValue
% I-HyperparameterValue
can O
be O
regarded O
as O
a O
large O
number O
for O
approaching O
the O
unlabeled B-HyperparameterName
dataset I-HyperparameterName
. O
Thus O
, O
D O
u O
T O
( O
50 B-HyperparameterValue
% I-HyperparameterValue
) O
is O
close O
to O
D O
u O
T O
( O
100 B-HyperparameterValue
% I-HyperparameterValue
) O
and O
d O
H∆H O
( O
D O
T O
, O
D O
u O
T O
( O
50 B-HyperparameterValue
% I-HyperparameterValue
) O
) O
approximates O
d O
H∆H O
( O
D O
T O
, O
D O
u O
T O
( O
100 B-HyperparameterValue
% I-HyperparameterValue
) O
) O
, O
which O
leads O
to O
the O
performance O
saturation O
. O

6 O
Related O
Work O

Domain B-TaskName
Adaptation I-TaskName

Inspired O
by O
the O
taxonomy O
in O
Ramponi O
and O
Plank O
( O
2020 O
) O
, O
we O
categorize O
the O
domain B-TaskName
adaptation I-TaskName
approaches O
into O
two O
categories O
: O
Feature O
- O
Alignment O
approaches O
and O
Data O
- O
Centric O
approaches O
. O
Feature O
- O
Alignment O
approaches O
( O
Tzeng O
et O
al O
. O
, O
2014 O
; O
Ganin O
et O
al O
. O
, O
2016 O
; O
Saito O
et O
al O
. O
, O
2019 O
) O
focus O
on O
aligning O
the O
feature O
space O
across O
domains O
. O
The O
most O
well O
- O
known O
feature O
- O
alignment O
approach O
is O
DANN B-MethodName
( O
Ganin O
et O
al O
. O
, O
2016 O
) O
, O
which O
aligns O
the O
feature O
space O
by O
min O
- O
max O
the O
domain O
classification O
loss O
. O
With O
similar O
efforts O
, O
MME B-MethodName
( O
Saito O
et O
al O
. O
, O
2019 O
) O
min O
- O
max O
the O
conditional O
entropy O
on O
the O
unlabeled O
data O
. O
VAT O
( O
Miyato O
et O
al O
. O
, O
2018 O
) O
, O
as O
well O
as O
BiAT B-MethodName
( O
Jiang O
et O
al O
. O
, O
2020 O
) O
, O
propose O
to O
decouple O
the O
min O
- O
max O
optimization O
process O
, O
which O
first O
imposes O
a O
gradient O
- O
based O
perturbation O
on O
the O
input O
space O
to O
maximize O
the O
risk O
loss O
and O
then O
minimize O
the O
final O
objective O
on O
the O
perturbed O
input O
cases O
. O
In O
contrast O
, O
Data O
- O
Centric O
approaches O
exploit O
the O
unlabeled O
data O
in O
the O
target O
domain O
or O
select O
the O
relevant O
data O
from O
the O
source O
domain O
. O
To O
select O
relevant O
data O
, O
( O
Moore O
and O
Lewis O
, O
2010 O
; O
Plank O
and O
van O
Noord O
, O
2011 O
) O
design O
a O
technique O
based O
on O
topic O
models O
for O
measuring O
the O
domain O
similarity O
. O
To O
exploit O
the O
unlabeled O
data O
, O
pseudo O
labeling O
approaches O
, O
including O
self O
- O
training O
( O
Zou O
et O
al O
. O
, O
2019 O
) O
, O
co O
- O
training O
( O
Chen O
et O
al O
. O
, O
2011 O
) O
, O
and O
tri O
- O
training O
( O
Saito O
et O
al O
. O
, O
2017 O
) O
, O
are O
widely O
applied O
and O
become O
an O
important O
direction O
. O
In O
the O
research O
of O
self O
- O
training O
for O
domain B-TaskName
adaptation I-TaskName
, O
many O
efforts O
are O
put O
into O
reducing O
the O
label O
noise O
of O
pseudo O
instances O
( O
Zou O
et O
al O
. O
, O
2019 O
( O
Zou O
et O
al O
. O
, O
, O
2018Liu O
et O
al O
. O
, O
2021 O
) O
. O
Among O
them O
, O
CRST O
( O
Zou O
et O
al O
. O
, O
2019 O
) O
proposes O
a O
label O
regularization O
technique O
to O
reduce O
label O
noise O
while O
CST O
( O
Liu O
et O
al O
. O
, O
2021 O
) O
takes O
Tsallis O
- O
entropy O
as O
a O
confidence O
- O
friendly O
regularize O
. O
In O
this O
paper O
, O
we O
propose O
to O
adopt O
metalearning O
to O
automatically O
reduce O
label O
noise O
. O

Meta O
- O
Learning O

Meta O
- O
learning O
is O
an O
emerging O
new O
branch O
in O
machine O
learning O
that O
focuses O
on O
providing O
better O
hyperparameters O
for O
model O
training O
, O
including O
but O
not O
limited O
to O
better O
initial O
model O
parameters O
, O
e.g. O
, O
MAML O
( O
Finn O
et O
al O
. O
, O
2017 O
) O
, O
better O
learning O
rates O
, O
e.g. O
, O
MetaSGD O
( O
Li O
et O
al O
. O
, O
2017 O
) O
, O
and O
better O
neural O
network O
architect O
, O
e.g. O
, O
DARTs O
( O
Liu O
et O
al O
. O
, O
2018 O
) O
. O
Recent O
studies O
revealed O
the O
prospect O
of O
providing O
better O
instance O
weights O
( O
Ren O
et O
al O
. O
, O
2018 O
; O
Shu O
et O
al O
. O
, O
2019 O
; O
Kye O
et O
al O
. O
, O
2020 O
) O
. O
When O
using O
prototypi O
- O
cal O
learning O
on O
the O
few O
- O
shot O
image O
classification O
task O
, O
MCT O
( O
Kye O
et O
al O
. O
, O
2020 O
) O
involves O
a O
reweighing O
process O
to O
obtain O
a O
more O
accurate O
class O
prototype O
. O
Oriented O
to O
natural O
language O
processing O
tasks O
, O
Chen O
et O
al O
. O
, O
2021 O
) O
use O
the O
optimization O
- O
based O
meta O
- O
reweighting O
algorithm O
to O
refine O
the O
training O
set O
. O
Similar O
to O
DaMSTF B-MethodName
, O
Wang O
et O
al O
. O
( O
2021 O
) O
also O
proposes O
to O
combine O
the O
metalearning O
algorithm O
and O
the O
self O
- O
training O
approach O
, O
but O
their O
method O
focuses O
on O
the O
neural O
sequence O
labeling O
task O
rather O
than O
the O
domain B-TaskName
adaptation I-TaskName
task O
. O
Also O
, O
they O
do O
not O
consider O
the O
bias O
between O
the O
meta O
- O
validation O
set O
and O
the O
test O
set O
, O
whereas O
reducing O
such O
bias O
is O
an O
important O
contribution O
of O
the O
DaMSTF B-MethodName
. O
WIND B-MethodName
( O
Chen O
et O
al O
. O
, O
2021 O
) O
is O
a O
meta O
- O
learning O
- O
based O
domain B-TaskName
adaptation I-TaskName
approach O
, O
the O
differences O
between O
WIND B-MethodName
and O
DaMSTF B-MethodName
are O
discussed O
in O
§ O
5.1 O
. O

Conclusion O

This O
paper O
proposes O
an O
improved O
self O
- O
training O
framework O
for O
domain B-TaskName
adaptation I-TaskName
, O
named O
DaMSTF B-MethodName
. O
DaMSTF B-MethodName
extends O
the O
basic O
framework O
for O
selftraining O
approaches O
by O
involving O
a O
meta O
- O
learning O
module O
, O
which O
alleviates O
the O
label O
noise O
problem O
in O
self O
- O
training O
. O
To O
guarantee O
the O
effectiveness O
of O
the O
meta O
- O
learning O
module O
, O
we O
propose O
a O
meta O
constructor O
to O
improve O
the O
quality O
of O
the O
meta O
validation O
set O
, O
and O
propose O
a O
domain B-MethodName
adversarial I-MethodName
module O
to O
prevent O
the O
training O
guidance O
vanishment O
. O
Also O
, O
the O
domain O
adversarial O
learning O
module O
can O
align O
the O
feature O
space O
along O
with O
the O
self O
- O
training O
iterations O
. O
Extensive O
experiments O
on O
two O
popular O
models O
, O
BiGCN B-MethodName
and O
BERT B-MethodName
, O
verify O
the O
effectiveness O
of O
DaMSTF B-MethodName
. O
The O
ablation O
studies O
demonstrate O
that O
the O
meta O
- O
learning O
module O
, O
the O
meta O
constructor O
, O
and O
the O
domain O
adversarial O
module O
are O
indispensable O
for O
the O
effectiveness O
of O
the O
DaMSTF B-MethodName
. O
The O
limitation O
, O
ethical O
considerations O
, O
and O
social O
impacts O
of O
this O
paper O
are O
in O
Appendix O
F O
and O
G O
. O

A O
Proof O
For O
Theorem O
1 O
Theorem O
1 O
. O
Let O
w O
i O
be O
the O
weight O
of O
the O
training O
instance O
i O
, O
denoted O
as O
( O
x O
i O
, O
y O
i O
) O
, O
in O
B O
, O
the O
gradient O
of O
w O
i O
on O
L O
M O
can O
be O
represented O
by O
the O
similarity O
between O
the O
gradients O
on O
training O
instance O
i O
and O
the O
gradients O
on O
the O
meta O
validation O
set O
: O

∂LM O
( O
θ O
( O
w O
) O
) O
∂wi O
= O
− O
η O
|B| O
• O
[ O
1 O
|DM O
| O
|D O
M O
| O
j=1 O
gθ O
( O
xj O
, O
yj O
) O
T O
] O
• O
g O
θ O
( O
xi O
, O
yi O
) O

where O

1 O
|D O
M O
| O
|D O
M O
| O
j=1 O
gθ O
( O
x O
j O
, O
y O
j O
) O
T O
is O
the O
gradients O
of O
θ O
on O
D O
M O
, O
g O
i O
θ O
( O
x O
i O
, O
y O
i O
) O

is O
the O
gradients O
of O
θ O
on O
the O
training O
instance O
i O
, O
η O
is O
the O
learning O
rate O
in O
Eq O
. O
( O
3 O
) O
Proof O
. O
Based O
on O
Eq O
. O
( O
2 O
) O
and O
Eq O
. O
( O
3 O
) O
in O
§ O
3.2 O
, O
we O
conclude O
the O
pseudo O
updated O
parametersθ O
( O
w O
) O
as O
: O

θ O
( O
w O
) O
= O
θ O
− O
η O
• O
1 O
|B| O
• O
x O
i O
, O
y O
i O
∈B O
σ O
( O
wi O
) O
• O
∂E O
( O
Φ O
( O
xi O
; O
θ O
) O
, O
yi O
) O
∂θ O
( O
11 O
) O

We O
then O
take O
the O
gradient O
of O
w O
i O
onθ O
( O
w O
) O
as O
: O

∂θ O
( O
w O
) O
∂σ O
( O
wi O
) O
= O
− O
η O
|B| O
• O
∂E O
( O
Φ O
( O
xi O
; O
θ O
) O
, O
yi O
) O
∂θ O
( O
12 O
) O

Based O
on O
Eq O
. O
( O
12 O
) O
, O
we O
derivate O
the O
gradient O
of O
w O
i O
on O
L O
M O
as O
: O

∂LM O
( O
θ O
( O
w O
) O
) O
∂wi O
= O
[ O
∂LM O
( O
θ O
( O
w O
) O
) O
∂θ O
( O
w O
) O
] O
T O
• O
[ O
∂θ O
( O
w O
) O
∂σ O
( O
wi O
) O
] O
• O
[ O
∂σ O
( O
w O
) O
∂w O
] O
= O
[ O
1 O
|DM O
| O
• O
|D O
M O
| O
j=1 O
∂E O
( O
Φ O
( O
xj O
; O
θ O
( O
w O
) O
) O
, O
yj O
) O
∂θ O
( O
w O
) O
] O
T O
• O
[ O
− O
η O
|B| O
• O
∂E O
( O
Φ O
( O
xi O
; O
θ O
) O
, O
yi O
) O
∂θ O
] O
• O
[ O
σ O
( O
wi O
) O
( O
1 O
− O
σ O
( O
wi O
) O
) O
] O
= O
− O
ησ O
( O
wi O
) O
( O
1 O
− O
σ O
( O
wi O
) O
) O
|B| O
• O
[ O
1 O
|DM O
| O
|D O
M O
| O
j=1 O
gθ O
( O
xj O
, O
yj O
) O
T O
] O
• O
g O
θ O
( O
xi O
, O
yi O
) O
( O
13 O
) O

where O
the O
second O
line O
is O
obtained O
by O
substituting O
L O
M O
andθ O
with O
Eq O
. O
( O
4 O
) O
and O
Eq O
. O
( O
11 O
) O
. O
Substitute O
gθ O
( O
x O
j O
, O
y O
j O
) O
= O
∂E O
( O
Φ O
( O
x O
j O
; O
θ O
( O
w O
) O
) O
, O
y O
j O
) O
∂θ O
( O
w O
) O
and O
g O
θ O
( O
x O
i O
, O
y O
i O
) O
= O
∂E O
( O
Φ O
( O
x O
i O
; O
θ O
) O
, O
y O
i O
) O
∂θ O
and O
rearrange O
the O
terms O
, O
we O
obtain O
the O
third O
line O
. O
The O
proof O
of O
Theorem O
1 O
is O
completed O
. O

B O
Proof O
For O
Theorem O
2 O
and O
Theorem O
3 O
Definition O
1 O
. O
disagreement O
is O
a O
measure O
to O
quantify O
the O
different O
performances O
of O
two O
different O
hypotheses O
on O
a O
specific O
dataset O
. O
Denote O
the O
two O
hypotheses O
as O
h O
1 O
and O
h O
2 O
, O
and O
denote O
the O
specific O
dataset O
as O
D O
, O
then O
the O
disagreement O
of O
h O
1 O
and O
h O
2 O
on O
D O
is O
formulated O
as O
: O

D O
( O
h1 O
, O
h2 O
) O
= O
1 O
|D| O
|D| O
i=1 O
[ O
1 O
C O
* O
||h1 O
( O
x O
) O
− O
h2 O
( O
x O
) O
||1 O
] O
( O
14 O
) O

where O
C O
is O
the O
number O
of O
classes O
, O
h O
1 O
( O
x O
) O
and O
h O
2 O
( O
x O
) O
are O
one O
- O
hot O
vectors O
representing O
the O
models O
' O
predictions O
. O
Definition O
2 O
. O
H∆H O
- O
distance O
is O
a O
metric O
for O
evaluating O
the O
divergence O
of O
the O
data O
distribution O
between O
two O
datasets O
. O
Formally O
, O
H∆H O
- O
distance O
is O
computed O
as O
: O

dH∆H O
( O
D1 O
, O
D2 O
) O
= O
2 O
sup O
h O
1 O
, O
h O
2 O
∈H O
| O
D O
1 O
( O
h1 O
, O
h2 O
) O
− O
D O
2 O
( O
h1 O
, O
h2 O
) O
| O
( O
15 O

) O

where O
H O
is O
the O
hypothesis O
space O
and O
sup O
denotes O
the O
supremum O
. O

The O
concepts O
disagreement O
and O
H∆H O
- O
distance O
are O
introduced O
in O
Definition O
1 O
and O
Definition O
2 O
, O
respectively O
. O
Based O
on O
the O
disagreement O
and O
H∆Hdistance O
, O
the O
proof O
for O
Theorem O
2 O
is O
presented O
as O
below O
. O

Lemma O
1 O
. O
Assume O
there O
exists O
two O
dataset O
, O
i.e. O
, O

D O
1 O
, O
D O
2 O
. O
Let O
X O
1 O
= O
{ O
x O
i O
| O
( O
x O
i O
, O
y O
i O
) O
∈ O
D O
1 O
} O
and O
X O
2 O
= O
{ O
x O
i O
| O
( O
x O
i O
, O
y O
i O
) O
∈ O
D O
2 O
} O
denotes O
the O
set O
of O
input O
case O
from O
D O
1 O
and O
D O
2 O
. O
If O
X O
1 O
⊆ O
X O
2 O
, O
then O
d O
H∆H O
( O
D O
1 O
, O
D O
2 O
) O
= O
2 O
• O
|D O
2 O
| O
− O
|D O
1 O
| O
|D O
2 O
| O
holds O
. O
Proof O
. O
Let O
I O
k O
( O
h O
1 O
, O
h O
2 O
) O
= O
1 O
C O
* O
||h O
1 O
( O
x O
k O
) O
− O
h O
2 O
( O
x O
k O
) O

|| O
1 O
denote O
the O
difference O
of O
two O
hypothesis O
h O
1 O
and O
h O
2 O
on O
instance O
x O
k O
, O
then O
the O
disagreement O
of O
h O
1 O
and O
h O
2 O
on O
the O
dataset O
D O
can O
be O
rewritten O
as O
: O

D O
( O
h O
1 O
, O
h O
2 O
) O
= O
1 O
|D| O
|D| O
i=1 O
I O
i O
( O
h O
1 O
, O
h O
2 O
) O

Based O
on O
the O
Definition O
2 O
, O
the O
H∆H O
distance O
between O
D O
1 O
and O
D O
2 O
is O
as O
below O
: O

dH∆H O
( O
D1 O
, O
D2 O
) O
= O
2 O
sup O
h O
1 O
, O
h O
2 O
∈H O
| O
D O
1 O
( O
h1 O
, O
h2 O
) O
− O
D O
1 O
( O
h1 O
, O
h2 O
) O
| O
( O
16 O
) O

Expanding O
the O
item O
D O
1 O
( O
h1 O
, O
h2 O
) O
and O
D O
1 O
( O
h1 O
, O
h2 O
) O
, O
we O
can O
obtain O
: O

| O
D O
2 O
( O
h1 O
, O
h2 O
) O
− O
D O
1 O
( O
h1 O
, O
h2 O
) O
| O
= O
| O
1 O
|X2| O
x O
i O
∈X O
2 O
Ii O
( O
h1 O
, O
h2 O
) O
− O
1 O
|X1| O
x O
i O
∈X O
1 O
Ii O
( O
h1 O
, O
h2 O
) O
| O
= O
| O
|X1| O
|X2| O
* O
1 O
|X1| O
x O
i O
∈X O
1 O
Ii O
( O
h1 O
, O
h2 O
) O
+ O
|X1| O
|X2| O
* O
1 O
|X1| O
x O
k O
∈X O
1 O
Ii O
( O
h1 O
, O
h2 O
) O
− O
1 O
|X1| O
x O
i O
∈X O
1 O
Ii O
( O
h1 O
, O
h2 O
) O
| O
= O
| O
1 O
|X2| O
x O
k O
∈X O
1 O
I O
k O
( O
h1 O
, O
h2 O
) O
− O
|X2| O
− O
|X1| O
|X2| O
• O
1 O
|X1| O
x O
i O
∈X O
1 O
Ii O
( O
h1 O
, O
h2 O
) O
| O
= O
1 O
|X2| O
| O
x O
k O
∈X O
1 O
I O
k O
( O
h1 O
, O
h2 O
) O
− O
|X1| O
|X1| O
• O
x O
i O
∈X O
1 O
Ii O
( O
h1 O
, O
h2 O
) O
| O
= O
|X1| O
|X2| O
| O
D O
1 O
( O
h1 O
, O
h2 O
) O
− O
D O
1 O
( O
h1 O
, O
h2 O
) O
| O
( O
17 O

) O

whereX O
1 O
is O
the O
complement O
set O
of O
X O
1 O
in O
X O
2 O
, O
i.e O
, O
X O
1 O
= O
X O
2 O
− O
X O
1 O
. O
Correspondingly O
, O
D O
1 O
= O
{ O
( O
x O
i O
, O
y O
i O
) O
| O
( O
x O
i O
, O
y O
i O
) O
∈ O
D O
2 O
and O
x O
i O
∈X O
} O
, O
and O
thus O
|X O
1 O
| O
= O
|D O
1 O
| O
holds O
. O
As O
0 O
≤ O
D O
1 O
( O
h O
1 O
, O
h O
2 O
) O
≤ O
1 O
and O
0 O
≤ O
D O
1 O
( O
h O
1 O
, O
h O
2 O
) O
≤ O
1 O
, O

we O
conclude O
the O
inequation O
below O
: O

| O
D O
1 O
( O
h O
1 O
, O
h O
2 O
) O
− O
D O
1 O
( O
h O
1 O
, O
h O
2 O
) O
| O
≤ O
1 O
( O
18 O
) O

Since O
D O
1 O
andD O
1 O
do O
not O
overlap O
, O
D O
1 O
( O
h O
1 O
, O
h O
2 O
) O
is O
independent O
of O
D O
1 O
( O
h O
1 O
, O
h O
2 O
) O
. O
Thus O
, O
we O
can O
maximize O
the O
left O
term O
in O
inequation O
( O
18 O
) O
by O
finding O
two O
hypothesesĥ O
1 O
andĥ O
2 O
, O
which O
make O
D O
1 O
( O
ĥ O
1 O
, O
ĥ O
2 O
) O
= O
1 O
and O
D O
1 O
( O
ĥ O
1 O
, O
ĥ O
2 O
) O
= O
0 O
. O
Thus O
, O

dH∆H O
( O
D1 O
, O
D2 O
) O
= O
2 O
sup O
h O
1 O
, O
h O
2 O
∈H O
| O
D O
2 O
( O
h1 O
, O
h2 O
) O
− O
D O
1 O
( O
h1 O
, O
h2 O
) O
| O
= O
2 O
• O
|X1| O
|X2| O
sup O
h O
1 O
, O
h O
2 O
∈H O
| O
D O
1 O
( O
h1 O
, O
h2 O
) O
− O
D O
1 O
( O
h1 O
, O
h2 O
) O
| O
= O
2 O
• O
|D1| O
|D2| O
sup O
h O
1 O
, O
h O
2 O
∈H O
| O
D O
1 O
( O
h1 O
, O
h2 O
) O
− O
D O
1 O
( O
h1 O
, O
h2 O
) O
| O
= O
2 O
• O
|D1| O
|D2| O
| O
D O
1 O
( O
ĥ1 O
, O
ĥ2 O
) O
− O
D O
1 O
( O
ĥ1 O
, O
ĥ2 O
) O
| O
= O
2 O
• O
|D1| O
|D2| O
= O
2 O
• O
|D2| O
− O
|D1| O
|D2| O

The O
proof O
of O
Lemma O
1 O
is O
completed O
. O

Theorem O
2 O
. O
Assume O
there O
exists O
an O
ideal O
hypothesis O
, O
denoted O
as O
h O
* O
, O
which O
correctly O
map O
all O
instances O
in O
the O
target O
domain O
to O
their O
groud O
- O
truth O
labels O
. O
In O
the O
self O
- O
training O
iteration O
t O
, O
let O
D O
l O
T O
( O
h O
t O
) O
and O
D O
E O
( O
h O
t O
) O
be O
the O
error O
rate O
of O
the O
hypothesis O
h O
t O
on O
D O
l O
T O
and O
D O
E O
, O
respectively O
. O
Then O
, O
the O
error O
rate O
of O
the O
hypothesis O
h O
t O
on O
the O
target O
domain O
is O
upper O
bounded O
by O
: O

D O
T O
( O
h O
t O
) O
≤ O
D O
l O
T O
∪D O
E O
( O
h O
t O
) O
+ O
1 O
2 O
dH∆H O
( O
DT O
, O
D O
l O
T O
∪ O
DE O
) O
+ O
ρ O
• O
D O
E O
( O
h O
* O
, O
h O
t−1 O
) O
( O
19 O
) O

where O

ρ O
= O
|D O
E O
| O
|D O
l O

T O
|+|D O
E O
| O
is O
a O
coefficient O
related O
to O
the O
size O
of O
D O
l O
T O
and O
D O
E O
, O
D O
l O
T O
∪D O
E O
( O
h O
t O
) O
is O
the O
error O
rate O
of O
the O
hypothesis O
h O
t O
on O
the O
union O
of O
D O
l O
T O
and O
D O
E O
. O
Proof O
. O
In O
the O
meta O
- O
learning O
module O
, O
the O
final O
objective O
is O
to O
minimize O
the O
risk O
loss O
on O
the O
meta O
validation O
set O
D O
l O
T O
∪ O
D O
E O
. O
Thus O
, O
according O
to O
the O
learning O
theory O
( O
Ben O
- O
David O
et O
al O
. O
, O
2010 O
) O
, O
the O
upper O
bound O
of O
the O
error O
rate O
on O
the O
test O
set O
( O
i.e. O
, O
the O
target O
domain O
) O
is O
: O

D O
T O
( O
h O
t O
) O
≤ O
D O
l O
T O
∪D O
E O
( O
h O
t O
) O
+ O
1 O
2 O
dH∆H O
( O
DT O
, O
D O
l O
T O
∪ O
DE O
) O
+ O
D O
T O
( O
h O
* O
) O
+ O
D O
l O
T O
∪D O
E O
( O
h O
* O
) O
( O
20 O
) O

Because O
h O
* O
is O
an O
ideal O
hypothesis O
on O
the O
target O
domain O
, O
D O
T O
( O
h O
* O
) O
= O
0 O
holds O
true O
. O
Expanding O
D O
l O
T O
∪D O
E O
( O
h O
* O
) O
with O
the O
definition O
in O
Eq O
. O
( O
14 O
) O
, O

D O
l O
T O
∪D O
E O
( O
h O
* O
) O
= O
1 O
|D O
l O
T O
| O
+ O
|DE| O
( O
x O
, O
y O
) O
∈D O
l O
T O
∪D O
E O
[ O
1 O
C O
* O
||h O
* O
( O
x O
) O
− O
y||1 O
] O
= O
1 O
|D O
l O
T O
| O
+ O
|DE| O
{ O
( O
x O
, O
y O
) O
∈D O
l O
T O
[ O
1 O
C O
* O
||h O
* O
( O
x O
) O
− O
y||1 O
] O
+ O
( O
x O
, O
y O
) O
∈D O
E O
[ O
1 O
C O
* O
||h O
* O
( O
x O
) O
− O
y||1 O
] O
} O
= O
1 O
|D O
l O
T O
| O
+ O
|DE| O
{ O
|D O
l O
T O
| O
• O
D O
l O
T O
( O
h O
* O
) O
+ O
|DE| O
• O
D O
E O
( O
h O
* O
) O
} O
( O
21 O
) O

Substituting O
Eq O
. O
( O
21 O
) O
into O
Eq O
. O
( O
20 O
) O
, O
we O
have O
: O

D O
T O
( O
h O
t O
) O
≤ O
D O
l O
T O
∪D O
E O
( O
h O
t O
) O
+ O
1 O
2 O
dH∆H O
( O
DT O
, O
D O
l O
T O
∪ O
DE O
) O
+ O
D O
T O
( O
h O
* O
) O
+ O
1 O
|D O
l O
T O
| O
+ O
|DE| O
{ O
|D O
l O
T O
| O
• O
D O
l O
T O
( O
h O
* O
) O
+ O
|DE| O
• O
D O
E O
( O
h O
* O
) O
} O
( O
22 O

) O

For O
any O
instance O
( O
x O
, O
y O
) O
∈ O
D O
E O
, O
y O
is O
the O
pseudo O
label O
, O
i.e. O
, O
the O
prediction O
of O
hypothesis O
h O
t−1 O
. O
Thus O
, O
we O
have O
: O

D O
E O
( O
h O
* O
) O
= O
1 O
|DE| O
( O
x O
, O
y O
) O
∈D O
E O
[ O
1 O
C O
* O
||h O
* O
( O
x O
) O
− O
y||1 O
] O
= O
1 O
|DE| O
( O
x O
, O
y O
) O
∈D O
E O
[ O
1 O
C O
* O
||h O
* O
( O
x O
) O
− O
h O
t−1 O
( O
x O
) O
||1 O
] O
= O
D O
E O
( O
h O
* O
, O
h O
t−1 O
) O
( O
23 O
) O

Since O
D O
l O
T O
is O
a O
subset O
of O
D O
T O
, O
D O
l O
T O
( O
h O
* O
) O
= O
0 O
holds O
true O
. O

By O
eliminating O
D O
T O
( O
h O
* O
) O
and O
D O
l O
T O
( O
h O
* O
) O
in O
Eq O
. O
( O
22 O
) O
, O
and O
substituting O
D O
E O
( O
h O
* O
) O
with O
D O
E O
( O
h O
* O
, O
h O
t−1 O
) O
, O
we O
have O
: O

D O
T O
( O
h O
t O
) O
≤ O
D O
l O
T O
∪D O
E O
( O
h O
t O
) O
+ O
1 O
2 O
dH∆H O
( O
DT O
, O
D O
l O
T O
∪ O
DE O
) O
+ O
|DE| O
|D O
l O
T O
| O
+ O
|DE| O
• O
D O
E O
( O
h O
* O
, O
h O
t−1 O
) O
} O

The O
proof O
of O
Theorem O
2 O
is O
completed O
. O

Theorem O
3 O
. O
Assume O
there O
exists O
three O
datasets O
, O
D O
1 O
, O
D O
2 O
, O
D O
3 O
, O
and O
let O
X O
1 O
, O
X O
2 O
, O
X O
3 O
denotes O
the O
set O
of O
input O
cases O
in O
these O
three O
datasets O
, O
i.e. O
, O
X O
1 O
= O

{ O
x O
i O
| O
( O
x O
i O
, O
y O
i O
) O
∈ O
D O
1 O
} O
, O
X O
2 O
= O
{ O
x O
i O
| O
( O
x O
i O
, O
y O
i O
) O
∈ O
D O
2 O
} O
, O
X O
3 O
= O
{ O
x O
i O
| O
( O
x O
i O
, O
y O
i O
) O
∈ O
D O
3 O
} O
. O
If O
X O
1 O
⊆ O
X O
2 O
⊆ O
X O
3 O
, O
then O
d O
H∆H O
( O
D O
2 O
, O
D O
3 O
) O
≤ O
d O
H∆H O
( O
D O
1 O
, O
D O
3 O
) O
holds O
Proof O
. O
According O
to O
Lemma O
1 O
, O
d O
H∆H O
( O
D O
2 O
, O
D O
3 O
) O
= O
2 O
• O
|D O
3 O
| O
− O
|D O
2 O
| O
|D O
3 O
| O
d O
H∆H O
( O
D O
1 O
, O
D O
3 O
) O
= O
2 O
• O
|D O
3 O
| O
− O
|D O
1 O
| O
|D O
3 O
| O
Since O
X O
1 O
⊆ O
X O
2 O
, O
|D O
1 O
| O
≤ O
|D O
2 O
| O
holds O
. O
Thus O
, O
d O
H∆H O
( O
D O
2 O
, O
D O
3 O
) O
< O
d O
H∆H O
( O
D O
1 O
, O
D O
3 O
) O
holds O
. O

The O
proof O
of O
Theorem O
3 O
is O
completed O
. O

C O
Implementation O
Details O

The O
base O
model O
on O
the O
rumor B-TaskName
detection I-TaskName
task O
is O
BiGCN B-MethodName
( O
Bian O
et O
al O
. O
, O
2020 O
) O
, O
while O
the O
base O
model O
on O
the O
sentiment B-TaskName
classification I-TaskName
task O
is O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
. O
On O
the O
benchmark O
datasets O
, O
we O
conduct O
domain B-TaskName
adaptation I-TaskName
experiments O
on O
every O
domain O
. O
When O
one O
domain O
is O
taken O
as O
the O
target O
domain O
for O
evaluation O
, O
the O
rest O
domains O
are O
merged O
as O
the O
source O
domain O
. O
For O
example O
, O
when O
the O
" O
books O
" O
domain O
in O
the O
Amazon B-DatasetName
dataset O
is O
taken O
as O
the O
target O
domain O
, O
the O
" O
dvd O
" O
, O
" O
electronics O
" O
and O
" O
kitchen O
" O
domains O
are O
merged O
as O
the O
source O
domain O
. O

The O
unlabeled O
data O
from O
the O
target O
domain O
are O
used O
for O
training O
the O
model O
, O
and O
the O
labeled O
data O
from O
the O
target O
domain O
are O
used O
for O
testing O
and O
validating O
the O
model O
( O
with O
a O
ratio O
of O
7:3 B-HyperparameterValue
) O
. O
Notes O
that O
the O
TWITTER B-DatasetName
dataset O
does O
not O
contain O
extra O
unlabeled O
data O
, O
we O
take O
70 B-HyperparameterValue
% I-HyperparameterValue
of O
the O
labeled O
data O
on O
the O
target O
domain O
as O
the O
unlabeled O
data O
for O
training B-HyperparameterName
, O
and O
the O
rest O
will O
be O
preserved O
for O
testing O
and O
validating O
. O
The O
experiments O
on O
TWITTER B-DatasetName
are O
conducted O
on O
" O
Cha O
. O
" O
, O
" O
Fer O
. O
" O
, O
" O
Ott O
. O
" O
, O
and O
" O
Syd O
. O
" O
1 O
. O

The O
implementation O
of O
BiGCN B-MethodName
to O
realize O
the O
rumor B-TaskName
detection I-TaskName
task O
is O
provided O
in O
( O
Bian O
et O
al O
. O
, O
2020 O
) O
, O
and O
we O
follow O
the O
description O
in O
( O
Bian O
et O
al O
. O
, O
2020 O
) O
to O
train O
the O
BiGCN B-MethodName
model O
with O
the O
TWIT B-DatasetName
- I-DatasetName
TER I-DatasetName
dataset O
. O
The O
implementation O
of O
BERT B-MethodName
to O
realize O
the O
sentiment B-TaskName
analysis I-TaskName
task O
can O
be O
found O
in O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
. O
We O
download O
the O
pretrained O
BERT B-MethodName
from O
https O
: O
/ O
/ O
huggingface O
. O
co O
/ O
bert O
- O
base O
- O
uncased O
2 O
and O
fit O
the O
BERT B-MethodName
on O
the O
Amazon B-DatasetName
dataset O
with O
the O
instruction O
in O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
. O
Since O
DANN B-MethodName
, O
FixMatch B-MethodName
, O
CST B-MethodName
, O
MME B-MethodName
, O
WIND B-MethodName
, O
and O
BiAT B-MethodName
are O
model O
agnostic O
, O
we O
implement O
them O
according O
to O
the O
cited O
references O
( O
Ganin O
et O
al O
. O
, O
2016 O
; O
Sohn O
et O
al O
. O
, O
2020 O
; O
Liu O
et O
al O
. O
, O
2021 O
; O
Saito O
et O
al O
. O
, O
2019 O
; O
Chen O
et O
al O
. O
, O
2021 O
; O
Wang O
and O
Zhang O
, O
2019 O
) O
. O
For O
the O
symbols O
in O
Algorithm O
1 O
, O
we O
set O
T B-HyperparameterName
M I-HyperparameterName
as O
5 B-HyperparameterValue
, O
T B-HyperparameterName
D I-HyperparameterName
as O
5 B-HyperparameterValue
, O
T B-HyperparameterName
G I-HyperparameterName
as O
1 B-HyperparameterValue
. O
We O
set O
η B-HyperparameterName
1 I-HyperparameterName
and O
η B-HyperparameterName
2 I-HyperparameterName
in O
Algorithm O
1 O
as O
5e B-HyperparameterValue
− I-HyperparameterValue
4 I-HyperparameterValue
and O
5e B-HyperparameterValue
− I-HyperparameterValue
3 I-HyperparameterValue
for O
the O
BiGCN B-MethodName
model O
, O
and O
as O
5e B-HyperparameterValue
− I-HyperparameterValue
6 I-HyperparameterValue
and O
2e B-HyperparameterValue
− I-HyperparameterValue
5 I-HyperparameterValue
for O
the O
BERT B-MethodName
model O
. O
We O
set O
η B-HyperparameterName
in O
Eq O
. O
( O
3 O
) O
as O
5e B-HyperparameterValue
− I-HyperparameterValue
5 I-HyperparameterValue
for O
the O
BERT B-MethodName
model O
, O
and O
5e B-HyperparameterValue
− I-HyperparameterValue
3 I-HyperparameterValue
for O
the O
BiGCN B-MethodName
model O
. O
We O
set O
γ B-HyperparameterName
in O
Eq O
. O
( O
6 O
) O
as O
0.1 B-HyperparameterValue
for O
both O
the O
BERT B-MethodName
and O
the O
BiGCN B-MethodName
model O
. O
We O
conduct O
all O
experiments O
the O
GeForce O
RTX O
3090 O
GPU O
with O
24 O
GB O
memory O
. O

E O
Extra O
Experiments O
E.1 O
Instance O
Reweighting O

To O
investigate O
the O
effectiveness O
of O
the O
metalearning O
module O
, O
we O
conduct O
an O
experiment O
to O
visualize O
the O
optimized O
instance O
weights O
on O
different O
pseudo O
instances O
. O
In O
detail O
, O
the O
experiments O
are O
conducted O
on O
the O
' O
Cha O
. O
' O
domain O
of O
the O
TWITTER B-DatasetName
3 O
https O
: O
/ O
/ O
figshare.com O
/ O
ndownloader O
/ O
articles O
/ O
6392078 O
/ O
dataset O
. O
Since O
the O
unlabeled O
data O
in O
the O
TWITTER B-DatasetName
dataset O
is O
constructed O
with O
the O
labeled O
data O
in O
the O
target O
domain O
( O
illustrated O
in O
§ O
5 O
) O
, O
we O
are O
aware O
of O
the O
pseudo O
labels O
' O
correctness O
. O
Thus O
, O
we O
can O
visualize O
the O
relevance O
among O
the O
instance O
weights O
, O
pseudo O
labels O
' O
correctness O
, O
and O
pseudo O
labels O
' O
confidence O
, O
the O
experiment O
results O
are O
shown O
in O
Fig O
. O
3 O
. O
Fig O
. O
3 O
is O
a O
violin O
plot O
in O
a O
horizontal O
direction O
, O
where O
each O
curve O
represents O
a O
distribution O
of O
the O
instance O
weights O
. O
The O
height O
of O
the O
curve O
represents O
the O
probability O
density O
. O
In O
each O
confidence O
interval O
, O
the O
yellow O
curve O
is O
the O
distribution O
over O
the O
correct O
pseudo O
instances O
while O
the O
blue O
curve O
is O
the O
distribution O
over O
the O
wrong O
pseudo O
instances O
. O
It O
should O
be O
noted O
that O
the O
probability O
density O
is O
normalized O
in O
each O
confidence O
interval O
. O
Thus O
, O
the O
area O
of O
the O
two O
kinds O
curves O
is O
equal O
to O
1.0 O
in O
each O
confidence O
interval O
. O
From O
Fig O
. O
3 O
, O
we O
can O
obtain O
the O
following O
observations O
. O
Firstly O
, O
the O
meta O
- O
learning O
module O
is O
effective O
in O
reducing O
label O
noise O
. O
In O
different O
confidence O
intervals O
, O
especially O
in O
[ O
0.5 O
- O
0.6 O
] O
and O
[ O
0.6 O
- O
0.7 O
] O
, O
the O
peak O
of O
the O
blue O
curve O
is O
smaller O
than O
0.2 O
, O
meaning O
that O
the O
wrong O
pseudo O
instances O
are O
mainly O
allocated O
low O
instance O
weights O
. O
Thus O
, O
the O
adverse O
impact O
from O
the O
wrong O
pseudo O
instances O
is O
reduced O
. O

Secondly O
, O
larger O
instance O
weights O
are O
allocated O
to O
the O
correct O
pseudo O
instances O
with O
low O
confidence O
. O
In O
specific O
, O
large O
instance O
weights O
( O
i.e. O
, O
> O
0.5 O
) O
mainly O
appears O
in O
the O
bottom O
two O
sub O
- O
graph O
, O
so O
the O
large O
instance O
weights O
are O
mainly O
allocated O
to O
the O
correct O
pseudo O
instances O
whose O
confidence O
is O
lower O
than O
0.7 O
. O
Thus O
, O
the O
meta O
- O
learning O
module O
is O
also O
effective O
in O
mining O
hard O
pseudo O
examples O
. O

E.2 O
Error O
rates O
on O
the O
expansion O
examples O

According O
to O
Theorem O
2 O
in O
§ O
4 O
, O
the O
performance O
of O
the O
DaMSTF B-MethodName
is O
limited O
by O
the O
error O
rate O
of O
the O
expansion O
examples O
, O
i.e. O
, O
D O
E O
( O
h O
* O
, O
h O
t−1 O
) O
. O
By O
selecting O
the O
examples O
with O
the O
lowest O
prediction O
entropy O
as O
the O
expansion O
example O
, O
the O
meta O
constructor O
can O
reduce O
D O
E O
( O
h O
* O
, O
h O
t−1 O
) O
, O
thereby O
can O
improve O
the O
performance O
of O
the O
DaMSTF B-MethodName
. O
In O
this O
subsection O
, O
we O
examine O
the O
reliability O
of O
the O
meta O
constructor O
, O
i.e. O
, O
visualizing O
the O
relationship O
between O
the O
prediction O
entropy O
and O
the O
prediction O
correctness O
. O
Specifically O
, O
we O
first O
compute O
and O
sort O
the O
prediction O
entropy O
on O
the O
" O
Syd O
. O
" O
domain O
. O
We O
then O
select O
the O
top O
5 O
% O
, O
10 O
% O
, O
20 O
% O
, O
30 O
% O
, O
40 O
% O
, O
50 O
% O
, O
60 O
% O
, O
70 O
% O
, O
80 O
% O
, O
90 O
% O
, O
100 O
% O
of O
the O
pseudo O
instances O
to O
compute O
the O
error O
rate O
between O
the O
selected O
predictions O
and O
their O
ground O
- O
truth O
labels O
. O
We O
summarize O
the O
experiment O
results O
in O
Fig O
. O
4 O
. O

E.3 O
Risk O
loss O
on O
the O
expansion O
examples O

As O
discussed O
in O
§ O
4.1 O
, O
expanding O
the O
meta O
validation O
set O
is O
challenged O
by O
the O
training O
guidance O
vanishment O
problem O
, O
since O
the O
model O
's O
risk O
loss O
, O
as O
well O
as O
the O
model O
's O
gradient O
, O
on O
the O
expansion O
examples O
is O
negligible O
. O
As O
a O
complementary O
, O
we O
design O
a O
domain B-MethodName
adversarial I-MethodName
learning I-MethodName
module O
to O
perturb O
the O
model O
's O
parameters O
, O
thereby O
increasing O
the O
model O
's O
gradients O
on O
the O
expansion O
examples O
. O
Here O
, O
we O
provide O
an O
intuitive O
explanation O
for O
the O
necessity O
of O
introducing O
domain O
adversarial O
learning O
. O
Specifically O
, O
we O
exhibit O
the O
relationship O
between O
the O
predictive O
entropy O
and O
the O
risk O
loss O
, O
and O
present O
the O
changes O
of O
the O
risk O
loss O
before O
and O
after O
the O
parameters O
perturbation O
. O
The O
experimental O
settings O
are O
the O
same O
as O
§ O
E.2 O
, O
and O
we O
summarize O
the O
results O
in O
Fig O
. O
5 O
. O
From O
Fig O
. O
5 O
, O
we O
observe O
that O
the O
mean O
risk B-MetricName
loss I-MetricName
decreases O
along O
with O
the O
decrease O
of O
the O
selection O
rate O
, O
and O
the O
risk O
loss O
on O
the O
examples O
with O
small O
predictive O
entropy O
is O
negligible O
. O
On O
the O
examples O
with O
the O
lowest O
10 O
% O
predictive O
entropy O
( O
i.e. O
, O
expansion O
examples O
in O
our O
setting O
) O
, O
the O
mean O
risk B-MetricName
loss I-MetricName
is O
only O
0.015 B-MetricValue
. O
Considering O
that O
the O
gradient O
is O
back O
- O
propagated O
from O
the O
risk O
loss O
, O
these O
expansion O
examples O
can O
not O
produce O
acceptable O
gradients O
. O
Accordingly O
, O
these O
expansion O
examples O
can O
not O
provide O
indicative O
training O
guidance O
. O
After O
perturbing O
the O
model O
parameters O
with O
the O
domain B-MethodName
adversarial I-MethodName
learning I-MethodName
module O
, O
the O
risk B-MetricName
loss I-MetricName
on O
the O
expansion O
examples O
( O
Selection O
Ratio=0.1 O
) O
sharply O
increases O
from O
0.015 B-MetricValue
to O
0.288 B-MetricValue
. O
Thus O
, O
the O
domain O
adversarial O
learning O
module O
is O
an O
indispensable O
complement O
to O
the O
meta O
constructor O
. O

F O
Limitation O

Although O
our O
approach O
produces O
promising O
results O
on O
two O
datasets O
, O
there O
are O
certain O
limitations O
. O
In O
the O
future O
, O
we O
will O
continue O
to O
dig O
into O
these O
concerns O
. O

Firstly O
, O
we O
evaluate O
the O
DaMSTF B-MethodName
on O
two O
classification B-TaskName
tasks O
. O
We O
do O
not O
conduct O
experiments O
on O
other O
NLP O
tasks O
, O
such O
as O
machine O
translation O
( O
Yang O
et O
al O
. O
, O
2018 O
) O
or O
named O
entity O
recognition O
( O
Jia O
et O
al O
. O
, O
2019 O
) O
. O
Nonetheless O
, O
as O
text B-TaskName
classification I-TaskName
is O
a O
fundamental O
task O
, O
other O
NLP O
applications O
can O
be O
specified O
as O
a O
case O
of O
classification B-TaskName
. O
For O
example O
, O
named O
entity O
recognition O
can O
be O
formulated O
as O
a O
wordword O
relation O
classification O
task O
. O

Secondly O
, O
the O
meta O
- O
learning O
module O
carries O
out O
extra O
computation O
overhead O
. O
As O
the O
bi O
- O
level O
hyperparameters O
optimization O
involves O
a O
second O
- O
order O
derivate O
on O
the O
model O
's O
parameters O
, O
their O
computation O
overhead O
is O
quadratic O
to O
the O
model O
's O
parameters O
. O
In O
DaMSTF B-MethodName
, O
we O
use O
the O
approximation O
techniques O
in O
WIND B-MethodName
to O
compute O
the O
derivate O
, O
which O
is O
linear O
to O
the O
model O
's O
parameters O
. O
In O
the O
future O
, O
we O

Acknowledgements O

WhitenedCSE B-MethodName
: O
Whitening B-MethodName
- I-MethodName
based I-MethodName
Contrastive I-MethodName
Learning I-MethodName
of I-MethodName
Sentence I-MethodName
Embeddings I-MethodName

This O
paper O
presents O
a O
whitening B-MethodName
- I-MethodName
based I-MethodName
contrastive I-MethodName
learning I-MethodName
method I-MethodName
for I-MethodName
sentence I-MethodName
embedding I-MethodName
learning I-MethodName
( O
WhitenedCSE B-MethodName
) O
, O
which O
combines O
contrastive O
learning O
with O
a O
novel O
shuffled B-MethodName
group I-MethodName
whitening I-MethodName
. O
Generally O
, O
contrastive O
learning O
pulls O
distortions O
of O
a O
single O
sample O
( O
i.e. O
, O
positive O
samples O
) O
close O
and O
push O
negative O
samples O
far O
away O
, O
correspondingly O
facilitating O
the O
alignment O
and O
uniformity O
in O
the O
feature O
space O
. O
A O
popular O
alternative O
to O
the O
" O
pushing O
" O
operation O
is O
whitening O
the O
feature O
space O
, O
which O
scatters O
all O
the O
samples O
for O
uniformity B-MetricName
. O
Since O
the O
whitening O
and O
the O
contrastive O
learning O
have O
large O
redundancy O
w.r.t O
. O
the O
uniformity B-MetricName
, O
they O
are O
usually O
used O
separately O
and O
do O
not O
easily O
work O
together O
. O
For O
the O
first O
time O
, O
this O
paper O
integrates O
whitening O
into O
the O
contrastive O
learning O
scheme O
and O
facilitates O
two O
benefits O
. O
1 O
) O
Better O
uniformity B-MetricName
. O
We O
find O
that O
these O
two O
approaches O
are O
not O
totally O
redundant O
but O
actually O
have O
some O
complementarity O
due O
to O
different O
uniformity B-MetricName
mechanism O
. O
2 O
) O
Better O
alignment B-MetricName
. O
We O
randomly O
divide O
the O
feature O
into O
multiple O
groups O
along O
the O
channel O
axis O
and O
perform O
whitening O
independently O
within O
each O
group O
. O
By O
shuffling O
the O
group O
division O
, O
we O
derive O
multiple O
distortions O
of O
a O
single O
sample O
and O
thus O
increase O
the O
positive O
sample O
diversity O
. O
Consequently O
, O
using O
multiple O
positive O
samples O
with O
enhanced O
diversity O
further O
improves O
contrastive O
learning O
due O
to O
better O
alignment B-MetricName
. O
Extensive O
experiments O
on O
seven O
semantic O
textual O
similarity O
tasks O
show O
our O
method O
achieves O
consistent O
improvement O
over O
the O
contrastive O
learning O
baseline O
and O
sets O
new O
states O
of O
the O
art O
, O
e.g. O
, O
78.78 B-MetricValue
% I-MetricValue
( O
+2.53 B-MetricValue
% I-MetricValue
based O
on O
BERT B-MethodName
base I-MethodName
) O
Spearman B-MetricName
correlation I-MetricName
on O
STS B-TaskName
tasks O
. O
1 O

Introduction O

This O
paper O
considers O
self B-TaskName
- I-TaskName
supervised I-TaskName
sentence I-TaskName
representation I-TaskName
( I-TaskName
embedding I-TaskName
) I-TaskName
learning I-TaskName
. O
It O
is O
a O
fundamental O
task O
in O
language O
processing O
( O
NLP O
) O
and O
can O
†Corresponding O
author O
. O

1 O
Our O
code O
will O
be O
available O
at O
https O
: O
/ O
/ O
github.com O
/ O
SupstarZh O
/ O
WhitenedCSE B-MethodName
. O
Meanwhile O
, O
in O
( O
d O
) O
, O
the O
positive O
samples O
after O
SGW B-MethodName
( O
red O
) O
obtain O
higher O
diversity O
than O
the O
original O
bert O
features O
( O
green O
) O
. O
Using O
these O
diverse O
positive O
samples O
for O
contrastive O
learning O
, O
the O
proposed O
WhitenedCSE B-MethodName
achieves O
better O
alignment B-MetricName
. O

benefit O
a O
wide O
range O
of O
downstream O
tasks O
( O
Qiao O
et O
al O
. O
, O
2016 O
; O
Le O
and O
Mikolov O
, O
2014 O
; O
Lan O
et O
al O
. O
, O
2019 O
; O
Logeswaran O
and O
Lee O
, O
2018 O
) O
. O
Two O
characteristics O
matter O
for O
sentence B-TaskName
embeddings I-TaskName
, O
i.e. O
, O
uniformity B-MetricName
( O
of O
the O
overall O
feature O
distribution O
) O
and O
alignment B-MetricName
( O
of O
the O
positive O
samples O
) O
, O
according O
to O
a O
common O
sense O
in O
deep O
representation O
learning O
( O
Wang O
and O
Isola O
, O
2020 O
) O
. O
Alignment B-MetricName
expects O
minimal O
distance O
between O
positive O
pairs O
, O
while O
uniformity B-MetricName
expects O
the O
features O
are O
uniformly O
distributed O
in O
the O
representation O
space O
in O
overall O
. O
From O
this O
viewpoint O
, O
the O
popular O
masked O
language O
modeling O
( O
MLM O
) O
( O
Devlin O
et O
al O
. O
, O
2018 O
; O
Liu O
et O
al O
. O
, O
2019 O
; O
Brown O
et O
al O
. O
, O
2020b O
; O
Reimers O
and O
Gurevych O
, O
2019 O
) O
is O
not O
an O
optimal O
choice O
for O
sentence B-TaskName
embedding I-TaskName
: O
MLM O
methods O
do O
not O
explicitly O
enforce O
the O
objective O
of O
uniformity B-MetricName
and O
alignment B-MetricName
and O
thus O
do O
not O
quite O
fit O
the O
objective O
of O
sentence B-TaskName
representation I-TaskName
learning I-TaskName
. O

To O
improve O
the O
uniformity B-MetricName
as O
well O
as O
the O
alignment B-MetricName
, O
there O
are O
two O
popular O
approaches O
, O
i.e. O
, O
contrastive O
learning O
and O
post O
- O
processing O
. O
1 O
) O
The O
contrastive O
learning O
methods O
( O
Yan O
et O
al O
. O
, O
2021 O
; O
Kim O
et O
al O
. O
, O
2021 O
; O
pulls O
similar O
sentences O
close O
to O
each O
other O
and O
pushes O
dissimilar O
sentences O
far O
- O
away O
in O
the O
latent O
feature O
space O
. O
Pulling O
similar O
sentences O
close O
directly O
enforces O
alignment B-MetricName
, O
while O
pushing O
dissimilar O
sentences O
apart O
implicitly O
enforces O
uniformity B-MetricName
( O
Wang O
and O
Isola O
, O
2020 O
) O
. O
2 O
) O
In O
contrast O
, O
the O
postprocessing O
methods O
mainly O
focus O
on O
improving O
the O
uniformity B-MetricName
. O
They O
use O
normalizing O
flows O
( O
Li O
et O
al O
. O
, O
2020 O
) O
or O
whitening O
operation O
( O
Su O
et O
al O
. O
, O
2021 O
) O
) O
to O
project O
the O
already O
- O
learned O
representations O
into O
an O
isotropic O
space O
. O
In O
other O
words O
, O
these O
methods O
scatter O
all O
the O
samples O
into O
the O
feature O
space O
and O
thus O
improve O
the O
uniformity B-MetricName
. O

In O
this O
paper O
, O
we O
propose O
a O
whitening B-MethodName
- I-MethodName
based I-MethodName
contrastive I-MethodName
learning I-MethodName
method I-MethodName
for I-MethodName
sentence I-MethodName
representation I-MethodName
learning I-MethodName
( O
WhitenedCSE B-MethodName
) O
. O
For O
the O
first O
time O
, O
we O
integrate O
whitening O
into O
the O
contrastive O
learning O
scheme O
and O
demonstrate O
substantial O
improvement O
. O
Specifically O
, O
WhitenedCSE B-MethodName
combines O
contrastive O
learning O
with O
a O
novel O
Shuffled B-MethodName
Group I-MethodName
Whitening I-MethodName
( O
SGW B-MethodName
) O
. O
Given O
a O
backbone O
feature O
, O
SGW B-MethodName
randomly O
divides O
the O
feature O
into O
multiple O
groups O
along O
the O
channel O
axis O
and O
perform O
whitening O
independently O
within O
each O
group O
. O
The O
whitened O
features O
are O
then O
fed O
into O
the O
contrastive O
loss O
for O
optimization O
. O

Although O
the O
canonical O
whitening O
( O
or O
group O
whitening O
) O
is O
only O
beneficial O
for O
uniformity B-MetricName
, O
SGW B-MethodName
in O
WhitenedCSE B-MethodName
improves O
not O
only O
the O
uniformity B-MetricName
but O
also O
the O
alignment B-MetricName
. O
We O
explain O
these O
two O
benefits O
in O
details O
as O
below O
: O

• O
Better O
uniformity B-MetricName
. O
We O
notice O
that O
the O
pushing O
effect O
in O
contrastive O
learning O
and O
the O
scattering O
effect O
in O
the O
whitening O
have O
large O
redundancy O
to O
each O
other O
, O
because O
they O
both O
facilitate O
the O
uniformity B-MetricName
. O
This O
redundancy O
is O
arguably O
the O
reason O
why O
no O
prior O
literature O
tries O
to O
combine O
them O
. O
Under O
this O
background O
, O
our O
finding O
i.e. O
, O
these O
two O
approaches O
are O
not O
totally O
redundant O
but O
actually O
have O
some O
complementarity O
is O
non O
- O
trivial O
. O
We O
think O
such O
complemenetarity O
is O
because O
these O
two O
approaches O
have O
different O
uniformity B-MetricName
mechanism O
and O
will O
discuss O
the O
differences O
in O
Section O
3.2.3 O
. O
In O
Fig O
. O
1 O
, O
we O
observe O
that O
while O
the O
contrastive O
learning O
( O
Fig O
. O
1 O
( O
b O
) O
) O
already O
improves O
the O
uniformity B-MetricName
over O
the O
original O
bert O
features O
( O
Fig O
. O
1 O
( O
a O
) O
) O
, O
applying O
whitening O
( O
Fig O
. O
1 O
( O
c O
) O
) O
brings O
another O
round O
of O
uniformity B-MetricName
improvement O
. O

• O
Better O
alignment B-MetricName
. O
In O
the O
proposed O
Whitened B-MethodName
- I-MethodName
CSE I-MethodName
, O
SGW B-MethodName
is O
featured O
for O
its O
shuffled O
grouping O
operation O
, O
i.e. O
, O
randomly O
dividing O
a O
backbone O
feature O
into O
multiple O
groups O
before O
whitening O
. O
Therefore O
, O
given O
a O
same O
backbone O
feature O
, O
we O
may O
repeat O
SGW B-MethodName
multiple O
times O
to O
get O
different O
grouping O
results O
, O
and O
then O
different O
whitened O
features O
. O
These O
" O
duplicated O
" O
features O
are O
different O
from O
each O
other O
and O
thus O
increase O
the O
diversity O
of O
positive O
samples O
, O
as O
shown O
in O
Fig O
. O
1 O
( O
d O
) O
. O
Using O
these O
diverse O
positive O
samples O
for O
contrastive O
learning O
, O
WhitenedCSE B-MethodName
improves O
the O
alignment B-MetricName
. O

Another O
important O
advantage O
of O
SGW B-MethodName
is O
: O
since O
it O
is O
applied O
onto O
the O
backbone O
features O
, O
it O
incurs O
very O
slight O
computational O
overhead O
for O
generating O
additional O
positive O
samples O
. O
This O
high O
efficiency O
allows O
WhitenedCSE B-MethodName
to O
increase O
the O
number O
of O
positive O
samples O
( O
more O
than O
common O
setting O
of O
2 O
) O
in O
a O
mini O
- O
batch O
with O
little O
cost O
. O
Ablation O
study O
shows O
that O
the O
enlarged O
positive O
- O
sample O
number O
brings O
a O
further O
benefit O
. O

Our O
contributions O
are O
summarized O
as O
follows O
: O

( O
1 O
) O
We O
propose O
WhitenedCSE B-MethodName
for O
the O
selfsupervised B-TaskName
sentence I-TaskName
representation I-TaskName
learning I-TaskName
task O
. O
WhitenedCSE B-MethodName
combines O
the O
contrastive O
learning O
with O
a O
novel O
Shuffled B-MethodName
Group I-MethodName
Whitening I-MethodName
( O
SGW B-MethodName
) O
. O

( O
2 O
) O
We O
show O
that O
through O
SGW B-MethodName
, O
WhitenedCSE B-MethodName
improves O
not O
only O
the O
uniformity B-MetricName
but O
also O
the O
alignment B-MetricName
. O
Moreover O
, O
SGW B-MethodName
enables O
efficient O
multipositive O
training O
, O
which O
is O
also O
beneficial O
. O

( O
3 O
) O
We O
evaluate O
our O
method O
on O
seven O
semantic B-TaskName
textual I-TaskName
similarity I-TaskName
tasks O
and O
seven O
transfer B-TaskName
tasks O
. O
Experimental O
results O
show O
that O
WhitenedCSE B-MethodName
brings O
consistent O
improvement O
over O
the O
contrastive O
learning O
baseline O
and O
sets O
new O
states O
of O
the O
art O
. O

Related O
Work O

Sentence B-TaskName
Representation I-TaskName
Learning I-TaskName

As O
a O
fundamental O
task O
in O
natural O
language O
processing O
, O
sentence B-TaskName
representation I-TaskName
learning I-TaskName
has O
been O
extensively O
studied O
. O
Early O
works O
mainly O
based O
on O
bag O
- O
of O
- O
words O
( O
Wu O
et O
al O
. O
, O
2010 O
; O
Tsai O
, O
2012 O
) O
or O
context O
prediction O
tasks O
( O
Kiros O
et O
al O
. O
, O
2015 O
; O
Hill O
et O
al O
. O
, O
2016 O
) O
, O
etc O
. O
Recently O
, O
with O
the O
advent O
of O
pretrained O
language O
model O
( O
Devlin O
et O
al O
. O
, O
2018 O
; O
Liu O
et O
al O
. O
, O
2019 O
; O
Brown O
et O
al O
. O
, O
2020a O
) O
, O
many O
works O
tend O
to O
directly O
use O
PLMs O
, O
such O
as O
BERT O
( O
Devlin O
et O
al O
. O
, O
2018 O
) O
, O
to O
generate O
sentence O
representations O
. O
However O
, O
some O
studies O
( O
Ethayarajh O
, O
2019 O
; O
Yan O
et O
al O
. O
, O
2021 O
) O
found O
that O
directly O
use O
the O
[ O
CLS O
] O
representation O
or O
the O
average O
pooling O
of O
token O
embeddings O
at O
the O
last O
layer O
will O
suffer O
from O
anisotropy O
problem O
, O
i.e. O
, O
the O
learned O
embeddings O
are O
collasped O
into O
a O
small O
area O
. O
To O
alleviate O
this O
problem O
, O
BERTflow O
( O
Li O
et O
al O
. O
, O
2020 O
) O
adopts O
a O
standardized O
flow O
transformation O
while O
BERT O
- O
Whitening O
( O
Su O
et O
al O
. O
, O
2021 O
) O
adopts O
a O
whitening O
transformation O
, O
both O
of O
them O
transform O
the O
representation O
space O
to O
a O
smooth O
and O
isotropic O
space O
. O
Most O
recently O
, O
contrastive O
learning O
( O
Chen O
et O
al O
. O
, O
2020 O
; O
has O
become O
a O
powerful O
tool O
to O
obtain O
the O
sentence O
representations O
. O

Contrastive O
Learning O

Contrastive O
learning O
( O
Chen O
et O
al O
. O
, O
2020 O
; O
has O
achieved O
great O
success O
in O
sentence O
representation O
learning O
tasks O
Yan O
et O
al O
. O
, O
2021 O
; O
Kim O
et O
al O
. O
, O
2021 O
; O
. O
It O
pulls O
semantically O
similar O
samples O
together O
, O
and O
pushes O
the O
dissimilar O
samples O
away O
, O
which O
can O
be O
formulated O
as O
: O

L O
cl O
= O
−log O
e O
sim O
( O
h O
i O
, O
h O
* O
i O
) O
/ O
τ O
n O
j=1 O
e O
sim O
( O
h O
i O
, O
h O
* O
j O
) O
/ O
τ O
( O
1 O
) O

where O
τ O
is O
a O
temperature O
hyperparameter O
, O
h O
* O
i O
, O
h O
* O
j O
are O
the O
positive O
sample O
and O
negative O
samples O
respectively O
. O
Recently O
, O
alignment O
and O
uniformity O
( O
Wang O
and O
Isola O
, O
2020 O
) O
are O
proposed O
to O
measure O
the O
quality O
of O
representations O
. O
Alignment O
measures O
whether O
the O
distance O
between O
positive O
samples O
is O
close O
, O
while O
uniformity O
measures O
the O
dispersion O
of O
embedding O
in O
vector O
space O
. O
A O
typical O
method O
called O
SimCSE O
uses O
dropout O
as O
a O
feature O
- O
wise O
data O
augmentation O
to O
construct O
the O
positive O
sample O
, O
and O
randomly O
sample O
negatives O
from O
the O
batch O
, O
which O
can O
achieve O
a O
great O
balance O
between O
alignment O
and O
uniformity O
. O
Some O
new O
works O
further O
improved O
the O
quality O
of O
sentence O
representations O
based O
on O
SimCSE O
, O
such O
as O
ESimCSE O
, O
MixCSE O
( O
Zhang O
et O
al O
. O
, O
2022a O
) O
and O
VaSCL O
( O
Zhang O
et O
al O
. O
, O
2021b O
) O
, O
each O
of O
them O
proposed O
a O
new O
data O
augmentation O
strategy O
to O
construct O
the O
positive O
pair O
. O
Besides O
, O
DCLR O
( O
Zhou O
et O
al O
. O
, O
2022 O
) O
focus O
on O
optimizing O
the O
strategy O
of O
sampling O
negatives O
, O
and O
ArcCSE O
( O
Zhang O
et O
al O
. O
, O
2022b O
) O
optimized O
the O
objective O
function O
, O
etc O
. O
In O
this O
paper O
, O
we O
find O
that O
contrastive O
learning O
can O
be O
further O
combined O
with O
whitening O
to O
obtain O
better O
sentence O
representations O
. O

Whitening O
Transformation O

In O
computer O
vision O
, O
recent O
works O
( O
Ermolov O
et O
al O
. O
, O
2021 O
; O
Zhang O
et O
al O
. O
, O
2021c O
; O
Hua O
et O
al O
. O
, O
2021 O
) O
use O
whitening O
transformation O
as O
an O
alternative O
method O
to O
the O
" O
pushing O
negatives O
away O
" O
operation O
in O
contrastive O
learning O
to O
disperse O
the O
data O
uniformly O
throughout O
the O
spherical O
space O
( O
i.e. O
, O
the O
feature O
space O
) O
, O
and O
then O
pull O
the O
positive O
samples O
together O
, O
which O
have O
achieved O
great O
success O
in O
unsupervised O
representation O
learning O
. O

Whitening O
( O
aka O
. O
, O
sphering O
) O
is O
a O
common O
transformation O
that O
transforms O
a O
set O
of O
variables O
into O
a O
new O
set O
of O
isotropic O
variables O
, O
and O
makes O
the O
covariance O
matrix O
of O
whitened O
variables O
equal O
to O
the O
identity O
matrix O
. O
In O
natural O
language O
processing O
, O
Su O
et O
al O
. O
( O
2021 O
) O
use O
whitening O
as O
a O
post O
- O
processing O
method O
to O
alleviate O
the O
anisotropic O
problem O
in O
pretrained O
language O
models O
. O
In O
this O
paper O
, O
we O
use O
whitening O
as O
an O
explicit O
operation O
to O
further O
improve O
the O
uniformity O
of O
the O
representation O
space O
, O
and O
further O
explore O
the O
potential O
of O
whitening O
in O
improving O
alignment O
, O
so O
as O
to O
obtain O
a O
better O
sentence O
representation O
model O
. O

Methods O

In O
this O
section O
, O
we O
first O
describe O
the O
overall O
architecture O
of O
WhitenedCSE B-MethodName
and O
then O
present O
the O
details O
of O
all O
the O
modules O
, O
including O
the O
shuffled B-MethodName
group I-MethodName
whitening I-MethodName
module O
and O
the O
new O
contrastive O
learning O
module O
. O

General O
Framework O

As O
shown O
in O
Fig O
. O
2 O
, O
WhitenedCSE B-MethodName
has O
three O
major O
components O
: O

• O
An O
BERT O
- O
like O
encoder O
, O
which O
we O
use O
to O
extract O
features O
from O
native O
sentences O
, O
and O
take O
the O
[ O
CLS O
] O
token O
as O
our O
native O
sentence O
representations O
. O

• O
Shuffled O
- O
group O
- O
whitening O
module O
, O
we O
use O
it O
as O
a O
complementary O
module O
to O
contrastive O
learning O
to O
further O
improve O
the O
uniformity O
and O
alignment O
of O
the O
representation O
space O
. O

• O
Multi O
- O
positives O
contrastive O
module O
, O
in O
this O
module O
, O
we O
pull O
distortions O
of O
the O
representations O
close O
and O
push O
the O
negative O
samples O
away O
in O
the O
latent O
feature O
space O
. O

Specifically O
, O
given O
a O
batch O
of O
sentences O
X O
, O
WhitenedCSE B-MethodName
use O
the O
feature O
encoder O
f O
θ O
( O
x O
i O
, O
γ O
) O
to O
map O
them O
to O
a O
higher O
dimensional O
space O
, O
where O
γ O
is O
a O
random O
mask O
for O
dropout O
, O
then O
we O
take O
the O
[ O
CLS O
] O
output O
as O
the O
native O
sentence O
representations O
. O
After O
this O
, O
we O
feed O
the O
na- O
In O
the O
middle O
column O
, O
WhitenedCSE B-MethodName
consists O
of O
three O
components O
, O
i.e. O
, O
1 O
) O
An O
BERTlike O
encoder O
for O
generating O
the O
backbone O
features O
from O
input O
samples O
, O
2 O
) O
A O
Shuffled B-MethodName
Group I-MethodName
Whitenning I-MethodName
( O
SGW B-MethodName
) O
module O
for O
scattering O
the O
backbone O
features O
and O
augmenting O
the O
positive O
feature O
diversity O
, O
and O
3 O
) O
A O
multi O
- O
positive O
contrastive O
loss O
for O
optimizing O
the O
features O
. O
In O
the O
left O
column O
, O
when O
the O
mini O
- O
batch O
flows O
through O
these O
three O
components O
sequentially O
, O
the O
feature O
space O
undergoes O
" O
anisotropy O
" O
→ O
" O
good O
uniformity O
+ O
augmented O
positives O
" O
→ O
" O
pulling O
close O
the O
positives O
" O
. O
The O
right O
column O
illustrates O
the O
SGW B-MethodName
module O
in O
details O
. O
Specifically O
, O
SGW B-MethodName
randomly O
shuffles O
the O
backbone O
feature O
along O
the O
axis O
and O
then O
divides O
the O
feature O
into O
multiple O
groups O
. O
Afterwards O
, O
SGW B-MethodName
whitens O
each O
group O
independently O
and O
re O
- O
shuffles O
the O
whitened O
feature O
. O
Given O
a O
single O
backbone O
feature O
, O
we O
repeat O
the O
SGW B-MethodName
process O
several O
times O
so O
as O
to O
generate O
multiple O
positive O
features O
. O

tive O
sentence O
representations O
to O
the O
shuffled B-MethodName
- I-MethodName
groupwhitening I-MethodName
( O
SGW B-MethodName
) O
module O
, O
in O
this O
module O
we O
randomly O
dividing O
each O
sentence O
representation O
into O
multiple O
groups O
along O
the O
axis O
, O
then O
we O
operate O
group O
whitening O
on O
each O
group O
. O
We O
repeat O
SGW B-MethodName
multiple O
times O
to O
get O
different O
grouping O
results O
, O
and O
then O
different O
whitened O
representations O
. O
These O
" O
duplicated O
" O
features O
are O
different O
from O
each O
other O
. O
Finally O
, O
we O
use O
multi O
- O
positives O
contrastive O
loss O
function O
to O
pull O
one O
representation O
and O
all O
its O
corresponding O
augmentations O
together O
, O
and O
push O
it O
away O
from O
others O
. O
We O
will O
discuss O
feasible O
loss O
function O
in O
Section O
3.3 O
, O
and O
present O
our O
final O
form O
of O
loss O
function O
. O

From O
Whitening O
to O
SGW B-MethodName

Preliminaries O
for O
Whitening O

Given O
a O
batch O
of O
normalized O
sentence O
representations O
Z O
∈ O
R O
N O
×d O
, O
the O
whitening O
transformation O
can O
be O
formulated O
as O
: O

H O
= O
Z O
T O
W O
( O
2 O
) O

where O
H O
∈ O
R O
d×N O
is O
the O
whitened O
embeddings O
and O
W O
∈ O
R O
d×d O
is O
the O
whitening O
matrix O
. O
We O
denote O
the O
covariance O
matrix O
of O
ZZ O
T O
as O
Σ. O
the O
goal O
of O
whitening O
is O
to O
make O
the O
covariance O
matrix O
of O
HH O
T O
equal O
to O
the O
identity O
matrix O
I O
, O
i.e. O
, O
WΣW O
T O
= O
I. O
There O
are O
many O
different O
whitening O
methods O
, O
such O
as O
PCA O
( O
Jégou O
and O
Chum O
, O
2012 O
) O
, O
ZCA B-MethodName
( O
Bell O
and O
Sejnowski O
, O
1997 O
) O
, O
etc O
. O
Group B-MethodName
whitening I-MethodName
use O
ZCA B-MethodName
as O
its O
whitening O
method O
to O
prevent O
the O
stochastic O
axis O
swapping O
( O
Huang O
et O
al O
. O
, O
2018 O
) O
. O
2 O
. O

ZCA B-MethodName
Whitening I-MethodName
. O
The O
whitening O
matrix O
of O
ZCA B-MethodName
whitening I-MethodName
transformation O
can O
be O
formulated O
as O
: O

W O
ZCA O
= O
UΛ O
−1 O
/ O
2 O
U O
T O
( O
3 O

) O

where O
U O
∈ O
R O
d×d O
is O
the O
stack O
of O
eigenvector O
of O
cov O
( O
Z O
, O
Z O
T O
) O
, O
and O
Λ O
is O
the O
correspond O
eigenvalue O
matrix O
. O
U O
and O
Λ O
are O
obtained O
by O
matrix O
decomposition O
. O
Therefore O
, O
Eq O
. O
2 O
becomes O
: O

H O
= O
Z O
T O
UΛ O
−1 O
/ O
2 O
U O
T O
( O
4 O
) O

Group B-MethodName
Whitening I-MethodName
. O
Since O
whitening O
module O
needs O
a O
large O
batch O
size O
to O
obtain O
a O
suitable O
estimate O
for O
the O
full O
covariance O
matrix O
, O
while O
in O
NLP O
, O
large O
batch O
size O
can O
be O
detrimental O
to O
unsupervised O
contrastive O
learning O
. O
To O
address O
this O
problem O
, O
we O
use O
group B-MethodName
whitening I-MethodName
( O
Huang O
et O
al O
. O
, O
2018 O
) O
, O
which O
controls O
the O
extent O
of O
whitening O
by O
decorrelating O
smaller O
groups O
. O
Specifically O
, O
give O
a O
sentence O
representation O
of O
dimension O
d O
, O
group B-MethodName
whitening I-MethodName
first O
divide O
it O
into O
k O
groups O
( O
Z O
0 O
, O
Z O
1 O
, O
... O
, O
Z O
k−1 O
) O
, O
i.e. O
, O
Z O
k O
∈ O
R O
N O
× O
d O
k O
and O
then O
apply O
whitening O
on O
each O
group O
. O
That O
is O
: O

H O
= O
concat O
( O
Z O
i O
W O
ZCA O
i O
) O
, O
i O
∈ O
[ O
0 O
, O
k O
) O
( O
5 O
) O

Shuffled B-MethodName
Group I-MethodName
Whitening I-MethodName

In O
order O
to O
further O
improve O
the O
quality O
of O
the O
sentence O
representation O
model O
, O
we O
proposed O
shuffledgroup B-MethodName
- I-MethodName
whitening I-MethodName
( O
SGW B-MethodName
) O
. O
We O
randomly O
divide O
the O
feature O
into O
multiple O
groups O
along O
the O
channel O
axis O
, O
and O
then O
perform O
ZCA B-MethodName
whitening I-MethodName
independently O
within O
each O
group O
. O
After O
whitening O
, O
we O
do O
a O
reshuffled O
operation O
to O
recover O
features O
to O
their O
original O
arrangement O
. O
The O
process O
can O
be O
formulated O
as O
: O

H O
= O
shuffled O
−1 O
( O
GW O
( O
shuffled O
( O
Z O
T O
) O
) O
) O
( O
6 O
) O

This O
can O
bring O
two O
benefits O
. O
One O
is O
that O
it O
can O
avoid O
the O
limitation O
that O
only O
adjacent O
features O
can O
be O
put O
into O
the O
same O
group O
, O
so O
as O
to O
better O
decorrelation O
and O
then O
achieve O
better O
uniformity O
in O
the O
representation O
space O
. O
Another O
is O
that O
it O
brings O
a O
disturbance O
to O
samples O
, O
we O
can O
use O
it O
as O
a O
data O
augmentation O
method O
. O
Specifically O
, O
we O
repeat O
SGW B-MethodName
multiple O
times O
and O
we O
can O
get O
different O
grouping O
results O
and O
then O
different O
whitened O
features O
. O
These O
" O
duplicated O
" O
features O
are O
different O
from O
each O
other O
, O
and O
thus O
increase O
the O
diversity O
of O
postive O
samples O
. O

Connection O
to O
contrastive O
learning O

We O
find O
that O
whitening O
and O
contrastive O
learning O
are O
not O
totally O
redundant O
but O
actually O
have O
some O
complementarity O
is O
non O
- O
trivia O
. O
Specifically O
, O
whitening O
decorrelates O
features O
through O
matrix O
decomposition O
, O
and O
makes O
the O
variance O
of O
all O
features O
equal O
to O
1 O
, O
that O
is O
, O
to O
project O
features O
into O
a O
spherical O
space O
. O
The O
" O
pushing O
" O
operation O
in O
contrastive O
learning O
is O
to O
approach O
a O
uniform O
spherical O
spatial O
distribution O
step O
by O
step O
through O
learning O
/ O
iteration O
. O
Therefore O
, O
conceptually O
, O
whitening O
and O
contrastive O
learning O
are O
redundant O
in O
optimizing O
the O
uniformity O
of O
the O
representation O
space O
. O
However O
, O
contrastive O
learning O
achieves O
uniformity O
by O
widening O
the O
distance O
between O
positive O
samples O
and O
all O
negative O
samples O
, O
but O
there O
is O
no O
explicit O
separation O
between O
negative O
samples O
. O
Whitening O
is O
the O
uniform O
dispersion O
of O
the O
entire O
samples O
, O
so O
there O
is O
complementarity O
between O
them O
. O
That O
is O
, O
whitening O
can O
supplement O
the O
lack O
of O
contrastive O
learning O
for O
the O
" O
pushing O
" O
operation O
between O
negative O
samples O
. O

Multi O
- O
Positive O
Contrastive O
Loss O

Since O
we O
get O
multi O
- O
positive O
samples O
from O
SGW B-MethodName
module O
, O
however O
, O
the O
original O
contrastive O
loss O
in O
Eq O
. O
1 O
is O
unable O
to O
handle O
multiple O
positives O
. O
We O
provide O
two O
possible O
options O
of O
contrastive O
loss O
which O
can O
adapt O
multi O
- O
positives O
. O
Given O
m O
positive O
samples O
, O
the O
objective O
function O
can O
be O
formulated O
as O
: O

L O
1 O
= O
−λ O
m O
m O
p=1 O
log O
e O
sim O
( O
h O
i O
, O
h O
+ O
i O
, O
p O
) O
/ O
τ O
N O
j=1 O
e O
sim O
( O
h O
i O
, O
h O
+ O
j O
) O
/ O
τ O
( O
7 O
) O
L O
2 O
= O
− O
log O
m O
p=1 O
λ O
m O
e O
−sim O
( O
h O
i O
, O
h O
+ O
i O
, O
p O
) O
/ O
τ O
N O
j=1 O
e O
sim O
( O
h O
i O
, O
h O
+ O
j O
) O
/ O
τ O
( O
8 O
) O

where O
λ O
m O
is O
a O
hyperparameter O
, O
it O
controls O
the O
impact O
of O
each O
positive O
. O
Eq O
. O
7 O
puts O
the O
summation O
over O
positives O
outside O
of O
the O
log O
while O
Eq O
. O
8 O
puts O
the O
sum O
of O
positives O
inside O
the O
log O
. O
It O
should O
be O
noted O
that O
in O
Eq O
. O
8 O
, O
there O
is O
a O
negative O
sign O
before O
the O
sum O
of O
positives O
, O
without O
it O
, O
the O
Eq O
. O
8 O
will O
conduct O
hard O
mining O
, O
which O
means O
the O
maximum O
of O
m O
p=1 O
e O
sim O
( O
h O
i O
, O
h O
+ O
i O
, O
p O
) O
/ O
τ O
is O
mainly O
determined O
by O
max O
( O
e O
−sim O
( O
h O
i O
, O
h O
+ O
i O
, O
p O
) O
/ O
τ O
) O
. O
If O
we O
add O
the O
negative O
sign O
, O
the O
loss O
function O
will O
be O
committed O
to O
punish O
the O
items O
with O
less O
similarity O
, O
which O
is O
good O
for O
bringing O
all O
positive O
samples O
closer O
to O
the O
anchor O
samples O
. O
In O
our O
framework O
, O
we O
adopt O
Eq O
. O
7 O
as O
our O
final O
loss O
function O
because O
it O
can O
achieve O
better O
performance O
. O

Experiments O

Experiment O
Setup O

In O
this O
section O
, O
We O
evaluate O
our O
method O
on O
seven O
Semantic B-TaskName
Textual I-TaskName
Similarity I-TaskName
( O
STS B-TaskName
) O
tasks O
and O
seven O
transfer B-TaskName
tasks O
. O
We O
use O
the O
SentEval B-DatasetName
( O
Conneau O
and O
Kiela O
, O
2018 O
) O
toolkit O
for O
all O
of O
tasks O
. O

Datasets O
. O
Semantic B-TaskName
Textual I-TaskName
Similarity I-TaskName
( O
STS B-TaskName
) O
tasks O
consist O
of O
seven O
tasks O
: O
STS B-DatasetName
2012 O
- O
2016 O
( O
Agirre O
et O
al O
. O
, O
2012 O
( O
Agirre O
et O
al O
. O
, O
, O
2013 O
( O
Agirre O
et O
al O
. O
, O
, O
2014 O
( O
Agirre O
et O
al O
. O
, O
, O
2015 O
( O
Agirre O
et O
al O
. O
, O
, O
2016 O
STS B-DatasetName
Benchmark I-DatasetName
( O
Cer O
et O
al O
. O
, O
2017 O
) O
and O
SICK B-DatasetName
- I-DatasetName
Relatedness I-DatasetName
( O
Marelli O
et O
al O
. O
, O
2014 O
) O
. O
Each O
sample O
in O
those O
datasets O
has O
two O
sentences O
and O
a O
manually O
annotated O
similarity O
score O
from O
0 O
to O
5 O
to O
measure O
their O
similarity O
. O
The O
transfer B-TaskName
tasks O
include O
MR B-DatasetName
( O
Pang O
and O
Lee O
, O
2005 O
) O
, O
CR B-DatasetName
( O
Hu O
and O
Liu O
, O
2004 O
) O
, O
SUBJ B-DatasetName
( O
Pang O
and O
Lee O
, O
2004 O
) O
, O
MPQA B-DatasetName
( O
Wiebe O
et O
al O
. O
, O
2005 O
) O
, O
SST-2 B-DatasetName
( O
Socher O
et O
al O
. O
, O
2013 O
) O
, O
TREC B-DatasetName
( O
Voorhees O
and O
Tice O
, O
2000 O
) O
, O
MRPC B-DatasetName
( O
Dolan O
and O
Brockett O
, O
2005 O
) O
. O
In O
this O
tasks O
, O
we O
use O
a O
logistic O
regression O
classifier O
trained O
on O
top O
of O
the O
frozen O
sentence O
embeddings O
. O

Baseline O
and O
competing O
methods O
. O
We O
compare O
WhitenedCSE B-MethodName
against O
several O
classic O
methods O
on O
Semantic B-TaskName
Textual I-TaskName
Similarity I-TaskName
datasets O
, O
i.e. O
, O
GloVe B-MethodName
embeddings O
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
, O
average O
BERT B-MethodName
embeddings O
from O
the O
last O
layer O
( O
Devlin O
et O
al O
. O
, O
2018 O
) O
, O
BERT B-MethodName
- I-MethodName
flow I-MethodName
( O
Li O
et O
al O
. O
, O
2020 O
) O
, O
BERTwhitening B-MethodName
( O
Su O
et O
al O
. O
, O
2021 O
) O
, O
IS B-MethodName
- I-MethodName
BERT I-MethodName
( O
Zhang O
et O
al O
. O
, O
2020 O
) O
, O
CT B-MethodName
( O
Carlsson O
, O
2021 O
) O
, O
ConSERT B-MethodName
( O
Yan O
et O
al O
. O
, O
2021 O
) O
, O
SimCSE B-MethodName
, O
as O
well O
as O
some O
most O
recent O
state O
- O
of O
- O
the O
- O
art O
methods O
, O
i.e. O
, O
MixCSE B-MethodName
( O
Zhang O
et O
al O
. O
, O
2022a O
) O
, O
ArcCSE B-MethodName
( O
Zhang O
et O
al O
. O
, O
2021a O
) O
, O
DCLR B-MethodName
( O
Zhou O
et O
al O
. O
, O
2022 O
) O
. O

Among O
these O
methods O
, O
SimCSE B-MethodName
may O
be O
viewed O
as O
our O
direct O
baseline O
, O
because O
WhitenedCSE B-MethodName
may O
be O
viewed O
as O
being O
transformed O
from O
SimCSE B-MethodName
by O
adding O
the O
SGW B-MethodName
and O
replacing O
the O
dual O
- O
positive O
contrastive O
loss O
with O
multi O
- O
positive O
contrastive O
loss O
. O

Therefore O
, O
when O
conduct O
ablation O
study O
, O
we O
use O
SimCSE B-MethodName
as O
our O
baseline O
. O

Implementation O
details O
. O
We O
use O
the O
output O
of O
the O
MLP O
layer O
on O
top O
of O
the O
[ O
CLS O
] O
as O
the O
our O
sentence O
representation O
. O
The O
MLP O
layer O
is O
consist O
of O
three O
components O
, O
which O
are O
a O
shuffled O
group O
whitening O
module O
, O
a O
768 O
× O
768 O
linear O
layer O
and O
a O
activation O
layer O
. O
Following O
SimCSE B-MethodName
, O
we O
use O
1 O
× O
10 O
6 O
randomly O
sampled O
sentences O
from O
English O
Wikipedia O
as O
our O
training O
corpus O
. O
We O
start O
from O
pretrained O
checkpoints O
of O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2018 O
) O
and O
RoBERTa B-MethodName
( O
Liu O
et O
al O
. O
, O
2019 O
) O
. O
At O
training O
time O
, O
we O
set O
the O
learning B-HyperparameterName
rate I-HyperparameterName
as O
3e-5 B-HyperparameterValue
, O
the O
batch B-HyperparameterName
size I-HyperparameterName
as O
64 B-HyperparameterValue
. O
We O
train O
our O
model O
for O
1 O
epoch O
with O
temperature B-HyperparameterName
τ O
= O
0.05 B-HyperparameterValue
. O
For O
BERT B-MethodName
- O
base O
and O
BERT B-MethodName
- O
large O
, O
we O
set O
the O
number O
of O
group B-HyperparameterName
size I-HyperparameterName
as O
384 B-HyperparameterValue
, O
for O
RoBERTabase B-MethodName
and O
RoBERTa B-MethodName
- O
large O
, O
we O
set O
the O
number O
of O
group B-HyperparameterName
size I-HyperparameterName
as O
256 B-HyperparameterValue
. O
We O
set O
the O
number O
of O
positives O
as O
3 O
for O
all O
of O
models O
. O
We O
evaluate O
the O
model O
every O
125 O
training O
steps O
on O
the O
development O
set O
of O
STS B-DatasetName
- I-DatasetName
B I-DatasetName
, O
and O
keep O
the O
best O
checkpoint O
for O
evaluation O
on O
test O
sets O
. O
We O
conduct O
our O
experiments O
on O
two O
3090 O
GPUs O
. O

STS B-TaskName
tasks O

We O
conduct O
experiments O
on O
7 O
semantic B-TaskName
textual I-TaskName
similarity I-TaskName
( O
STS B-TaskName
) O
tasks O
, O
and O
use O
SentEval B-DatasetName
toolkit O
( O
Conneau O
and O
Kiela O
, O
2018 O
) O
Spearman B-MetricName
's I-MetricName
correlation I-MetricName
coefficient I-MetricName
as O
our O
evaluation O
metrics O
. O
The O
Spearman B-MetricName
's I-MetricName
correlation I-MetricName
uses O
a O
monotonic O
equation O
to O
evaluate O
the O
correlation O
of O
two O
statistical O
variables O
, O
it O
varies O
between O
-1 O
and O
1 O
with O
0 O
implying O
no O
correlation O
, O
and O
the O
closer O
the O
value O
is O
to O
1 O
, O
the O
closer O
the O
two O
statistical O
variables O
are O
to O
positive O
correlation O
. O
Tab O
. O
1 O
shows O
the O
evaluation O
results O
on O
7 O
STS B-TaskName
tasks O
, O
from O
which O
we O
can O
see O
that O
WhitenedCSE B-MethodName
achieves O
competitive O
performance O
. O
Compared O
with O
SimCSE B-MethodName
, O
WhitenedCSE B-MethodName
achieves O
2.53 B-MetricValue
and O
1.56 B-MetricValue
points O
of O
improvement O
based O
on O
BERT B-MethodName
base O
and O
BERT B-MethodName
large O
. O
It O
also O
raise O
the O
performance O
from O
76.57 B-MetricValue
% I-MetricValue
to O
78.22 B-MetricValue
% I-MetricValue
base O
on O
RoBERTa B-MethodName
base O
. O
Compared O
with O
recent O
works O
, O
WhitenedCSE B-MethodName
also O
achieves O
the O
best O
performance O
in O
most O
of O
the O
STS B-TaskName
tasks O
. O

Transfer B-TaskName
tasks O

We O
also O
conduct O
experiments O
on O
7 O
transfer B-TaskName
tasks O
, O
and O
use O
SentEval B-DatasetName
toolkit O
( O
Conneau O
and O
Kiela O
, O
2018 O
) O
for O
evaluation O
. O
For O
each O
task O
, O
we O
train O
a O
logistic O
regression O
classifier O
on O
top O
of O
the O
frozen O
sentence O
embeddings O
and O
test O
the O
accuracy B-MetricName
on O
the O
downstream O
task O
. O
In O
our O
experiment O
settings O
, O
we O
do O
not O
include O
models O
with O
auxiliary O
tasks O
, O
i.e. O
, O
masked O
language O
modeling O
, O
for O
a O
fair O
comparison O
. O

Tab O
. O
2 O
shows O
the O
evaluation O
results O
. O
Comparied O
with O
the O
SimCSE B-MethodName
baseline O
, O
WhitenedCSE B-MethodName
achieves O
0.59 B-MetricValue
and O
0.33 B-MetricValue
accuracy B-MetricName
improvement O
on O
average O
results O
based O
on O
BERT B-MethodName
base O
and O
BERT B-MethodName
large O
. O
Compared O
with O
recent O
works O
, O
WhitenedCSE B-MethodName
also O
achieves O
the O
best O
performance O
in O
most O
of O
the O
transfer O
tasks O
, O
which O
further O
demonstrates O
the O
effectiveness O
of O
our O
method O
. O

Alignment O
and O
Uniformity O

In O
order O
to O
further O
quantify O
the O
improvement O
in O
uniformity B-MetricName
and O
alignment B-MetricName
of O
WhitenedCSE B-MethodName
, O
we O
follow O
SimCSE B-MethodName
, O
and O
use O
alignment B-MetricName
loss O
and O
uniformity B-MetricName
loss O
( O
Wang O
and O
Isola O
, O
2020 O
) O
to O
measure O
the O
quality O
of O
representations O
. O
Alignment O
is O
used O
to O
measure O
the O
expected O
distance O
between O
the O
embeddings O
of O
the O
positive O
pairs O
, O
and O
can O
be O
formulated O
as O
: O

ℓ O
align O
= O
E O
( O
x O
, O
x O
+ O
) O
∼ppos O
∥f O
( O
x O
) O
− O
f O
( O
x O
+ O
) O
∥ O
2 O
( O
9 O
) O

while O
uniformity B-MetricName
measures O
how O
well O
the O
embeddings O
are O
uniformly O
distributed O
in O
the O
representation O
space O
: O

ℓ O
uniform O
= O
log O
E O
x O
, O
y O
i.i.d O
. O
∼ O
p O
data O
e O
−2∥f O
( O
x O
) O
−f O
( O
y O
) O
∥ O
2 O
( O
10 O
) O

We O
calculate O
the O
alignment B-MetricName
loss O
and O
uniformity B-MetricName
loss O
every O
125 O
training O
steps O
on O
the O
STS B-DatasetName
- I-DatasetName
B I-DatasetName
development O
set O
. O
From O
Fig O
. O
3 O
, O
we O
can O
see O
that O
compared O
with O
SimCSE B-MethodName
, O
WhitenedCSE B-MethodName
performs O
better O
both O
on O
the O
alignment B-MetricName
measure O
and O
the O
uniformity B-MetricName
measure O
. O
We O
also O
find O
that O
the O
uniformity B-MetricName
of O
our O
models O
is O
well O
optimized O
at O
the O
beginning O
and O
remains O
stable O
throughout O
the O
training O
process O
. O
This O
further O
confirms O
that O
our O
method O
can O
improve O
the O
quality O
of O
sentence B-TaskName
representation I-TaskName
more O
effectively O
. O

Ablation O
Analysis O

In O
this O
section O
, O
we O
further O
investigate O
the O
effectiveness O
of O
our O
proposed O
model O
WhitenedCSE B-MethodName
. O
For O
all O
experiments O
, O
we O
use O
BERT B-MethodName
base O
as O
our O
base O
model O
, O
and O
evaluate O
WhitenedCSE B-MethodName
on O
the O
STS B-TaskName
tasks O
unless O
otherwise O
specified O
. O

Shuffling O
augments O
the O
positive O
samples O

We O
prove O
theoretically O
and O
practically O
that O
SGW B-MethodName
can O
be O
regarded O
as O
an O
effective O
data O
augmentation O
. O
We O
know O
different O
whitening O
transformations O
will O
get O
different O
whitened O
results O
, O
but O
all O
of O
them O
are O
representations O
for O
the O
same O
sample O
, O
so O
they O
can O
be O
regarded O
as O
positive O
samples O
for O
each O
other O
. O
In O
WhitenedCSE B-MethodName
, O
we O
operate O
randomly O
shuffled O
on O
feature O
dimension O
, O
and O
divide O
the O
representations O
along O
the O
feature O
dimension O
into O
k O
groups O
. O
Since O
each O
time O
we O
use O
a O
different O
permutation O
, O
we O
can O
get O
a O
different O
representations O
Z O
and O
the O
corresponding O
whitening O
matrix O
W O
, O
We O
find O
it O
can O
be O
written O
as O
a O
form O
of O
feature O
- O
wise O
disturbance O
Z O
* O
= O
Z O
+ O
ϵ O
: O

Z O
* O
= O
Z O
+ O
( O
W O
ZCA O
− O
1 O
) O
Z O
= O
Z O
+ O
( O
UΛ O
−1 O
/ O
2 O
U O
T O
− O
1 O
) O
Z O
( O
11 O
) O

Here O
, O
we O
treat O
( O
UΛ O
−1 O
/ O
2 O
U O
T O
− O
1 O
) O
Z O
as O
a O
perturbation O
ϵ O
on O
the O
feature O
dimension O
. O
Thus O
, O
in O
Whitened B-MethodName
- I-MethodName
CSE I-MethodName
, O
we O
use O
it O
as O
a O
data O
augmentation O
and O
generate O
more O
diverse O
positive O
samples O
. O
From O
Tab.4 O
, O
we O
can O
see O
that O
, O
shuffling O
plays O
a O
very O
important O
role O
in O
the O
performance O
of O
the O
model O
. O

The O
importance O
of O
Group B-MethodName
Whitening I-MethodName

Recently O
, O
Su O
et O
al O
. O
( O
2021 O
) O
directly O
apply O
whitening O
on O
the O
output O
of O
the O
BERT B-MethodName
and O
have O
achieved O
remarkable O
performance O
at O
the O
time O
. O
This O
lead O
us O
to O
think O
whether O
whitening O
can O
be O
directly O
applied O
to O
the O
output O
of O
contrastive O
learning O
model O
to O
further O
improve O
the O
uniformity O
of O
the O
model O
representation O
space O
. O
We O
consider O
two O
different O
whitening O
methods O
: O
PCA O
Whitening O
and O
ZCA B-MethodName
Whitening I-MethodName
. O
The O
difference O
between O
them O
is O
that O
ZCA B-MethodName
Whitening I-MethodName
uses O
an O
additional O
rotation O
matrix O
to O
rotate O
the O
PCA O
whitened O
data O
back O
to O
the O
original O
feature O
space O
, O
which O
can O
make O
the O
transformed O
data O
closer O
to O
the O
original O
input O
data O
. O

W O
ZCA O
= O
U O
rotate O
W O
P O
CA O
( O
12 O
) O

We O
use O
the O
in O
- O
batch O
sentence O
representations O
to O
calculate O
the O
mean O
valuex O
and O
the O
covariance O
matrix O
σ O
, O
and O
use O
the O
momentum O
to O
estimate O
the O
overall O
mean O
value O
µ O
and O
covariance O
matrix O
Σ O
. O

µ O
n O
= O
βµ O
n−1 O
+ O
( O
1 O
− O
β O
) O
x O
n−1 O
Σ O
n O
= O
βΣ O
n−1 O
+ O
( O
1 O
− O
β O
) O
σ O
n−1 O
( O
13 O
) O

As O
the O
results O
shown O
in O
the O
Tab O
. O
3 O
, O
we O
found O
that O
directly O
applying O
the O
whitening O
transformation O
on O
contrastive O
learning O
models O
is O
detrimental O
to O
the O
performance O
. O
we O
attribute O
this O
to O
two O
reasons O
: O
( O
1 O
) O
small O
batch O
size O
may O
not O
provide O
enough O
samples O
to O
obtain O
a O
suitable O
estimate O
for O
the O
full O
covariance O
matrix O
. O
( O
2 O
) O
The O
covariance O
matrix O
obtained O
by O
high O
- O
dimensional O
features O
is O
not O
necessarily O
a O
positive O
definite O
matrix O
( O
maybe O
a O
semi O
- O
positive O
definite O
matrix O
) O
, O
which O
may O
leads O
to O
errors O
in O
matrix O
decomposition O
. O
To O
alleviate O
this O
problem O
, O
we O
use O
the O
group B-MethodName
whitening I-MethodName
to O
control O
the O
extent O
of O
whitening O
. O
From O
Tab O
. O
3 O
we O
can O
see O
that O
group B-MethodName
whitening I-MethodName
can O
significantly O
improve O
the O
performance O
. O

Hyperparameters O
Analysis O

For O
hyperparameters O
analysis O
, O
we O
want O
to O
explore O
the O
sensitivity B-MetricName
of O
WhitenedCSE B-MethodName
to O
these O
parameters O
. O
Concretely O
, O
we O
study O
the O
impact O
of O
the O
group B-HyperparameterName
size I-HyperparameterName
, O
the O
number B-HyperparameterName
of I-HyperparameterName
positive I-HyperparameterName
samples I-HyperparameterName
. O
We O
evaluate O
our O
model O
with O
varying O
values O
, O
and O
report O
the O
performances O
on O
the O
seven O
STS B-TaskName
tasks O
. O

The O
influence O
of O
group B-HyperparameterName
size I-HyperparameterName
. O
In O
WhitenedCSE B-MethodName
, O
we O
divide O
the O
representation O
into O
k B-HyperparameterName
groups O
. O
However O
, O
we O
know O
the O
size O
of O
group O
controls O
the O
degree O
of O
whitening O
, O
and O
has O
a O
great O
effect O
on O
the O
effectiveness O
of O
WhitenedCSE B-MethodName
, O
so O
we O
carry O
out O
an O
experiment O
with O
k B-HyperparameterName
varying O
from O
32 B-HyperparameterValue
to O
384 B-HyperparameterValue
. O
As O
shown O
in O
Tab O
. O
4 O
, O
we O
can O
see O
that O
the O
best O
performance O
is O
achieved O
when O
k B-HyperparameterName
= O
384 B-HyperparameterValue
, O
and O
the O
second O
best O
performance O
is O
achieved O
when O
k B-HyperparameterName
= O
128 B-HyperparameterValue
. O
When O
k B-HyperparameterName
takes O
other O
values O
, O
the O
performance O
will O
drop O
slightly O
. O

The O
influence O
of O
positive B-HyperparameterName
samples I-HyperparameterName
number I-HyperparameterName
. O
Sampling O
multi O
- O
positive O
samples O
can O
significantly O
enrich O
semantic O
diversity O
, O
we O
want O
to O
explore O
how O
the O
number B-HyperparameterName
of I-HyperparameterName
positive I-HyperparameterName
samples I-HyperparameterName
affect O
the O
performance O
of O
our O
model O
, O
so O
we O
conduct O
an O
experiment O
with O
positive O
number O
m B-HyperparameterName
varying O
from O
2 B-HyperparameterValue
to O
5 B-HyperparameterValue
. O
From O
Tab O
. O
5 O
, O
we O
can O
see O
that O
when O
m B-HyperparameterName
= O
3 B-HyperparameterValue
, O
our O
model O
achieve O
the O
best O
performance O
. O
However O
, O
due O
to O
the O
limitation O
of O
the O
memory O
size O
, O
we O
can O
not O
exhaust O
all O
the O
possibilities O
, O
but O
we O
found O
that O
when O
m B-HyperparameterName
≥ O
2 B-HyperparameterValue
, O
the O
performance O
of O
the O
model O
is O
always O
better O
than O
when O
m B-HyperparameterName
= O
2 B-HyperparameterValue
, O
which O
confirms O
that O
mult O
- O
positive O
samples O
can O
bring O
richer O
semantics O
, O
allowing O
the O
model O
to O
learn O
better O
sentence B-TaskName
representations I-TaskName
. O

The O
influence O
of O
different O
modules O
. O
In O
Whitened B-MethodName
- I-MethodName
CSE I-MethodName
, O
using O
the O
proposed O
shuffled B-MethodName
- I-MethodName
group I-MethodName
- I-MethodName
whitening I-MethodName
method O
is O
beneficial O
, O
and O
further O
using O
multi O
- O
hot O
positive O
samples O
brings O
additional O
benefit O
. O
We O
investigate O
their O
respective O
contributions O
to O
Whitened B-MethodName
- I-MethodName
CSE I-MethodName
in O
Tab O
. O
6 O
. O
In O
our O
ablation O
study O
, O
we O
replace O
the O
whitening O
method O
with O
ordinary O
dropout O
technique O
, O
and O
still O
retain O
the O
multi O
- O
hot O
positive O
sample O
loss O
. O
Meanwhile O
, O
we O
use O
the O
proposed O
whitening O
method O
alone O
, O
and O
keep O
the O
number B-HyperparameterName
of I-HyperparameterName
positive I-HyperparameterName
samples I-HyperparameterName
2 B-HyperparameterValue
. O
From O
Tab O
. O
6 O
, O
we O
find O
that O
multi O
- O
hot O
positive O
samples O
based O
on O
dropout O
only O
brings O
+0.12 B-MetricValue
% I-MetricValue
improvement O
. O
This O
is O
reasonable O
because O
when O
the O
data O
augmentation O
is O
subtle O
( O
i.e. O
, O
the O
dropout O
) O
, O
using O
extra O
positive O
samples O
barely O
increases O
the O
diversity O
. O
In O
contrast O
, O
the O
proposed O
SGW B-MethodName
gener- O
ates O
informative O
data O
augmentation O
, O
and O
thus O
well O
accommodates O
the O
multi O
- O
hot O
positive O
samples O
. O

Conclusion O

In O
this O
paper O
, O
we O
proposed O
WhitenedCSE B-MethodName
, O
a O
whitening B-MethodName
- I-MethodName
based I-MethodName
contrastive I-MethodName
learning I-MethodName
framework I-MethodName
for I-MethodName
unsupervised I-MethodName
sentence I-MethodName
representation I-MethodName
learning I-MethodName
. O
We O
proposed O
a O
novel O
shuffled B-MethodName
group I-MethodName
whitening I-MethodName
, O
which O
reinforces O
the O
contrastive O
learning O
effect O
regarding O
both O
the O
uniformity B-MetricName
and O
the O
alignment B-MetricName
. O
Specifically O
, O
it O
retains O
the O
role O
of O
whitening O
in O
dispersing O
data O
, O
and O
can O
further O
improve O
uniformity B-MetricName
on O
the O
basis O
of O
contrastive O
learning O
. O
Additionally O
, O
it O
shuffles O
and O
groups O
features O
on O
channel O
axis O
, O
and O
performs O
whitening O
independently O
within O
each O
group O
. O
This O
kind O
of O
operation O
can O
be O
regarded O
as O
a O
disturbance O
on O
feature O
dimension O
. O
We O
obtain O
multiple O
positive O
samples O
through O
this O
operation O
, O
and O
learn O
the O
invariance O
to O
this O
disturbance O
to O
obtain O
better O
alignment O
. O
Experimental O
results O
on O
seven O
semantic B-TaskName
textual I-TaskName
similarity I-TaskName
tasks O
have O
shown O
that O
our O
approach O
achieve O
consistent O
improvement O
over O
the O
contrastive O
learning O
baseline O
. O

Limitations O

In O
this O
paper O
, O
we O
limit O
the O
proposed O
WhitenedCSE B-MethodName
for O
sentence B-TaskName
embedding I-TaskName
learning I-TaskName
. O
Conceptually O
, O
WhitenedCSE B-MethodName
is O
potential O
to O
benefit O
contrastive O
learning O
on O
some O
other O
tasks O
, O
e.g. O
, O
self O
- O
supervised O
image O
representation O
learning O
and O
self O
- O
supervised O
vision O
- O
language O
contrastive O
learning O
. O
However O
, O
we O
did O
not O
investigate O
the O
self O
- O
supervised O
image O
representation O
learning O
because O
this O
domain O
is O
currently O
dominated O
by O
masked O
image O
modeling O
. O
We O
will O
consider O
extending O
WhitenedCSE B-MethodName
for O
visionlanguage O
contrastive O
learning O
when O
we O
have O
sufficient O
training O
resources O
for O
the O
extraordinary O
largescale O
text O
- O
image O
pairs O
. O
Since O
the O
number O
of O
pages O
in O
the O
text O
is O
limited O
and O
our O
model O
does O
not O
have O
significant O
potential O
risks O
, O
we O
do O
not O
discuss O
this O
. O

Ethics O
Statement O

Visual O
Commonsense O
in O
Pretrained O
Unimodal B-MethodName
and O
Multimodal B-MethodName
Models O

Our O
commonsense O
knowledge O
about O
objects O
includes O
their O
typical O
visual O
attributes O
; O
we O
know O
that O
bananas O
are O
typically O
yellow O
or O
green O
, O
and O
not O
purple O
. O
Text O
and O
image O
corpora O
, O
being O
subject O
to O
reporting O
bias O
, O
represent O
this O
worldknowledge O
to O
varying O
degrees O
of O
faithfulness O
. O
In O
this O
paper O
, O
we O
investigate O
to O
what O
degree O
unimodal B-MethodName
( O
language O
- O
only O
) O
and O
multimodal B-MethodName
( O
image O
and O
language O
) O
models O
capture B-TaskName
a I-TaskName
broad I-TaskName
range I-TaskName
of I-TaskName
visually I-TaskName
salient I-TaskName
attributes I-TaskName
. O
To O
that O
end O
, O
we O
create O
the O
Visual B-DatasetName
Commonsense I-DatasetName
Tests I-DatasetName
( O
ViComTe B-DatasetName
) O
dataset O
covering O
5 O
property O
types O
( O
color O
, O
shape O
, O
material O
, O
size O
, O
and O
visual O
co O
- O
occurrence O
) O
for O
over O
5000 O
subjects O
. O
We O
validate O
this O
dataset O
by O
showing O
that O
our O
grounded O
color O
data O
correlates O
much O
better O
than O
ungrounded O
text O
- O
only O
data O
with O
crowdsourced O
color O
judgments O
provided O
by O
Paik O
et O
al O
. O
( O
2021 O
) O
. O
We O
then O
use O
our O
dataset O
to O
evaluate O
pretrained O
unimodal B-MethodName
models O
and O
multimodal B-MethodName
models O
. O
Our O
results O
indicate O
that O
multimodal B-MethodName
models O
better O
reconstruct O
attribute O
distributions O
, O
but O
are O
still O
subject O
to O
reporting O
bias O
. O
Moreover O
, O
increasing O
model O
size O
does O
not O
enhance O
performance O
, O
suggesting O
that O
the O
key O
to O
visual O
commonsense O
lies O
in O
the O
data O
. O
1 O

Introduction O

The O
observation O
that O
human O
language O
understanding O
happens O
in O
a O
rich O
multimodal O
environment O
has O
led O
to O
an O
increased O
focus O
on O
visual O
grounding O
in O
natural O
language O
processing O
( O
NLP O
) O
( O
Baltrusaitis O
et O
al O
. O
, O
2019 O
; O
Bisk O
et O
al O
. O
, O
2020 O
) O
, O
driving O
comparisons O
between O
traditional O
unimodal B-MethodName
text O
- O
only O
models O
and O
multimodal B-MethodName
models O
which O
take O
both O
text O
and O
image O
inputs O
. O
In O
this O
work O
, O
we O
explore O
to O
what O
extent O
unimodal B-MethodName
and O
multimodal B-MethodName
models O
are O
able O
to O
capture O
commonsense O
visual O
concepts O
across O
five O
types O
of O
relations O
: O
color O
, O
shape O
, O
material O
, O
size O
, O
and O
visual O
cooccurrence O
( O
cf O
. O
Fig O
. O
1 O
) O
. O
We O
further O
explore O
how O
this O
ability O
is O
influenced O
by O
reporting O
bias O
( O
Gordon O
and O
Van O
Durme O
, O
2013 O
) O
, O
the O
tendency O
of O
large O
corpora O
to O
over O
- O
or O
under O
- O
report O
events O
. O
We O
define O
visual B-TaskName
commonsense I-TaskName
as O
knowledge O
about O
generic O
visual O
concepts O
, O
e.g. O
" O
knobs O
are O
usually O
round O
" O
, O
and O
we O
measure O
this O
knowledge O
via O
frequency O
distributions O
over O
potential O
properties O
( O
e.g. O
round O
, O
square O
, O
etc O
) O
. O
A O
visually O
- O
informed O
language O
model O
should O
be O
able O
to O
capture O
such O
properties O
. O
Our O
color O
, O
shape O
, O
material O
, O
and O
co O
- O
occurrence O
data O
are O
mined O
from O
Visual O
Genome O
( O
Krishna O
et O
al O
. O
, O
2016 O
) O
, O
and O
our O
size O
data O
are O
created O
from O
object O
lists O
. O
They O
contain O
a O
large O
number O
of O
examples O
of O
per O
- O
object O
attribute O
distributions O
and O
" O
object O
- O
attribute O
" O
pairs O
. O
Paik O
et O
al O
. O
( O
2021 O
) O
evaluate O
language O
models O
' O
color O
perception O
using O
a O
human O
- O
annotated O
color O
dataset O
( O
CoDa O
) O
, O
finding O
that O
reporting O
bias O
negatively O
influences O
model O
performance O
and O
that O
multimodal O
training O
can O
mitigate O
those O
effects O
. O
In O
this O
work O
, O
we O
confirm O
those O
findings O
while O
extending O
the O
evaluation O
to O
a O
broader O
range O
of O
visually O
salient O
properties O
, O
resulting O
in O
a O
more O
comprehensive O
metric O
for O
visual O
commonsense O
. O
In O
order O
to O
elicit O
visual B-TaskName
commonsense I-TaskName
from O
language O
models O
, O
we O
utilize O
soft O
prompt O
tuning O
( O
Qin O
and O
Eisner O
, O
2021 O
) O
, O
which O
trains O
optimal O
templates O
by O
gradient O
descent O
for O
each O
model O
and O
relation O
type O
that O
we O
explore O
. O
We O
also O
utilize O
knowledge O
distillation O
to O
enhance O
a O
textonly O
model O
's O
visual B-TaskName
commonsense I-TaskName
ability O
, O
where O
the O
vision O
- O
language O
model O
serves O
as O
the O
teacher O
. O

The O
major O
contributions O
of O
this O
work O
are O
: O
( O
1 O
) O
we O
design O
a O
comprehensive O
analytic O
dataset O
, O
Vi B-DatasetName
- I-DatasetName
ComTe I-DatasetName
, O
for O
probing O
English O
visual O
commonsense O
, O
that O
is O
applicable O
to O
any O
language O
model O
; O
( O
2 O
) O
we O
use O
ViComTe B-DatasetName
to O
study O
models O
' O
ability O
to O
capture O
empirical O
distributions O
of O
visually O
salient O
properties O
. O
We O
examine O
unimodal B-MethodName
language O
models O
, O
multimodal B-MethodName
vision O
- O
language O
( O
VL O
) O
models O
, O
and O
a O
knowledgedistilled O
version O
of O
a O
VL O
model O
; O
and O
( O
3 O
) O
we O
analyze O
the O
effects O
of O
reporting O
bias O
on O
the O
visuallygrounded O
vs. O
ungrounded O
datasets O
and O
models O
. O

Does O
the O
model O
know O
… O

It O
is O
larger O
than O
: O
It O
is O
smaller O
than O
: O

Unimodal B-MethodName
Multimodal B-MethodName

BERT B-MethodName
, O
… O
Oscar B-MethodName
, O
… O

A O
girl O
is O
looking O
at O
the O
penguin O
. O

Penguins O
are O
a O
group O
of O
aquatic O
flightless O
birds O
. O

The O
word O
penguin O
first O
appears O
in O
the O
16th O
century O
as O
a O
name O
for O
the O
great O
auk O
. O
what O
is O
the O
color O
of O
a O
penguin O
? O

Figure O
1 O
: O
We O
compare O
unimodal B-MethodName
and O
multimodal B-MethodName
models O
' O
abilities O
to O
capture O
visual B-TaskName
commonsense I-TaskName
knowledge O
. O
The O
commonsense O
knowledge O
is O
evaluated O
on O
five O
relation O
types O
: O
color O
, O
shape O
, O
material O
, O
size O
, O
and O
visual O
co O
- O
occurrence O
. O

We O
compare O
the O
model O
outputs O
with O
the O
gold O
distribution O
from O
ViComTe B-DatasetName
, O
which O
is O
mined O
from O
Visual O
Genome O
. O

2 O
Related O
Work O

Vision O
- O
Language O
Modeling O

Recent O
advances O
in O
vision O
- O
language O
( O
VL B-MethodName
) O
modeling O
have O
led O
to O
increased O
success O
on O
benchmark O
tasks O
. O
Most O
VL O
models O
learn O
joint O
image O
and O
text O
representations O
from O
cross O
- O
modal O
training O
of O
transformers O
with O
self O
- O
attention O
, O
including O
LXMERT O
( O
Tan O
and O
Bansal O
, O
2019 O
) O
, O
ViLBERT O
( O
Lu O
et O
al O
. O
, O
2019 O
) O
, O
VisualBERT O
( O
Li O
et O
al O
. O
, O
2019 O
) O
, O
UNITER O
, O
etc O
. O
Oscar O
additionally O
uses O
object O
tags O
in O
images O
as O
anchor O
points O
to O
facilitate O
the O
learning O
of O
image O
- O
text O
alignments O
and O
VinVL O
presents O
an O
improved O
object O
detection O
model O
. O
CLIP O
( O
Radford O
et O
al O
. O
, O
2021 O
) O
learns O
by O
predicting O
caption O
- O
image O
alignment O
from O
a O
large O
internet O
corpus O
of O
( O
image O
, O
text O
) O
pairs O
. O
While O
our O
work O
uses O
textual O
prompt O
tuning O
techniques O
, O
there O
have O
also O
been O
work O
on O
visual O
prompt O
engineering O
to O
enhance O
the O
performance O
of O
pretrained O
vision O
- O
language O
models O
. O
Zhou O
et O
al O
. O
( O
2021 O
) O
model O
context O
in O
prompts O
as O
continuous O
representations O
and O
learn O
to O
optimize O
that O
context O
. O
Yao O
et O
al O
. O
( O
2021 O
) O
develop O
a O
cross O
- O
modal O
prompt O
tuning O
framework O
that O
reformulates O
visual O
grounding O
as O
a O
fill O
- O
in O
- O
the O
- O
blank O
problem O
for O
both O
image O
and O
text O
. O

Visual B-TaskName
Commonsense I-TaskName

In O
one O
of O
the O
early O
attempts O
at O
learning O
visual B-TaskName
commonsense I-TaskName
, O
Vedantam O
et O
al O
. O
( O
2015 O
) O
measure O
the O
plausibility O
of O
a O
commonsense O
assertion O
in O
the O
form O
of O
( O
obj1 O
, O
relation O
, O
obj2 O
) O
based O
on O
its O
similarity O
to O
known O
plausible O
assertions O
, O
using O
both O
visual O
scenes O
and O
accompanying O
text O
. O
Zellers O
et O
al O
. O
( O
2021 O
) O
learn O
physical O
commonsense O
via O
interaction O
, O
and O
use O
this O
knowledge O
to O
ground O
language O
. O
Frank O
et O
al O
. O
( O
2021 O
) O
probe O
whether O
VL O
models O
have O
learned O
to O
construct O
cross O
- O
modal O
representations O
from O
both O
modalities O
via O
cross O
- O
modal O
input O
ablation O
. O

Note O
that O
our O
definition O
of O
visual B-TaskName
commonsense I-TaskName
differs O
from O
that O
of O
Zellers O
et O
al O
. O
( O
2019 O
) O
, O
where O
the O
model O
is O
required O
to O
perform O
commonsense O
reasoning O
based O
on O
an O
image O
. O
Our O
definition O
of O
visual O
commonsense O
is O
more O
similar O
to O
the O
idea O
of O
stereotypic O
tacit O
assumptions O
( O
Prince O
, O
1978 O
) O
the O
propositional O
beliefs O
that O
humans O
hold O
about O
generic O
concepts O
, O
such O
as O
" O
dogs O
have O
to O
be O
walked O
" O
. O
Weir O
et O
al O
. O
( O
2020 O
) O
probe O
neural O
language O
models O
for O
such O
human O
tacit O
assumptions O
and O
demonstrate O
the O
models O
' O
success O
. O
We O
extend O
this O
intuition O
to O
visual O
concepts O
and O
explore O
how O
visual O
information O
may O
help O
language O
models O
to O
capture O
such O
assumptions O
. O

There O
has O
also O
been O
earlier O
work O
on O
the O
McRae O
feature O
norms O
( O
McRae O
et O
al O
. O
, O
2005 O
) O
, O
in O
which O
human O
annotators O
wrote O
down O
attributes O
that O
describe O
the O
meaning O
of O
words O
. O
For O
instance O
, O
" O
car O
" O
can O
be O
labeled O
as O
" O
has O
four O
wheels O
" O
and O
" O
apple O
" O
can O
be O
labeled O
as O
" O
is O
green O
" O
. O
Silberer O
et O
al O
. O
( O
2013 O
) O
expand O
the O
McRae O
dataset O
into O
a O
set O
of O
images O
and O
their O
visual O
attributes O
and O
construct O
visually O
grounded O
distributional O
models O
that O
can O
represent O
image O
features O
with O
visual O
attributes O
. O
Zhu O
et O
al O
. O
( O
2020 O
) O
examine O
the O
" O
language O
prior O
" O
problem O
in O
Visual O
Question O
Answering O
models O
, O
where O
models O
tend O
to O
answer O
based O
on O
word O
frequencies O
in O
the O
data O
, O
ignoring O
the O
image O
contents O
. O
In O
this O
work O
, O
we O
explore O
to O
what O
extent O
such O
a O
language O
prior O
is O
recruited O
absent O
a O
visual O
input O
. O

Reporting O
Bias O

Pretrained O
language O
models O
such O
as O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
are O
trained O
on O
billions O
of O
tokens O
of O
text O
, O
capturing O
statistical O
regularities O
present O
in O
the O
training O
corpora O
. O
However O
, O
their O
textual O
training O
data O
can O
suffer O
from O
reporting O
bias O
, O
where O
the O
frequency O
distribution O
of O
specific O
events O
and O
properties O
in O
text O
may O
not O
reflect O
the O
real O
- O
world O
distribution O
of O
such O
properties O
( O
Gordon O
and O
Van O
Durme O
, O
2013 O
) O
. O
For O
example O
, O
while O
grass O
is O
typically O
green O
, O
this O
may O
be O
under O
- O
reported O
in O
web O
corpora O
( O
as O
it O
is O
assumed O
to O
be O
true O
) O
, O
and O
while O
motorcycle O
crashes O
may O
be O
more O
common O
in O
the O
real O
world O
, O
plane O
crashes O
are O
mentioned O
far O
more O
in O
news O
text O
( O
Gordon O
and O
Van O
Durme O
, O
2013 O
) O
. O
Misra O
et O
al O
. O
( O
2016 O
) O
highlight O
the O
reporting O
bias O
in O
" O
human O
- O
centric O
" O
image O
annotations O
and O
find O
that O
the O
noise O
in O
annotations O
exhibits O
a O
structure O
that O
can O
be O
modeled O
. O

3 O
Dataset O
: O
ViComTe B-DatasetName

Dataset O
Mining O

For O
each O
relation O
color O
, O
shape O
, O
material O
, O
size O
, O
and O
object O
co O
- O
occurrence O
, O
our O
data O
take O
the O
form O
of O
( O
subject O
, O
object O
) O
tuples O
extracted O
from O
object O
distributions O
per O
subject O
. O
The O
goal O
is O
to O
predict O
the O
object O
and O
its O
distribution O
from O
the O
subject O
and O
relation O
. O
Table O
1 O
summarizes O
the O
number O
of O
classes O
and O
subject O
- O
object O
pairs O
for O
each O
relation O
. O
2 O
Color O
, O
Shape O
, O
Material O
For O
color O
, O
shape O
, O
and O
material O
, O
the O
subject O
is O
a O
noun O
and O
the O
object O
is O
the O
color O
, O
shape O
, O
or O
material O
property O
of O
the O
noun O
, O
mined O
from O
attributes O
of O
Visual O
Genome O
( O
VG O
) O
( O
Krishna O
et O
al O
. O
, O
2016 O
) O
. O
3 O
We O
manually O
create O
a O
list O
of O
single O
- O
word O
attributes O
for O
each O
relation O
, O
and O
only O
VG O
subjects O
that O
are O
matched O
with O
a O
specific O
attribute O
for O
more O
than O
a O
threshold O
number O
of O
times O
are O
recorded O
, O
in O
order O
to O
avoid O
noise O
in O
the O
dataset O
. O
The O
thresholds O
for O
color O
, O
material O
, O
and O
shape O
are O
5 O
, O
2 O
, O
and O
1 O
, O
respectively O
, O
chosen O
based O
on O
the O
availability O
of O
attributes O
of O
each O
relation O
in O
VG O
. O
VG O
attributes O
are O
filtered O
with O
the O
following O
steps O
: O
( O
1 O
) O
attribute O
" O
Y O
colored O
/ O
made O
/ O
shaped O
" O
is O
treated O
as O
" O
Y O
" O
; O
( O
2 O
) O
select O
only O
the O
last O
word O
for O
compound O
attributes O
( O
e.g. O
treat O
" O
forest O
green O
" O
as O
" O
green O
" O
) O
; O
( O
3 O
) O
similar O
attributes O
are O
merged O
into O
a O
main O
attribute O
class O
( O
e.g. O
" O
maroon O
" O
and O
" O
crimson O
" O
become O
" O
red O
" O
) O
. O

The O
above O
procedure O
produces O
a O
distribution O
over O
the O
set O
of O
attributes O
for O
each O
subject O
noun O
. O
From O
that O
distribution O
, O
a O
( O
subject O
, O
object O
) O
data O
instance O
is O
generated O
for O
each O
subject O
where O
the O
object O
is O
the O
attribute O
that O
associates O
with O
it O
the O
most O
. O
See O
the O
first O
three O
rows O
of O
Table O
1 O
for O
examples O
. O

Size O
Size O
is O
separated O
into O
size_smaller O
and O
size_larger O
, O
where O
the O
subject O
is O
a O
noun O
and O
the O
object O
is O
another O
noun O
that O
is O
smaller O
or O
larger O
, O
respectively O
, O
than O
the O
subject O
. O
To O
form O
the O
size O
dataset O
, O
we O
obtain O
a O
set O
of O
concrete O
nouns O
that O
appears O
in O
VG O
, O
which O
we O
manually O
classify O
into O
5 O
size O
categories O
( O
tiny O
, O
small O
, O
medium O
, O
large O
, O
and O
huge O
) O
. O
Typical O
objects O
in O
each O
category O
includes O
pill O
, O
book O
, O
table O
, O
lion O
, O
mountain O
, O
respectively O
. O
We O
randomly O
pick O
two O
nouns O
from O
different O
categories O
to O
form O
a O
( O
subject O
, O
object O
) O
pair O
. O

Visual O
Co O
- O
occurrence O
The O
visual O
co O
- O
occurrence O
dataset O
is O
generated O
in O
a O
similar O
way O
to O
the O
color O
, O
shape O
, O
and O
material O
datasets O
. O
Co O
- O
occurrence O
distribution O
is O
extracted O
from O
Visual O
Genome O
where O
two O
objects O
that O
occur O
in O
the O
same O
scene O
graph O
together O
for O
more O
than O
8 O
times O
are O
recorded O
, O
and O
a O
( O
subject O
, O
object O
) O
instance O
is O
generated O
for O
each O
subject O
, O
where O
the O
object O
is O
the O
noun O
that O
co O
- O
occurs O
with O
the O
subject O
the O
most O
. O

Data O
Grouping O

Following O
Paik O
et O
al O
. O
( O
2021 O
) O
, O
we O
split O
the O
color O
, O
shape O
, O
and O
material O
datasets O
each O
into O
three O
groups O
: O
SINGLE O
, O
MULTI O
, O
and O
ANY O
. O
The O
SINGLE O
group O
is O
for O
subjects O
whose O
most O
common O
attribute O
covers O
more O
than O
80 O
% O
of O
the O
probability O
, O
e.g. O
, O
the O
color O
of O
snow O
is O
almost O
always O
white O
. O
The O
MULTI O
group O
is O
defined O
as O
subjects O
not O
in O
the O
SINGLE O
group O
where O
more O
than O
90 O
% O
of O
the O
probability O
falls O
in O
the O
top O
4 O
attribute O
classes O
, O
e.g. O
, O
the O
color O
of O
a O
penguin O
in O
Fig O
. O
1 O
. O
The O
rest O
of O
the O
subjects O
are O
in O
the O
ANY O
group O
. O
Lower O
model O
performance O
for O
the O
SINGLE O
group O
would O
indicate O
the O
influence O
of O
reporting O
bias O
. O
For O
example O
, O
if O
the O
model O
is O
unable O
to O
correctly O
capture O
the O
distribution O
of O
the O
color O
of O
snow O
, O
it O
is O
likely O
because O
the O
color O
of O
snow O
has O
low O
probability O
of O
being O
reported O
in O
the O
training O
corpus O
, O
as O
people O
know O
it O
is O
white O
by O
default O
. O

Templates O

In O
order O
to O
elicit O
model O
response O
and O
extract O
target O
objects O
and O
distributions O
from O
text O
, O
we O
manually O
design O
a O
set O
of O
templates O
for O
each O
relation O
. O
There O
are O
7 O
templates O
for O
color O
, O
shape O
, O
and O
material O
each O
, O
8 O
for O
size O
, O
and O
4 O
for O
visual O
co O
- O
occurrence O
. O
See O
Table O
1 O
for O
example O
templates O
. O

Wikipedia O
Data O

In O
order O
to O
compare O
text O
- O
based O
and O
visuallygrounded O
data O
, O
we O
mine O
the O
color O
, O
shape O
, O
and O
material O
datasets O
from O
Wikipedia O
data O
, O
which O
is O
typically O
used O
in O
model O
pretraining O
. O
To O
mine O
these O
text O
- O
based O
datasets O
, O
we O
combine O
the O
sets O
of O
subjects O
in O
VG O
, O
take O
the O
manual O
list O
of O
attributes O
as O
objects O
again O
, O
and O
extract O
( O
subject O
, O
object O
) O
pairs O
if O
the O
pair O
matches O
any O
of O
the O
pre O
- O
defined O
templates O
. O
In O
Section O
3.5 O
we O
will O
show O
the O
advantages O
of O
the O
VG O
- O
mined O
dataset O
over O
this O
text O
- O
based O
dataset O
. O

Dataset O
Evaluation O

To O
ensure O
the O
validity O
of O
ViComTe B-DatasetName
, O
we O
compare O
our O
color O
dataset O
with O
the O
human O
- O
annotated O
CoDa B-DatasetName
dataset O
( O
Paik O
et O
al O
. O
, O
2021 O
) O
, O
which O
we O
assume O
is O
close O
to O
real O
- O
world O
color O
distributions O
and O
has O
minimal O
reporting O
bias O
. O
We O
see O
a O
reasonably O
strong O
correlation O
with O
CoDa B-DatasetName
, O
indicating O
that O
the O
ViComTe B-DatasetName
dataset O
is O
a O
good O
and O
cost O
- O
effective O
approximation O
to O
human O
annotations O
. O

Metrics O
We O
report O
the O
Spearman B-MetricName
's I-MetricName
rank I-MetricName
- I-MetricName
order I-MetricName
correlation I-MetricName
between O
the O
two O
distributions O
in O
comparison O
, O
averaged O
across O
all O
subjects O
. O
The O
Spearman B-MetricName
correlation I-MetricName
is O
used O
instead O
of O
the O
Pearson O
correlation O
since O
for O
our O
purpose O
the O
rank O
of O
the O
object O
distributions O
is O
more O
important O
than O
the O
exact O
values O
, O
which O
may O
change O
due O
to O
data O
variability O
. O
The O
top-1 O
accuracy B-MetricName
( O
Acc B-MetricName
@ I-MetricName
1 I-MetricName
) O
is O
the O
percentage O
of O
the O
objects O
with O
the O
highest O
probability O
in O
the O
source O
distributions O
matching O
those O
in O
the O
target O
distributions O
. O
These O
two O
metrics O
are O
also O
used O
in O
later O
sections O
when O
evaluating O
model O
distributions O
. O

Analysis O
Table O
2 O
shows O
the O
detailed O
results O
of O
the O
evaluation O
of O
the O
ViComTe B-DatasetName
and O
Wikipedia B-DatasetName
color I-DatasetName
datasets O
by O
comparing O
with O
the O
human O
- O
annotated O
dataset O
, O
CoDa B-DatasetName
. O
We O
can O
see O
that O
ViComTe B-DatasetName
has O
much O
higher O
Spearman B-MetricName
correlation I-MetricName
with O
CoDa B-DatasetName
, O
as O
well O
as O
substantially O
higher O
top-1 O
accuracy B-MetricName
for O
the O
SINGLE O
group O
. O
The O
correlation B-MetricName
is O
expected O
to O
be O
low O
for O
the O
ANY O
group O
, O
because O
objects O
in O
the O
ANY O
group O
can O
have O
many O
possible O
colors O
. O

Reporting O
bias O
is O
present O
in O
both O
datasets O
, O
as O
the O
average O
number O
of O
occurrences O
of O
SINGLE O
group O
subjects O
are O
much O
fewer O
than O
that O
of O
the O
MULTI O
and O
ANY O
group O
subjects O
. O
Counter O
- O
intuitively O
, O
for O
ViComTe B-DatasetName
, O
the O
highly O
- O
correlated O
SINGLE O
group O
subjects O
have O
fewer O
average O
occurrences O
than O
the O
ones O
with O
low O
correlations O
. O
This O
is O
contrary O
to O
our O
expectation O
that O
more O
frequent O
objects O
would O
better O
reflect O
the O
human O
- O
perceived O
distribution O
and O
can O
be O
explained O
by O
SINGLE O
subjects O
being O
easier O
to O
represent O
even O
without O
a O
large O
amount O
of O
data O
. O

One O
example O
where O
the O
Wikipedia B-DatasetName
distribution O
diverges O
from O
the O
CoDa B-DatasetName
distribution O
is O
" O
penguin O
" O
, O
whose O
most O
likely O
color O
in O
CoDa B-DatasetName
is O
black O
, O
followed O
by O
white O
and O
gray O
; O
however O
, O
its O
top O
color O
in O
Wikipedia B-DatasetName
is O
blue O
, O
because O
" O
blue O
penguin O
" O
is O
a O
specific O
species O
with O
an O
entry O
in O
Wikipedia B-DatasetName
, O
even O
if O
it O
is O
not O
as O
common O
as O
black O
and O
white O
penguins O
. O
One O
example O
where O
the O
VG O
distributions O
diverge O
from O
CoDa B-DatasetName
is O
" O
mouse O
" O
, O
because O
in O
VG O
, O
most O
occurrences O
of O
" O
mouse O
" O
are O
computer O
mice O
, O
which O
are O
most O
commonly O
black O
, O
whereas O
when O
asked O
about O
the O
word O
" O
mouse O
" O
, O
human O
annotators O
typically O
think O
about O
the O
animal O
, O
so O
that O
the O
most O
likely O
colors O
in O
CoDa B-DatasetName
are O
white O
and O
gray O
. O
4 O

Dataset O
splits O

Each O
of O
the O
color O
, O
shape O
, O
material O
, O
size O
, O
and O
cooccurrence O
datasets O
is O
split O
into O
80 B-HyperparameterValue
% I-HyperparameterValue
training O
data O
and O
20 B-HyperparameterValue
% I-HyperparameterValue
test O
data O
. O
All O
evaluation O
metrics O
are O
reported O
on O
the O
test O
set O
. O
The O
training O
set O
is O
used O
for O
the O
logistic O
regression O
and O
the O
soft O
prompt O
tuning O
algorithm O
( O
Section O
4.2 O
) O
. O

Probing O
Visual B-TaskName
Commonsense I-TaskName

Models O

We O
examine O
7 O
pretrained O
transformer O
- O
based O
models O
and O
2 O
variations O
of O
them O
, O
trained O
on O
a O
variety O
of O
data O
. O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
, O
ALBERT B-MethodName
( O
Lan O
et O
al O
. O
, O
2020 O
) O
, O
and O
RoBERTa B-MethodName
( O
Liu O
et O
al O
. O
, O
2019 O
) O
are O
trained O
on O
text O
only O
using O
a O
masked O
language O
modeling O
objective O
( O
MLM O
) O
. O
Oscar B-MethodName
) O
is O
a O
vision O
- O
language O
model O
based O
on O
the O
BERT B-MethodName
architecture O
, O
trained O
with O
an O
combined O
MLM O
and O
contrastive O
loss O
on O
text O
- O
image O
pairs O
. O
VisualBERT B-MethodName
( O
Li O
et O
al O
. O
, O
2019 O
) O
is O
another O
vision O
- O
language O
model O
based O
on O
BERT B-MethodName
that O
learns O
joint O
representation O
of O
images O
and O
text O
. O
Tan O
and O
Bansal O
( O
2020 O
) O
introduce O
the O
" O
vokenization B-MethodName
" O
method O
, O
which O
aligns O
language O
tokens O
to O
their O
related O
images O
, O
mitigating O
the O
shortcomings O
of O
models O
trained O
on O
visually O
- O
grounded O
datasets O
in O
text O
- O
only O
tasks O
. O
Since O
our O
task O
is O
purely O
text O
- O
based O
, O
we O
also O
experiment O
with O
a O
pretrained O
vokenization O
model O
( O
BERT B-MethodName
+ I-MethodName
VLM I-MethodName
on O
Wiki O
) O
. O
Finally O
, O
we O
use O
representations O
from O
CLIP B-MethodName
( O
ViT O
- O
B O
/ O
32 O
) O
( O
Radford O
et O
al O
. O
, O
2021 O
) O
, O
which O
is O
trained O
with O
a O
contrastive O
image O
- O
caption O
matching O
loss O
. O

Distilled B-MethodName
Oscar I-MethodName

As O
our O
experiments O
involve O
exclusively O
textual O
inputs O
, O
we O
develop O
a O
knowledgedistilled O
version O
of O
Oscar B-MethodName
( O
" O
Distilled O
" O
) O
which O
corrects O
for O
the O
lack O
of O
image O
input O
in O
our O
task O
. O
Knowledge O
distillation O
( O
Hinton O
et O
al O
. O
, O
2015 O
; O
Sanh O
et O
al O
. O
, O
2019 O
) O
is O
the O
process O
of O
transferring O
knowledge O
from O
one O
model O
to O
another O
, O
where O
the O
student O
model O
is O
trained O
to O
produce O
the O
output O
of O
the O
teacher O
model O
. O
Here O
, O
we O
use O
Oscar B-MethodName
as O
the O
teacher O
and O
BERT B-MethodName
as O
the O
student O
. O
The O
training O
data O
is O
part O
of O
the O
Oscar B-MethodName
pretraining O
corpus O
: O
COCO O
( O
Lin O
et O
al O
. O
, O
2014 O
) O
, O
Flickr30k O
( O
Young O
et O
al O
. O
, O
2014 O
) O
, O
and O
GQA O
( O
Hudson O
and O
Manning O
, O
2019 O
) O
, O
and O
the O
Distilled B-MethodName
Oscar I-MethodName
model O
has O
access O
to O
the O
text O
data O
only O
. O
We O
use O
the O
Kullback B-HyperparameterName
- I-HyperparameterName
Leibler I-HyperparameterName
loss O
to O
measure O
the O
divergence O
between O
the O
output O
logits O
of O
BERT B-MethodName
and O
Oscar B-MethodName
, O
and O
optimize O
the O
pretrained O
BERT B-MethodName
on O
that O
loss O
to O
match O
the O
outputs O
of O
Oscar O
. O
Configurable O
parameters O
are O
set O
the O
same O
as O
for O
Oscar B-MethodName
pretraining O
. O

CaptionBERT B-MethodName
Since O
VL O
models O
are O
trained O
largely O
on O
caption O
data O
, O
it O
could O
be O
that O
the O
differences O
between O
a O
text O
- O
only O
model O
and O
a O
VL O
model O
come O
not O
from O
a O
difference O
in O
modalities O
-text O
vs. O
images O
and O
text O
-but O
from O
a O
difference O
in O
domainwebtext O
vs. O
image O
captions O
. O
In O
order O
to O
disentangle O
the O
effects O
of O
the O
domain O
difference O
from O
those O
of O
visual O
inputs O
, O
we O
train O
a O
BERT O
model O
from O
scratch O
( O
" O
CaptionBERT B-MethodName
" O
) O
on O
Oscar O
's O
caption O
- O
based O
text O
data O
( O
the O
same O
data O
as O
for O
the O
Distilled O
model O
) O
. O
If O
CaptionBERT B-MethodName
, O
which O
does O
not O
have O
exposure O
to O
visual O
inputs O
, O
performs O
better O
than O
BERT B-MethodName
and O
similarly O
to O
VL O
models O
( O
which O
are O
trained O
with O
visual O
inputs O
) O
, O
it O
would O
suggest O
that O
the O
training O
domain O
matters O
more O
than O
the O
modality O
. O
If O
, O
on O
the O
other O
hand O
, O
CaptionBERT B-MethodName
performs O
worse O
than O
VL O
models O
, O
it O
would O
highlight O
the O
importance O
of O
modality O
. O

Evaluation O
Methods O

We O
compare O
the O
visual B-TaskName
commonsense I-TaskName
abilities O
of O
pretrained O
unimodal B-MethodName
and O
multimodal B-MethodName
models O
. O
Given O
a O
list O
of O
prompts O
and O
a O
subject O
word O
, O
each O
model O
outputs O
the O
distribution O
of O
the O
target O
word O
. O
Following O
Paik O
et O
al O
. O
( O
2021 O
) O
, O
we O
apply O
zero B-HyperparameterName
- I-HyperparameterName
shot I-HyperparameterName
probes I-HyperparameterName
to O
models O
that O
are O
trained O
on O
a O
language O
modeling O
objective O
, O
and O
conduct O
representation O
probes O
for O
those O
that O
are O
not O
. O
We O
report O
the O
prediction O
accuracy O
and O
the O
Spearman B-MetricName
correlation I-MetricName
of O
the O
output O
distribution O
with O
the O
true O
distribution O
. O

We O
use O
models O
trained O
with O
an O
MLM O
objective O
( O
BERT B-MethodName
, O
Distilled B-MethodName
, O
etc O
) O
directly O
for O
zero B-HyperparameterName
- I-HyperparameterName
shot I-HyperparameterName
predic I-HyperparameterName
- I-HyperparameterName
tion I-HyperparameterName
of O
masked O
tokens O
. O
5 O
For O
Oscar B-MethodName
we O
add O
a O
wordprediction O
head O
on O
top O
of O
it O
. O
The O
results O
across O
templates O
are O
aggregated O
in O
two O
modes O
. O
In O
the O
" O
best O
template O
" O
mode O
, O
for O
each O
example O
, O
the O
highest O
Spearman B-MetricName
correlation I-MetricName
among O
all O
templates O
is O
reported O
, O
and O
the O
top-1 B-MetricName
result O
is O
regarded O
as O
correct O
if O
the O
true O
target O
object O
is O
the O
same O
as O
the O
top-1 B-MetricName
result O
of O
any O
of O
the O
templates O
. O
In O
the O
" O
average O
template O
" O
mode O
, O
the O
output O
distribution O
is O
the O
mean O
of O
the O
distributions O
across O
all O
templates O
. O

Since O
CLIP B-MethodName
is O
not O
trained O
on O
a O
token O
- O
prediction O
objective O
, O
we O
implement O
logistic O
regression O
on O
top O
of O
the O
frozen O
encoder O
output O
, O
to O
predict O
the O
target O
attribute O
or O
object O
. O
The O
input O
is O
each O
of O
the O
templates O
with O
the O
subject O
[ O
X O
] O
filled O
with O
an O
input O
in O
the O
dataset O
. O
Like O
Paik O
et O
al O
. O
( O
2021 O
) O
, O
to O
give O
the O
model O
ample O
chance O
of O
success O
, O
we O
take O
the O
template O
that O
results O
in O
the O
best O
test O
accuracy O
score O
, O
report O
that O
accuracy O
and O
the O
Spearman O
correlation O
associated O
with O
that O
template O
. O
For O
the O
classification O
head O
, O
we O
use O
the O
Scikit O
- O
Learn O
implementation O
of O
Logistic O
Regression O
( O
random_state=0 O
, O
C=0.316 O
, O
max_iter=2000 O
) O
( O
Pedregosa O
et O
al O
. O
, O
2011 O
) O
. O

Soft O
prompt O
tuning O
In O
order O
to O
overcome O
the O
limitation O
of O
self O
- O
designed O
prompts O
, O
we O
incorporate O
prompt O
tuning O
technique O
that O
learns O
soft O
prompts O
by O
gradient O
descent O
, O
from O
Qin O
and O
Eisner O
( O
2021 O
) O
. O
6 O
The O
algorithm O
minimizes O
the O
log O
loss O
: O

( O
x O
, O
y O
) O
∈Er O
− O
log O
t∈Tr O
p O
( O
y|t O
, O
x O
) O

for O
a O
set O
of O
example O
pairs O
E O
r O
and O
template O
set O
T O
r O
. O

Size O
Evaluation O

The O
size O
dataset O
differs O
from O
the O
other O
datasets O
in O
that O
we O
use O
relative O
sizes O
( O
X O
is O
larger O
/ O
smaller O
than O
Y O
) O
, O
as O
absolute O
size O
information O
is O
hard O
to O
obtain O
. O
Thus O
, O
we O
use O
two O
evaluation O
strategies O
for O
size O
. O

Rank O
partition O
First O
, O
as O
in O
the O
previous O
prediction O
task O
, O
given O
a O
template O
such O
as O
" O
[ O
X O
] O
is O
larger O
than O
[ O
Y O
] O
" O
and O
an O
object O
[ O
X O
] O
, O
we O
ask O
the O
model O
to O
predict O
the O
distribution O
of O
[ O
Y O
] O
, O
taking O
only O
the O
distribution O
D O
of O
nouns O
in O
the O
size O
dataset O
. O
For O
the O
current O
object O
[ O
X O
] O
, O
we O
take O
the O
nouns O
in O
size O
categories O
that O
are O
smaller O
than O
the O
category O
of O
[ O
X O
] O
( O
N O
sm O
) O
, O
and O
those O
that O
are O
in O
larger O
categories O
( O
N O
lg O
) O
. O

Let O
the O
length O
of O
N O
sm O
be O
m O
and O
the O
length O
of O
N O
lg O
be O
n. O
Then O
for O
the O
" O
larger O
" O
templates O
, O
we O
compute O
the O
average O
percentage O
of O
overlap O
between O
the O
top O
n O
objects O
in O
D O
and O
N O
lg O
and O
that O
between O
the O
bottom O
m O
objects O
in O
D O
and O
and O
N O
sm O
. O
For O
the O
" O
smaller O
" O
templates O
, O
the O
" O
top O
" O
and O
" O
bottom O
" O
are O
reversed O
. O

Adjective O
projection O

The O
second O
approach O
follows O
that O
of O
van O
Paridon O
et O
al O
. O
( O
2021 O
) O
, O
which O
projects O
the O
word O
to O
be O
evaluated O
onto O
an O
adjective O
scale O
. O
In O
this O
case O
, O
we O
compute O
the O
word O
embeddings O
of O
the O
adjectives O
" O
small O
" O
and O
" O
large O
" O
and O
the O
nouns O
from O
models O
, O
so O
the O
scale O
is O
−−→ O
large O
− O
− O
−− O
→ O
small O
and O
the O
projection O
is O
calculated O
by O
cosine O
similarity O
. O
For O
instance O
, O
for O
the O
example O
noun O
" O
bear O
" O
, O
the O
projection O
score O
is O
given O
by O
: O

cos_sim O
( O
−−→ O
large O
− O
− O
−− O
→ O
small O
, O
− O
− O
→ O
bear O
) O

With O
good O
word O
embeddings O
, O
larger O
nouns O
are O
expected O
to O
have O
higher O
projection O
scores O
. O
The O
validity O
of O
the O
adjective O
scales O
from O
word O
representations O
is O
shown O
by O
Kim O
and O
de O
Marneffe O
( O
2013 O
) O
. O

Measuring O
Model O
Reporting O
Bias O

We O
measure O
the O
reporting O
bias O
of O
our O
models O
by O
comparing O
model O
performance O
on O
datasets O
with O
different O
levels O
of O
reporting O
bias O
and O
on O
the O
SINGLE O
, O
MULTI O
, O
ANY O
groups O
of O
the O
ViComTe B-DatasetName
dataset O
. O
We O
assume O
that O
CoDa B-DatasetName
contains O
no O
reporting O
bias O
, O
in O
which O
case O
we O
can O
interpret O
Table O
2 O
as O
showing O
that O
ViComTe B-DatasetName
contains O
a O
relatively O
small O
amount O
of O
it O
, O
and O
Wikipedia B-DatasetName
contains O
a O
relatively O
large O
amount O
. O
Thus O
, O
a O
larger O
correlation O
of O
model O
outputs O
with O
ViComTe B-DatasetName
and O
a O
smaller O
one O
with O
Wikipedia B-DatasetName
would O
indicate O
less O
model O
reporting O
bias O
. O

Also O
, O
since O
the O
SINGLE O
group O
subjects O
are O
those O
whose O
attribute O
distribution O
concentrates O
on O
a O
single O
attribute O
, O
these O
subject O
- O
attribute O
pairs O
are O
less O
likely O
to O
be O
reported O
in O
text O
corpora O
or O
even O
image O
annotations O
. O
Therefore O
, O
lower O
model O
correlation O
on O
the O
SINGLE O
group O
than O
the O
MULTI O
and O
the O
ANY O
groups O
would O
be O
a O
sign O
of O
model O
reporting O
bias O
. O

Results O

The O
experimental O
results O
show O
that O
multimodal B-MethodName
models O
outperform O
text O
- O
only O
models O
, O
suggesting O
their O
advantage O
in O
capturing O
visual B-TaskName
commonsense I-TaskName
. O
However O
, O
all O
models O
are O
subject O
to O
the O
influence O
of O
reporting O
bias O
, O
as O
they O
correlate B-MetricName
better O
with O
the O
distributions O
from O
Wikipedia B-DatasetName
than O
those O
from O
CoDa B-DatasetName
Table O
3 O
: O
Spearman B-MetricName
correlation I-MetricName
and O
top-1 B-MetricName
accuracy I-MetricName
( O
both O
× O
100 O
) O
of O
zero O
shot O
probing O
, O
before O
and O
after O
soft O
prompt O
tuning O
( O
" O
N O
" O
and O
" O
Y O
" O
for O
the O
" O
Tune O
" O
column O
) O
. O
This O
is O
the O
" O
average O
template O
" O
case O
where O
the O
output O
distribution O
is O
the O
mean O
of O
distributions O
across O
all O
templates O
. O
The O
Spearman B-MetricName
correlation I-MetricName
reported O
is O
the O
mean O
across O
all O
subjects O
± O
standard O
deviation O
, O
comparing O
the O
output O
distribution O
and O
the O
Visual O
Genome O
distribution O
. O
The O
subscripts O
b O
and O
l O
indicate O
the O
size O
of O
the O
model O
, O
and O
Distilled B-MethodName
is O
the O
BERT B-MethodName
model O
after O
distilling O
from O
Oscar B-MethodName
. O
Asterisk O
indicates O
where O
there O
is O
no O
significant O
difference O
between O
BERT B-MethodName
b O
and O
Oscar B-MethodName
b O
( O
t O
- O
test O
p O
- O
value O
> O
0.05 O
) O
. O

and O
ViComTe B-DatasetName
. O
Prompt O
tuning O
and O
knowledge O
distillation O
substantially O
enhance O
model O
performance O
, O
while O
increasing O
model O
size O
does O
not O
. O

Color O
, O
Shape O
, O
Material O
The O
resulting O
model O
performance O
for O
the O
" O
average O
template O
" O
mode O
is O
shown O
in O
Table O
3 O
. O
Prompt O
tuning O
is O
done O
in O
this O
mode O
only O
. O
Note O
that O
because O
the O
top-1 B-MetricName
accuracy I-MetricName
is O
taken O
among O
all O
possible O
classes O
of O
each O
relation O
, O
it O
should O
be O
interpreted O
together O
with O
the O
number O
of O
classes O
( O
Table O
1 O
) O
. O

We O
can O
see O
from O
Table O
3 O
that O
Oscar B-MethodName
does O
better O
than O
BERT B-MethodName
in O
almost O
all O
cases O
. O
Significant O
difference O
between O
Oscar B-MethodName
( O
base O
) O
and O
BERT B-MethodName
( O
base O
) O
is O
seen O
in O
most O
cases O
. O
Also O
, O
after O
soft O
prompt O
tuning O
, O
both O
the O
Spearman B-MetricName
correlation I-MetricName
and O
the O
accuracy B-MetricName
substantially O
improved O
. O
Although O
there O
is O
considerable O
variation O
of O
the O
Spearman B-MetricName
correlations I-MetricName
, O
we O
find O
consistent O
improvement O
per O
example O
with O
both O
prompt B-HyperparameterName
tuning I-HyperparameterName
and O
multimodal B-HyperparameterName
pretraining I-HyperparameterName
( O
Appendix O
A.2 O
) O
. O

Table O
3 O
also O
shows O
that O
knowledge O
distillation O
helps O
improve O
the O
performance O
of O
BERT B-MethodName
in O
all O
cases O
, O
and O
the O
distilled O
model O
can O
sometimes O
even O
outperform O
the O
teacher O
model O
, O
Oscar B-MethodName
. O
Moreover O
, O
the O
large O
version O
of O
each O
model O
does O
not O
always O
outperform O
its O
base O
counterpart O
, O
suggesting O
that O
increasing O
the O
size O
of O
the O
model O
does O
not O
enhance O
the O
model O
's O
ability O
to O
understand O
visual B-TaskName
commonsense I-TaskName
. O
Instead O
, O
training O
with O
visually O
grounded O
data O
does O
. O

Fig O
. O
2 O
illustrates O
the O
Spearman B-MetricName
correlations I-MetricName
of O
different O
models O
with O
the O
color O
distributions O
from O
CoDa B-DatasetName
, O
ViComTe B-DatasetName
and O
Wikipedia B-DatasetName
, O
under O
the O
" O
best O
template O
" O
mode O
. O
7 O
All O
models O
correlate O
moderately O
Table O
5 O
: O
Per B-MetricName
- I-MetricName
group I-MetricName
Spearman I-MetricName
correlation I-MetricName
and O
top-1 B-MetricName
accuracy I-MetricName
( O
both O
× O
100 O
) O
with O
a O
logistic O
regression O
head O
on O
model O
encoder O
outputs O
. O
Note O
that O
the O
ANY O
group O
for O
shape O
only O
has O
one O
example O
, O
so O
the O
accuracy O
is O
less O
meaningful O
and O
is O
omitted O
. O
All O
models O
have O
higher O
correlations O
in O
the O
MULTI O
and O
ANY O
groups O
than O
the O
SINGLE O
group O
, O
which O
is O
a O
sign O
of O
reporting O
bias O
. O

Results O
with O
Classification O
Head O

Table O
4 O
shows O
the O
results O
of O
BERT B-MethodName
, O
CLIP B-MethodName
, O
and O
Oscar B-MethodName
when O
topped O
with O
a O
classification O
head O
. O
We O
observe O
that O
Oscar B-MethodName
and O
CLIP B-MethodName
achieve O
similar O
performance O
and O
both O
outperform O
BERT B-MethodName
. O
Note O
that O
, O
while O
Visual O
Genome O
is O
part O
of O
Oscar B-MethodName
's O
pretraining O
corpus O
and O
one O
might O
suspect O
that O
that O
gives O
it O
an O
advantage O
, O
CLIP B-MethodName
is O
trained O
on O
a O
large O
corpus O
from O
web O
search O
that O
is O
unrelated O
to O
Visual O
Genome O
. O
Therefore O
, O
we O
can O
conclude O
that O
multimodal O
models O
pretrained O
on O
both O
images O
and O
text O
outperform O
text O
- O
only O
models O
. O
Table O
5 O
breaks O
down O
the O
results O
in O
Table O
4 O
into O
three O
subject O
groups O
. O
Oscar B-MethodName
and O
CLIP B-MethodName
outperform O
BERT B-MethodName
in O
almost O
all O
cases O
. O
The O
top-1 O
accuracy O
is O
higher O
for O
the O
SINGLE O
group O
than O
for O
the O
MULTI O
and O
ANY O
groups O
, O
perhaps O
because O
the O
SINGLE O
group O
subjects O
have O
only O
one O
most O
likely O
target O
attribute O
, O
which O
may O
be O
easier O
to O
predict O
. O
Note O
that O
the O
Spearman O
correlations O
for O
all O
three O
models O
become O
higher O
from O
group O
SINGLE O
to O
MULTI O
to O
ANY O
. O
Paik O
et O
al O
. O
( O
2021 O
) O
argue O
that O
higher O
correlation O
for O
the O
ANY O
and O
MULTI O
groups O
is O
a O
sign O
of O
model O
reporting O
bias O
, O
as O
objects O
in O
those O
two O
groups O
are O
more O
often O
reported O
. O
Thus O
, O
the O
results O
here O
indicate O
that O
reporting O
bias O
is O
still O
present O
in O
multimodal O
models O
. O

Results O
: O
Size O
Relation O

Table O
6 O
shows O
results O
of O
the O
rank B-HyperparameterName
partition I-HyperparameterName
method O
( O
Section O
4.3 O
) O
, O
before O
and O
after O
prompt B-HyperparameterName
tuning I-HyperparameterName
. O
Sur- O
prisingly O
, O
prompt O
tuning O
does O
not O
help O
in O
this O
case O
. O
Moreover O
, O
the O
performance O
for O
the O
" O
larger O
" O
templates O
is O
higher O
than O
that O
of O
the O
" O
smaller O
" O
templates O
, O
suggesting O
that O
the O
models O
contain O
inherent O
preference O
towards O
the O
" O
larger O
" O
templates O
. O
Fig O
. O
3 O
shows O
the O
results O
of O
the O
adjective O
projection O
method O
. O
8 O
For O
BERT B-MethodName
and O
Oscar B-MethodName
, O
we O
use O
the O
average O
embedding O
of O
the O
subword O
tokens O
of O
the O
nouns O
projected O
onto O
that O
of O
the O
adjectives O
" O
large O
" O
and O
" O
small O
" O
. O
For O
CLIP B-MethodName
, O
we O
take O
the O
textual O
encoder O
outputs O
as O
the O
embeddings O
, O
resulting O
in O
a O
different O
score O
range O
from O
that O
of O
BERT B-MethodName
and O
Oscar B-MethodName
. O
The O
results O
show O
the O
following O
trend O
: O
larger O
objects O
are O
projected O
onto O
the O
" O
large O
" O
end O
of O
the O
spectrum O
, O
although O
the O
trend O
is O
sometimes O
broken O
towards O
the O
" O
huge O
" O
end O
. O
This O
may O
be O
due O
to O
the O
" O
huge O
" O
group O
including O
nouns O
such O
as O
" O
pool O
" O
and O
" O
house O
" O
which O
can O
be O
modified O
by O
a O
relative O
size O
indicator O
" O
small O
" O
. O

Analysis O
and O
Limitations O

In O
Table O
3 O
, O
the O
accuracy O
of O
BERT B-MethodName
for O
shape O
is O
particularly O
low O
( O
only O
6.7 O
% O
) O
, O
despite O
that O
shape O
has O
only O
12 O
classes O
. O
We O
hypothesize O
that O
this O
is O
due O
to O
reporting O
bias O
on O
shape O
in O
the O
text O
corpora O
that O
BERT B-MethodName
is O
trained O
on O
. O
This O
hypothesis O
is O
supported O
by O
mining O
sentences O
from O
Wikipedia B-DatasetName
that O
contain O
( O
noun O
, O
attribute O
) O
pairs O
, O
where O
we O
see O
that O
the O
relation O
shape O
has O
fewer O
number O
of O
occurrences O
than O
material O
and O
color O
( O
Appendix O
A.3 O
) O
. O
We O
also O
investigate O
whether O
the O
advantage O
of O
the O
visually O
- O
grounded O
models O
over O
pure O
- O
language O
models O
comes O
from O
the O
domain O
difference O
between O
web O
corpora O
and O
image O
captions O
, O
or O
the O
presence O
of O
actual O
visual O
input O
. O
Although O
its O
teacher O
is O
trained O
with O
visual O
inputs O
, O
the O
Distilled B-MethodName
model O
is O
trained O
only O
on O
captions O
data O
and O
its O
performance O
matches O
that O
of O
Oscar B-MethodName
, O
so O
we O
hypothesize O
that O
grounded O
training O
data O
enhance O
models O
' O
ability O
to O
capture O
visual O
commonsense O
. O
The O
CaptionBERT B-MethodName
results O
support O
the O
hypothesis O
in O
favor O
of O
domain O
difference O
, O
since O
it O
performs O
better O
than O
BERT B-MethodName
in O
both O
CoDa B-DatasetName
and O
VG O
( O
Fig O
. O
2 O
) O
. O
Nevertheless O
, O
the O
visual O
inputs O
also O
have O
an O
effect O
, O
as O
Oscar B-MethodName
has O
a O
higher O
correlation O
than O
CaptionBERT B-MethodName
on O
CoDa B-DatasetName
. O
Thus O
, O
it O
seems O
that O
both O
domain O
and O
modality O
affect O
the O
ultimate O
model O
performance O
. O

Finally O
, O
although O
multimodal B-MethodName
models O
show O
improvement O
on O
the O
task O
, O
sometimes O
the O
improvement O
is O
not O
significant O
and O
the O
resulting O
correlations B-MetricName
are O
still O
weak O
. O
Further O
work O
is O
needed O
to O
enhance O
the O
visual O
commonsense O
abilities O
of O
the O
models O
and O
mitigate O
reporting O
bias O
, O
and O
our O
datasets O
can O
serve O
as O
an O
evaluation O
method O
. O

Conclusion O

In O
this O
paper O
, O
we O
probe O
knowledge O
about O
visually O
salient O
properties O
from O
pretrained O
neural O
networks O
. O
We O
automatically O
extract O
dataset O
of O
five O
visual O
relations O
: O
color O
, O
shape O
, O
material O
, O
size O
, O
and O
cooccurrence O
, O
and O
show O
that O
our O
ViComTe B-DatasetName
dataset O
has O
a O
much O
higher O
correlation B-MetricName
with O
human O
perception O
data O
for O
color O
than O
data O
mined O
from O
Wikipedia B-DatasetName
. O
We O
then O
apply O
several O
probing O
techniques O
and O
discover O
that O
visually O
- O
supervised O
models O
perform O
better O
than O
pure O
language O
models O
, O
which O
indicates O
that O
they O
can O
better O
capture O
such O
visual O
properties O
. O
Distilling O
the O
knowledge O
from O
a O
visually O
- O
supervised O
model O
into O
a O
pure O
language O
model O
results O
in O
comparable O
performance O
with O
the O
teacher O
model O
. O

We O
also O
observe O
less O
reporting O
bias O
in O
both O
visually O
- O
grounded O
text O
( O
VG O
- O
mined O
datasets O
) O
than O
Wikipedia O
text O
and O
visually O
- O
grounded O
models O
( O
Oscar B-MethodName
, O
DistilledOscar B-MethodName
, O
VisualBERT B-MethodName
, O
and O
CLIP B-MethodName
) O
than O
pure O
language O
models O
. O
However O
, O
visuallygrounded O
models O
are O
still O
subject O
to O
the O
influence O
of O
reporting O
bias O
, O
as O
seen O
in O
the O
per O
- O
group O
analysis O
, O
where O
both O
types O
of O
models O
perform O
better O
for O
the O
MULTI O
group O
than O
the O
SINGLE O
group O
. O

A O
Appendix O

A.1 O
List O
of O
Objects O

Table O
7 O
shows O
the O
list O
of O
all O
possible O
attributes O
for O
relations O
color O
, O
shape O
, O
and O
material O
. O
Table O
8 O
shows O
the O
list O
of O
objects O
in O
the O
five O
categories O
of O
relation O
size O
. O
Visual O
co O
- O
ocurrence O
has O
a O
large O
number O
of O
objects O
that O
are O
not O
listed O
here O
for O
space O
reasons O
. O

A.2 O
Additional O
Probing O

Best O
template O
mode O
Table O
9 O
contains O
zero O
- O
shot O
results O
under O
the O
" O
best O
template O
" O
mode O
, O
for O
BERT B-MethodName
( O
base O
) O
, O
Oscar B-MethodName
( O
base O
) O
, O
BERT B-MethodName
distilled O
from O
Oscar B-MethodName
, O
RoBERTa B-MethodName
( O
base O
) O
, O
ALBERT B-MethodName
( O
base O
) O
, O
Vokenization B-MethodName
, O
and O
VisualBERT B-MethodName
( O
base O
) O
. O
These O
results O
demonstrate O
similar O
trends O
as O
the O
ones O
in O
the O
" O
average O
template O
" O
mode O
. O
Per O
- O
object O
analysis O
Fig O
. O
4 O
illustrates O
the O
finegrained O
Spearman B-MetricName
correlation I-MetricName
± O
standard O
deviation O
per O
object O
group O
for O
BERT B-MethodName
and O
CLIP O
. O
Size O
per O
- O
object O
Fig O
. O
5 O
shows O
how O
the O
per O
- O
object O
projection O
scores O
on O
the O
size O
spectrum O
from O
BERT B-MethodName
and O
Oscar O
are O
correlated O
. O
Per O
- O
Subject O
Comparison O
Fig O
. O
6 O
and O
Fig O
. O
7 O
show O
how O
the O
Spearman B-MetricName
correlations I-MetricName
of O
10 O
individual O
subjects O
improve O
after O
soft O
prompt O
tuning O
and O
after O
multimodal O
pretraining O
. O
Consistent O
improvement O
can O
be O
seen O
in O
color O
, O
material O
, O
and O
cooccurrence O
. O
Although O
we O
report O
average O
Spearman B-MetricName
correlations I-MetricName
in O
Table O
3 O
and O
there O
are O
large O
standard O
deviations O
, O
here O
we O
show O
that O
when O
improvement O
is O
observed O
collectively O
, O
it O
is O
also O
consistent O
across O
subjects O
. O
With O
shape O
, O
the O
improvement O
is O
less O
obvious O
( O
45.9 B-MetricValue
to O
50.4 B-MetricValue
for O
prompt B-HyperparameterName
tuning I-HyperparameterName
and O
49.2 B-MetricValue
to O
50.4 B-MetricValue
for O
multimodal B-HyperparameterName
pretraining I-HyperparameterName
) O
. O

A.3 O
Error O
Analysis O

Data O
The O
three O
subjects O
with O
the O
highest O
and O
lowest O
Spearman B-MetricName
correlation I-MetricName
are O
shown O
in O
Fig O
. O
8 O
and O
Fig O
. O
9 O
. O

Wikipedia B-DatasetName
Table O
10 O
shows O
the O
number O
of O
( O
noun O
, O
attribute O
) O
pairs O
of O
the O
three O
relation O
types O
in O
Wikipedia B-DatasetName
. O
Shape O
has O
fewer O
occurrences O
than O
material O
and O
color O
. O
Model O
Table O
11 O
shows O
the O
errors O
made O
by O
BERT B-MethodName
and O
Oscar B-MethodName
in O
the O
" O
average O
template O
" O
mode O
before O
prompt O
tuning O
. O
Overall O
, O
subjects O
with O
low O
correlation O
are O
those O
that O
are O
less O
often O
reported O
in O
Visual O
Genome O
as O
well O
as O
in O
textual O
data O
. O

Acknowledgments O

We O
would O
like O
to O
thank O
the O
reviewers O
for O
their O
comments O
and O
suggestions O
. O
Chenyu O
Zhang O
is O
supported O
by O
the O
Pistritto O
Research O
Fellowship O
. O
Elias O
Stengel O
- O
Eskin O
is O
supported O
by O
an O
NSF O
Graduate O
Research O
Fellowship O
. O
Zhuowan O
Li O
is O
supported O
by O
NSF O
1763705 O
. O

No O
clues O
, O
good O
clues O
: O
Out O
of O
context O
Lexical B-TaskName
Relation I-TaskName
Classification I-TaskName

The O
accurate O
prediction B-TaskName
of I-TaskName
lexical I-TaskName
relations I-TaskName
between O
words O
is O
a O
challenging O
task O
in O
Natural O
Language O
Processing O
( O
NLP O
) O
. O
The O
most O
recent O
advances O
in O
this O
direction O
come O
with O
the O
use O
of O
pre B-MethodName
- I-MethodName
trained I-MethodName
language I-MethodName
models I-MethodName
( O
PTLMs B-MethodName
) O
. O
A O
PTLM B-MethodName
typically O
needs O
" O
well O
- O
formed O
" O
verbalized O
text O
to O
interact O
with O
it O
, O
either O
to O
fine O
- O
tune O
it O
or O
to O
exploit O
it O
. O
However O
, O
there O
are O
indications O
that O
commonly O
used O
PTLMs B-MethodName
already O
encode O
enough O
linguistic O
knowledge O
to O
allow O
the O
use O
of O
minimal O
( O
or O
none O
) O
textual O
context O
for O
some O
linguistically O
motivated O
tasks O
, O
thus O
notably O
reducing O
human O
effort O
, O
the O
need O
for O
data O
pre O
- O
processing O
, O
and O
favoring O
techniques O
that O
are O
language O
neutral O
since O
do O
not O
rely O
on O
syntactic O
structures O
. O
In O
this O
work O
, O
we O
explore O
this O
idea O
for O
the O
tasks O
of O
lexical B-TaskName
relation I-TaskName
classification I-TaskName
( O
LRC B-TaskName
) O
and O
graded B-TaskName
Lexical I-TaskName
Entailment I-TaskName
( O
LE B-TaskName
) O
. O
After O
finetuning O
PTLMs B-MethodName
for O
LRC B-TaskName
with O
different O
verbalizations O
, O
our O
evaluation O
results O
show O
that O
very O
simple O
prompts O
are O
competitive O
for O
LRC B-TaskName
and O
significantly O
outperform O
graded B-TaskName
LE I-TaskName
SoTA O
. O
In O
order O
to O
gain O
a O
better O
insight O
into O
this O
phenomenon O
, O
we O
perform O
a O
number O
of O
quantitative O
statistical O
analyses O
on O
the O
results O
, O
as O
well O
as O
a O
qualitative O
visual O
exploration O
based O
on O
embedding O
projections O
. O

Introduction O

Lexical B-TaskName
Relation I-TaskName
Classification I-TaskName
( O
LRC B-TaskName
) O
is O
the O
task O
of O
predicting O
which O
lexical O
relation O
exists O
between O
two O
given O
words O
( O
e.g. O
, O
' O
tall O
' O
and O
' O
small O
' O
are O
related O
by O
the O
antonymy O
relation O
) O
, O
from O
a O
finite O
catalogue O
of O
lexical O
relations O
. O
Discovering O
lexico O
- O
semantic O
relations O
between O
words O
has O
received O
attention O
in O
the O
NLP O
community O
since O
Hearst O
's O
seminal O
research O
in O
1992 O
on O
the O
automatic O
acquisition O
of O
hyponyms O
from O
large O
text O
corpora O
based O
on O
pre O
- O
designed O
patterns O
( O
Hearst O
, O
1992 O
) O
. O
Despite O
many O
recent O
advancements O
, O
LRC B-TaskName
continues O
to O
be O
an O
open O
research O
topic O
in O
the O
NLP O
field O
( O
Wang O
et O
al O
. O
, O
2021 O
; O
Ushio O
et O
al O
. O
, O
2021 O
) O
. O
Applications O
of O
the O
task O
are O
numerous O
: O
automatic O
thesauri O
creation O
, O
paraphrasing O
, O
textual O
entailment O
, O
sentiment O
analysis O
, O
ontology O
learning O
, O
and O
ontology O
population O
, O
among O
others O
( O
Weeds O
et O
al O
. O
, O
2014 O
; O
Cimiano O
, O
2006 O
) O
. O

The O
most O
recent O
advances O
in O
LRC B-TaskName
come O
with O
the O
use O
of O
pre B-MethodName
- I-MethodName
trained I-MethodName
language I-MethodName
models I-MethodName
( O
PTLMs B-MethodName
) O
based O
on O
the O
transformers B-MethodName
architecture O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
, O
which O
have O
been O
proven O
to O
capture O
a O
large O
amount O
of O
lexico O
- O
semantic O
knowledge O
from O
text O
successfully O
. O
One O
of O
the O
main O
benefits O
of O
the O
adoption O
of O
PLTMs B-MethodName
is O
that O
, O
while O
they O
were O
trained O
for O
a O
general O
task O
( O
text O
generation O
) O
following O
a O
masked O
language O
model O
( O
MLM O
) O
objective O
in O
an O
unsupervised O
way O
, O
they O
can O
be O
easily O
adapted O
to O
different O
downstream O
tasks O
( O
e.g. O
, O
text O
classification O
, O
text O
summarization O
, O
sentiment O
analysis O
) O
by O
introducing O
additional O
parameters O
and O
fine O
- O
tuning O
them O
using O
objective O
functions O
specific O
to O
the O
task O
. O
That O
avoids O
the O
need O
to O
train O
the O
model O
from O
scratch O
, O
still O
obtaining O
SoTA O
results O
, O
while O
decreasing O
computational O
costs O
and O
the O
need O
for O
very O
large O
amounts O
of O
data O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
. O

More O
recently O
, O
the O
" O
pre O
- O
train O
, O
fine O
- O
tune O
" O
procedure O
is O
shifting O
in O
NLP O
tasks O
towards O
the O
" O
pre O
- O
train O
, O
prompt O
, O
and O
predict O
" O
paradigm O
( O
Liu O
et O
al O
. O
, O
2023 O
) O
. O
In O
that O
case O
, O
instead O
of O
adapting O
PTLMs B-MethodName
to O
the O
downstream O
task O
via O
fine O
- O
tuning O
, O
the O
task O
is O
reformulated O
to O
look O
more O
like O
those O
solved O
during O
the O
original O
model O
training O
with O
the O
help O
of O
a O
textual O
prompt O
. O
Following O
the O
example O
in O
( O
Liu O
et O
al O
. O
, O
2023 O
) O
, O
when O
recognizing O
the O
emotion O
of O
a O
sentence O
, O
" O
I O
missed O
the O
bus O
today O
. O
" O
, O
we O
may O
continue O
with O
a O
prompt O
" O
I O
felt O
very O
" O
, O
and O
ask O
the O
PTLM B-MethodName
to O
fill O
the O
blank O
with O
an O
emotion O
- O
bearing O
word O
. O

A O
PTLM B-MethodName
typically O
needs O
" O
well O
- O
formed O
" O
verbalized O
text O
to O
interact O
with O
it O
, O
either O
to O
fine O
- O
tune O
it O
or O
to O
exploit O
it O
via O
prompt O
engineering O
. O
While O
some O
authors O
claim O
that O
longer O
, O
more O
complex O
verbalizations O
of O
the O
input O
data O
work O
best O
for O
real O
- O
world O
text B-TaskName
classification I-TaskName
tasks O
( O
Schick O
and O
Schütze O
, O
2022 O
) O
, O
or O
relation O
classification O
( O
Bouraoui O
et O
al O
. O
, O
2020 O
) O
, O
other O
authors O
( O
LoganIV O
et O
al O
. O
, O
2022 O
) O
have O
collected O
indications O
in O
the O
opposite O
direction O
for O
a O
wide O
range O
of O
NLP O
tasks O
( O
such O
as O
paraphrasing O
, O
textual O
similarity O
, O
or O
sentiment O
analysis O
) O
. O

We O
share O
the O
hypothesis O
that O
commonly O
used O
PTLMs B-MethodName
already O
encode O
enough O
linguistic O
knowledge O
to O
allow O
the O
use O
of O
minimal O
( O
or O
none O
) O
textual O
context O
for O
some O
linguistically O
motivated O
tasks O
. O
In O
such O
cases O
, O
very O
simple O
prompts O
work O
almost O
as O
well O
or O
even O
better O
than O
hand O
- O
crafted O
, O
more O
complex O
verbalizations O
. O
Reducing O
the O
need O
of O
complex O
prompting O
notably O
reduces O
the O
need O
of O
human O
effort O
and O
the O
need O
for O
data O
pre O
- O
processing O
, O
and O
favors O
techniques O
that O
are O
language O
neutral O
since O
they O
do O
not O
rely O
on O
syntactic O
structures O
. O

In O
this O
work O
1 O
, O
we O
explore O
this O
idea O
for O
the O
LRC B-TaskName
task I-TaskName
, O
and O
we O
extend O
it O
to O
graded B-TaskName
lexical I-TaskName
entailment I-TaskName
( O
LE B-TaskName
) O
, O
i.e. O
, O
discovering O
the O
strength O
of O
the O
taxonomical O
asymmetric O
hyponymy O
- O
hypernymy O
relation O
between O
two O
words O
( O
Vulić O
et O
al O
. O
, O
2017 O
) O
. O
In O
previous O
works O
, O
other O
authors O
have O
explored O
complex O
verbalizations O
for O
LRC B-TaskName
( O
Ushio O
et O
al O
. O
, O
2021 O
) O
while O
others O
have O
essayed O
shorter O
ones O
( O
Wachowiak O
et O
al O
. O
, O
2020 O
) O
. O
However O
, O
there O
has O
been O
no O
systematic O
study O
on O
the O
impact O
of O
long O
/ O
short O
prompting O
for O
LRC B-TaskName
so O
far O
. O
To O
that O
end O
, O
we O
have O
experimented O
with O
different O
verbalizations O
of O
the O
training O
and O
test O
data O
in O
an O
LRC B-TaskName
experiment O
. O
Then O
, O
we O
analysed O
which O
verbalization O
produces O
better O
predictions O
for O
at O
least O
one O
of O
the O
lexico O
- O
semantic O
relations O
entailed O
between O
a O
pair O
of O
words O
. O
We O
experiment O
with O
widely O
used O
benchmarks O
for O
LRC B-TaskName
namely O
, O
CogALexV B-DatasetName
( O
Santus O
et O
al O
. O
, O
2016a O
) O
, O
BLESS B-DatasetName
( O
Baroni O
and O
Lenci O
, O
2011 O
) O
, O
EVALution B-DatasetName
( O
Santus O
et O
al O
. O
, O
2015 O
) O
, O
K B-DatasetName
& I-DatasetName
H+N I-DatasetName
( O
Necsulescu O
et O
al O
. O
, O
2015 O
) O
, O
and O
ROOT9 B-DatasetName
( O
Santus O
et O
al O
. O
, O
2016b O
) O
. O
Besides O
, O
we O
evaluate O
such O
models O
with O
the O
Hyperlex B-DatasetName
( O
Vulić O
et O
al O
. O
, O
2017 O
) O
dataset O
for O
graded B-TaskName
LE I-TaskName
. O

Our O
main O
contributions O
are O
: O

1 O
. O
We O
show O
empirically O
that O
SoTA O
results O
for O
LRC B-TaskName
can O
be O
reached O
by O
providing O
very O
simple O
verbalizations O
of O
the O
data O
or O
even O
no O
verbalization O
at O
all O
( O
null O
prompting O
) O
when O
fine O
- O
tuning O
and O
testing O
a O
PTLM B-MethodName
. O

2 O
. O
We O
test O
the O
generalizability O
of O
such O
models O
trained O
with O
minimal O
prompting O
to O
similar O
tasks O
by O
testing O
them O
in O
graded B-TaskName
LE I-TaskName
, O
where O
they O
outperform O
SoTA O
results O
. O

3 O
. O
We O
provide O
an O
extensive O
analysis O
of O
the O
results O
( O
including O
error O
analysis O
) O
to O
further O
observe O
the O
strengths O
and O
limitations O
of O
minimal O
prompting O
for O
LRC B-TaskName
. O

4 O
. O
To O
further O
understand O
the O
models O
' O
behaviour O
, O
we O
add O
a O
qualitative O
analysis O
of O
their O
learning O
process O
based O
on O
the O
visualisation O
of O
the O
embeddings O
that O
are O
built O
in O
their O
different O
layers O
. O

Our O
paper O
is O
structured O
as O
follows O
: O
first O
, O
in O
Section O
3 O
, O
we O
formally O
describe O
both O
the O
LRC B-TaskName
task O
and O
the O
LE B-TaskName
task O
. O
Secondly O
, O
in O
Section O
4 O
, O
we O
describe O
the O
chosen O
templates O
for O
the O
input O
verbalizations O
, O
the O
used O
datasets O
and O
baselines O
we O
compare O
with O
, O
as O
well O
as O
the O
hyper O
parameter O
and O
fine O
- O
tuning O
setting O
of O
our O
models O
. O
Then O
, O
in O
Section O
5 O
, O
we O
analyze O
our O
results O
showing O
: O
a O
) O
our O
quantitative O
results O
, O
analyzing O
which O
template O
, O
model O
, O
and O
method O
work O
best O
on O
each O
dataset O
, O
b O
) O
the O
error O
analysis O
, O
checking O
how O
the O
distribution O
and O
linguistic O
characteristics O
of O
the O
different O
datasets O
affected O
the O
performance O
of O
our O
models O
and O
what O
examples O
and O
categories O
were O
the O
most O
difficult O
ones O
, O
and O
c O
) O
a O
visualization O
of O
the O
embedding O
projection O
, O
highlighting O
which O
layers O
are O
more O
informative O
for O
relation O
classification O
and O
how O
the O
model O
learns O
them O
through O
the O
different O
epochs O
. O
Finally O
, O
in O
Section O
6 O
, O
we O
summarize O
the O
conclusions O
and O
possible O
future O
work O
, O
stating O
the O
limitations O
of O
our O
work O
. O

Related O
Work O

In O
this O
section O
we O
give O
an O
overview O
of O
some O
related O
approaches O
that O
are O
relevant O
to O
our O
work O
. O

Prompt O
- O
based O
Learning O

In O
their O
extensive O
review O
, O
Liu O
et O
al O
. O
( O
2023 O
) O
have O
analyzed O
the O
prompt O
- O
based O
learning O
paradigm O
, O
exploring O
different O
verbalization O
techniques O
used O
to O
input O
text O
to O
PTLMs B-MethodName
, O
as O
a O
key O
point O
to O
reach O
SoTA O
results O
in O
few O
and O
zero O
- O
shot O
learning O
scenarios O
. O
The O
currently O
under O
research O
question O
is O
: O
what O
kind O
of O
verbalizations O
work O
better O
? O
Here O
, O
two O
different O
trends O
arise O
: O
a O
) O
automatically O
searched O
prompts O
( O
Shin O
et O
al O
. O
, O
2020 O
; O
Liu O
et O
al O
. O
, O
2022 O
; O
Li O
and O
Liang O
) O
and O
b O
) O
handcrafted O
prompts O
Schütze O
, O
2021 O
, O
2022 O
) O
. O
The O
main O
drawback O
of O
the O
first O
one O
is O
the O
necessity O
of O
additional O
training O
and O
computational O
resources O
to O
find O
the O
best O
prompt O
, O
and O
the O
second O
's O
major O
issue O
is O
the O
necessity O
of O
manual O
effort O
( O
Lo O
- O
ganIV O
et O
al O
. O
, O
2022 O
; O
Mahabadi O
et O
al O
. O
, O
2022 O
) O
. O
A O
third O
option O
is O
however O
possible O
: O
null O
prompts O
( O
LoganIV O
et O
al O
. O
, O
2022 O
) O
where O
the O
mask O
token O
is O
simply O
added O
to O
the O
input O
sentence O
. O

Currently O
, O
no O
consensus O
has O
been O
reached O
on O
which O
kind O
of O
verbalizations O
work O
best O
, O
and O
, O
while O
authors O
such O
as O
Schick O
and O
Schütze O
( O
2022 O
) O
obtain O
the O
best O
results O
in O
a O
variety O
of O
NLP O
tasks O
with O
handcrafted O
verbalizations O
, O
others O
( O
LoganIV O
et O
al O
. O
, O
2022 O
; O
Mahabadi O
et O
al O
. O
, O
2022 O
) O
defend O
the O
advantages O
of O
short O
or O
even O
null O
prompts O
while O
still O
achieving O
competitive O
results O
. O
Liu O
et O
al O
. O
( O
2022 O
) O
found O
different O
behavior O
for O
their O
Ptuning O
- O
v2 O
method O
depending O
on O
the O
task O
: O
simple O
classification B-TaskName
tasks O
prefer O
shorter O
prompts O
, O
while O
hard O
sequence O
labeling O
tasks O
prefer O
longer O
ones O
. O

Other O
open O
questions O
about O
prompting O
rely O
on O
the O
selection O
of O
the O
label O
to O
verbalize O
the O
mask O
and O
the O
order O
in O
which O
the O
mask O
and O
input O
are O
provided O
. O
Labels O
given O
in O
benchmark O
datasets O
are O
often O
multiword O
or O
rare O
expressions O
consisting O
of O
more O
than O
one O
token O
, O
however O
, O
the O
mask O
needs O
to O
be O
filled O
by O
just O
one O
token O
( O
Schick O
and O
Schütze O
, O
2022 O
) O
thus O
there O
is O
a O
need O
to O
select O
the O
label O
either O
automatically O
or O
manually O
. O
The O
order O
in O
which O
input O
and O
mask O
are O
entered O
is O
also O
under O
current O
research O
( O
Mahabadi O
et O
al O
. O
, O
2022 O
) O
. O

Previous O
comparisons O
of O
different O
prompting O
techniques O
have O
been O
mostly O
applied O
to O
highly O
context O
- O
dependent O
NLP O
tasks O
such O
as O
sentiment O
analysis O
, O
subjectivity O
, O
classification B-TaskName
, O
question O
classification O
, O
natural O
language O
inference O
, O
question O
answering O
, O
word O
sense O
disambiguation O
or O
paraphrasing O
( O
LoganIV O
et O
al O
. O
, O
2022 O
; O
Schick O
and O
Schütze O
, O
2022 O
; O
Mahabadi O
et O
al O
. O
, O
2022 O
) O
were O
the O
input O
example O
already O
consists O
of O
a O
well O
- O
formed O
sentence O
. O
Yet O
, O
other O
NLP O
tasks O
that O
are O
less O
context O
- O
sensitive O
such O
as O
LRC B-TaskName
, O
Relation O
Extraction O
, O
or O
Lexical B-TaskName
Entailment I-TaskName
, O
have O
received O
little O
or O
no O
attention O
so O
far O
in O
prompt O
comparison O
studies O
. O

Lexical B-TaskName
Relation I-TaskName
Classification I-TaskName

Seminal O
work O
on O
LRC B-TaskName
started O
exploring O
patternbased O
techniques O
( O
Hearst O
, O
1992 O
) O
, O
where O
a O
set O
of O
patterns O
that O
elicit O
the O
relation O
entailed O
between O
a O
pair O
of O
words O
is O
defined O
. O
A O
drawback O
of O
this O
method O
is O
that O
not O
all O
lexical O
relations O
are O
explicit O
in O
texts O
by O
a O
closed O
set O
of O
patterns O
. O
Then O
, O
the O
approach O
towards O
LRC B-TaskName
shifted O
to O
distributional O
semantics O
with O
static O
embeddings O
, O
meaning O
one O
vector O
is O
given O
to O
represent O
each O
word O
in O
the O
embeddings O
space O
( O
Weeds O
et O
al O
. O
, O
2014 O
; O
Santus O
et O
al O
. O
, O
2016a O
; O
Wang O
et O
al O
. O
, O
2019 O
; O
. O
Such O
techniques O
were O
found O
beneficial O
to O
LRC B-TaskName
tasks O
, O
in O
which O
words O
were O
normally O
provided O
without O
additional O
context O
( O
Barkan O
et O
al O
. O
, O
2020 O
) O
. O

Recent O
work O
in O
LRC B-TaskName
has O
focused O
on O
PTLMs B-MethodName
and O
their O
dynamic O
embeddings O
, O
owing O
to O
their O
capacity O
to O
better O
capture O
polysemy O
than O
static O
embeddings O
, O
which O
led O
to O
better O
results O
( O
Karmakar O
and O
McCrae O
, O
2020 O
; O
Ushio O
et O
al O
. O
, O
2021 O
; O
Wang O
et O
al O
. O
, O
2021 O
) O
. O
Such O
works O
have O
already O
used O
prompting O
to O
fine O
- O
tune O
PTLMs B-MethodName
. O
However O
, O
none O
of O
them O
has O
focused O
on O
analyzing O
what O
kind O
of O
verbalization O
can O
be O
better O
used O
to O
extract O
relation O
information O
, O
as O
we O
do O
. O
For O
instance O
, O
while O
in O
( O
Ushio O
et O
al O
. O
, O
2021 O
) O
the O
authors O
opted O
to O
use O
hand O
- O
crafted O
complex O
verbalizations O
motivated O
by O
previous O
research O
( O
Bouraoui O
et O
al O
. O
, O
2020 O
; O
Jiang O
et O
al O
. O
, O
2020 O
) O
, O
Wachowiak O
et O
al O
. O
( O
2020 O
) O
used O
minimal O
prompts O
, O
and O
in O
( O
Karmakar O
and O
Mc O
- O
Crae O
, O
2020 O
) O
null O
prompting O
was O
used O
. O

The O
focus O
of O
our O
work O
is O
comparing O
the O
verbalizations O
enumerated O
by O
Schick O
and O
Schütze O
( O
2022 O
) O
in O
their O
work O
: O
null O
- O
prompting O
, O
null O
- O
prompting O
with O
punctuation O
, O
short O
templates O
and O
long O
templates O
and O
see O
how O
they O
interact O
with O
a O
lexical O
- O
focused O
task O
when O
some O
artificial O
context O
( O
i.e. O
, O
not O
initially O
available O
in O
the O
dataset O
) O
is O
added O
to O
the O
prompt O
, O
versus O
when O
no O
context O
other O
than O
two O
words O
is O
provided O
( O
as O
in O
null O
prompting O
) O
. O

Problem O
Statement O

Let O
V O
= O
{ O
w O
1 O
, O
. O
. O
. O
, O
w O
n O
} O
be O
a O
set O
of O
words O
( O
our O
vocabulary O
) O
, O
and O
a O
sentence O
s O
be O
any O
finite O
sequence O
of O
words O
from O
V O
. O
The O
set O
of O
all O
sentences O
over O
V O
is O
denoted O
by O
S. O
Given O
a O
word O
w O
∈ O
V O
, O
a O
context O
c O
of O
w O
is O
any O
sentence O
such O
that O
w O
∈ O
c. O
The O
set O
of O
all O
contexts O
of O
a O
word O
w O
is O
denoted O
by O
C O
w O
. O

A O
binary O
relation O
r O
between O
words O
is O
a O
subset O
of O
V O
× O
V O
. O
Let O
us O
denote O
by O
R O
the O
set O
of O
all O
binary O
relations O
over O
the O
vocabulary O
V O
, O
that O
is O
, O
R O
is O
the O
power O
set O
of O
V O
× O
V O
. O
We O
say O
that O
a O
set O
of O
relations O
, O
R O
= O
{ O
r O
1 O
, O
. O
. O
. O
, O
r O
k O
} O
, O
where O
r O
i O
∈ O
R O
, O
is O
mutually O
exclusive O
if O
the O
relations O
in O
R O
are O
disjoint O
; O
and O
we O
say O
that O
R O
is O
complete O
if O
the O
union O
of O
the O
relations O
is O
equal O
to O
V O
× O
V O
. O
Note O
that O
we O
can O
make O
a O
relation O
set O
R O
complete O
by O
adding O
a O
relation O
named O
unknown O
, O
which O
is O
the O
complementary O
of O
all O
the O
relations O
in O
R O
. O

We O
consider O
that O
any O
context O
of O
two O
words O
induces O
a O
relation O
from O
a O
predefined O
set O
of O
relations O
, O
that O
is O
, O
there O
exists O
a O
function O
f O
R O
: O
P O
→ O
R O
, O
where O

P O
= O
{ O
c O
∈ O
S O
| O
c O
∈ O
C O
w O
1 O
∩ O
C O
w O
2 O
, O
w O
1 O
, O
w O
2 O
∈ O
V O
} O
. O

For O
instance O
, O
given O
the O
set O
of O
relations O
R O
= O
{ O
partOf O
, O
unknown O
} O
, O
the O
common O
context O
for O
the O
words O
bank O
and O
river O
, O
" O
I O
play O
by O
the O
bank O
of O
the O
river O
" O
, O
induces O
the O
relation O
partOf O
, O
while O
" O
I O
will O
deposit O
the O
money O
in O
the O
bank O
beside O
the O
river O
" O
would O
induce O
the O
unknown O
relation O
. O
Thus O
, O
Relation O
Classification O
( O
RC O
) O
is O
the O
task O
of O
using O
a O
functionf O
R O
that O
estimates O
f O
R O
. O

Lexical B-TaskName
Relation I-TaskName
Classification I-TaskName
( O
LRC B-TaskName
) O
is O
a O
subtype O
of O
RC O
where O
the O
relation O
between O
words O
is O
a O
lexical O
one O
. O
The O
most O
usual O
and O
important O
lexical O
relations O
are O
hyponymy O
, O
hyperonymy O
, O
antonymy O
, O
synonymy O
, O
and O
meronymy O
. O
Among O
these O
relations O
, O
hyponymy O
and O
, O
its O
counterpart O
, O
hyperonymy O
are O
especially O
important O
in O
NLP O
and O
ontology O
engineering O
. O

Finally O
, O
Lexical B-TaskName
Entailment I-TaskName
( O
LE B-TaskName
) O
is O
the O
task O
of O
detecting O
the O
hyponymy O
relationship O
between O
two O
words O
. O
This O
task O
becomes O
graded B-TaskName
LE I-TaskName
when O
we O
have O
to O
calculate O
the O
numerical O
degree O
to O
which O
a O
word O
w O
1 O
is O
a O
type O
of O
w O
2 O
, O
becoming O
a O
more O
challenging O
regression O
task O
. O

Experimental O
Setup O

The O
main O
goals O
of O
our O
experiments O
are O
: O
1 O
) O
to O
check O
if O
LRC B-TaskName
can O
be O
conducted O
without O
adding O
artificial O
context O
when O
just O
a O
pair O
of O
words O
out O
of O
context O
is O
given O
, O
2 O
) O
if O
so O
, O
to O
analyze O
which O
verbalization O
works O
best O
for O
model O
fine O
- O
tuning O
, O
and O
3 O
) O
to O
check O
the O
generalizability O
of O
our O
model O
to O
other O
languagerelated O
tasks O
such O
as O
graded B-TaskName
LE I-TaskName
. O

Chosen O
Verbalization O

Similarly O
to O
( O
Schick O
and O
Schütze O
, O
2022 O
) O
, O
we O
compare O
null O
prompts O
to O
punctuated O
ones O
( O
just O
the O
target O
and O
source O
words O
with O
added O
punctuation O
) O
, O
and O
a O
longer O
template O
( O
the O
best O
performing O
one O
in O
( O
Ushio O
et O
al O
. O
, O
2021 O
) O
) O
. O
The O
chosen O
mask O
order O
and O
wording O
placement O
in O
the O
verbalization O
is O
the O
best O
performing O
one O
in O
( O
Mahabadi O
et O
al O
. O
, O
2022 O
) O
, O
inserting O
the O
mask O
token O
between O
both O
words O
. O
Table O
1 O
presents O
our O
chosen O
prompts O
. O

We O
explore O
two O
different O
options O
: O
a O
) O
adopting O
a O
sentence O
classification O
scheme O
, O
where O
a O
classification O
layer O
is O
added O
on O
top O
of O
the O
output O
layer O
( O
templates O
T1 O
, O
T2 O
, O
T3 O
, O
and O
T4 O
) O
to O
classify O
the O
CLS O
( O
special O
classification O
token O
) O
that O
is O
added O
at O
the O
beginning O
of O
every O
template O
, O
and O
b O
) O
instantiating O
the O
task O
as O
a O
fill O
in O
the O
blank O
task O
( O
templates O
TM1 O
, O
TM2 O
, O
and O
TM3 O
) O
. O
We O
use O
T4 O
as O
a O
control O
case O
to O
check O
what O
happens O
when O
train O
and O
test O
templates O
are O
different O
. O

Datasets O
and O
Baselines O

LRC B-TaskName
We O
conducted O
experiments O
on O
five O
datasets O
2 O
: O
CogALexV B-DatasetName
( O
Santus O
et O
al O
. O
, O
2016a O
) O
, O
BLESS B-DatasetName
( O
Baroni O
and O
Lenci O
, O
2011 O
) O
, O
EVALution B-DatasetName
( O
Santus O
et O
al O
. O
, O
2015 O
) O
, O
K B-DatasetName
& I-DatasetName
H+N I-DatasetName
( O
Necsulescu O
et O
al O
. O
, O
2015 O
) O
, O
and O
ROOT9 B-DatasetName
( O
Santus O
et O
al O
. O
, O
2016b O
) O
. O
These O
datasets O
contain O
a O
variety O
of O
lexical O
relations O
, O
including O
hypernyms O
, O
meronyms O
, O
synonyms O
, O
antonyms O
, O
and O
random O
( O
equivalent O
to O
unknown O
relation O
defined O
in O
S3 O
) O
3 O
. O
For O
a O
deeper O
analysis O
( O
error O
analysis O
and O
visualization O
) O
, O
we O
focus O
on O
CogALexV B-DatasetName
as O
it O
contains O
a O
subset O
of O
the O
most O
complicated O
examples O
of O
EVALution B-DatasetName
. O
To O
compare O
the O
performance O
of O
the O
different O
verbalizations O
in O
PTLM B-MethodName
fine O
- O
tuning O
to O
SoTA O
methods O
, O
we O
selected O
the O
following O
baseline O
models O
: O
LexNet B-MethodName
, O
SphereRE B-MethodName
( O
Wang O
et O
al O
. O
, O
2019 O
) O
, O
KEML B-MethodName
( O
Wang O
et O
al O
. O
, O
2021 O
) O
, O
and O
RelBERT B-MethodName
( O
Ushio O
et O
al O
. O
, O
2021 O
) O
. O

Graded B-TaskName
LE I-TaskName
We O
use O
Hyperlex B-DatasetName
dataset O
( O
Vulić O
et O
al O
. O
, O
2017 O
) O
, O
which O
consists O
of O
2616 O
pairs O
of O
words O
( O
2163 O
nouns O
and O
453 O
verbs O
) O
. O
Each O
pair O
was O
presented O
to O
at O
least O
ten O
human O
annotators O
to O
answer O
the O
question O
To O
what O
degree O
X O
is O
a O
type O
of O
Y O
? O
rang O
- O
ing O
from O
0 O
to O
6 O
. O
The O
final O
given O
score O
for O
each O
pair O
is O
the O
median O
of O
the O
human O
annotations O
. O
The O
authors O
of O
Hyperlex B-DatasetName
provide O
an O
upper O
bound O
of O
the O
Inter B-MetricName
- I-MetricName
Annotator I-MetricName
Agreement I-MetricName
( O
IAA B-MetricName
) O
calculated O
as O
the O
average O
Spearman O
correlation O
of O
a O
human O
rater O
with O
the O
average O
of O
all O
the O
other O
raters O
; O
in O
particular O
, O
the O
annotation O
reaches O
an O
IAA B-MetricName
- I-MetricName
ρ I-MetricName
of O
0.864 B-MetricValue
( O
for O
nouns O
, O
IAA B-MetricName
- I-MetricName
ρ I-MetricName
= O
0.864 B-MetricValue
, O
and O
for O
verbs O
, O
IAAρ B-MetricName
= O
0.862 B-MetricValue
) O
. O
To O
train O
supervised O
systems O
, O
Hyperlex B-DatasetName
is O
split O
into O
train O
/ O
val O
/ O
test O
datasets O
in O
two O
configurations O
: O
a O
) O
random O
split O
: O
data O
are O
randomly O
split O
into O
1831 B-HyperparameterValue
/ I-HyperparameterValue
130 I-HyperparameterValue
/ I-HyperparameterValue
655 I-HyperparameterValue
train B-HyperparameterName
/ I-HyperparameterName
val I-HyperparameterName
/ I-HyperparameterName
test I-HyperparameterName
pairs O
, O
respectively O
( O
all O
the O
words O
in O
the O
test O
split O
appear O
in O
the O
train O
/ O
val O
splits O
) O
; O
b O
) O
lexical O
split O
: O
to O
avoid O
lexical O
memorization O
, O
words O
in O
the O
test O
split O
are O
forced O
not O
to O
appear O
in O
the O
train B-HyperparameterName
/ I-HyperparameterName
val I-HyperparameterName
splits O
, O
leading O
to O
fewer O
pairs O
in O
each O
split O
, O
1133 B-HyperparameterValue
/ I-HyperparameterValue
85 I-HyperparameterValue
/ I-HyperparameterValue
269 I-HyperparameterValue
, O
respectively O
. O
To O
compare O
our O
proposal O
, O
we O
have O
considered O
the O
following O
SoTA O
models O
as O
baselines O
: O
LEAR B-MethodName
( O
Vulić O
and O
Mrkšić O
, O
2018 O
) O
, O
SDNS B-MethodName
( O
Rei O
et O
al O
. O
, O
2018 O
) O
, O
GLEN B-MethodName
, O
POSTLE B-MethodName
( O
Kamath O
et O
al O
. O
, O
2019 O
) O
, O
LexSub B-MethodName
( O
Arora O
et O
al O
. O
, O
2020 O
) O
and O
Hierarchy B-MethodName
- I-MethodName
fitting I-MethodName
( O
HF B-MethodName
) O
. O
Note O
that O
all O
these O
models O
use O
non O
- O
contextual O
embeddings O
; O
however O
, O
as O
far O
as O
our O
knowledge O
, O
there O
are O
no O
models O
in O
the O
literature O
that O
use O
contextual O
embeddings O
for O
graded B-TaskName
LE I-TaskName
as O
we O
do O
. O

Fine O
- O
tuning O
Setting O

We O
begin O
by O
briefly O
describing O
the O
models O
we O
use O
, O
continue O
by O
explaining O
how O
the O
models O
are O
finetuned O
for O
LRC B-TaskName
and O
graded B-TaskName
LE I-TaskName
, O
and O
how O
the O
finetuned O
models O
are O
used O
for O
inference O
, O
and O
conclude O
the O
section O
by O
describing O
the O
hyperparameter O
setup O
. O

Chosen O
PTLMs B-MethodName
In O
this O
work O
, O
we O
chose O
to O
use O
RoBERTa B-MethodName
and O
BERT B-MethodName
, O
both O
recognized O
as O
SoTA O
models O
for O
general O
domains O
and O
tasks O
in O
English O
. O
In O
particular O
, O
we O
use O
both O
their O
base O
and O
large O
versions O
that O
can O
be O
downloaded O
using O
the O
Huggingface O
transformers O
library O
( O
Wolf O
et O
al O
. O
, O
2020 O
) O
4 O
. O
Moreover O
, O
we O
use O
the O
appropriate O
version O
depending O
on O
the O
actual O
underlying O
task O
we O
are O
fine O
- O
tuning O
, O
whether O
it O
is O
sequence B-TaskName
classification I-TaskName
( O
T1 O
- O
4 O
) O
or O
fillin B-TaskName
- I-TaskName
the I-TaskName
- I-TaskName
mask I-TaskName
( O
TM1 O
- O
3 O
) O
. O
Finally O
, O
note O
that O
BERT B-MethodName
and O
RoBERTa B-MethodName
have O
different O
- O
sized O
vocabularies O
and O
treat O
white O
spaces O
differently O
; O
thus O
, O
we O
must O
bear O
in O
mind O
these O
differences O
to O
adapt O
the O
templates O
and O
prompts O
for O
each O
model O
. O
4 O
Both O
models O
are O
open O
source O
with O
Apache O
2.0 O
and O
MIT O
licenses O
LRC B-TaskName
Our O
setup O
for O
fine O
- O
tuning O
a O
model O
has O
four O
components O
: O
1 O
) O
a O
PTLM B-MethodName
M O
and O
its O
token O
vocabulary O
V O
M O
; O
2 O
) O
a O
training O
set O
T O
= O
{ O
( O
w O
i O
, O
y O
i O
) O
| O
i O
= O
1 O
, O
. O
. O
. O
n O
} O
, O
where O
w O
i O
= O
( O
w O
1 O
i O
, O
w O
2 O
i O
) O
is O
a O
pair O
of O
words O
and O
y O
i O
∈ O
Y O
is O
the O
label O
of O
a O
lexical O
relation O
( O
|Y O
| O
= O
K O
) O
; O
3 O
) O
an O
injective O
function O
from O
the O
set O
of O
labels O
to O
the O
vocabulary O
of O
tokens O
V O
M O
, O
v O
: O
Y O
→ O
V O
M O
, O
called O
the O
mask O
verbalizer O
function O
; O
and O
4 O
) O
a O
training O
and O
a O
testing O
template O
, O
T O
t O
and O
T O
e O
, O
used O
to O
verbalize O
w O
i O
. O
In O
this O
context O
, O
a O
template O
T O
is O
a O
function O
, O
T O
: O
V O
× O
V O
→ O
S O
, O
from O
pairs O
of O
the O
word O
vocabulary O
to O
the O
set O
of O
sentences O
where O
the O
CLS O
, O
SEP O
and O
MASK O
special O
tokens O
of O
the O
PTLM B-MethodName
can O
appear O
in O
the O
sentence O
. O
We O
denote O
by O
T O
( O
w O
) O
C O
and O
T O
( O
w O
) O
M O
to O
the O
CLS O
and O
MASK O
tokens O
in O
the O
sentence O
T O
( O
w O
) O
, O
respectively O
. O

Depending O
on O
the O
template O
used O
, O
we O
adopt O
one O
of O
the O
following O
two O
training O
objectives O
: O
( O
T1 O
- O
4 O
) O
a O
classification B-TaskName
objective O
to O
estimate O
the O
probability O
P O
( O
Y O
= O
y O
j O
|T O
t O
( O
w O
i O
) O
C O
) O
; O
and O
( O
TM1 O
- O
3 O
) O
a O
mask B-TaskName
prediction I-TaskName
objective O
to O
estimate O
P O
( O
T O
t O
( O
w O
i O
) O
M O
= O
t O
j O
|T O
t O
( O
w O
i O
) O
) O
, O
where O
t O
j O
∈ O
V O
M O
is O
any O
token O
in O
the O
vocabulary O
of O
the O
PTLM B-MethodName
. O
At O
inference O
time O
, O
for O
a O
model O
trained O
with O
a O
classification B-TaskName
objective O
, O
we O
use O
the O
testing O
template O
T O
e O
to O
predict O
the O
label O
with O
argmax O
y O
i O
∈Y O
{ O
P O
( O
Y O
= O
y O
i O
|T O
e O
( O
w O
) O
C O
) O
} O
, O
and O
for O
the O
mask O
objective O
, O
argmax O
y O
i O
∈Y O
{ O
P O
( O
T O
e O
( O
w O
) O
M O
= O
v O
( O
y O
j O
) O
|T O
e O
( O
w O
) O
) O
} O
. O
For O
this O
latter O
case O
, O
note O
that O
at O
inference O
time O
, O
we O
only O
use O
the O
tokens O
given O
by O
the O
mask O
verbalizer O
function O
v O
. O

Graded B-TaskName
LE I-TaskName
In O
this O
task O
, O
we O
have O
a O
similar O
setup O
to O
the O
LRC B-TaskName
one O
, O
but O
the O
training O
set O
tuples O
are O
extended O
with O
the O
hyponymy O
score O
for O
the O
pair O
of O
words O
, O
s O
i O
∈ O
R O
; O
thus O
, O
T O
= O
{ O
( O
w O
i O
, O
s O
i O
, O
y O
i O
) O
} O
. O
We O
first O
fine O
- O
tune O
a O
model O
M O
using O
only O
the O
labels O
y O
i O
as O
for O
the O
LRC B-TaskName
task O
. O
The O
model O
M O
produces O
a O
logit O
, O
l O
j O
i O
∈ O
R O
for O
each O
pair O
w O
i O
∈ O
T O
and O
label O
y O
j O
( O
token O
v O
( O
y O
j O
) O
) O
for O
a O
model O
fine O
- O
tuned O
with O
a O
classification O
( O
masked O
) O
objective O
. O
Let O
us O
denote O
by O
M O
( O
w O
i O
) O
= O
( O
l O
1 O
i O
, O
. O
. O
. O
, O
l O
K O
i O
) O
the O
logit O
vector O
produced O
by O
the O
model O
and O
by O
A O
= O
[ O
M O
( O
w O
i O
) O
] O
∈ O
R O
n×K O
the O
matrix O
of O
logits O
. O
Then O
, O
a O
linear O
regression O
model O
is O
fitted O
to O
predict O
the O
scores O
in O
the O
training O
set O
{ O
s O
i O
| O
i O
= O
1 O
, O
. O
. O
. O
n O
} O
with O
the O
logits O
A. O
We O
obtain O
K O
regression O
coefficients O
β O
= O
( O
β O
1 O
, O
. O
. O
. O
, O
β O
K O
) O
. O
For O
an O
unseen O
pair O
w O
, O
the O
predicted O
score O
is O
the O
linear O
combination O
of O
the O
fitted O
regression O
coefficients O
and O
the O
logits O
produced O
by O
the O
model O
M O
, O
that O
is O
, O
the O
scalar O
product O
score O
( O
w O
) O
= O
β O
• O
M O
( O
w O
) O
. O

Hyperparameters O
and O
Fine O
- O
tuning O
Setup O

Training O
and O
evaluation O
were O
performed O
on O
a O
Tesla O
- O
T4 O
GPU O
through O
Google O
Colab O
. O
Overall O
we O
consumed O
around O
850h O
of O
GPU O
usage O
. O
To O
fine O
- O
tune O
the O
models O
, O
we O
used O
the O
following O
hyperparameters O
: O
batch B-HyperparameterName
size I-HyperparameterName
of O
32 B-HyperparameterValue
, O
Adam O
weight O
optimizer O
, O
learning B-HyperparameterName
rate I-HyperparameterName
of O
2e B-HyperparameterValue
−5 I-HyperparameterValue
, O
weight B-HyperparameterName
decay I-HyperparameterName
of O
0.01 B-HyperparameterValue
, O
no O
warmup O
, O
10 B-HyperparameterValue
epochs B-HyperparameterName
, O
and O
5 B-HyperparameterValue
runs I-HyperparameterValue
of O
training B-HyperparameterName
and I-HyperparameterName
evaluation I-HyperparameterName
to O
asses O
model O
's O
performance O
variability O
. O
We O
use O
the O
train B-HyperparameterName
, I-HyperparameterName
validation I-HyperparameterName
, I-HyperparameterName
and I-HyperparameterName
test I-HyperparameterName
splits I-HyperparameterName
provided O
by O
the O
original O
datasets O
, O
and O
, O
when O
no O
validation O
split O
was O
provided O
, O
we O
did O
not O
use O
any O
. O
We O
report O
the O
F1 B-MetricName
- O
score O
weighted O
by O
the O
support O
of O
the O
labels O
to O
compare O
ourselves O
with O
the O
other O
baselines O
. O
In O
the O
case O
of O
CogALexV B-DatasetName
, O
we O
take O
out O
the O
results O
for O
RANDOM B-DatasetName
before O
reporting O
the O
results O
as O
advised O
by O
its O
authors O
in O
( O
Santus O
et O
al O
. O
, O
2016a O
) O
. O
For O
graded B-TaskName
LE I-TaskName
and O
Hyperlex B-DatasetName
dataset O
, O
the O
Spearman B-MetricName
correlation I-MetricName
between O
the O
median O
human O
annotators O
scores O
and O
our O
proposed O
score O
is O
reported O
. O
We O
also O
report O
the O
Spearman B-MetricName
correlation I-MetricName
restricted O
to O
nouns O
and O
verbs O
. O

Results O

In O
this O
section O
, O
we O
report O
the O
qualitative O
and O
quantitative O
results O
of O
our O
experiments O
. O

Quantitative O
Results O

LRC B-TaskName
Results O

We O
report O
our O
results O
5 O
in O
Tables O
2 O
and O
3 O
, O
comparing O
them O
to O
the O
SoTA O
6 O
results O
. O
We O
report O
the O
mean O
value O
of O
the O
5 O
runs O
for O
each O
measure O
, O
underlining O
the O
highest O
value O
achieved O
for O
each O
dataset O
( O
column O
- O
wise O
) O
. O
Boldened O
numbers O
mark O
no O
statistical O
significance O
( O
at O
confident O
level O
α O
= O
0.01 O
) O
to O
be O
different O
from O
the O
greatest O
mean O
value O
applying O
Welch O
's O
t O
- O
test O
. O
Except O
for O
KHN B-DatasetName
, O
we O
improve O
the O
F1 B-MetricName
- O
score O
in O
all O
the O
datasets O
. O
In O
some O
of O
them O
( O
EVALution B-DatasetName
and O
CogALexV B-DatasetName
) O
, O
we O
outperform O
the O
baselines O
by O
almost O
10 B-MetricValue
points O
. O
We O
hypothesize O
that O
not O
biasing O
the O
model O
by O
adding O
external O
artificial O
context O
might O
let O
it O
choose O
the O
best O
sense O
of O
both O
words O
. O
Coincidentally O
with O
( O
Schick O
and O
Schütze O
, O
2022 O
) O
, O
the O
longer O
hand O
- O
crafted O
template O
( O
T3 O
) O
obtained O
the O
best O
results O
in O
most O
datasets O
. O
However O
, O
the O
difference O
with O
simpler O
templates O
( O
T1 O
, O
T2 O
) O
, O
was O
very O
small O
and O
statistically O
not O
significant O
in O
most O
cases O
. O
T4 O
reported O
the O
worst O
performance O
due O
to O
the O
differences O
between O
train O
and O
test O
which O
misguided O
the O
model O
's O
learning O
. O
We O
must O
point O
out O
that O
masked O
variants O
exhibited O
more O
stability O
when O
small O
models O
, O
small O
prompts O
, O
and O
small O
datasets O
are O
jointly O
used O
, O
as O
, O
in O
some O
instances O
with O
this O
setting O
, O
T1 O
and O
T2 O
did O
not O
manage O
to O
converge O
, O
entering O
a O
poor O
minimal O
local O
. O
Such O
situations O
were O
solved O
by O
relaunching O
the O
training O
. O

Graded B-TaskName
LE I-TaskName
results O

The O
results O
for O
graded B-TaskName
LE I-TaskName
are O
shown O
in O
Table O
4 O
. O
We O
can O
see O
how O
models O
trained O
with O
a O
mask O
objective O
( O
TM1 O
- O
TM3 O
) O
obtain O
the O
best O
results O
, O
and O
improve O
the O
SoTA O
results O
by O
more O
than O
10 B-MetricValue
points O
globally O
( O
all O
) O
and O
focusing O
only O
on O
noun O
pairs O
( O
nouns O
) O
. O
In O
particular O
, O
in O
the O
lexical O
split O
, O
our O
results O
are O
about O
20 B-MetricValue
points O
above O
previous O
proposals O
. O
Note O
as O
well O
that O
the O
difference O
of O
the O
results O
in O
the O
lexical O
split O
is O
only O
about O
4 B-MetricValue
points O
less O
than O
in O
the O
random O
split O
, O
which O
is O
a O
good O
indicator O
of O
the O
generalization O
capabilities O
of O
our O
models O
. O
To O
the O
best O
of O
our O
knowledge O
, O
previous O
studies O
reported O
results O
just O
on O
all O
POS O
together O
, O
and O
some O
focused O
on O
nouns O
as O
well O
. O
We O
expand O
this O
research O
to O
verbs O
considering O
the O
results O
promising O
as O
, O
even O
if O
they O
are O
lower O
than O
for O
nouns O
, O
they O
show O
that O
the O
part O
of O
speech O
has O
influence O
in O
our O
models O
. O
Finally O
, O
we O
want O
to O
remark O
that O
our O
models O
push O
up O
the O
results O
for O
nouns O
near O
to O
the O
IAA B-MetricName
given O
by O
humans O
( O
0.837 B-MetricValue
vs. O
0.864 B-MetricValue
) O
. O

LRC B-TaskName
Error O
Analysis O

Results O
obtained O
for O
EVALution B-DatasetName
and O
CogALexV B-DatasetName
datasets O
are O
noticeably O
lower O
. O
We O
hypothesize O
a O
reason O
for O
this O
is O
that O
EVALution B-DatasetName
is O
an O
extended O
version O
of O
BLESS B-DatasetName
dataset O
where O
the O
relations O
of O
synonyms O
and O
antonyms O
were O
added O
. O
Adding O
such O
relations O
makes O
the O
task O
of O
LRC B-TaskName
more O
challenging O
as O
, O
particularly O
, O
synonyms O
are O
a O
very O
heterogeneous O
class O
difficult O
to O
be O
delimited O
even O
for O
humans O
. O
CogALexV B-DatasetName
becomes O
even O
more O
challenging O
as O
it O
consists O
of O
a O
selected O
subset O
of O
EVALution B-DatasetName
, O
where O
words O
were O
stemmed O
, O
decreasing O
possible O
morpho O
- O
semantic O
cues O
. O
Moreover O
, O
both O
EVALution B-DatasetName
and O
CogALexV B-DatasetName
were O
created O
to O
avoid O
lexical O
memorization O
, O
this O
meaning O
, O
they O
consistently O
use O
words O
that O
participate O
in O
various O
relations O
. O
Finally O
, O
the O
bigger O
dataset O
size O
of O
BLESS B-DatasetName
, O
ROOT09 B-DatasetName
, O
and O
K B-DatasetName
& I-DatasetName
H+N I-DatasetName
should O
also O
have O
a O
beneficial O
impact O
on O
the O
results O
. O

From O
now O
on O
, O
we O
focus O
our O
error O
analysis O
on O
EVALution B-DatasetName
and O
CogALexV B-DatasetName
as O
they O
contain O
the O
most O
challenging O
examples O
7 O
. O
Unknown O
( O
or O
equivalently O
Random O
) O
relations O
and O
models O
trained O
with O
the O
T4 O
control O
template O
have O
been O
excluded O
from O
this O
analysis O
. O
We O
focused O
this O
analysis O
on O
the O
best O
- O
performing O
model O
in O
our O
experiments O
, O
Roberta B-MethodName
- O
large O
, O
and O
we O
got O
two O
groups O
of O
word O
pairs O
, O
those O
which O
were O
well O
and O
wrongly O
classified O
with O
all O
templates O
. O
For O
these O
two O
groups O
, O
we O
analyzed O
different O
features O
( O
presented O
below O
) O
, O
checking O
whether O
there O
was O
a O
statistically O
significant O
difference O
between O
the O
two O
groups O
by O
using O
χ O
2 O
-tests O
or O
Welch O
's O
t O
- O
tests O
. O
We O
considered O
that O
a O
feature O
had O
a O
significant O
impact O
when O
the O
p O
- O
value O
was O
below O
0.05 O
. O

-- O
-- O
-0.174 B-MetricValue
/ O
-- O
-- O
- O
/ O
-- O
-- O
- O
SDNS B-MethodName
0.692 B-MetricValue
/ O
-- O
-- O
- O
/ O
-- O
-- O
-- O
-- O
-- O
/ O
-- O
-- O
- O
/ O
-- O
-- O
- O
GLEN B-MethodName
0.520 B-MetricValue
/ O
-- O
-- O
- O
/ O
-- O
-- O
-0.481 B-MetricValue
/ O
-- O
-- O
- O
/ O
-- O
-- O
- O
POSTLE B-MethodName
0.686 B-MetricValue
/ O
-- O
-- O
- O
/ O
-- O
-- O
-- O
-- O
-- O
/ O
0.600 B-MetricValue
/ O
-- O
-- O
- O
LexSub B-MethodName
0.533 B-MetricValue
/ O
-- O
-- O
- O
/ O
-- O
-- O
-- O
-- O
-- O
/ O
-- O
-- O
- O
/ O
-- O
-- O
- O
HF B-MethodName
0.690 B-MetricValue
/ O
-- O
-- O
- O
/ O
-- O
-- O
-- O
-- O
-- O
/ O
-- O
-- O
- O
/ O
-- O
-- O
- O
IAA B-MetricName
0.864 B-MetricValue
/ O
0.864 B-MetricValue
/ O
0.862 B-MetricValue

Relationship O
Type O

We O
observed O
that O
, O
in O
both O
datasets O
, O
all O
the O
trained O
models O
struggled O
correctly O
classifying O
synonyms O
, O
while O
they O
are O
particularly O
good O
at O
predicting O
antonyms O
. O
In O
comparison O
to O
previous O
studies O
with O
static O
embeddings O
( O
Etcheverry O
and O
Wonsever O
, O
2019 O
; O
Samenko O
et O
al O
. O
, O
2020 O
) O
, O
where O
antonyms O
and O
synonyms O
were O
mutually O
confused O
in O
the O
classification O
, O
with O
our O
setting O
we O
overcame O
this O
problem O
. O
Yet O
, O
synonyms O
, O
in O
line O
with O
previous O
studies O
( O
Santus O
et O
al O
. O
, O
2016a O
) O
, O
remain O
the O
most O
challenging O
class O
. O

Polysemy O
Initially O
, O
we O
expected O
more O
polysemous O
words O
would O
be O
more O
problematic O
and O
worse O
predicted O
, O
as O
, O
at O
first O
sight O
, O
a O
wider O
range O
of O
categories O
could O
describe O
different O
relations O
between O
source O
and O
target O
words O
. O
Moreover O
, O
we O
expected O
that O
the O
lack O
of O
context O
( O
or O
the O
addition O
of O
an O
artificial O
one O
, O
not O
adapted O
to O
the O
word O
pair O
context O
) O
in O
our O
approach O
would O
make O
it O
more O
difficult O
to O
disambiguate O
between O
the O
different O
senses O
, O
and O
thus O
to O
choose O
the O
best O
relation O
. O
However O
, O
counterintuitively O
, O
we O
did O
not O
find O
statistical O
evidence O
that O
polysemy O
8 O
affected O
our O
results O
. O

POS O
When O
looking O
at O
the O
part O
of O
speech O
, O
we O
found O
out O
that O
adjectives O
were O
the O
best O
- O
predicted O
ones O
, O
compared O
to O
verbs O
and O
nouns O
. O
To O
extract O
the O
part O
of O
speech O
, O
the O
predominant O
part O
of O
speech O
annotated O
for O
the O
CogALexV B-DatasetName
and O
EVALution B-DatasetName
datasets O
were O
selected O
. O

Semantic O
Domains O
and O
Prototypicality O
These O
datasets O
provide O
us O
for O
each O
word O
pair O
with O
humanannotated O
semantic O
domains O
9 O
for O
both O
the O
source O
and O
target O
words O
as O
well O
as O
their O
prototypical O
relation O
. O
We O
found O
out O
that O
our O
model O
predicted O
better O
word O
pairs O
that O
contained O
abstract O
rather O
than O
concrete O
words O
, O
and O
objects O
better O
than O
events O
. O
Our O
error O
analysis O
strengthens O
previous O
studies O
( O
Necsulescu O
et O
al O
. O
, O
2015 O
) O
that O
suggest O
LRC B-TaskName
is O
sensitive O
to O
domain O
bias O
. O
Regarding O
prototypicality O
, O
as O
previously O
noted O
in O
( O
Santus O
et O
al O
. O
, O
2016a O
) O
, O
categories O
more O
generally O
associated O
with O
a O
pair O
of O
words O
were O
the O
best O
- O
predicted O
ones O
( O
in O
contrast O
to O
categories O
where O
human O
annotators O
doubted O
the O
accuracy O
of O
the O
provided O
annotations O
) O
. O

Sampled O
Errors O

Embedding O
Projection O
Visualization O

In O
Figure O
1 O
In O
the O
visualization O
of O
the O
embedding O
projections O
, O
we O
annotated O
our O
data O
with O
some O
linguistic O
features O
such O
as O
polysemy O
, O
word O
frequency O
, O
and O
linguistic O
register O
( O
formal O
vs O
colloquial O
and O
geographical O
differences O
) O
extracted O
from O
WordNet O
to O
check O
whether O
any O
clear O
clusters O
appeared O
for O
the O
unattested O
relations O
group O
. O
Yet O
, O
in O
this O
initial O
exploration O
, O
we O
could O
not O
find O
any O
clear O
clustering O
. O

Conclusions O
and O
Future O
Work O

Our O
experiments O
show O
that O
minimal O
prompts O
work O
equally O
well O
to O
more O
complex O
ones O
for O
the O
LRC B-TaskName
task O
, O
thus O
, O
allowing O
less O
human O
effort O
and O
computational O
cost O
, O
and O
following O
a O
language O
- O
neutral O
approach O
. O
Moreover O
, O
we O
show O
that O
minimal O
prompting O
outperforms O
SoTA O
results O
in O
graded B-TaskName
LE I-TaskName
. O
We O
conducted O
an O
extensive O
error O
analysis O
showing O
that O
: O
synonymy O
remains O
the O
hardest O
category O
to O
classify O
, O
there O
is O
some O
domain O
and O
POS O
bias O
, O
and O
polysemy O
was O
proven O
to O
be O
an O
issue O
. O
We O
highlight O
the O
need O
of O
crafting O
more O
balanced O
datasets O
in O
terms O
of O
POS O
and O
domain O
, O
with O
finer O
- O
graded O
annotations O
for O
the O
different O
types O
of O
synonyms O
. O
As O
future O
work O
, O
we O
would O
like O
to O
a O
) O
address O
LRC B-TaskName
as O
a O
multilabel O
classification O
task O
to O
alleviate O
the O
polysemy O
challenge O
, O
b O
) O
check O
the O
approach O
with O
other O
languages O
, O
c O
) O
extend O
the O
study O
to O
other O
semantic O
relations O
, O
and O
d O
) O
gain O
insights O
in O
why O
null O
prompting O
improves O
the O
SoTA O
for O
LRC B-TaskName
and O
if O
this O
line O
of O
research O
could O
be O
generalized O
to O
other O
relations O
, O
or O
if O
not O
, O
what O
characterizes O
Lexico O
- O
Semantic O
relations O
to O
fit O
this O
well O
the O
null O
prompting O
approach O
. O

Limitations O

1 O
. O
Computational O
cost O
: O
For O
our O
experiments O
, O
we O
used O
almost O
850h O
of O
GPUs O
. O
In O
future O
research O
, O
we O
could O
try O
to O
lower O
this O
cost O
by O
experimenting O
with O
prompting O
for O
LRC B-TaskName
task O
in O
few O
- O
shot O
scenarios O
, O
which O
would O
also O
help O
when O
conducting O
the O
task O
for O
low O
- O
researched O
languages O
. O

2 O
. O
Language O
: O
Our O
experiments O
were O
conducted O
just O
for O
the O
English O
language O
. O
Thus O
, O
and O
with O
the O
advantage O
derived O
from O
minimal O
prompting O
of O
being O
language O
independent O
, O
in O
further O
research O
we O
would O
like O
to O
expand O
our O
experiments O
to O
multilingual O
datasets O
such O
as O
the O
ones O
from O
( O
Wachowiak O
et O
al O
. O
, O
2020 O
) O
. O

3 O
. O
Original O
dataset O
limitations O
: O
In O
line O
with O
( O
Lang O
et O
al O
. O
, O
2021 O
) O
, O
we O
found O
some O
misleading O
annotations O
in O
CogALexV B-DatasetName
dataset O
. O
This O
not O
only O
decrease O
the O
performance O
of O
the O
model O
but O
can O
also O
lead O
to O
hard O
- O
to O
- O
detect O
biases O
. O

Once O
again O
, O
few O
- O
shot O
tuning O
would O
decrease O
the O
annotation O
cost O
, O
making O
it O
possible O
to O
train O
with O
, O
although O
less O
, O
better O
- O
annotated O
examples O
. O
Additionally O
, O
synonymy O
remains O
the O
most O
difficult O
relation O
to O
capture O
, O
a O
more O
fine O
- O
graded O
annotation O
of O
the O
different O
kinds O
of O
synonyms O
could O
improve O
their O
classification O
. O

Domain O
dependence O
: O

The O
limitation O
spotted O
by O
( O
Necsulescu O
et O
al O
. O
, O
2015 O
) O
is O
persistent O
in O
our O
model O
. O
A O
richer O
domain O
annotation O
would O
be O
advised O
to O
better O
research O
domain O
bias O
in O
the O
LRC B-TaskName
task O
. O

Aknowledgements O

Supported O
by O
the O
Spanish O
project O
PID2020 O
- O
113903RB O
- O
I00 O
( O
AEI O
/ O
FEDER O
, O
UE O
) O
, O
by O
DGA O
/ O
FEDER O
, O
by O
the O
Agencia O
Estatal O
de O
Investigación O
of O
the O
Spanish O
Ministry O
of O
Economy O
and O
Competitiveness O
and O
the O
European O
Social O
Fund O
through O
the O
" O
Ramón O
y O
Cajal O
" O
program O
( O
RYC2019 O
- O
028112 O
- O
I O
) O
, O
and O
by O
the O
EU O
research O
and O
innovation O
program O
HORIZON O
Europe O
2021 O
through O
the O
" O
4D O
PICTURE O
" O
project O
under O
grant O
agreement O
101057332 O
. O

A O
Datasets O
Description O

All O
the O
five O
datasets O
used O
for O
LRC B-TaskName
, O
except O
K B-DatasetName
& I-DatasetName
H+N I-DatasetName
, O
are O
to O
some O
extent O
expansions O
and O
modified O
versions O
of O
the O
BLESS B-DatasetName
dataset O
. O
BLESS B-DatasetName
aimed O
to O
provide O
pair O
of O
words O
to O
conduct O
research O
on O
distributional O
semantics O
through O
analogies O
. O
This O
first O
dataset O
used O
the O
McRae O
norms O
, O
Wordnet O
and O
Con O
- O
ceptNet O
as O
sources O
. O
They O
used O
single O
words O
instead O
of O
multiwords O
and O
crowdsourced O
random O
words O
to O
create O
noise O
in O
the O
dataset O
at O
the O
same O
time O
that O
they O
assured O
no O
relation O
between O
them O
was O
entailed O
. O
They O
tried O
to O
avoid O
ambiguities O
, O
and O
relied O
on O
prototypical O
terms O
to O
stay O
as O
' O
little O
controversial O
as O
possible O
' O
. O
As O
categories O
, O
they O
study O
meronyms O
and O
hyponyms O
, O
excluding O
synonyms O
due O
the O
alleged O
problematic O
description O
and O
heterogeneity O
. O

EVAlution B-DatasetName
was O
developed O
as O
an O
expansion O
of O
BLESS B-DatasetName
, O
to O
which O
synonyms O
and O
antonyms O
were O
added O
, O
containing O
IsA O
( O
hypernymy O
) O
, O
antonymy O
, O
synonymy O
, O
meronymy O
( O
part O
of O
, O
member O
of O
, O
and O
made O
of O
) O
, O
entailment O
, O
hasA O
( O
possession O
) O
, O
has O
property O
( O
attribution O
) O
relations O
with O
heterogeneous O
distribution O
of O
them O
. O
Complementary O
linguistic O
data O
is O
also O
provided O
, O
as O
for O
example O
the O
domain O
11 O
. O
Co B-DatasetName
- I-DatasetName
gALexV I-DatasetName
dataset O
was O
provided O
at O
the O
ACL O
lexical O
relation O
classification O
workshop O
in O
2016 O
as O
a O
challenging O
subset O
of O
Evalution O
, O
where O
words O
were O
stemmed O
. O
ROOT9 B-DatasetName
is O
an O
expansion O
of O
CogALexV B-DatasetName
. O

K B-DatasetName
& I-DatasetName
+N I-DatasetName
is O
an O
expansion O
of O
Kozareva O
and O
Hongs O
, O
2010 O
dataset O
, O
which O
extracted O
its O
original O
data O
from O
hyponymy O
and O
hypernymy O
relations O
in O
Wordnet O
, O
for O
animal O
, O
plant O
and O
vehicle O
domains O
. O
In O
the O
current O
K B-DatasetName
& I-DatasetName
H+N I-DatasetName
dataset O
, O
cohyponyms O
and O
meronyms O
were O
added O
. O
As O
in O
the O
previous O
datasets O
, O
multiwords O
were O
avoided O
. O

Most O
datasets O
, O
by O
being O
descendants O
of O
BLESS B-DatasetName
, O
contain O
the O
same O
limitations O
, O
being O
mostly O
the O
elusion O
of O
rare O
vocabulary O
and O
ambiguous O
words O
. O

For O
graded B-TaskName
LE I-TaskName
, O
in O
the O
original O
Hyperlex B-DatasetName
dataset O
, O
the O
hyponym O
pairs O
are O
annotated O
in O
four O
levels O
, O
namely O
hyp O
- O
i O
, O
1 O
≤i≤ O
4 O
, O
where O
i O
is O
the O
path O
length O
in O
the O
WordNet O
hierarchy O
. O
We O
collapse O
all O
labels O
hyp O
- O
i O
to O
hyp O
in O
our O
experiments O
. O
The O
same O
rationale O
is O
applied O
to O
the O
hyperonym O
labels O
r O
- O
hyp O
- O
i O
. O

In O
Table O
6 O
, O
we O
show O
the O
number O
of O
pairs O
for O
relation O
in O
the O
train B-HyperparameterName
/ I-HyperparameterName
validation I-HyperparameterName
/ I-HyperparameterName
test I-HyperparameterName
splits I-HyperparameterName
. O

11 O
Domain O
information O
was O
crowdsourced O
and O
not O
always O
reliable O
, O
thus O
, O
authors O
advised O
to O
only O
take O
domains O
as O
valid O
when O
two O
or O
more O
raters O
annotated O
the O
word O
as O
belonging O
to O
the O
same O
domain O

B O
Detailed O
Error O
Analysis O

To O
conduct O
the O
error O
analysis O
, O
we O
take O
the O
easiest O
and O
the O
most O
difficult O
examples O
to O
classify O
trained O
with O
RoBERTa B-MethodName
( O
large O
) O
for O
CogaALexV B-DatasetName
and O
EVA B-DatasetName
- I-DatasetName
Lution I-DatasetName
datasets O
. O
We O
take O
two O
groups O
of O
pairs O
: O
those O
which O
were O
well O
and O
wrongly O
classified O
in O
all O
of O
the O
5 O
runs O
and O
all O
templates O
, O
except O
for O
template O
T O
4 O
. O
We O
test O
if O
there O
is O
statistical O
evidence O
that O
some O
features O
influence O
the O
well O
/ O
wrongly O
classified O
pairs O
. O
We O
have O
a O
total O
of O
1527 O
pairs O
, O
586 O
from O
CogALexV B-DatasetName
and O
941 O
from O
EVALution B-DatasetName
, O
divided O
into O
1359 O
/ O
168 O
well O
/ O
wrongly O
predicted O
pairs O
. O

The O
first O
studied O
feature O
is O
the O
relation O
between O
the O
words O
, O
that O
is O
, O
we O
ask O
if O
there O
is O
some O
lexical O
relation O
that O
it O
is O
easier O
/ O
harder O
to O
predict O
. O
Figure O
2 O
contains O
a O
visualization O
of O
the O
contingency O
tables O
of O
the O
well O
/ O
wrongly O
predicted O
pairs O
by O
relation O
. O
In O
both O
datasets O
, O
applying O
a O
χ O
2 O
-test O
, O
there O
is O
statistical O
evidence O
that O
the O
relation O
type O
influences O
the O
prediction O
( O
p O
- O
values O
< O
< O
0.05 O
) O
. O
In O
particular O
, O
there O
is O
a O
great O
difference O
in O
the O
predictions O
for O
antonyms O
and O
synonyms O
, O
the O
former O
being O
better O
predicted O
than O
the O
latter O
. O

We O
check O
if O
the O
pairs O
containing O
polysemous O
words O
are O
more O
difficult O
to O
predict O
. O
We O
use O
Word O
- O
Net O
to O
obtain O
the O
number O
of O
synsets O
for O
each O
word O
, O
and O
we O
consider O
that O
the O
polysemous O
level O
of O
a O
pair O
is O
the O
product O
of O
the O
number O
of O
synsets O
of O
the O
words O
in O
the O
pair O
. O
Although O
the O
mean O
of O
the O
polysemous O
level O
is O
less O
for O
well O
- O
predicted O
pairs O
, O
108.5 O
vs. O
120.6 O
, O
performing O
a O
Welch O
's O
t O
- O
test O
to O
evaluate O
if O
the O
means O
are O
different O
, O
we O
find O
that O
there O
is O
no O
statistical O
evidence O
, O
with O
a O
high O
p O
- O
value O
equal O
to O
0.40 O
. O

We O
also O
study O
if O
the O
part O
of O
the O
speech O
( O
POS O
) O
influences O
the O
predictions O
. O
CogALexV B-DatasetName
and O
EVA B-DatasetName
- I-DatasetName
Lution I-DatasetName
datasets O
are O
also O
annotated O
with O
the O
predominant O
POS O
and O
a O
list O
of O
the O
different O
possible O
POS O
of O
each O
word O
. O
We O
restrict O
our O
POS O
study O
to O
the O
well O
/ O
wrongly O
predicted O
pairs O
where O
both O
words O
in O
the O
pairs O
have O
the O
same O
predominant O
POS O
or O
there O
is O
only O
one O
POS O
in O
the O
intersection O
lists O
of O
possible O
POS O
. O
As O
it O
is O
appreciated O
in O
the O
contingency O
table O
( O
Figure O
3 O
) O
, O
adjectives O
are O
easier O
to O
predict O
than O
nouns O
and O
verbs O
. O

The O
domain O
of O
the O
words O
in O
CogALexV B-DatasetName
and O
EVALution B-DatasetName
were O
annotated O
by O
humans O
. O
We O
get O
pairs O
with O
common O
domains O
, O
and O
we O
restrict O
the O
study O
to O
the O
most O
common O
domains O
: O
abstract O
, O
concrete O
, O
event O
and O
object O
domains O
. O
The O
visualization O
of O
the O
contingency O
table O
can O
be O
seen O
in O
Figure O
4 O
. O
There O
is O
statistical O
evidence O
( O
p O
- O
value O
< O
< O
0.5 O
) O
that O
the O
domain O
influences O
the O
correctness O
of O
the O
prediction O
: O
words O
in O
the O
abstract O
and O
object O
domains O
are O
better O
predicted O
. O
Finally O
, O
CogALexV B-DatasetName
and O
EVALution B-DatasetName
were O
annotated O
by O
humans O
with O
the O
prototypicality O
of O
the O
annotated O
relation O
. O
The O
pairs O
of O
words O
in O
the O
datasets O
were O
exposed O
to O
five O
humans O
to O
answer O
to O
what O
extent O
they O
agreed O
with O
the O
annotated O
relation O
( O
from O
0 O
- O
strongly O
disagree O
to O
5 O
- O
strongly O
agree O
) O
. O
So O
, O
it O
is O
interesting O
to O
check O
if O
the O
prototypicality O
is O
higher O
for O
well O
- O
predicted O
pairs O
. O
We O
perform O
a O
Welch O
's O
t O
- O
test O
to O
test O
if O
the O
prototypicality O
means O
for O
well O
/ O
wrongly O
predicted O
pairs O
are O
equal O
. O
We O
get O
that O
well O
/ O
wrongly O
means O
are O
4.63 O
/ O
4.51 O
with O
p O
- O
value O
< O
< O
0.05 O
, O
so O
they O
are O
different O
. O
Although O
the O
means O
seem O
quite O
similar O
, O
take O
into O
account O
that O
about O
90 O
% O
of O
the O
prototypicality O
in O
the O
datasets O
range O
from O
4 O
to O
5 O
. O

C O
Mask O
Verbalizer O

In O
Table O
7 O
it O
is O
shown O
the O
used O
tokens O
to O
verbalize O
the O
mask O
token O
in O
templates O
TM1 O
, O
TM2 O
and O
TM3 O
. O

D O
Complete O
Results O

We O
present O
the O
results O
for O
BERT B-MethodName
and O
RoBERTa B-MethodName
( O
large O
and O
base O
) O
models O
. O
Table O
8 O
contains O
the O
mean O
of O
the O
weighted O
by O
the O
support O
labels O
of O
precision B-MetricName
of O
the O
5 O
runs O
, O
recall B-MetricName
and O
F1 B-MetricName
- O
score O
. O
The O
greatest O
value O
for O
each O
measure O
( O
column O
) O
is O
underlined O
. O
A O
value O
is O
boldened O
if O
there O
is O
no O
statistical O
evidence O
to O
be O
different O
from O
the O
greatest O
one O
performing O
a O
Welch O
's O
t O
- O
test O
for O
the O
mean O
values O
. O
A O
similar O
rationale O
is O
applied O
for O
Table O
9 O
, O
with O
the O
complete O
results O
for O
CogALexV B-DatasetName
dataset O
and O
Table O
10 O
CogALexV B-DatasetName
EVALution B-DatasetName

NL B-MethodName
- I-MethodName
EDIT I-MethodName
: O
Correcting O
Semantic B-TaskName
Parse I-TaskName
Errors O
through O
Natural O
Language O
Interaction O

We O
study O
semantic B-TaskName
parsing I-TaskName
in O
an O
interactive O
setting O
in O
which O
users O
correct O
errors O
with O
natural O
language O
feedback O
. O
We O
present O
NL B-MethodName
- I-MethodName
EDIT I-MethodName
, O
a O
model O
for O
interpreting O
natural O
language O
feedback O
in O
the O
interaction O
context O
to O
generate O
a O
sequence O
of O
edits O
that O
can O
be O
applied O
to O
the O
initial O
parse O
to O
correct O
its O
errors O
. O
We O
show O
that O
NL B-MethodName
- I-MethodName
EDIT I-MethodName
can O
boost O
the O
accuracy O
of O
existing O
text O
- O
to O
- O
SQL O
parsers O
by O
up O
to O
20 O
% O
with O
only O
one O
round O
of O
correction O
. O
We O
analyze O
the O
limitations O
of O
the O
model O
and O
discuss O
directions O
for O
improvement O
and O
evaluation O
. O
The O
code O
and O
datasets O
used O
in O
this O
paper O
are O
publicly O
available O
at O
http O
: O
/ O
/ O
aka.ms O
/ O
NLEdit B-MethodName
. O

Introduction O

Major O
progress O
in O
natural O
language O
processing O
has O
been O
made O
towards O
fully O
automating O
challenging O
tasks O
such O
as O
question O
answering O
, O
translation O
, O
and O
summarization O
. O
On O
the O
other O
hand O
, O
several O
studies O
have O
argued O
that O
machine O
learning O
systems O
that O
can O
explain O
their O
own O
predictions O
( O
Doshi O
- O
Velez O
and O
Kim O
, O
2017 O
) O
and O
learn O
interactively O
from O
their O
endusers O
( O
Amershi O
et O
al O
. O
, O
2014 O
) O
can O
result O
in O
better O
user O
experiences O
and O
more O
effective O
learning O
systems O
. O
We O
develop O
NL B-MethodName
- I-MethodName
EDIT I-MethodName
- O
an O
approach O
that O
employs O
both O
explanations O
and O
interaction O
in O
the O
context O
of O
semantic B-TaskName
parsing I-TaskName
. O

Most O
existing O
systems O
frame O
semantic B-TaskName
parsing I-TaskName
as O
a O
one O
- O
shot O
translation O
from O
a O
natural O
language O
question O
to O
the O
corresponding O
logical O
form O
( O
e.g. O
, O
SQL O
query O
) O
( O
Yu O
et O
al O
. O
, O
2018a O
; O
Guo O
et O
al O
. O
, O
2019 O
; O
Wang O
et O
al O
. O
, O
2020 O
, O
inter O
alia O
) O
. O
A O
growing O
body O
of O
recent O
work O
demonstrates O
that O
semantic O
parsing O
systems O
can O
be O
improved O
by O
including O
users O
in O
the O
parsing O
loop O
- O
giving O
them O
the O
affordance O
to O
examine O
the O
parses O
, O
judge O
their O
correctness O
, O
and O
provide O
feedback O
accordingly O
. O
The O
feedback O
often O
comes O
in O
the O
form O
of O
a O
binary O
correct O
/ O
incorrect O
Figure O
1 O
: O
Example O
human O
interaction O
with O
NL B-MethodName
- I-MethodName
EDIT I-MethodName
to O
correct O
an O
initial O
parse O
through O
natural O
language O
feedback O
. O
In O
the O
Semantic B-TaskName
Parsing I-TaskName
Phase O
( O
top O
) O
, O
an O
offthe O
- O
shelf O
parser O
generates O
an O
initial O
SQL O
query O
and O
provides O
an O
answer O
paired O
with O
an O
explanation O
of O
the O
generated O
SQL O
. O
In O
the O
Correction O
Phase O
( O
bottom O
) O
, O
the O
user O
reviews O
the O
explanation O
and O
provides O
feedback O
that O
describes O
how O
the O
explanation O
should O
be O
corrected O
. O
The O
system O
parses O
the O
feedback O
as O
a O
set O
of O
edits O
that O
are O
applied O
to O
the O
initial O
parse O
to O
generate O
a O
corrected O
SQL O
. O

signal O
( O
Iyer O
et O
al O
. O
, O
2017 O
) O
, O
answers O
to O
a O
multiplechoice O
question O
posed O
by O
the O
system O
( O
Gur O
et O
al O
. O
, O
2018 O
; O
Yao O
et O
al O
. O
, O
2019 O
) O
, O
or O
suggestions O
of O
edits O
that O
can O
be O
applied O
to O
the O
parse O
. O

Unlike O
other O
frameworks O
for O
interactive O
semantic B-TaskName
parsing I-TaskName
that O
typically O
expect O
users O
to O
judge O
the O
correctness O
of O
the O
execution O
result O
or O
induced O
logical O
form O
, O
Elgohary O
et O
al O
. O
( O
2020 O
) O
introduced O
a O
framework O
for O
interactive O
text O
- O
to O
- O
SQL O
in O
which O
induced O
SQL O
queries O
are O
fully O
explained O
in O
natural O
lan O
- O
guage O
to O
users O
, O
who O
in O
turn O
, O
can O
correct O
such O
parses O
through O
natural O
language O
feedback O
( O
Figure O
1 O
) O
. O
They O
construct O
the O
SPLASH B-DatasetName
dataset O
and O
use O
it O
to O
evaluate O
baselines O
for O
the O
semantic O
parse O
correction O
with O
natural O
language O
feedback O
task O
they O
introduce O
. O

We O
present O
a O
detailed O
analysis O
of O
the O
feedback O
and O
the O
differences O
between O
the O
initial O
( O
incorrect O
) O
and O
the O
correct O
parse O
. O
We O
argue O
that O
a O
correction O
model O
should O
be O
able O
to O
interpret O
the O
feedback O
in O
the O
context O
of O
other O
elements O
of O
the O
interaction O
( O
the O
original O
question O
, O
the O
schema O
, O
and O
the O
explanation O
of O
the O
initial O
parse O
) O
. O
We O
observe O
from O
SPLASH B-DatasetName
that O
most O
feedback O
utterances O
tend O
to O
describe O
a O
few O
edits O
that O
the O
user O
desires O
to O
apply O
to O
the O
initial O
parse O
. O
As O
such O
, O
we O
pose O
the O
correction O
task O
as O
a O
semantic O
parsing O
problem O
that O
aims O
to O
convert O
natural O
language O
feedback O
to O
a O
sequence O
of O
edits O
that O
can O
be O
deterministically O
applied O
to O
the O
initial O
parse O
to O
correct O
it O
. O
We O
use O
the O
edit O
- O
based O
modeling O
framework O
to O
show O
that O
we O
can O
effectively O
generate O
synthetic O
data O
to O
pre O
- O
train O
the O
correction O
model O
leading O
to O
clear O
performance O
gains O
. O

We O
make O
the O
following O
contributions O
: O
( O
1 O
) O
We O
present O
a O
scheme O
for O
representing O
SQL O
query O
Edits O
that O
benefits O
both O
the O
modeling O
and O
the O
analysis O
of O
the O
correction O
task O
, O
( O
2 O
) O
we O
present O
NL B-MethodName
- I-MethodName
EDIT I-MethodName
, O
an O
edit O
- O
based O
model O
for O
interactive O
text O
- O
to O
- O
SQL O
with O
natural O
language O
feedback O
. O
We O
show O
that O
NL O
- O
EDIT O
outperforms O
baselines O
in O
( O
Elgohary O
et O
al O
. O
, O
2020 O
) O
by O
more O
than O
16 O
points O
, O
( O
3 O
) O
We O
demonstrate O
that O
we O
can O
generate O
synthetic O
data O
through O
the O
edit O
- O
based O
framing O
and O
that O
the O
model O
can O
effectively O
use O
this O
data O
to O
improve O
its O
accuracy O
and O
( O
4 O
) O
We O
present O
a O
detailed O
analysis O
of O
the O
model O
performance O
including O
studying O
the O
effect O
of O
different O
components O
, O
generalization O
to O
errors O
of O
state O
- O
of O
- O
the O
- O
art O
parsers O
, O
and O
outline O
directions O
for O
future O
research O
. O

Background O

In O
the O
task O
of O
text O
- O
to O
- O
SQL O
parsing O
, O
the O
objective O
is O
given O
a O
database O
schema O
( O
tables O
, O
columns O
, O
and O
primary O
- O
foreign O
key O
relations O
) O
and O
a O
natural O
language O
question O
, O
generate O
a O
SQL O
query O
that O
answers O
the O
question O
when O
executed O
against O
the O
database O
. O
Several O
recent O
text O
- O
to O
- O
SQL O
models O
have O
been O
introduced O
( O
Yu O
et O
al O
. O
, O
2018a O
; O
Guo O
et O
al O
. O
, O
2019 O
; O
Wang O
et O
al O
. O
, O
2020 O
, O
inter O
alia O
) O
as O
a O
result O
of O
the O
availability O
of O
SPIDER O
( O
Yu O
et O
al O
. O
, O
2018b O
) O
, O
a O
large O
dataset O
of O
schema O
, O
questions O
and O
gold O
parses O
spanning O
several O
databases O
in O
different O
domains O
. O

The O
task O
of O
SQL O
parse O
correction O
with O
natural O
language O
feedback O
( O
Elgohary O
et O
al O
. O
, O
2020 O
) O
aims O
to O
correct O
an O
erroneous O
parse O
based O
on O
natural O
language O
feedback O
collected O
from O
the O
user O
. O
Given O
a O
question O
, O
a O
database O
schema O
, O
an O
incorrect O
initial O
parse O
, O
natural O
language O
feedback O
on O
the O
initial O
parse O
, O
the O
task O
is O
to O
generate O
a O
corrected O
parse O
. O

To O
study O
this O
problem O
, O
Elgohary O
et O
al O
. O
( O
2020 O
) O
introduced O
the O
SPLASH B-DatasetName
dataset O
. O
SPLASH B-DatasetName
was O
created O
by O
showing O
annotators O
questions O
and O
a O
natural O
language O
explanation O
of O
incorrect O
parses O
and O
asking O
them O
to O
provide O
feedback O
, O
in O
natural O
language O
, O
to O
correct O
the O
parse O
. O
The O
dataset O
contained O
9,314 O
question O
- O
feedback O
pairs O
. O
Like O
the O
SPIDER O
dataset O
, O
it O
was O
split O
into O
train O
- O
dev O
- O
test O
sets O
by O
database O
to O
encourage O
the O
models O
to O
generalize O
to O
new O
unseen O
databases O
. O
They O
contrast O
the O
task O
with O
conversational O
semantic O
parsing O
( O
Suhr O
et O
al O
. O
, O
2018 O
; O
Yu O
et O
al O
. O
, O
2019b O
, O
a O
; O
Andreas O
et O
al O
. O
, O
2020 O
) O
and O
show O
that O
the O
two O
tasks O
are O
distinct O
and O
are O
addressing O
different O
aspects O
of O
utilizing O
context O
. O
They O
establish O
several O
baseline O
models O
and O
show O
that O
the O
task O
is O
challenging O
for O
state O
- O
of O
- O
the O
- O
art O
semantic O
parsing O
models O
. O
We O
use O
these O
as O
baselines O
for O
this O
work O
. O

SQL O
Edits O

We O
define O
a O
scheme O
for O
representing O
the O
edits O
required O
to O
transform O
one O
SQL O
query O
to O
another O
. O
We O
use O
that O
scheme O
both O
in O
our O
model O
and O
analysis O
. O
Our O
goal O
is O
to O
balance O
the O
granularity O
of O
the O
editstoo O
fine O
- O
grained O
edits O
result O
in O
complex O
structures O
that O
are O
challenging O
for O
models O
to O
learn O
, O
and O
too O
coarse O
- O
grained O
edits O
result O
in O
less O
compact O
structures O
that O
are O
harder O
for O
models O
to O
generate O
. O

We O
view O
a O
SQL O
query O
as O
a O
set O
of O
clauses O
( O
e.g O
, O
SELECT O
, O
FROM O
, O
WHERE O
) O
, O
each O
clause O
has O
a O
sequence O
of O
arguments O
( O
Figure O
2 O
) O
. O
We O
mirror O
the O
SQL O
clauses O
SELECT O
, O
FROM O
, O
WHERE O
, O
GROUP O
- O
BY O
, O
ORDER O
- O
BY O
, O
HAVING O
, O
and O
LIMIT O
. O
For O
subqueries O
, O
we O
define O
a O
clause O
SUBS O
whose O
arguments O
are O
recursively O
defined O
as O
sets O
of O
clauses O
. O
Subqueries O
can O
be O
linked O
to O
the O
main O
query O
in O
two O
ways O
: O
either O
through O
an O
IEU O
clause O
( O
mirrors O
SQL O
INTERSECT O
/ O
EXCEPT O
/ O
UNION O
) O
whose O
first O
argument O
is O
one O
of O
the O
keywords O
INTERSECT O
, O
EXCEPT O
, O
UNION O
and O
its O
second O
argument O
is O
a O
pointer O
to O
a O
subquery O
in O
SUBS O
. O
The O
second O
is O
through O
nested O
queries O
where O
the O
arguments O
of O
some O
of O
the O
clauses O
( O
e.g. O
, O
WHERE O
) O
can O
point O
at O
subqueries O
in O
SUBS O
( O
e.g. O
, O
" O
i O
d O
NOT O
IN O
SUBS O
1 O
" O
) O
. O

With O
such O
view O
of O
two O
queries O
P O
source O
and O
P O
target O
, O
we O
define O
their O
edit O
D O
source→target O
as O
SELECT O
: O
arg O
1 O
: O
" O
i O
d O
" O
, O
arg O
2 O
: O
" O
MAX O
( O
grade O
) O
" O
FROM O
: O
arg O
1 O
: O
" O
assignments O
" O
GROUP O
- O
BY O
: O
arg O
1 O
: O
" O
i O
d O
" O
the O
set O
of O
clause O
- O
level O
edits O
{ O
D O
c O
source→target O
} O
for O
all O
types O
of O
clauses O
c O
that O
appear O
in O
P O
source O
or O
P O
target O
( O
Figure O
2 O
) O
. O
To O
compare O
two O
clauses O
of O
type O
c O
, O
we O
simply O
exact O
- O
match O
their O
arguments O
: O
unmatched O
arguments O
in O
the O
source O
( O
e.g. O
, O
MAX O
( O
grade O
) O
in O
SELECT O
) O
are O
added O
as O
toremove O
arguments O
to O
the O
corresponding O
edit O
clause O
, O
and O
unmatched O
arguments O
in O
the O
target O
( O
e.g. O
, O
" O
i O
d O
" O
in O
the O
ORDER O
- O
BY O
) O
are O
added O
as O
to O
- O
add O
arguments O
. O

Our O
current O
implementation O
follows O
SPIDER O
's O
assumption O
that O
the O
number O
of O
subqueries O
is O
at O
most O
one O
which O
implies O
that O
computing O
edits O
for O
different O
clauses O
can O
be O
done O
independently O
even O
for O
the O
clauses O
that O
reference O
a O
subquery O
( O
e.g. O
, O
WHERE O
in O
Figure O
2 O
) O
. O
The O
edit O
of O
the O
SUBS O
clause O
is O
recursively O
computed O
as O
the O
edit O
between O
two O
queries O
( O
any O
of O
them O
can O
be O
empty O
) O
; O
the O
subquery O
of O
source O
and O
the O
subquery O
of O
target O
, O
i.e. O
, O
D O
SUBS O
source→target O
= O
D O
source O
: O
SUBS O
1 O
→target O
: O
SUBS O
1 O
. O
We O
keep O
track O
of O
the O
edits O
to O
the O
arguments O
that O
reference O
the O
subquery O
. O
After O
all O
edit O
clauses O
are O
computed O
, O
we O
prune O
the O
edits O
of O
the O
SUBS O
clause O
if O
the O
subquery O
will O
no O
longer O
be O
referenced O
( O
SUBS O
1 O
in O
Figure O
2 O
) O
. O
We O
follow O
the O
SPIDER O
evaluation O
and O
discard O
the O
values O
in O
WHERE O
/ O
HAVING O
clauses O
. O

Throughout O
this O
paper O
, O
we O
refer O
to O
the O
number O
of O
add O
/ O
remove O
operations O
in O
an O
edit O
as O
the O
Edit O
Size O
, O
and O
we O
denote O
it O
as O
|D O
source→target O
| O
. O
For O
example O
, O
the O
edit O
in O
Figure O
2 O
is O
of O
size O
four O
. O

Model O

We O
follow O
the O
task O
description O
in O
Section O
2 O
: O
the O
inputs O
to O
the O
model O
are O
the O
elements O
of O
the O
interaction O
- O
question O
, O
schema O
, O
an O
initial O
parse O
P O
, O
and O
feedback O
. O
The O
model O
predicts O
a O
corrected O
P O
. O
The O
gold O
parse O
P O
is O
available O
for O
training O
. O
Our O
model O
is O
based O
on O
integrating O
two O
key O
ideas O
in O
an O
encoder O
- O
decoder O
architecture O
. O
We O
start O
with O
a O
discussion O
of O
the O
intuitions O
behind O
the O
two O
ideas O
followed O
by O
the O
model O
details O
. O

Intuitions O

Feedback O
describes O
a O
set O
of O
edits O
: O
The O
difference O
between O
the O
erroneous O
parse O
and O
the O
correct O
one O
can O
mostly O
be O
described O
as O
a O
few O
edits O
that O
need O
to O
be O
applied O
to O
the O
initial O
parse O
to O
correct O
its O
errors O
( O
Section O
7 O
) O
. O
Also O
, O
the O
feedback O
often O
only O
describes O
the O
edits O
to O
be O
made O
( O
Elgohary O
et O
al O
. O
, O
2020 O
) O
. O
As O
such O
, O
we O
can O
pose O
the O
task O
of O
correction O
with O
NL O
feedback O
as O
a O
semantic O
parsing O
task O
where O
we O
convert O
a O
natural O
language O
deception O
of O
5602 O
the O
edits O
to O
a O
canonical O
form O
that O
can O
be O
applied O
deterministically O
to O
the O
initial O
parse O
to O
generate O
the O
corrected O
one O
. O
We O
train O
our O
model O
to O
generate O
SQL O
Edits O
( O
Section O
3 O
) O
rather O
than O
SQL O
queries O
. O

Encoder O

Our O
encoder O
( O
Figure O
3 O
) O
starts O
with O
passing O
the O
concatenation O
of O
the O
feedback O
, O
explanation O
, O
question O
, O
and O
schema O
through O
BERT O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
. O
Following O
( O
Wang O
et O
al O
. O
, O
2020 O
; O
Suhr O
et O
al O
. O
, O
2018 O
; O
Scholak O
et O
al O
. O
, O
2020 O
) O
, O
we O
tokenize O
the O
column O
/ O
table O
names O
and O
concatenate O
them O
in O
one O
sequence O
( O
Schema O
) O
starting O
with O
the O
tokens O
of O
the O
tables O
followed O
by O
the O
tokens O
of O
the O
columns O
. O
Then O
, O
we O
average O
the O
BERT O
embeddings O
of O
the O
tokens O
corresponding O
to O
each O
column O
( O
table O
) O
to O
obtain O
one O
representation O
for O
the O
column O
( O
table O
) O
. O
Wang O
et O
al O
. O
( O
2020 O
) O
study O
the O
text O
- O
to O
- O
SQL O
problem O
using O
the O
SPIDER O
dataset O
and O
show O
the O
benefit O
of O
injecting O
preexisting O
relations O
within O
the O
schema O
( O
column O
exists O
in O
a O
table O
, O
primary O
- O
foreign O
key O
) O
, O
and O
between O
the O
question O
and O
schema O
items O
( O
column O
and O
table O
names O
) O
by O
: O
( O
1 O
) O
name O
linking O
: O
link O
a O
question O
token O
to O
a O
column O
/ O
table O
if O
the O
token O
and O
the O
item O
name O
match O
and O
( O
2 O
) O
value O
linking O
: O
link O
a O
question O
token O
to O
a O
column O
if O
the O
token O
appears O
as O
a O
value O
under O
that O
column O
. O
To O
incorporate O
such O
relations O
in O
their O
model O
, O
they O
use O
the O
relation O
- O
aware O
self O
- O
attention O
formulation O
presented O
in O
( O
Shaw O
et O
al O
. O
, O
2018 O
) O
. O
The O
relation O
- O
aware O
transformer O
( O
Shaw O
et O
al O
. O
, O
2018 O
) O
assigns O
a O
learned O
embedding O
for O
each O
relation O
type O
and O
combines O
such O
embeddings O
with O
the O
self O
- O
attention O
of O
the O
original O
transformer O
model O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
: O
If O
a O
preexisting O
relation O
r O
holds O
between O
two O
tokens O
, O
the O
embedding O
of O
r O
is O
added O
as O
a O
bias O
term O
to O
the O
self O
- O
attention O
computation O
between O
the O
two O
tokens O
. O

In O
addition O
to O
those O
relations O
, O
we O
define O
a O
new O
set O
of O
relations O
that O
aim O
at O
contextualizing O
the O
feedback O
with O
respect O
to O
the O
other O
elements O
of O
the O
interaction O
in O
our O
setup O
: O
( O
1 O
) O
[ O
Feedback O
- O
Schema O
] O
We O
link O
the O
feedback O
to O
the O
schema O
the O
same O
way O
the O
question O
is O
linked O
to O
the O
schema O
via O
both O
name O
and O
value O
linking O
, O
( O
2 O
) O
[ O
Explanation O
- O
Schema O
] O
Columns O
and O
tables O
are O
mentioned O
with O
their O
exact O
names O
in O
the O
explanation O
. O
We O
link O
the O
explanation O
to O
the O
schema O
only O
through O
exact O
name O
matching O
, O
( O
3 O
) O
[ O
Feedback O
- O
Question O
] O
We O
use O
partial O
( O
at O
the O
lemma O
level O
) O
and O
exact O
matching O
to O
link O
tokens O
in O
the O
feedback O
and O
the O
question O
, O
( O
4 O
) O
[ O
Feedback O
- O
Explanation O
] O
We O
link O
tokens O
in O
the O
feedback O
to O
tokens O
in O
the O
explanation O
through O
partial O
and O
exact O
token O
matching O
. O
Since O
the O
feedback O
often O
refers O
to O
particular O
steps O
, O
we O
link O
the O
feedback O
tokens O
to O
explanation O
tokens O
that O
occur O
in O
steps O
that O
are O
referred O
to O
in O
the O
feedback O
with O
a O
separate O
relation O
type O
that O
indicates O
step O
reference O
in O
the O
feedback O
, O
and O
( O
5 O
) O
[ O
Explanation O
- O
Explanation O
] O
We O
link O
explanation O
tokens O
that O
occur O
within O
the O
same O
step O
. O
We O
use O
the O
same O
formulation O
of O
relationaware O
self O
- O
attention O
as O
( O
Wang O
et O
al O
. O
, O
2020 O
) O
and O
add O
the O
relation O
- O
aware O
layers O
on O
top O
of O
BERT O
to O
integrate O
all O
relations O
into O
the O
model O
( O
Figure O
3 O
) O
. O

Decoder O

Using O
a O
standard O
teacher O
- O
forced O
cross O
- O
entropy O
loss B-HyperparameterName
, O
we O
train O
our O
model O
to O
generate O
linearized O
SQL O
Edits O
( O
Figure O
2 O
) O
. O
At O
training O
time O
, O
we O
compute O
the O
reference O
SQL O
Edit O
D O
P O
→P O
of O
the O
initial O
parse O
P O
and O
the O
gold O
parse O
P O
( O
Section O
3 O
) O
. O
Then O
we O
linearize O
D O
P O
→P O
by O
listing O
the O
clause O
edits O
in O
a O
fixed O
order O
( O
FROM O
, O
WHERE O
, O
GROUP O
- O
BY O
, O
... O
etc O
. O
) O
. O
The O
argument O
of O
each O
clause O
- O
representing O
one O
add O
or O
remove O
operation O
- O
is O
formatted O
as O
< O
CLAUSE O
> O
ADD O
/ O
REMOVE O
ARG O
< O
/ O
CLAUSE O
> O
. O
We O
express O
SQL O
operators O
in O
ARG O
with O
natural O
language O
explanation O
as O
in O
( O
Elgohary O
et O
al O
. O
, O
2020 O
) O
. O
For O
example O
, O
the O
argument O
" O
AVG O
( O
grade O
) O
" O
is O
expressed O
as O
" O
average O
grade O
" O
. O
At O
inference O
time O
, O
we O
generate O
a O
corrected O
parse O
P O
by O
applying O
the O
produced O
edit O
to O
the O
initial O
parse O
P O
. O

We O
use O
a O
standard O
transformer O
decoder O
that O
either O
generates O
tokens O
from O
the O
output O
vocab O
or O
copies O
columns O
and O
tables O
from O
the O
encoder O
output O
. O
Since O
all O
editing O
operations O
should O
be O
directed O
by O
the O
feedback O
, O
we O
tried O
splitting O
the O
attention O
to O
the O
encoder O
into O
two O
phases O
: O
First O
, O
we O
attend O
to O
the O
feedback O
only O
and O
update O
the O
decoder O
state O
Replace O
- O
Select O
- O
Column O
: O

-replace O
{ O
NEW O
- O
COL O
} O
with O
{ O
OLD O
- O
COL O
} O
-you O
should O
find O
{ O
OLD O
- O
COL O
} O
instead O
Add O
- O
Where O
- O
Condition O
: O

-delete O
{ O
COL O
} O
{ O
OPERATOR O
} O
{ O
VALUE O
} O
Remove O
- O
Limit O
: O

-only O
top O
{ O
LIMIT O
- O
VALUE O
} O
rows O
are O
needed O
accordingly O
. O
Then O
, O
we O
use O
the O
updated O
decoder O
state O
to O
attend O
to O
the O
other O
inputs O
. O
With O
that O
, O
we O
only O
observed O
a O
marginal O
improvement O
of O
0.5 O
% O
in O
the O
accuracy B-MetricName
. O
We O
conduct O
all O
our O
experiments O
with O
standard O
decoder O
- O
encoder O
attention O
and O
plan O
to O
investigate O
other O
attention O
patterns O
in O
the O
future O
. O

Synthetic O
Feedback O

In O
this O
section O
, O
we O
describe O
our O
process O
for O
automatically O
synthesizing O
additional O
examples O
for O
training O
the O
correction O
model O
. O
Recall O
that O
each O
example O
consists O
of O
a O
question O
about O
a O
given O
schema O
paired O
with O
a O
gold O
parse O
, O
an O
initial O
erroneous O
parse O
, O
and O
feedback O
. O
Starting O
with O
a O
seed O
of O
questions O
and O
their O
corresponding O
gold O
parses O
from O
SPIDER O
's O
training O
set O
( O
8,099 O
pairs O
) O
1 O
, O
our O
synthesis O
process O
applies O
a O
sequence O
of O
SQL O
editing O
operations O
to O
the O
gold O
parse O
to O
reach O
an O
altered O
parse O
that O
we O
use O
as O
the O
initial O
parse O
( O
Algorithm O
1 O
) O
. O
By O
manually O
inspecting O
the O
edits O
( O
Section O
3 O
) O
we O
induce O
for O
the O
initial O
and O
gold O
parses O
in O
SPLASH B-DatasetName
training O
set O
, O
we O
define O
26 O
SQL O
editors O
and O
pair O
each O
editor O
with O
their O
most O
frequent O
corresponding O
feedback O
template O
( O
s O
) O
( O
Examples O
in O
Table O
1 O
) O
. O
We O
also O
associate O
each O
editor O
with O
a O
set O
of O
constraints O
that O
determines O
whether O
it O
can O
be O
applied O
to O
a O
given O
SQL O
query O
( O
e.g. O
, O
the O
" O
Remove O
- O
Limit O
" O
editor O
can O
only O
be O
applied O
to O
a O
query O
that O
has O
a O
limit O
clause O
) O
. O

Algorithm O
1 O
summarizes O
the O
synthesis O
process O
. O
We O
start O
by O
creating O
N O
( O
controls O
the O
size O
of O
the O
dataset O
) O
clones O
of O
each O
seed O
example O
. O
Elgohary O
et O
al O
. O
( O
2020 O
) O
's O
analysis O
of O
SPLASH B-DatasetName
shows O
that O
multiple O
mistakes O
might O
be O
present O
in O
the O
initial O
SQL O
, O
hence O
we O
allow O
our O
synthesis O
process O
to O
introduce O
up O
to O
four O
edits O
( O
randomly O
decided O
in O
line:4 O
) O
to O
each O
clone O
p. O
For O
each O
editing O
step O
, O
we O
sample O
a O
feasible O
edit O
for O
the O
current O
parse O
( O
line:5 O
) O
with O
man- O
We O
use O
BERT B-HyperparameterName
- I-HyperparameterName
base I-HyperparameterName
- I-HyperparameterName
uncased I-HyperparameterName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
in O
all O
our O
experiments O
. O
We O
set O
the O
number B-HyperparameterName
of I-HyperparameterName
layers I-HyperparameterName
in O
the O
relational O
- O
aware O
transformer O
to O
eight O
( O
Wang O
et O
al O
. O
, O
2020 O
) O
and O
the O
number O
of O
decoder B-HyperparameterName
layers I-HyperparameterName
to O
two O
. O
We O
train O
with O
batches B-HyperparameterName
of O
size B-HyperparameterName
24 B-HyperparameterValue
. O
We O
use O
the O
Adam O
optimizer B-HyperparameterName
( O
Kingma O
and O
Ba O
, O
2015 O
) O
for O
training O
. O
We O
freeze O
BERT O
parameters O
during O
the O
first O
5,000 O
warm O
- O
up O
steps O
and O
update O
the O
rest O
of O
the O
parameters O
with O
a O
linearly O
increasing O
learning B-HyperparameterValue
rate I-HyperparameterValue
from O
zero O
to O
5 B-HyperparameterValue
× I-HyperparameterValue
10 I-HyperparameterValue
−4 I-HyperparameterValue
. O
Then O
, O
we O
linearly O
decrease O
the O
learning B-HyperparameterName
rates I-HyperparameterName
from O
5 B-HyperparameterValue
× I-HyperparameterValue
10 I-HyperparameterValue
−5 I-HyperparameterValue
for O
BERT O
and O
5 B-HyperparameterValue
× I-HyperparameterValue
10 I-HyperparameterValue
−4 I-HyperparameterValue
for O
the O
other O
parameters O
to O
zero O
. O
2 O
We O
use O
beam O
search O
with O
a O
beam O
of O
size O
20 O
and O
take O
the O
top O
- O
ranked O
beam O
that O
results O
in O
a O
valid O
SQL O
after O
applying O
the O
inferred O
edit O
. O
Evaluation O
: O
We O
follow O
( O
Elgohary O
et O
al O
. O
, O
2020 O
) O
and O
use O
the O
correction B-MetricName
accuracy I-MetricName
as O
our O
main O
evaluation O
measure O
: O
each O
example O
in O
SPLASH B-DatasetName
test O
set O
contains O
an O
initial O
parse O
P O
and O
a O
gold O
parse O
P O
. O
With O
a O
predicted O
( O
corrected O
) O
parse O
by O
a O
correction O
model O
P O
, O
they O
compute O
the O
correction O
accuracy O
using O
the O
exact O
- O
set O
- O
match O
( O
Yu O
et O
al O
. O
, O
2018b O
) O
between O
P O
and O
P O
averaged O
over O
all O
test O
examples O
. O
While O
useful O
, O
correction O
accuracy O
also O
has O
limitations O
. O
It O
expects O
models O
to O
be O
able O
to O
fully O
correct O
an O
erroneous O
parse O
with O
only O
one O
utterance O
of O
feedback O
as O
such O
, O
it O
is O
defined O
in O
terms O
of O
the O
exact O
match O
between O
the O
corrected O
and O
the O
gold O
parse O
. O
We O
find O
( O
Table O
2 O
) O
that O
in O
several O
cases O
, O
models O
were O
still O
able O
to O
make O
progress O
by O
reducing O
the O
number O
of O
errors O
as O
measured O
by O
the O
edit O
size O
( O
Section O
3 O
) O
after O
correction O
. O
As O
such O
, O
we O
define O
another O
set O
of O
metrics O
to O
measure O
partial O
progress O
. O
We O
report O
( O
Edit B-MetricName
↓ I-MetricName
and O
Edit B-MetricName
↑ I-MetricName
in O
Table O
2 O
) O
the O
percentage O
of O
examples O
on O
which O
the O
size O
of O
the O
edit O
set O
strictly O
decreased O
/ O
increased O
. O
To O
combine O
Edit O
↓ O
and O
Edit B-MetricName
↑ I-MetricName
in O
one O
measure O
and O
account O
for O
the O
relative O
reduction O
( O
increase O
) O
in O
the O
number O
of O
edits O
, O
we O
define O

Progress B-MetricName
( O
S O
) O
= O
1 O
|S| O
∑︂ O
P O
, O
P O
, O
P O
∈S O
|D O
P O
→P O
| O
− O
|D O
P O
→P O
| O
|D O
P O
→P O
| O
. O

Given O
a O
test O
set O
S O
, O
the O
Progress B-MetricName
of O
a O
correction O
model O
is O
computed O
as O
the O
average O
relative O
edit O
reduction O
between O
the O
initial O
parse O
P O
and O
the O
gold O
parse O
P O
by O
predicting O
a O
correction O
P O
of O
P O
. O
A O
perfect O
model O
that O
can O
fully O
correct O
all O
errors O
in O
the O
initial O
parse O
would O
achieve O
a O
100 B-MetricValue
% I-MetricValue
progress B-MetricName
. O
A O
model O
can O
have O
a O
negative O
progress B-MetricName
( O
e.g. O
, O
Rulebased O
re O
- O
ranking O
in O
Table O
2 O
) O
when O
it O
frequently O
predicts O
corrections O
with O
more O
errors O
than O
those O
in O
the O
initial O
parse O
. O
Unlike O
correction O
accuracy O
, O
Progress B-MetricName
is O
more O
aligned O
with O
user O
experience O
in O
an O
interactive O
environment O
as O
it O
assigns O
partial O
credit O
for O
fixing O
a O
subset O
of O
the O
errors O
and O
also O
, O
it O
penalizes O
models O
that O
predict O
an O
even O
more O
erroneous O
parse O
after O
receiving O
feedback O
. O

Results O
: O
We O
compare O
( O
Table O
2 O
) O
NL B-MethodName
- I-MethodName
EDIT I-MethodName
to O
the O
two O
top O
- O
performing O
baselines O
in O
( O
Elgohary O
et O
al O
. O
, O
2020 O
) O
and O
also O
to O
the O
beam O
re O
- O
ranking O
upper O
- O
bound O
they O
report O
. O
NL B-MethodName
- I-MethodName
EDIT I-MethodName
significantly O
increases O
the O
correction B-MetricName
accuracy I-MetricName
over O
the O
top O
baseline O
( O
Edit O
- O
SQL+Feedback O
) O
by O
more O
than O
16 B-MetricValue
% I-MetricValue
and O
it O
also O
outperforms O
oracle O
re O
- O
ranking O
by O
around O
5 B-MetricValue
% I-MetricValue
. O
We O
also O
note O
that O
in O
72.4 O
% O
of O
the O
test O
examples O
, O
NL B-MethodName
- I-MethodName
EDIT I-MethodName
was O
able O
to O
strictly O
reduce O
the O
number O
of O
errors O
in O
the O
initial O
parse O
( O
Edit O
↓ O
) O
which O
potentially O
indicates O
a O
more O
positive O
user O
experience O
than O
the O
other O
models O
. O
NL B-MethodName
- I-MethodName
EDIT I-MethodName
achieves O
37 O
% O
Progress O
which O
indicates O
faster O
convergence O
to O
the O
fully O
corrected O
parse O
than O
all O
the O
other O
models O
. O

Analysis O

Ablations O

Following O
the O
same O
experimental O
setup O
in O
Section O
6 O
, O
we O
compare O
NL B-MethodName
- I-MethodName
EDIT I-MethodName
to O
other O
variants O
with O
one O
ablated O
component O
at O
a O
time O
( O
Table O
3 O
) O
. O
We O
ablate O
the O
feedback O
, O
the O
explanation O
, O
and O
the O
question O
from O
the O
encoder O
input O
. O
We O
also O
ablate O
the O
interaction O
relations O
( O
Section O
4.2 O
) O
that O
we O
incorporate O
in O
the O
relation O
- O
aware O
transformer O
module O
. O
We O
only O
ablate O
the O
new O
relations O
we O
introduce O
to O
model O
the O
interaction O
( O
shown O
in O
Figure O
3 O
) O
, O
but O
we O
keep O
the O
Question O
- O
Schema O
and O
Schema O
- O
Schema O
relations O
introduced O
in O
( O
Wang O
et O
al O
. O
, O
2020 O
) O
. O
For O
each O
such O
variant O
, O
we O
train O
for O
20,000 O
steps O
on O
the O
synthetic O
dataset O
then O
continue O
training O
on O
SPLASH B-DatasetName
until O
step O
100,000 O
. O
We O
also O
train O
an O
ablated O
variant O
that O
does O
not O
use O
the O
synthetic O
feedback O
where O
we O
train O
for O
100,000 O
steps O
only O
on O
SPLASH B-DatasetName
. O
For O
all O
variants O
, O
we O
choose O
the O
checkpoint O
with O
the O
largest O
correction O
accuracy O
on O
the O
dev O
set O
and O
report O
the O
accuracy O
on O
the O
SPLASH B-DatasetName
test O
set O
. O
The O
results O
in O
Table O
3 O
confirm O
the O
effectiveness O
of O
each O
component O
in O
our O
model O
. O
We O
find O
that O
the O
model O
is O
able O
to O
correct O
19.8 B-MetricValue
% I-MetricValue
of O
the O
examples O
without O
the O
feedback O
. O
We O
noticed O
that O
the O
ablatedfeedback O
model O
almost O
reaches O
that O
accuracy O
only O
after O
training O
on O
the O
synthetic O
data O
with O
very O
minor O
improvement O
( O
< O
1 O
% O
) O
after O
training O
on O
SPLASH B-DatasetName
. O
Only O
using O
the O
question O
and O
the O
explanation O
, O
the O
model O
is O
able O
to O
learn O
about O
a O
set O
of O
systematic O
errors O
that O
parsers O
make O
and O
how O
they O
can O
be O
corrected O
( O
Gupta O
et O
al O
. O
, O
2017 O
; O
Yin O
and O
Neubig O
, O
2019 O
) O
. O

Error O
Analysis O

In O
Figure O
4 O
, O
we O
breakdown O
the O
correction B-MetricName
accuracy I-MetricName
by O
the O
feedback O
and O
explanation O
lengths O
( O
in O
number O
of O
tokens O
) O
and O
by O
the O
reference O
edit O
size O
( O
number O
of O
required O
edit O
operations O
to O
fully O
correct O
the O
initial O
parse O
) O
. O
The O
accuracy O
drops O
significantly O
when O
the O
reference O
edit O
size O
exceeds O
two O
( O
Figure O
4c O
) O
, O
while O
it O
declines O
more O
gradually O
as O
the O
feedback O
and O
explanation O
increase O
in O
length O
. O
We O
manually O
( O
Examples O
in O
Table O
4 O
) O
inspected O
the O
examples O
with O
longer O
feedback O
than O
24 O
, O
and O
found O
that O
8 O
% O
of O
them O
the O
feedback O
is O
long O
because O
it O
describes O
how O
to O
rewrite O
the O
whole O
query O
rather O
than O
being O
lim O
- O
Long O
Feedback O
Not O
Describing O
an O
Edit O
: O
" O
you O
should O
determine O
the O
major O
record O
format O
from O
the O
orchestra O
table O
and O
make O
sure O
it O
is O
arranged O
in O
ascending O
order O
of O
number O
of O
rows O
that O
appear O
for O
each O
major O
record O
format O
. O
" O

Long O
Feedback O
Describing O
an O
Edit O
: O
" O
replace O
course O
i O
d O
( O
both O
) O
with O
degree O
program O
i O
d O
, O
first O
courses O
with O
student O
enrolment O
, O
course O
description O
with O
degree O
summary O
name O
, O
second O
courses O
with O
degree O
programs O
. O
" O
Table O
4 O
: O
Example O
long O
feedback O
that O
NL B-MethodName
- I-MethodName
EDIT I-MethodName
struggles O
with O
. O
Top O
: O
The O
feedback O
describes O
a O
rewriting O
of O
the O
query O
rather O
than O
how O
to O
edit O
it O
. O
Bottom O
: O
The O
initial O
query O
has O
several O
errors O
and O
the O
feedback O
enumerates O
how O
to O
edit O
all O
of O
them O
. O
ited O
to O
only O
the O
edits O
to O
be O
made O
. O
In O
the O
remaining O
92 O
% O
, O
the O
initial O
query O
had O
several O
errors O
( O
edit O
size O
of O
5.5 O
on O
average O
) O
with O
the O
corresponding O
feedback O
enumerating O
all O
of O
them O
. O

Figure O
4d O
shows O
how O
the O
number O
of O
errors O
( O
measured O
in O
edit O
size O
) O
changes O
after O
correction O
. O
The O
figure O
shows O
that O
even O
for O
examples O
with O
a O
large O
number O
of O
errors O
( O
four O
and O
five O
) O
, O
the O
model O
is O
still O
able O
to O
reduce O
the O
number O
of O
errors O
in O
most O
cases O
. O
We O
manually O
inspected O
the O
examples O
with O
only O
one O
error O
that O
the O
model O
failed O
to O
correct O
. O
We O
found O
15 O
% O
of O
them O
have O
either O
wrong O
or O
non O
- O
editing O
feedback O
and O
in O
29 O
% O
the O
model O
produced O
the O
correct O
edit O
but O
with O
additional O
irrelevant O
ones O
. O
The O
dominant O
source O
of O
error O
in O
the O
remaining O
examples O
is O
because O
of O
failures O
with O
linking O
the O
feedback O
to O
the O
schema O
( O
Examples O
in O
Table O
5 O
) O
. O

Cross O
- O
Parser O
Generalization O

So O
far O
, O
we O
have O
been O
using O
SPLASH B-DatasetName
for O
both O
training O
and O
testing O
. O
The O
erroneous O
parses O
( O
and O
corresponding O
feedback O
) O
in O
SPLASH O
are O
based O
on O
the O
Seq2Struct O
parser O
( O
Shin O
, O
2019 O
) O
. O
Recent O
progress O
in O
model O
architectures O
( O
Wang O
et O
al O
. O
, O
2020 O
) O
and O
pretraining O
( O
Yin O
et O
al O
. O
, O
2020 O
; O
Yu O
et O
al O
. O
, O
2021a O
) O
has O
led O
to O
parsers O
that O
already O
outperform O
Seq2Struct O
by O
more O
than O
30 O
% O
in O
parsing O
accuracy O
. O
3 O
Here O
, O
we O
ask O
whether O
NL B-MethodName
- I-MethodName
EDIT I-MethodName
that O
we O
train O
on O
SPLASH B-DatasetName
( O
and O
synthetic O
feedback O
) O
can O
generalize O
to O
parsing O
errors O
made O
by O
more O
recent O
parsers O
without O
additional O
parser O
- O
specific O
training O
data O
. O
We O
follow O
the O
same O
crowdsourcing O
process O
used O
to O
construct O
SPLASH B-DatasetName
( O
Section O
2 O
) O
to O
collect O
three O
new O
test O
sets O
based O
on O
three O
recent O
textto O
- O
SQL O
parsers O
: O
EditSQL B-HyperparameterName
, O
TaBERT B-HyperparameterName
( O
Yin O
et O
al O
. O
, O
2020 O
) O
and O
RAT B-HyperparameterName
- I-HyperparameterName
SQL I-HyperparameterName
( O
Wang O
et O
al O
. O
, O
2020 O
) O
. O
Following O
Elgohary O
et O
al O
. O
( O
2020 O
) O
, O
we O
run O
each O
parser O
on O
SPIDER O
dev O
set O
and O
only O
collect O
feedback O
for O
the O
examples O
with O
incorrect O
parses O
that O
can O
be O
explained O
using O
their O
SQL O
explanation O
3 O
https O
: O
/ O
/ O
yale-lily.github.io O
/ O
spider O
framework O
. O
Table O
6 O
( O
Top O
) O
summarizes O
the O
three O
new O
test O
sets O
and O
compares O
them O
to O
SPLASH O
test O
set O
. O
We O
note O
that O
the O
four O
datasets O
are O
based O
on O
the O
same O
set O
of O
questions O
and O
databases O
( O
SPIDER O
dev O
) O
. O

Table O
6 O
( O
Bottom O
) O
compares O
the O
parsing O
accuracy B-MetricName
( O
measure O
by O
exact O
query O
match O
( O
Yu O
et O
al O
. O
, O
2018b O
) O
) O
of O
each O
parser O
when O
used O
by O
itself O
( O
No O
Interaction O
) O
to O
integrating O
it O
with O
NL B-MethodName
- I-MethodName
EDIT I-MethodName
. O
We O
report O
both O
the O
accuracy O
on O
the O
examples O
provided O
to O
NL B-MethodName
- I-MethodName
EDIT I-MethodName
( O
Error O
Correction O
) O
and O
the O
End O
- O
to O
- O
End O
accuracy O
on O
the O
full O
SPIDER O
dev O
set O
. O
NL B-MethodName
- I-MethodName
EDIT I-MethodName
significantly O
boosts O
the O
accuracy O
of O
all O
parsers O
, O
but O
with O
a O
notable O
drop O
in O
the O
gains O
as O
the O
accuracy O
of O
the O
parser O
improves O
. O
To O
explain O
that O
, O
in O
Figure O
5 O
we O
compare O
the O
distribution O
of O
reference O
edit O
size O
across O
the O
four O
datasets O
. O
The O
figure O
does O
not O
show O
any O
significant O
differences O
in O
the O
distributions O
that O
would O
lead O
to O
such O
a O
drop O
in O
accuracy O
gain O
. O
Likewise O
, O
the O
distributions O
of O
the O
feedback O
lengths O
are O
very O
similar O
( O
the O
mean O
is O
shown O
in O
Table O
6 O
) O
. O
As O
parsers O
improve O
in O
accuracy O
, O
they O
tend O
to O
make O
most O
of O
their O
errors O
on O
complex O
SQL O
queries O
. O
Although O
the O
number O
of O
errors O
with O
each O
query O
does O
not O
significantly O
change O
( O
Figure O
5 O
) O
, O
we O
hypothesize O
that O
localizing O
the O
errors O
in O
a O
complex O
initial O
parse O
, O
with O
a O
long O
explanation O
( O
Table O
6 O
) O
, O
is O
the O
main O
generalization O
bottleneck O
that O
future O
work O
needs O
to O
address O
. O

Related O
Work O
and O
Discussion O

Natural O
language O
to O
SQL O
: O
Natural O
language O
interfaces O
to O
databases O
have O
been O
an O
active O
field O
of O
study O
for O
many O
years O
( O
Woods O
et O
al O
. O
, O
1972 O
; O
Warren O
and O
Pereira O
, O
1982 O
; O
Popescu O
et O
al O
. O
, O
2003 O
; O
Li O
and O
Jagadish O
, O
2014 O
) O
. O
The O
development O
of O
new O
large O
scale O
datasets O
, O
such O
as O
WikiSQL O
( O
Zhong O
et O
al O
. O
, O
2017 O
) O
and O
SPIDER O
( O
Yu O
et O
al O
. O
, O
2018b O
) O
, O
has O
reignited O
the O
interest O
in O
this O
area O
with O
several O
new O
models O
introduced O
recently O
( O
Choi O
et O
al O
. O
, O
2020 O
; O
Wang O
et O
al O
. O
, O
2020 O
; O
Scholak O
et O
al O
. O
, O
2020 O
) O
. O
Another O
related O
line O
of O
work O
has O
focused O
on O
conversation O
semantic O
parsing O
, O
e.g. O
SParC O
( O
Yu O
et O
al O
. O
, O
2019b O
) O
, O
CoSQL O
( O
Yu O
et O
al O
. O
, O
2019a O
) O
, O
and O
SMCalFlow O
( O
Andreas O
et O
al O
. O
, O
2020 O
) O
, O
where O
parsers O
aim O
at O
modeling O
utterance O
sequentially O
and O
in O
context O
of O
previous O
utterances O
. O

Interactive O
Semantic B-TaskName
Parsing I-TaskName
: O
Several O
previous O
studies O
have O
looked O
at O
the O
problem O
of O
improving O
semantic O
parser O
with O
feedback O
or O
human O
interactions O
( O
Clarke O
et O
al O
. O
, O
2010 O
; O
Artzi O
and O
Zettlemoyer O
, O
2013 O
) O
. O
Interactions O
are O
supported O
in O
multiple O
ways O
including O
binary O
correct O
/ O
incorrect O
signal O
( O
Iyer O
et O
al O
. O
, O
2017 O
) O
, O
answers O
to O
a O
yes O
/ O
no O
or O
a O
multiple O
- O
choice O
question O
posed O
by O
the O
system O
( O
Yao O
et O
al O
. O
, O
2019 O
; O
Gur O
et O
al O
. O
, O
2018 O
) O
or O
suggestions O
of O
edits O
that O
can O
be O
applied O
to O
the O
parse O
. O
Yao O
et O
al O
. O
( O
2019 O
) O
and O
Gur O
et O
al O
. O
( O
2018 O
) O
ask O
yes O
/ O
no O
and O
multiple O
- O
choice O
questions O
and O
use O
the O
answers O
in O
generating O
the O
pars O
. O
Elgohary O
et O
al O
. O
( O
2020 O
) O
introduce O
SPLASH B-DatasetName
( O
Section O
2 O
) O
, O
a O
dataset O
for O
correcting O
semantic O
parsing O
with O
natural O
language O
feedback O
. O
Using O
language O
as O
a O
medium O
for O
providing O
feedback O
enables O
the O
human O
to O
provide O
rich O
open O
- O
form O
feedback O
in O
their O
natural O
way O
of O
communication O
giving O
them O
control O
and O
flexibility O
specifying O
what O
is O
wrong O
and O
how O
it O
should O
be O
corrected O
. O
Our O
work O
uses O
SPLASH B-DatasetName
and O
proposes O
to O
pose O
the O
problem O
of O
semantic O
parse O
correction O
as O
a O
parser O
editing O
problem O
with O
natural O
language O
feedback O
input O
. O
This O
is O
also O
related O
to O
recent O
work O
on O
casting O
text O
generation O
( O
e.g. O
summarization O
, O
grammatical O
error O
correction O
, O
sentence O
splitting O
, O
etc O
. O
) O
as O
a O
text O
editing O
task O
( O
Malmi O
et O
al O
. O
, O
2019 O
; O
Panthaplackel O
et O
al O
. O
, O
2020 O
; O
Stahlberg O
and O
Kumar O
, O
2020 O
) O
where O
target O
texts O
are O
reconstructed O
from O
inputs O
using O
several O
edit O
operations O
. O

Semantic B-TaskName
Parsing I-TaskName
with O
Synthetic O
Data O
: O
Semantic O
parsing O
systems O
have O
frequently O
used O
synthesized O
data O
to O
alleviate O
the O
challenge O
of O
labeled O
data O
scarcity O
. O
In O
their O
semantic O
parser O
overnight O
work O
, O
Wang O
et O
al O
. O
( O
2015 O
) O
proposed O
a O
method O
for O
training O
semantic O
parsers O
quickly O
in O
a O
new O
domain O
using O
synthetic O
data O
. O
They O
generate O
logical O
forms O
and O
canonical O
utterances O
and O
then O
paraphrase O
the O
canonical O
utterances O
via O
crowd O
- O
sourcing O
. O
Several O
other O
approaches O
have O
demonstrated O
the O
benefit O
of O
adopting O
this O
approach O
to O
train O
semantic O
parsers O
in O
low O
- O
resource O
settings O
( O
Su O
et O
al O
. O
, O
2017 O
; O
Zhong O
et O
al O
. O
, O
2017 O
; O
Cheng O
et O
al O
. O
, O
2018 O
; O
Xu O
et O
al O
. O
, O
2020 O
) O
. O
Most O
recently O
, O
synthetic O
data O
was O
used O
to O
continue O
to O
pre O
- O
train O
language O
models O
for O
semantic O
parsing O
tasks O
( O
Herzig O
et O
al O
. O
, O
2020 O
; O
Yu O
et O
al O
. O
, O
2021a O
, O
b O
) O
. O
We O
build O
on O
this O
line O
work O
by O
showing O
that O
we O
can O
generate O
synthetic O
data O
automatically O
without O
human O
involvement O
to O
simulate O
edits O
between O
an O
erroneous O
parse O
and O
a O
correct O
one O
. O

Conclusions O
and O
Future O
Work O

We O
introduced O
a O
model O
, O
a O
data O
augmentation O
method O
, O
and O
analysis O
tools O
for O
correcting O
semantic O
parse O
errors O
in O
text O
- O
to O
- O
SQL O
through O
natural O
language O
feedback O
. O
Compared O
to O
previous O
models O
, O
our O
model O
improves O
the O
correction B-MetricName
accuracy I-MetricName
by O
16 O
% O
and O
boosts O
the O
end O
- O
to O
- O
end O
parsing O
accuracy O
by O
up O
to O
20 O
% O
with O
only O
one O
turn O
of O
feedback O
. O
Our O
work O
creates O
several O
avenues O
for O
future O
work O
: O
( O
1 O
) O
improving O
the O
model O
by O
better O
modeling O
the O
interaction O
between O
the O
inputs O
and O
exploring O
different O
patterns O
for O
decoder O
- O
encoder O
attention O
, O
( O
2 O
) O
evaluating O
existing O
methods O
for O
training O
with O
synthetic O
data O
( O
e.g. O
, O
curriculum O
learning O
( O
Bengio O
et O
al O
. O
, O
2009 O
) O
) O
, O
( O
3 O
) O
optimizing O
the O
correction O
model O
for O
better O
user O
experience O
using O
the O
progress O
measure O
we O
introduce O
, O
and O
( O
4 O
) O
using O
the O
SQL O
edits O
scheme O
in O
other O
related O
tasks O
such O
as O
conversational O
text O
- O
to O
- O
SQL O
parsing O
. O

Acknowledgments O

This O
work O
has O
benefited O
greatly O
from O
discussions O
with O
Xiang O
Deng O
, O
Alex O
Polozov O
, O
Tao O
Yu O
, O
and O
Guoqing O
Zheng O
. O
We O
thank O
Pengcheng O
Yin O
for O
sharing O
TaBERT B-HyperparameterName
predictions O
before O
the O
official O
code O
release O
. O
We O
are O
very O
grateful O
to O
our O
reviewers O
for O
their O
insightful O
feedback O
and O
suggestions O
. O

VGNMN B-MethodName
: O
Video B-MethodName
- I-MethodName
grounded I-MethodName
Neural I-MethodName
Module I-MethodName
Networks I-MethodName
for O
Video B-TaskName
- I-TaskName
Grounded I-TaskName
Dialogue I-TaskName
Systems O

Introduction O

Vision O
- O
language O
tasks O
have O
been O
studied O
to O
build O
intelligent O
systems O
that O
can O
perceive O
information O
from O
multiple O
modalities O
, O
such O
as O
images O
, O
videos O
, O
and O
text O
. O
Extended O
from O
image O
- O
grounded O
tasks O
, O
e.g. O
( O
Antol O
et O
al O
. O
, O
2015 O
) O
, O
recently O
Jang O
et O
al O
. O
( O
2017 O
) O
; O
Lei O
et O
al O
. O
( O
2018 O
) O
propose O
to O
use O
video O
as O
the O
grounding O
features O
. O
This O
modification O
poses O
a O
significant O
challenge O
to O
previous O
image O
- O
based O
models O
with O
the O
additional O
temporal O
variance O
through O
video O
frames O
. O

Recently O
further O
develop O
videogrounded O
language O
research O
into O
the O
dialogue O
domain O
. O
In O
the O
proposed O
task O
, O
video O
- O
grounded O
dialogues O
, O
the O
dialogue O
agent O
is O
required O
to O
answer O
questions O
about O
a O
video O
over O
multiple O
dialogue O
turns O
. O
Using O
Figure O
1 O
as O
an O
example O
, O
to O
answer O
Video O
Caption O
: O
a O
boy O
and O
a O
man O
walk O
to O
the O
room O
. O

The O
boy O
carries O
his O
backpack O
while O
the O
man O
… O
Visual O
: O
... O
Audio O
: O
... O

Question O

Dialogue O
Understanding O
Video O
Understanding O

Figure O
1 O
: O
A O
sample O
video O
- O
grounded O
dialogue O
with O
a O
demonstration O
of O
a O
reasoning O
process O
questions O
correctly O
, O
a O
dialogue O
agent O
has O
to O
resolve O
references O
in O
dialogue O
context O
, O
e.g. O
" O
he O
" O
and O
" O
it O
" O
, O
and O
identify O
the O
original O
entity O
, O
e.g. O
" O
a O
boy O
" O
and O
" O
a O
backpack O
" O
. O
Besides O
, O
the O
agent O
also O
needs O
to O
identify O
the O
actions O
of O
these O
entities O
, O
e.g. O
" O
carrying O
a O
backpack O
" O
to O
retrieve O
information O
from O
the O
video O
. O

Current O
state O
- O
of O
- O
the O
- O
art O
approaches O
to O
videogrounded O
dialogue O
tasks O
, O
e.g. O
( O
Le O
et O
al O
. O
, O
2019b O
; O
Fan O
et O
al O
. O
, O
2019 O
) O
have O
achieved O
remarkable O
performance O
through O
the O
use O
of O
deep O
neural O
networks O
to O
retrieve O
grounding O
video O
signals O
based O
on O
language O
inputs O
. O
However O
, O
these O
approaches O
often O
assume O
the O
reasoning O
structure O
, O
including O
resolving O
references O
of O
entities O
and O
detecting O
the O
corresponding O
actions O
to O
retrieve O
visual O
cues O
, O
is O
implicitly O
learned O
. O
An O
explicit O
reasoning O
structure O
becomes O
more O
beneficial O
as O
the O
tasks O
complicate O
in O
two O
scenarios O
: O
video O
with O
complex O
spatial O
and O
temporal O
dynamics O
, O
and O
language O
inputs O
with O
sophisticated O
semantic O
dependencies O
, O
e.g. O
questions O
positioned O
in O
a O
dialogue O
context O
. O
These O
scenarios O
often O
challenge O
researchers O
to O
interpret O
model O
hidden O
layers O
, O
identify O
errors O
, O
and O
assess O
model O
reasoning O
capability O
. O

Similar O
challenges O
have O
been O
observed O
in O
imagegrounded O
tasks O
in O
which O
deep O
neural O
networks O
exhibit O
shallow O
understanding O
capability O
as O
they O
exploit O
superficial O
visual O
cues O
( O
Agrawal O
et O
al O
. O
, O
2016 O
; O
Goyal O
et O
al O
. O
, O
2017 O
; O
Feng O
et O
al O
. O
, O
2018 O
; O
Serrano O
and O
Smith O
, O
2019 O
) O
. O
Andreas O
et O
al O
. O
( O
2016b O
) O
propose O
neural O
module O
networks O
( O
NMNs O
) O
by O
decomposing O
a O
question O
into O
sub O
- O
sequences O
called O
program O
and O
assembling O
a O
network O
of O
neural O
operations O
. O
Motivated O
by O
this O
line O
of O
research O
, O
we O
propose O
a O
new O
approach O
, O
VGNMN B-MethodName
, O
to O
video B-TaskName
- I-TaskName
grounded I-TaskName
language I-TaskName
tasks I-TaskName
. O
Our O
approach O
benefits O
from O
integrating O
neural O
networks O
with O
a O
compositional O
reasoning O
structure O
to O
exploit O
low O
- O
level O
information O
signals O
in O
video O
. O
An O
example O
of O
the O
reasoning O
structure O
can O
be O
seen O
on O
the O
right O
side O
of O
Figure O
1 O
. O

Video B-MethodName
- I-MethodName
grounded I-MethodName
Neural I-MethodName
Module I-MethodName
Network I-MethodName
( O
VGNMN B-MethodName
) O
tackles O
video O
understanding O
through O
action O
and O
entity O
- O
paramterized O
NMNs O
to O
retrieve O
video O
features O
. O
We O
first O
decompose O
question O
into O
a O
set O
of O
entities O
and O
extract O
video O
features O
related O
to O
these O
entities O
. O
VGNMN B-MethodName
then O
extracts O
the O
temporal O
steps O
by O
focusing O
on O
relevant O
actions O
that O
are O
associated O
with O
these O
entities O
. O
VGNMN B-MethodName
is O
analogous O
to O
how O
human O
processes O
information O
by O
gradually O
retrieving O
signals O
from O
input O
modalities O
using O
a O
set O
of O
discrete O
subjects O
and O
their O
actions O
. O

To O
tackle O
dialogue O
understanding O
, O
VGNMN B-MethodName
is O
trained O
to O
resolve O
any O
co O
- O
reference O
in O
language O
inputs O
, O
e.g. O
questions O
in O
a O
dialogue O
context O
, O
to O
identify O
the O
unique O
entities O
in O
each O
dialogue O
. O
Previous O
approaches O
to O
video O
- O
grounded O
dialogues O
often O
obtain O
question O
global O
representations O
in O
relation O
to O
dialogue O
context O
. O
These O
approaches O
might O
be O
suitable O
to O
represent O
general O
semantics O
in O
open O
- O
domain O
dialogues O
( O
Serban O
et O
al O
. O
, O
2016 O
) O
. O
However O
, O
they O
are O
not O
ideal O
to O
detect O
fine O
- O
grained O
information O
in O
a O
video O
- O
grounded O
dialogue O
which O
frequently O
entails O
dependencies O
between O
questions O
and O
past O
dialogue O
turns O
in O
the O
form O
of O
entity O
references O
. O

In O
summary O
, O
our O
contributions O
include O
: O

• O
VGNMN B-MethodName
, O
a O
neural O
module O
network O
- O
based O
approach O
for O
video B-TaskName
- I-TaskName
grounded I-TaskName
dialogues I-TaskName
. O

• O
The O
approach O
includes O
a O
modularized O
system O
that O
creates O
a O
reasoning O
pipeline O
parameterized O
by O
entity O
and O
action O
- O
based O
representations O
from O
both O
dialogue O
and O
video O
contexts O
. O

• O
Our O
experiments O
are O
conducted O
on O
the O
challenging O
benchmark O
for O
video O
- O
grounded O
dialogues O
, O
Audio B-DatasetName
- I-DatasetName
visual I-DatasetName
Scene I-DatasetName
- I-DatasetName
Aware I-DatasetName
Dialogues I-DatasetName
( O
AVSD B-DatasetName
) O
as O
well O
as O
TGIF B-DatasetName
- I-DatasetName
QA I-DatasetName
( O
Jang O
et O
al O
. O
, O
2017 O
) O
for O
video O
QA O
task O
. O

• O
Our O
results O
indicate O
strong O
performance O
of O
VGNMN B-MethodName
as O
well O
as O
improved O
model O
inter O
- O
pretability O
and O
robustness O
to O
difficult O
scenarios O
of O
dialogues O
, O
videos O
, O
and O
question O
structures O
. O

2 O
Related O
Work O

Video O
- O
Language O
Understanding O

The O
research O
of O
video O
- O
language O
understanding O
aims O
to O
develop O
a O
model O
's O
joint O
understanding O
capability O
of O
language O
, O
video O
, O
and O
their O
interactions O
. O
Jang O
et O
al O
. O
( O
2017 O
) O
; O
Gao O
et O
al O
. O
( O
2018 O
) O
; O
propose O
to O
learn O
attention O
guided O
by O
question O
global O
representation O
to O
retrieve O
spatial O
- O
level O
and O
temporal O
- O
level O
visual O
features O
. O
Li O
et O
al O
. O
( O
2019 O
) O
; O
Fan O
et O
al O
. O
( O
2019 O
) O
; O
Jiang O
and O
Han O
( O
2020 O
) O
model O
interaction O
between O
all O
pairs O
of O
question O
tokenlevel O
representations O
and O
temporal O
- O
level O
features O
of O
the O
input O
video O
through O
similarity O
matrix O
, O
memory O
networks O
, O
and O
graph O
networks O
respectively O
. O
; O
Le O
et O
al O
. O
( O
2019cLe O
et O
al O
. O
( O
, O
2020b O
; O
Lei O
et O
al O
. O
( O
2020 O
) O
; O
Huang O
et O
al O
. O
( O
2020 O
) O
extends O
the O
previous O
approach O
by O
dividing O
a O
video O
into O
equal O
segments O
, O
sub O
- O
sampling O
video O
frames O
, O
or O
considering O
objectlevel O
representations O
of O
input O
video O
. O
We O
propose O
to O
replace O
token O
- O
level O
and O
global O
question O
representations O
with O
question O
representations O
composed O
of O
specific O
entities O
and O
actions O
. O
Recently O
, O
we O
have O
witnessed O
emerging O
techniques O
in O
video O
- O
language O
systems O
that O
exploit O
deep O
transformer O
- O
based O
architectures O
such O
as O
BERT O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
for O
pretraining O
multimodal O
representations O
( O
Li O
et O
al O
. O
, O
2020a O
; O
Yang O
et O
al O
. O
, O
2020 O
; O
Kim O
et O
al O
. O
, O
2021 O
; O
Tang O
et O
al O
. O
, O
2021 O
; O
Zellers O
et O
al O
. O
, O
2021 O
) O
in O
very O
large O
- O
scale O
videolanguage O
datasets O
. O
While O
these O
systems O
can O
achieve O
impressive O
performance O
, O
they O
are O
not O
straightforward O
to O
apply O
in O
domains O
with O
limited O
data O
such O
as O
video O
- O
grounded O
dialogues O
. O
Moreover O
, O
as O
we O
shown O
in O
our O
qualitative O
examples O
, O
our O
approach O
facilitates O
better O
interpretability O
through O
the O
output O
of O
decoded O
functional O
programs O
. O

Video B-TaskName
- I-TaskName
grounded I-TaskName
Dialogues I-TaskName

Extended O
from O
video O
QA O
, O
video B-TaskName
- I-TaskName
grounded I-TaskName
dialogue I-TaskName
is O
an O
emerging O
task O
that O
combines O
dialogue O
response O
generation O
and O
video O
- O
language O
understanding O
research O
. O
This O
task O
entails O
a O
novel O
requirement O
for O
models O
to O
learn O
dialogue O
semantics O
and O
decode O
entity O
co O
- O
references O
in O
questions O
. O
Nguyen O
et O
al O
. O
( O
2018 O
) O
; O
; O
; O
Sanabria O
et O
al O
. O
( O
2019 O
) O
; O
Le O
et O
al O
. O
( O
2019a O
, O
b O
) O
extend O
traditional O
QA O
models O
by O
adding O
dialogue O
history O
neural O
encoders O
. O
Kumar O
et O
al O
. O
( O
2019 O
) O
en O
- O
hances O
dialogue O
features O
with O
topic O
- O
level O
representations O
to O
express O
the O
general O
topic O
in O
each O
dialogue O
. O
Schwartz O
et O
al O
. O
( O
2019 O
) O
treats O
each O
dialogue O
turn O
as O
an O
independent O
sequence O
and O
allows O
interaction O
between O
questions O
and O
each O
dialogue O
turn O
. O
Le O
et O
al O
. O
( O
2019b O
) O
encodes O
dialogue O
history O
as O
a O
sequence O
with O
embedding O
and O
positional O
representations O
. O
Different O
from O
prior O
work O
, O
we O
dissect O
the O
question O
sequence O
and O
explicitly O
detect O
and O
decode O
any O
entities O
and O
their O
references O
. O
Our O
approach O
also O
enables O
insights O
on O
how O
models O
extract O
deductive O
bias O
from O
dialogues O
to O
extract O
video O
information O
. O

Neural O
Module O
Network O

Neural O
Module O
Network O
( O
NMN O
) O
( O
Andreas O
et O
al O
. O
, O
2016b O
, O
a O
) O
is O
introduced O
to O
address O
visual O
QA O
by O
decomposing O
questions O
into O
linguistic O
sub O
- O
structures O
, O
known O
as O
programs O
, O
to O
instantiate O
a O
network O
of O
neural O
modules O
. O
NMN O
models O
have O
achieved O
success O
in O
synthetic O
image O
domains O
where O
a O
multi O
- O
step O
reasoning O
process O
is O
required O
( O
Johnson O
et O
al O
. O
, O
2017b O
; O
Hu O
et O
al O
. O
, O
2018 O
; O
Han O
et O
al O
. O
, O
2019 O
) O
. O
Yi O
et O
al O
. O
( O
2018 O
) O
; O
Han O
et O
al O
. O
( O
2019 O
) O
; O
improve O
NMN O
models O
by O
decoupling O
visual O
- O
language O
understanding O
and O
visual O
concept O
learning O
. O
Our O
work O
is O
related O
to O
the O
recent O
work O
( O
Kottur O
et O
al O
. O
, O
2018 O
; O
Jiang O
and O
Bansal O
, O
2019 O
; O
Gupta O
et O
al O
. O
, O
2020 O
) O
that O
extended O
NMNs O
to O
image O
reasoning O
in O
dialogues O
and O
reading O
comprehension O
reasoning O
. O
Our O
approach O
follows O
the O
previous O
approaches O
that O
learn O
to O
generate O
program O
structure O
and O
require O
no O
parser O
at O
evaluation O
time O
. O
Compared O
to O
prior O
work O
, O
we O
use O
NMN O
to O
learn O
dependencies O
between O
the O
composition O
in O
language O
inputs O
and O
the O
spatio O
- O
temporal O
dynamics O
in O
videos O
. O
Specifically O
, O
we O
propose O
to O
construct O
a O
reasoning O
structure O
from O
text O
, O
from O
which O
detected O
entities O
are O
used O
to O
extract O
visual O
information O
in O
the O
spatial O
space O
and O
detected O
actions O
are O
used O
to O
find O
visual O
information O
in O
the O
temporal O
space O
. O

Method O

In O
this O
section O
, O
we O
present O
the O
design O
of O
our O
model O
. O
An O
overview O
of O
the O
model O
can O
be O
seen O
in O
Figure O
2 O
. O

Task O
Definition O

The O
input O
to O
the O
model O
consists O
of O
a O
dialogue O
D O
which O
is O
grounded O
on O
a O
video O
V. O
The O
input O
components O
include O
the O
question O
of O
current O
dialogue O
turn O
Q O
, O
dialogue O
history O
H O
, O
and O
the O
features O
of O
the O
input O
video O
, O
including O
visual O
and O
audio O
input O
. O
The O
output O
is O
a O
dialogue O
response O
, O
denoted O
as O
R. O
Each O
text O
input O
component O
is O
a O
sequence O
of O
words O
w O
1 O
, O
... O
, O
w O
m O
∈ O
V O
in O
, O
the O
input O
vocabulary O
. O
Similarly O
, O
the O
output O
response O
R O
is O
a O
sequence O
of O
tokens O
w O
1 O
, O
... O
, O
w O
n O
∈ O
V O
out O
, O
the O
output O
vocabulary O
. O
The O
objective O
of O
the O
task O
is O
the O
generation O
objective O
that O
output O
answers O
of O
the O
current O
dialogue O
turn O
t O
: O

R O
t O
= O
arg O
max O
Rt O
P O
( O
R O
t O
|V O
, O
H O
t O
, O
Q O
t O
; O
θ O
) O
= O
arg O
max O
Rt O
L O
R O
n=1 O
P O
m O
( O
w O
n O
|R O
t,1 O
: O
n−1 O
, O
V O
, O
H O
t O
, O
Q O
t O
; O
θ O
) O

where O
L O
R O
is O
the O
length O
of O
the O
sequence O
R. O
In O
a O
Video O
- O
QA O
task O
, O
the O
dialogue O
history O
H O
is O
simply O
absent O
and O
the O
output O
response O
is O
typically O
collapsed O
to O
a O
single O
- O
token O
response O
. O

Encoders O

Text O
Encoder O
. O
A O
text O
encoder O
is O
shared O
to O
encode O
text O
inputs O
, O
including O
dialogue O
history O
, O
questions O
, O
and O
captions O
. O
The O
text O
encoder O
converts O
each O
text O
sequence O
X O
= O
w O
1 O
, O
... O
, O
w O
m O
into O
a O
sequence O
of O
embeddings O
X O
∈ O
R O
m×d O
. O
We O
use O
a O
trainable O
embedding O
matrix O
to O
map O
token O
indices O
to O
vector O
representations O
of O
d O
dimensions O
through O
a O
mapping O
function O
φ O
. O
These O
vectors O
are O
then O
integrated O
with O
ordering O
information O
of O
tokens O
through O
a O
positional O
encoding O
function O
with O
layer O
normalization O
( O
Ba O
et O
al O
. O
, O
2016 O
; O
Vaswani O
et O
al O
. O
, O
2017 O
1 O
: O
Description O
of O
the O
modules O
and O
their O
functionalities O
. O
We O
denote O
P O
as O
the O
parameter O
to O
instantiate O
each O
module O
, O
H O
as O
the O
dialogue O
history O
, O
Q O
as O
the O
question O
of O
the O
current O
dialogue O
turn O
, O
and O
V O
as O
video O
input O
. O

corresponding O
coordinates O
projected O
to O
d O
vis O
dimensions O
. O
We O
also O
make O
use O
of O
a O
CNN B-MethodName
- O
based O
pretrained O
model O
to O
obtain O
features O
of O
temporal O
dimension O
Z O
cnn O
∈ O
R O
F O
×d O
vis O
. O
The O
audio O
feature O
is O
obtained O
through O
a O
pretrained O
audio O
model O
, O
Z O
aud O
∈ O
R O
F O
×d O
aud O
. O
We O
passed O
all O
video O
features O
through O
a O
linear O
transformation O
layer O
with O
ReLU O
activation B-HyperparameterName
to O
the O
same O
embedding O
dimension O
d O
. O

Neural O
Modules O

We O
introduce O
neural O
modules O
that O
are O
used O
to O
assemble O
an O
executable O
program O
constructed O
by O
the O
generated O
sequence O
from O
question O
parsers O
. O
We O
provide O
an O
overview O
of O
neural O
modules O
in O
Table O
1 O
and O
demonstrate O
dialogue O
understanding O
and O
video O
understanding O
modules O
in O
Figure O
3 O
and O
4 O
respectively O
. O
Each O
module O
parameter O
, O
e.g. O
" O
a O
backpack O
" O
, O
is O
extracted O
from O
the O
parsed O
program O
( O
See O
Section O
3.4 O
) O
. O

For O
each O
parameter O
, O
we O
denote O
P O
∈ O
R O
d O
as O
the O
average O
pooling O
of O
component O
token O
embeddings O
. O
find O
( O
P O
, O
H O
) O
→H O
ent O
. O
This O
module O
handles O
entity O
tracing O
by O
obtaining O
a O
distribution O
over O
tokens O
in O
the O
dialogue O
history O
. O
We O
use O
an O
entity O
- O
todialogue O
- O
history O
attention O
mechanism O
applied O
from O
an O
entity O
P O
i O
to O
all O
tokens O
in O
the O
dialogue O
history O
. O
Any O
neural O
network O
that O
learn O
to O
generate O
attention O
between O
two O
tensors O
is O
applicable O
.e.g O
. O
( O
Bahdanau O
et O
al O
. O
, O
2015 O
; O
Vaswani O
et O
al O
. O
, O
2017 O
) O
. O
The O
attention O
matrix O
normalized O
by O
softmax O
, O
A O
find O
, O
i O
∈ O
R O
L O
H O
, O
is O
used O
to O
compute O
the O
weighted O
sum O
of O
dialogue O
history O
token O
representations O
. O
The O
output O
is O
combined O
with O
entity O
embedding O
P O
i O
to O
obtain O
contextual O
entity O
representation O
H O
ent O
, O
i O
∈ O
R O
d O
. O

summarize O
( O
H O
ent O
, O
Q O
) O
→Q O
ctx O
. O
For O
each O
contextual O
entity O
representation O
H O
ent O
, O
i O
, O
i O
= O
1 O
, O
... O
, O
N O
ent O
, O
it O
is O
projected O
to O
L O
Q O
dimensions O
and O
is O
combined O
with O
question O
token O
embeddings O
through O
elementwise O
summation O
to O
obtain O
entity O
- O
aware O
question O
representation O
Q O
ent O
, O
i O
∈ O
R O
L O
Q O
×d O
. O
It O
is O
fed O
to O
a O
onedimensional O
CNN O
with O
max O
- O
pooling O
layer O
( O
Kim O
, O
2014 O
) O
to O
obtain O
a O
contextual O
entity O
- O
aware O
question O
representation O
. O
We O
denote O
the O
final O
output O

as O
Q O
ctx O
∈ O
R O
Nent×d O
. O

While O
previous O
models O
usually O
focus O
on O
global O
or O
token O
- O
level O
dependencies O
Le O
et O
al O
. O
, O
2019b O
) O
to O
encode O
question O
features O
, O
our O
modules O
compress O
fine O
- O
grained O
question O
representations O
at O
the O
entity O
level O
. O
Specifically O
, O
find O
and O
summarize O
modules O
can O
generate O
entitydependent O
local O
and O
global O
representations O
of O
question O
semantics O
. O
We O
show O
that O
our O
modularized O
approach O
can O
achieve O
better O
performance O
and O
transparency O
than O
traditional O
approaches O
to O
encode O
dialogue O
context O
( O
Serban O
et O
al O
. O
, O
2016 O
; O
Vaswani O
et O
al O
. O
, O
2017 O
) O
( O
Section O
4 O
) O
. O

where O
( O
P O
, O
V O
) O
→V O
ent O
. O
Similar O
to O
the O
find O
module O
, O
this O
module O
handles O
entity O
- O
based O
attention O
to O
the O
video O
input O
. O
However O
, O
the O
entity O
representation O
P O
, O
in O
this O
case O
, O
is O
parameterized O
by O
the O
original O
entity O
in O
dialogue O
rather O
than O
in O
question O
( O
See O
Section O
3.4 O
for O
more O
description O
) O
. O
Each O
entity O
P O
i O
is O
stacked O
to O
match O
the O
number O
of O
sampled O
video O
frames O
/ O
clips O
F O
. O
An O
attention B-HyperparameterName
network I-HyperparameterName
is O
used O
to O
obtain O
entity O
- O
to O
- O
object O
attention O
matrix O
A O
where O
, O
i O
∈ O
R O
F O
×O O
. O
The O
attended O
feature O
are O
compressed O
through O
weighted O
sum O
pooling O
along O
the O
spatial O
dimension O
, O
resulting O
in O
V O
ent O
, O
i O
∈ O
R O
F O
×d O
, O
i O
= O
1 O
, O
... O
, O
N O
ent O
. O
when O
( O
P O
, O
V O
ent O
) O
→V O
ent+act O
. O
This O
module O
follows O
a O
similar O
architecture O
as O
the O
where O
module O
. O
However O
, O
the O
action O
parameter O
P O
i O
is O
stacked O
to O
match O
N O
ent O
dimensions O
. O
The O
attention O
matrix O
A O
when O
, O
i O
∈ O
R O
F O
is O
then O
used O
to O
compute O
the O
visual O
entity O
- O
action O
representations O
through O
weighted O
sum O
along O
the O
temporal O
dimension O
. O
We O
denote O
the O
output O
for O
all O
actions O
P O
i O
as O
V O
ent+act O
∈ O
R O
Nent×Nact×d O
describe O
( O
P O
, O
V O
ent+act O
) O
→V O
ctx O
. O
This O
module O
is O
a O
linear O
transformation O
to O
compute O
V O
ctx O
= O
W O
desc O
T O
[ O
V O
ent+act O
; O
P O
stack O
] O
∈ O
R O
Nent×Nact×d O
where O
W O
desc O
∈ O
R O
2d×d O
, O
P O
stack O
is O
the O
stacked O
representations O
of O
parameter O
embedding O
P O
to O
N O
ent O
× O
N O
act O
dimensions O
, O
and O
[ O
; O
] O
is O
the O
concatenation O
operation O
. O
Note O
that O
the O
parameter O
P O
here O
is O
extracted O
from O
questions O
, O
often O
as O
the O
type O
of O
questions O
e. O
and O
" O
how O
" O
. O
This O
eliminates O
the O
need O
to O
have O
different O
modules O
for O
different O
question O
types O
. O
However O
, O
we O
noted O
the O
current O
design O
may O
be O
challenged O
in O
rare O
cases O
in O
which O
an O
utterance O
contain O
numerous O
questions O
( O
refer O
to O
Figure O
5 O
) O
. O

The O
exist O
module O
is O
used O
when O
the O
questions O
are O
" O
yes O
/ O
no O
" O
questions O
. O
This O
module O
is O
a O
special O
case O
of O
describe O
module O
where O
the O
parameter O
P O
is O
simply O
the O
average O
pooled O
question O
embeddings O
. O
The O
above O
where O
module O
is O
applied O
to O
object O
- O
level O
features O
. O
For O
temporal O
- O
based O
features O
such O
as O
CNN O
- O
based O
and O
audio O
features O
, O
the O
same O
neural O
operation O
is O
applied O
along O
the O
temporal O
dimension O
. O
Each O
resulting O
entity O
- O
aware O
output O
is O
then O
incorporated O
to O
frame O
- O
level O
features O
through O
element O
- O
wise O
summation O
. O

An O
advantage O
of O
our O
architecture O
is O
that O
it O
separates O
dialogue O
and O
video O
understanding O
. O
We O
adopt O
a O
transparent O
approach O
to O
solve O
linguistic O
entity O
references O
during O
the O
dialogue O
understanding O
phase O
. O
The O
resolved O
entities O
are O
fed O
to O
the O
video O
understanding O
phase O
to O
learn O
entity O
- O
action O
dynamics O
in O
the O
video O
. O
We O
show O
that O
our O
approach O
is O
robust O
when O
dialogue O
evolves O
to O
many O
turns O
and O
video O
extends O
over O
time O
( O
Please O
refer O
to O
Section O
4 O
) O
. O

Question O
Parsers O

To O
learn O
compositional O
programs O
, O
we O
follow O
( O
Johnson O
et O
al O
. O
, O
2017a O
; O
Hu O
et O
al O
. O
, O
2017 O
) O
and O
consider O
program O
generation O
as O
a O
sequence O
- O
tosequence O
task O
. O
We O
adopt O
a O
simple O
template O
" O
param O
1 O
module O
1 O
param O
2 O
module O
2 O
... O
" O
as O
the O
target O
sequence O
. O
The O
resulting O
target O
sequences O
for O
dialogue O
and O
video O
understanding O
programs O
are O
sequences O
P O
dial O
and O
P O
vid O
respectively O
. O

The O
parsers O
decompose O
questions O
into O
sub O
- O
sequences O
to O
construct O
compositional O
reasoning O
programs O
for O
dialogue O
and O
video O
understanding O
. O
Each O
parser O
is O
a O
vanilla B-HyperparameterName
Transformer I-HyperparameterName
decoder I-HyperparameterName
, O
including O
multi B-HyperparameterName
- I-HyperparameterName
head I-HyperparameterName
attention I-HyperparameterName
layers O
on O
questions O
and O
past O
dialogue O
turns O
( O
Please O
refer O
to O
Appendix O
A.1 O
for O
more O
technical O
details O
) O
. O

Response O
Decoder O

System O
response O
is O
decoded O
by O
incorporating O
the O
dialogue O
context O
and O
video O
context O
outputs O
from O
the O
corresponding O
reasoning O
programs O
to O
target O
token O
representations O
. O
We O
follows O
a O
vanilla O
Transformer O
decoder O
architecture O
( O
Le O
et O
al O
. O
, O
2019b O
) O
, O
which O
consists O
of O
3 O
attention O
layers O
: O
self O
- O
attention O
to O
attend O
on O
existing O
tokens O
, O
attention O
to O
Q O
ctx O
from O
dialogue O
understanding O
program O
execution O
, O
and O
attention O
to O
V O
ctx O
from O
video O
understanding O
program O
execution O
. O

A O
( O
1 O
) O
res O
= O
Attention O
( O
R| O
j−1 O
0 O
, O
R| O
j−1 O
0 O
, O
R| O
j−1 O
0 O
) O
∈ O
R O
j×d O
A O
( O
2 O
) O
res O
= O
Attention O
( O
A O
( O
1 O
) O
res O
, O
Q O
ctx O
, O
Q O
ctx O
) O
∈ O
R O
j×d O
A O
( O
3 O
) O
res O
= O
Attention O
( O
A O
( O
2 O
) O
res O
, O
V O
ctx O
, O
V O
ctx O
) O
∈ O
R O
j×d O
Multimodal O
Fusion O
. O

For O
video O
features O
come O
from O
multiple O
modalities O
, O
visual O
and O
audio O
, O
the O
contextual O
features O
, O
denoted O
V O
ctx O
, O
is O
obtained O
through O
a O
weighted O
sum O
of O
component O
modalities O
, O
e.g. O
contextual O
visual O
features O
V O
vis O
ctx O
and O
contextual O
audio O
features O
V O
aud O
ctx O
. O
The O
scores O
S O
fusion O
to O
compute O
the O
weighted O
sum O
is O
defined O
as O
: O

S O
fusion O
= O
Softmax O
( O
W O
T O
fusion O
[ O
Q O
stack O
; O
V O
vis O
ctx O
; O
V O
aud O
ctx O
] O

) O
where O
Q O
stack O
is O
the O
mean O
pooling O
output O
of O
question O
embeddings O
Q O
which O
is O
then O
stacked O
to O
N O
ent O
+ O
N O
act O
dimensions O
, O
and O
W O
fusion O
∈ O
R O
3d×2 O
are O
trainable O
model O
parameters O
. O
The O
resulting O
S O
fusion O
has O
a O
dimension O
of O
∈ O
R O
( O
Nent+Nact O
) O
×2 O
. O

Response O
Generation O
. O
To O
generate O
response O
sequences O
, O
a O
special O
token O
" O
_ O
sos O
" O
is O
concatenated O
as O
the O
first O
token O
w O
0 O
. O
The O
decoded O
token O
w O
1 O
is O
then O
appended O
to O
w O
0 O
as O
input O
to O
decode O
w O
2 O
and O
so O
on O
. O
Similarly O
to O
input O
source O
sequences O
, O
at O
decoding O
time O
step O
j O
, O
the O
input O
target O
sequence O
is O
encoded O
to O
obtain O
representations O
of O
system O
response O
R| O
j−1 O
0 O
. O
We O
combine O
vocabulary O
of O
input O
and O
output O
sequences O
and O
share O
the O
embedding O
matrix O
E O
∈ O
R O
|V|×d O
where O
V O
= O
V O
in O
∩ O
V O
out O
. O
During O
training O
time O
, O
we O
directly O
use O
the O
ground O
- O
truth O
responses O
as O
input O
to O
the O
decoder O
and O
optimize O
VGNMN B-MethodName
with O
a O
cross O
- O
entropy O
loss B-HyperparameterName
to O
decode O
the O
next O
ground O
- O
truth O
tokens O
. O
During O
test O
time O
, O
responses O
are O
generated O
auto O
- O
regressively O
through O
beam O
search O
with O
beam O
size O
5 O
. O
Note O
that O
we O
apply O
the O
same O
procedure O
to O
generate O
reasoning O
programs O
from O
question O
parsers O
. O

Experiments O

Datasets O
. O
We O
use O
the O
AVSD B-DatasetName
benchmark O
from O
the O
Dialogue O
System O
Technology O
Challenge O
7 O
( O
DSTC7 O
) O
. O
The O
benchmark O
consists O
of O
dialogues O
grounded O
on O
the O
Charades O
videos O
( O
Sigurdsson O
et O
al O
. O
, O
2016 O
) O
. O
Each O
dialogue O
contains O
up O
to O
10 O
dialogue O
turns O
, O
each O
turn O
consists O
of O
a O
question O
and O
expected O
response O
about O
a O
given O
video O
. O
For O
visual O
features O
, O
we O
use O
the O
3D O
CNN O
- O
based O
features O
from O
a O
pretrained O
I3D O
model O
( O
Carreira O
and O
Zisserman O
, O
2017 O
) O
and O
object O
- O
level O
features O
from O
a O
pretrained O
FasterRNN O
model O
( O
Ren O
et O
al O
. O
, O
2015b O
) O
. O
The O
audio O
features O
are O
obtained O
from O
a O
pretrained O
VGGish O
model O
( O
Hershey O
et O
al O
. O
, O
2017 O
) O
. O
In O
the O
experiments O
with O
AVSD B-DatasetName
, O
we O
consider O
two O
settings O
: O
one O
with O
video O
summary O
and O
one O
without O
video O
summary O
as O
input O
. O
In O
the O
setting O
with O
video O
summary O
, O
the O
summary O
is O
concatenated O
to O
the O
dialogue O
history O
before O
the O
first O
dialogue O
turn O
. O
We O
also O
adapt O
VGNMN B-MethodName
to O
the O
video O
QA O
benchmark O
TGIF B-DatasetName
- I-DatasetName
QA I-DatasetName
( O
Jang O
et O
al O
. O
, O
2017 O
For O
the O
TGIF B-DatasetName
- I-DatasetName
QA I-DatasetName
benchmark O
, O
we O
use O
the O
extracted O
features O
from O
a O
pretrained O
ResNet O
model O
( O
He O
et O
al O
. O
, O
2016 O
) O
. O
Table O
2 O
shows O
a O
summary O
of O
the O
AVSD B-DatasetName
and O
TGIF B-DatasetName
- I-DatasetName
QA I-DatasetName
benchmarks I-DatasetName
. O

Training O
Details O
. O
We O
follow O
prior O
approaches O
( O
Hu O
et O
al O
. O
, O
2017 O
( O
Hu O
et O
al O
. O
, O
, O
2018Kottur O
et O
al O
. O
, O
2018 O
) O
by O
obtaining O
the O
annotations O
of O
the O
programs O
through O
a O
language O
parser O
( O
Hu O
et O
al O
. O
, O
2016 O
) O
and O
a O
reference O
resolution O
model O
( O
Clark O
and O
Manning O
, O
2016 O
) O
. O

During O
training O
, O
we O
directly O
use O
these O
as O
groundtruth O
labels O
of O
programs O
to O
train O
our O
models O
. O
The O
ground O
- O
truth O
responses O
are O
augmented O
with O
label O
smoothing O
technique O
( O
Szegedy O
et O
al O
. O
, O
2016 O
) O
. O
During O
inference O
time O
, O
we O
generate O
all O
programs O
and O
responses O
from O
given O
dialogues O
and O
videos O
. O
We O
run O
beam O
search O
to O
enumerate O
programs O
for O
dialogue O
and O
video O
understanding O
and O
dialogue O
responses O
. O
We O
use O
a O
training O
batch B-HyperparameterName
size I-HyperparameterName
of O
32 B-HyperparameterValue
and O
embedding B-HyperparameterName
dimension I-HyperparameterName
d B-HyperparameterName
= O
128 B-HyperparameterValue
in O
all O
experiments O
. O
Where O
Transformer O
attention O
is O
used O
, O
we O
fix O
the O
number O
of O
attention B-HyperparameterName
heads I-HyperparameterName
to O
8 B-HyperparameterValue
in O
all O
attention O
layers O
. O
In O
neural O
modules O
with O
MLP O
layers O
, O
the O
MLP O
network O
is O
fixed O
to O
2 B-HyperparameterValue
linear B-HyperparameterName
layers I-HyperparameterName
with O
a O
ReLU O
activation B-HyperparameterName
in O
between O
. O
In O
neural O
modules O
with O
CNN O
, O
we O
adopt O
a O
vanilla O
CNN B-MethodName
architecture O
for O
text O
classification O
( O
without O
the O
last O
MLP O
layer O
) O
where O
the O
number O
of O
input O
channels O
is O
1 O
, O
the O
kernel O
sizes O
are O
{ O
3 O
, O
4 O
, O
5 O
} O
, O
and O
the O
number O
of O
output O
channels O
is O
d. O
We O
initialize O
models O
with O
uniform O
distribution O
( O
Glorot O
and O
Bengio O
, O
2010 O
) O
. O
During O
training O
, O
we O
adopt O
the O
Adam O
optimizer O
( O
Kingma O
and O
Ba O
, O
2015 O
) O
and O
a O
decaying O
learning O
rate O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
where O
we O
fix O
the O
warm O
- O
up O
steps O
to O
15 O
K O
training O
steps O
. O
We O
employ O
dropout O
( O
Srivastava O
et O
al O
. O
, O
2014 O
) O
of O
0.2 O
at O
all O
networks O
except O
the O
last O
linear O
layers O
of O
question O
parsers O
and O
response O
decoder O
. O
We O
train O
models O
up O
to O
50 B-HyperparameterValue
epochs B-HyperparameterName
and O
select O
the O
best O
models O
based O
on O
the O
average O
loss B-HyperparameterName
per O
epoch B-HyperparameterName
in O
the O
validation O
set O
. O

All O
models O
are O
trained O
in O
a O
V100 O
GPU O
with O
a O
capacity O
of O
16 O
GB O
. O
We O
approximated O
each O
training O
epoch B-HyperparameterName
took O
about O
20 O
minutes O
to O
run O
. O
For O
each O
model O
experiment O
with O
VGNMN B-MethodName
, O
we O
obtained O
at O
least O
2 O
runs O
and O
reported O
the O
average O
results O
. O
We O
implemented O
models O
in O
Pytorch O
and O
released O
the O
code O
and O
model O
checkpoints O
1 O
. O

Optimization B-HyperparameterName
. O
We O
optimize O
models O
by O
joint O
training O
to O
minimize O
the O
cross O
- O
entropy O
losses B-HyperparameterName
to O
generate O
responses O
and O
functional O
programs O
. O

L O
= O
αL O
dial O
+ O
βL O
vid O
+ O
L O
res O
= O
α O
j O
− O
log O
( O
P O
dial O
( O
P O
dial O
, O
j O
) O
) O
+ O
β O
l O
− O
log O
( O
P O
video O
( O
P O
video O
, O
l O
) O
) O
+ O
n O
− O
log O
( O
P O
res O
( O
R O
n O
) O
) O

where O
P O
is O
the O
probability O
distribution O
of O
an O
output O
token O
. O
The O
probability O
is O
computed O
by O
passing O
output O
representations O
from O
the O
parsers O
and O
decoder O
to O
a O
linear O
layer O
W O
∈ O
R O
d×V O
with O
softmax O
activation O
. O
We O
share O
the O
parameters O
between O
W O
and O
embedding O
matrix O
E. O
AVSD B-MethodName
Results O
. O
We O
evaluate O
model O
performance O
by O
the O
objective O
metrics O
, O
including O
BLEU B-MetricName
( O
Papineni O
et O
al O
. O
, O
2002 O
) O
, O
METEOR O
( O
Banerjee O
and O
Lavie O
, O
2005 O
) O
, O
ROUGE B-MetricName
- O
L O
( O
Lin O
, O
2004 O
) O
, O
and O
CIDEr B-MetricName
( O
Vedantam O
et O
al O
. O
, O
2015 O
) O
, O
between O
each O
generated O
response O
and O
6 O
reference O
gold O
responses O
. O
As O
seen O
in O
Table O
3 O
, O
our O
models O
outperform O
most O
of O
existing O
approaches O
. O
We O
observed O
that O
our O
approach O
did O
not O
outperform O
the O
GPT O
- O
based O
baselines O
( O
Li O
et O
al O
. O
, O
2020b O
; O
Le O
and O
Hoi O
, O
2020 O
) O
in O
the O
setting O
that O
allows O
video O
summary O
/ O
caption O
input O
. O
However O
, O
the O
performance O
of O
our O
model O
in O
the O
setting O
without O
video O
summary O
/ O
caption O
input O
is O
on O
par O
with O
the O
GPT O
- O
based O
baseline O
( O
Li O
et O
al O
. O
, O
2020b O
) O
, O
even O
though O
our O
model O
did O
not O
rely O
on O
deep O
pretrained O
representations O
on O
large O
- O
scale O
text O
data O
. O
These O
observations O
imply O
that O
GPT O
- O
based O
models O
can O
better O
capture O
video O
context O
from O
video O
caption O
/ O
summary O
through O
rich O
pretrained O
representations O
. O
However O
, O
without O
access O
to O
video O
caption O
/ O
summary O
, O
these O
models O
may O
fail O
to O
understand O
video O
from O
visual O
- O
only O
representations O
. O
In O
this O
setting O
, O
GPT O
- O
based O
models O
may O
be O
inferior O
to O
VGNMN B-MethodName
, O
which O
explicitly O
exploits O
the O
compositional O
structures O
from O
textual O
inputs O
to O
integrate O
visual O
features O
. O
We O
also O
found O
that O
VGNMN B-MethodName
applied O
to O
object O
- O
level O
features O
is O
competitive O
to O
the O
model O
applied O
to O
CNN B-MethodName
- O
based O
features O
. O
The O
as O
well O
as O
model O
( O
1 O
) O
. O
We O
observed O
that O
related O
factors O
might O
affect O
the O
discrepancy O
, O
such O
as O
the O
complexity O
of O
the O
questions O
for O
these O
short O
and O
long O
- O
range O
videos O
. O
Potentially O
, O
our O
question O
parser O
for O
the O
video O
understanding O
program O
needs O
to O
be O
improved O
( O
e.g. O
for O
tree O
- O
based O
programs O
) O
to O
retrieve O
information O
in O
these O
ranges O
. O
Robustness O
to O
dialogue O
turn O
: O
In O
Table O
4b O
, O
we O
observed O
that O
model O
( O
1 O
) O
performs O
better O
than O
model O
( O
2 O
) O
overall O
, O
especially O
in O
higher O
turn O
positions O
, O
i.e. O
from O
the O
4 O
th O
turn O
to O
8 O
th O
turn O
. O
Interestingly O
, O
we O
noted O
some O
mixed O
results O
in O
very O
low O
turn O
position O
, O
i.e. O
the O
2 O
nd O
and O
3 O
rd O
turn O
, O
and O
very O
high O
turn O
position O
, O
i.e. O
the O
10 O
th O
turn O
. O
Potentially O
, O
with O
a O
large O
dialogue O
turn O
position O
, O
the O
neural O
- O
based O
approach O
such O
as O
hierarchical B-MethodName
RNN I-MethodName
can O
better O
capture O
the O
global O
dependencies O
within O
dialogue O
context O
than O
the O
entity O
- O
based O
compositional O
NMN O
method O
. O

Robustness O
to O
question O
structure O
: O
Finally O
, O
we O
compared O
performance O
of O
VGNMN B-MethodName
with O
the O
no O
- O
NMN O
variant O
( O
1 O
) O
in O
different O
cases O
of O
question O
structures O
: O
single O
- O
question O
vs. O
multiple O
- O
part O
structure O
. O
In O
single O
- O
question O
structures O
, O
we O
examined O
by O
the O
question O
types O
( O
e.g. O
yes O
/ O
no O
, O
wh O
- O
questions O
) O
. O
In O
multi O
- O
part O
structures O
, O
we O
further O
classified O
whether O
there O
are O
sentences O
preceding O
the O
question O
( O
e.g. O
" O
1Sent+Que O
" O
) O
or O
there O
are O
smaller O
( O
sub- O
) O
questions O
( O
e.g. O
" O
2SubQue O
" O
) O
within O
the O
question O
. O
In O
Table O
4c O
, O
we O
observed O
that O
VGNMN B-MethodName
has O
clearer O
performance O
gains O
in O
multi O
- O
part O
structures O
than O
singlequestion O
structures O
. O
In O
multi O
- O
part O
structures O
, O
we O
observed O
higher O
gaps O
between O
VGNMN B-MethodName
and O
model O
( O
1 O
) O
in O
highly O
complex O
cases O
e.g. O
" O
2Sent+Que O
" O
vs. O
" O
1Sent+Que O
" O
. O
These O
observations O
indicate O
the O
robustness O
of O
VGNMN B-MethodName
and O
the O
underlying O
compositionality O
principle O
to O
deal O
with O
complex O
question O
structures O
. O
We O
also O
noted O
that O
VGNMN B-MethodName
is O
still O
susceptible O
to O
extremely O
long O
questions O
( O
" O
> O
2Sent+Que O
" O
) O
and O
future O
work O
is O
needed O
to O
ad O
- O
dress O
these O
scenarios O
. O
Interpretability O
. O
In O
Figure O
5 O
, O
we O
show O
both O
success O
and O
failure O
cases O
of O
generated O
responses O
and O
corresponding O
generated O
functional O
programs O
. O
In O
each O
example O
, O
we O
marked O
predicted O
outputs O
as O
incorrect O
if O
they O
do O
not O
match O
the O
ground O
- O
truth O
completely O
( O
even O
though O
the O
outputs O
might O
be O
partially O
correct O
) O
. O
From O
Figure O
5 O
, O
we O
observe O
that O
in O
cases O
where O
generated O
dialogue O
programs O
and O
video O
programs O
match O
or O
are O
close O
to O
the O
gold O
labels O
, O
the O
model O
can O
generate O
generally O
correct O
responses O
. O
For O
cases O
where O
some O
module O
parameters O
do O
not O
exactly O
match O
but O
are O
closed O
to O
the O
gold O
labels O
, O
the O
model O
can O
still O
generate O
responses O
with O
the O
correct O
visual O
information O
( O
e.g. O
the O
4 O
th O
turn O
in O
example O
B O
) O
. O
In O
cases O
of O
wrong O
predicted O
responses O
, O
we O
can O
further O
look O
at O
how O
the O
model O
understands O
the O
questions O
based O
on O
predicted O
programs O
. O
In O
the O
3 O
rd O
turn O
of O
example O
A O
, O
the O
output O
response O
is O
missing O
a O
minor O
detail O
as O
compared O
to O
the O
label O
response O
because O
the O
video O
program O
fails O
to O
capture O
" O
rooftop O
" O
as O
a O
where O
parameter O
. O
These O
subtle O
yet O
important O
details O
can O
determine O
whether O
output O
responses O
can O
fully O
address O
user O
queries O
. O
In O
the O
3 O
rd O
turn O
of O
example O
B O
, O
the O
model O
wrongly O
identifies O
" O
what O
room O
" O
as O
a O
where O
parameter O
and O
subsequently O
generates O
a O
wrong O
response O
that O
it O
is O
" O
a O
living O
room O
" O
. O

TGIF B-DatasetName
- I-DatasetName
QA I-DatasetName
Results O
. O
We O
report O
the O
result O
using O
the O
L2 O
loss B-HyperparameterName
in O
Count O
task O
and O
accuracy B-MetricName
in O
other O
tasks O
. O
From O
Table O
5 O
, O
VGNMN B-MethodName
outperforms O
the O
majority O
of O
the O
baseline O
models O
in O
all O
tasks O
by O
a O
large O
margin O
. O
Compared O
to O
AVSD B-DatasetName
experiments O
, O
the O
TGIF B-DatasetName
- I-DatasetName
QA I-DatasetName
experiments O
emphasize O
the O
video O
understanding O
ability O
of O
the O
models O
, O
removing O
the O
requirement O
for O
dialogue O
understanding O
and O
natural O
language O
generation O
. O
Since O
TGIF B-DatasetName
- I-DatasetName
QA I-DatasetName
questions O
follow O
a O
very O
specific O
question O
type O
distribution O
( O
count O
, O
action O
, O
transition O
, O
and O
frameQA O
) O
, O
the O
question O
structures O
are O
simpler O
and O
easier O
to O
learn O
than O
AVSD B-MetricName
. O
Using O
exact B-MetricName
- I-MetricName
match I-MetricName
accuracy O
of O
parsed O
programs O
vs. O
label O
programs O
as O
a O
metric O
, O
our O
question O
parser O
can O
achieve O
a O
performance O
81 B-MetricValue
% I-MetricValue
to O
94 B-MetricValue
% I-MetricValue
accuracy O
in O
TGIF B-DatasetName
- I-DatasetName
QA I-DatasetName
vs. O
41 B-MetricValue
- I-MetricValue
45 I-MetricValue
% I-MetricValue
in O
AVSD B-DatasetName
. O
The O
higher O
accuracy B-MetricName
in O
decoding O
a O
reasoning O
structure O
translates O
to O
better O
adaptation O
between O
training O
and O
test O
time O
, O
resulting O
in O
higher O
performance O
gains O
. O
Cascading O
Errors O
. O
Compared O
to O
prior O
approaches O
, O
we O
noted O
that O
VGNMN O
is O
a O
modularized O
system O
which O
may O
result O
in O
cascading O
errors O
to O
downstream O
modules O
. O
One O
major O
error O
is O
the O
error O
of O
generated O
programs O
which O
is O
used O
as O
parameters O
in O
neural O
modules O
. O
To O
gauge O
this O
error O
, O
we O
compare O
the O
performance O
of O
VGNMN B-MethodName
between O
2 O
cases O
: O
with O
generated O
programs O
and O
with O
groundtruth O
programs O
. O
From O
Table O
6 O
, O
we O
noticed O
some O
performance O
gaps O
between O
these O
cases O
. O
These O
observations O
imply O
that O
: O
( O
1 O
For O
additional O
experiment O
results O
, O
qualitative O
samples O
, O
and O
analysis O
between O
model O
variants O
, O
refer O
to O
Appendix O
B O
and O
C O
. O

Conclusion O

In O
this O
work O
, O
we O
introduce O
Video B-MethodName
- I-MethodName
grounded I-MethodName
Neural I-MethodName
Module I-MethodName
Network I-MethodName
( O
VGNMN B-MethodName
) O
. O
VGNMN B-MethodName
consists O
of O
dialogue O
and O
video O
understanding O
neural O
modules O
, O
each O
of O
which O
performs O
entity O
and O
action O
- O
level O
operations O
on O
language O
and O
video O
components O
. O
Our O
comprehensive O
experiments O
on O
AVSD B-DatasetName
and O
TGIF B-DatasetName
- I-DatasetName
QA I-DatasetName
benchmarks O
show O
that O
our O
models O
can O
achieve O
competitive O
performance O
while O
promoting O
a O
compositional O
and O
interpretable O
learning O
approach O
. O

Broader O
Impacts O

During O
the O
duration O
of O
this O
work O
, O
there O
have O
been O
no O
ethical O
concerns O
regarding O
the O
model O
implementation O
, O
training O
, O
and O
testing O
. O
The O
data O
used O
in O
this O
work O
has O
been O
carefully O
reviewed O
and O
accordingly O
to O
the O
description O
from O
the O
original O
authors O
, O
we O
did O
not O
find O
any O
concerns O
on O
any O
significant O
biases O
. O
For O
any O
potential O
application O
or O
extension O
of O
this O
work O
, O
we O
would O
like O
to O
highlight O
some O
specific O
concerns O
. O
First O
, O
as O
the O
work O
is O
developed O
to O
build O
an O
intelligent O
dialogue O
agents O
, O
models O
should O
not O
be O
used O
with O
the O
intention O
to O
create O
fake O
human O
profiles O
for O
any O
harmful O
purposes O
( O
e.g. O
fishing O
or O
spreading O
fake O
news O
) O
. O
For O
wider O
use O
of O
dialogue O
systems O
, O
the O
application O
of O
work O
might O
result O
in O
certain O
impacts O
to O
some O
stakeholders O
whose O
jobs O
may O
be O
affected O
by O
this O
application O
( O
e.g. O
customer O
service O
call O
agents O
) O
. O
We O
hope O
any O
application O
should O
be O
carefully O
considered O
against O
these O
potential O
risks O
. O

Each O
attention O
is O
followed O
by O
a O
feed O
- O
forward O
network O
applied O
to O
each O
position O
identically O
. O
We O
exploit O
the O
multi O
- O
head O
and O
feed O
- O
forward O
architecture O
, O
which O
show O
good O
performance O
in O
NLP O
tasks O
such O
as O
NMT O
and O
QA O
( O
Vaswani O
et O
al O
. O
, O
2017 O
; O
Dehghani O
et O
al O
. O
, O
2019 O
) O
, O
to O
efficiently O
incorporate O
contextual O
cues O
from O
dialogue O
components O
to O
parse O
question O
into O
reasoning O
programs O
. O
At O
decoding O
step O
0 O
, O
we O
simply O
use O
a O
special O
token O
_ O
sos O
as O
the O
input O
to O
the O
parser O
. O
In O
each O
subsequent O
decoding O
step O
, O
we O
concatenate O
the O
prior O
input O
sequence O
with O
the O
generated O
token O
to O
decode O
in O
an O
auto O
- O
regressive O
manner O
. O
We O
share O
the O
vocabulary O
sets O
of O
input O
and O
output O
components O
and O
thus O
, O
use O
the O
same O
embedding O
matrix O
. O
Given O
the O
encoded O
question O
Q O
, O
to O
decode O
the O
program O
for O
dialogue O
understanding O
, O
the O
contextual O
signals O
are O
integrated O
through O
2 O
attention O
layers O
: O
one O
attention O
on O
previously O
generated O
tokens O
, O
and O
the O
other O
on O
question O
tokens O
. O
At O
time O
step O
j O
, O
we O
denote O
the O
output O
from O
an O
attention O
layer O
as O
A O
dial O
, O
j O
. O

A O

( O
1 O
) O
dial O
= O
Attention O
( O
P O
dial O
| O
j−1 O
0 O
, O
P O
dial O
| O
j−1 O
0 O
, O
P O
dial O
| O
j−1 O
0 O
) O
A O

( O
2 O
) O
dial O
= O
Attention O
( O
A O
( O
1 O
) O
dial O
, O
Q O
, O
Q O
) O
∈ O
R O
j×d O
To O
generate O
programs O
for O
video O
understanding O
, O
the O
contextual O
signals O
are O
learned O
and O
incorporated O
in O
a O
similar O
manner O
. O
However O
, O
to O
exploit O
dialogue O
contextual O
cues O
, O
the O
execution O
output O
of O
dialogue O
understanding O
neural O
modules O
Q O
ctx O
is O
incorporated O
to O
each O
vector O
in O
P O
dial O
through O
an O
additional O
attention O
layer O
. O
This O
layer O
integrates O
the O
resolved O
entity O
information O
to O
decode O
the O
original O
entities O
for O
video O
understanding O
. O
It O
is O
equivalent O
to O
a O
reasoning O
process O
that O
converts O
the O
question O
from O
its O
original O
multi O
- O
turn O
semantics O
to O
single O
- O
turn O
semantics O
. O

A O

( O
1 O
) O
vid O
= O
Attention O
( O
P O
vid O
| O
j−1 O
0 O
, O
P O
vid O
| O
j−1 O
0 O
, O
P O
vid O
| O
j−1 O
0 O
) O
A O
Noted O
that O
in O
the O
neural O
modules O
described O
in O
Section O
3.3 O
, O
during O
training O
, O
we O
simply O
feed O
the O
ground O
- O
truth O
programs O
to O
optimize O
these O
modules O
. O
For O
instance O
, O
the O
neural O
module O
where O
received O
the O
ground O
truth O
entities O
P O
which O
is O
then O
used O
to O
instantiate O
the O
neural O
network O
and O
retrieve O
from O
video O
V O
. O
During O
test O
time O
, O
we O
decode O
the O
programs O
token O
by O
token O
through O
the O
question O
parsers O
, O
and O
feed O
the O
predicted O
entitiesP O
to O
neural O
modules O
. O
Note O
that O
we O
do O
not O
assume O
, O
and O
hence O
not O
train O
model O
to O
retrieve O
ground O
- O
truth O
locations O
of O
visual O
entities O
in O
videos O
. O
This O
strategy O
enables O
the O
applicability O
of O
VGNMN O
as O
we O
consider O
these O
entity O
annotations O
mostly O
unavailable O
in O
real O
- O
world O
systems O
. O

B O
Additional O
Experimental O
Results O

B.1 O
Non O
- O
NMN O
Models O

We O
experiment O
with O
several O
Non B-MethodName
- I-MethodName
NMN I-MethodName
based O
variants O
of O
our O
models O
. O
As O
can O
be O
seen O
in O
Table O
7 O
, O
our O
approach O
to O
video O
and O
dialogue O
understanding O
through O
compositional O
reasoning O
programs O
exhibits O
better O
performance O
than O
non O
- O
compositional O
approaches O
. O
Compared O
to O
the O
approaches O
that O
directly O
process O
frame O
- O
level O
features O
in O
videos O
( O
Row O
B O
) O
or O
token O
- O
level O
features O
in O
dialogues O
( O
Row O
C O
, O
D O
) O
, O
our O
full O
VGNMN B-MethodName
( O
Row O
A O
) O
considers O
entitylevel O
and O
action O
- O
level O
information O
extraction O
and O
thus O
, O
avoids O
unnecessary O
and O
possibly O
noisy O
extraction O
. O
Compared O
to O
the O
approaches O
that O
obtain O
dialogue O
contextual O
cues O
through O
a O
hierarchical O
encoding O
architecture O
( O
Row O
E O
, O
F O
) O
such O
as O
( O
Serban O
et O
al O
. O
, O
2016 O
; O
, O
VGNMN B-MethodName
directly O
addresses O
the O
challenge O
of O
entity O
references O
in O
dialogues O
. O
As O
mentioned O
, O
we O
hypothesize O
that O
the O
hierarchical O
encoding O
architecture O
is O
more O
appropriate O
for O
less O
entity O
- O
sensitive O
dialogues O
such O
as O
chit O
- O
chat O
and O
open O
- O
domain O
dialogues O
. O

B.2 O
Dialogue O
context O
integration O

Experimenting O
with O
different O
ways O
to O
integrate O
dialogue O
context O
representations O
, O
we O
observe O
that O
adding O
an O
attention O
layer O
attending O
to O
question O
during O
response O
decoding O
( O
Row O
G O
) O
is O
not O
necessary O
. O
This O
can O
be O
explained O
as O
the O
representation O
Q O
ctx O
obtained O
from O
dialogue O
understanding O
program O
already O
contains O
contextual O
information O
of O
both O
dialogue O
history O
and O
question O
and O
question O
input O
is O
no O
longer O
needed O
in O
the O
decoding O
phase O
. O
Furthermore O
, O
we O
investigate O
the O
model O
sensitivity O
to O
natural O
language O
generation O
through O
its O
ability O
to O
construct O
linguistically O
correct O
programs O
and O
responses O
. O
To O
generate O
responses O
that O
are O
linguistically O
appropriate O
, O
VGNMN B-MethodName
needs O
dialogue O
context O
representation O
Q O
ctx O
as O
input O
to O
the O
response O
decoder O
( O
Row O
H O
) O
. O
The O
model O
also O
needs O
encoded O
question O
Q O
as O
input O
to O
the O
video O
understanding O
program O
parser O
to O
be O
able O
to O
decompose O
this O
sequence O
to O
entity O
and O
action O
module O
parameters O
( O
Row O
I O
) O
. O

A O
Additional O
Model O
Details O

A.1 O
Question O
Parsers O

To O
learn O
compositional O
programs O
, O
we O
follow O
( O
Johnson O
et O
al O
. O
, O
2017a O
; O
Hu O
et O
al O
. O
, O
2017 O
) O
and O
consider O
program O
generation O
as O
a O
sequence O
- O
tosequence O
task O
. O
We O
adopt O
a O
simple O
template O
" O
param O
1 O
module O
1 O
param O
2 O
module O
2 O
... O
" O
as O
the O
target O
sequence O
. O
The O
resulting O
target O
sequences O
for O
dialogue O
and O
video O
understanding O
programs O
are O
sequences O
P O
dial O
and O
P O
vid O
respectively O
. O

The O
parsers O
decompose O
questions O
into O
subsequences O
to O
construct O
compositional O
reasoning O
programs O
for O
dialogue O
and O
video O
understanding O
. O
Each O
parser O
is O
an O
attention O
- O
based O
Transformer O
decoder O
. O
The O
Transformer O
attention O
is O
a O
multi O
- O
head O
attention O
on O
query O
q O
, O
key O
k O
, O
and O
value O
v O
tensors O
, O
denoted O
as O
Attention O
( O
q O
, O
k O
, O
v O
) O
. O
For O
each O
token O
in O
the O
q O
sequence O
, O
the O
distribution O
over O
tokens O
in O
the O
k O
sequence O
is O
used O
to O
obtain O
the O
weighted O
sum O
of O
the O
corresponding O
representations O
in O
the O
v O
sequence O
. O

C O
Interpretability O

We O
extract O
the O
predicted O
programs O
and O
responses O
for O
some O
example O
dialogues O
in O
Figure O
6 O
, O
7 O
, O
8 O
, O
and O
9 O
and O
report O
our O
observations O
: O

• O
We O
observe O
that O
when O
the O
predicted O
programs O
are O
correct O
, O
the O
output O
responses O
generally O
match O
the O
ground O
- O
truth O
( O
See O
the O
1 O
st O
and O
2 O
nd O
turn O
in O
Figure O
6 O
, O
and O
the O
1 O
st O
and O
4 O
th O
turn O
in O
Figure O
8 O
) O
or O
close O
to O
the O
ground O
- O
truth O
responses O
( O
1 O
st O
turn O
in O
Figure O
7 O
) O
. O

• O
When O
the O
output O
responses O
do O
not O
match O
the O
ground O
truth O
, O
we O
can O
understand O
the O
model O
mistakes O
by O
interpreting O
the O
predicted O
programs O
. O
For O
example O
, O
in O
the O
3 O
rd O
turn O
in O
Figure O
6 O
, O
the O
output O
response O
describes O
a O
room O
because O
the O
predicted O
video O
program O
focuses O
on O
the O
entity O
" O
what O
room O
" O
instead O
of O
the O
entity O
" O
an O
object O
" O
in O
the O
question O
. O
Another O
example O
is O
the O
3 O
rd O
turn O
in O
Figure O
8 O
where O
the O
entity O
" O
rooftop O
" O
is O
missing O
in O
the O
video O
program O
. O
These O
mismatches O
can O
deviate O
the O
information O
retrieved O
from O
the O
video O
during O
video O
program O
execution O
, O
leading O
to O
wrong O
output O
responses O
with O
wrong O
visual O
contents O
. O

• O
We O
also O
note O
that O
in O
some O
cases O
, O
one O
or O
both O
of O
the O
predicted O
programs O
are O
incorrect O
, O
but O
the O
predicted O
responses O
still O
match O
the O
groundtruth O
responses O
. O
This O
might O
be O
explained O
as O
the O
predicted O
module O
parameters O
are O
still O
close O
enough O
to O
the O
" O
gold O
" O
labels O
( O
e.g. O
4 O
th O
turn O
in O
Figure O
6 O
) O
. O
Sometimes O
, O
our O
model O
predicted O
programs O
that O
are O
more O
appropriate O
than O
the O
ground O
truth O
. O
For O
example O
, O
in O
the O
2 O
nd O
turn O
in O
Figure O
7 O
, O
the O
program O
is O
added O
with O
a O
where O
module O
parameterized O
by O
the O
entity O
" O
the O
shopping O
bag O
" O
which O
was O
solved O
from O
the O
reference O
" O
them O
" O
mentioned O
in O
the O
question O
. O

• O
We O
observe O
that O
for O
complex O
questions O
that O
involve O
more O
than O
one O
queries O
( O
e.g. O
the O
3 O
rd O
turn O
in O
Figure O
8 O
) O
, O
it O
becomes O
more O
challenging O
to O
decode O
an O
appropriate O
video O
understanding O
program O
and O
generate O
responses O
that O
can O
address O
all O
queries O
. O

• O
In O
Figure O
9 O
, O
we O
demonstrate O
some O
output O
examples O
of O
VGNMN B-MethodName
and O
compare O
with O
two O
baselines O
: O
Baseline B-MethodName
and O
MTN B-MethodName
( O
Le O
et O
al O
. O
, O
2019b O
) O
. O
We O
noted O
that O
VGNMN B-MethodName
can O
include O
important O
entities O
relevant O
to O
the O
current O
dialogue O
turn O
to O
construct O
output O
responses O
while O
other O
models O
might O
miss O
some O
entity O
details O
, O
e.g. O
" O
them O
/ O
dishes O
" O
in O
example O
A O
and O
" O
the O
magazine O
" O
in O
example O
B. O
These O
small O
yet O
important O
details O
can O
determine O
the O
correctness O
of O
dialogue O
responses O
. O
Figure O
9 O
: O
Interpretability O
of O
example O
outputs O
from O
VGNMN B-MethodName
and O
baselines O
models O
Le O
et O
al O
. O
, O
2019b O
) O

Zero O
- O
Shot O
Cross B-TaskName
- I-TaskName
lingual I-TaskName
Semantic I-TaskName
Parsing I-TaskName

Recent O
work O
in O
cross B-TaskName
- I-TaskName
lingual I-TaskName
semantic I-TaskName
parsing I-TaskName
has O
successfully O
applied O
machine O
translation O
to O
localize O
parsers O
to O
new O
languages O
. O
However O
, O
these O
advances O
assume O
access O
to O
highquality O
machine O
translation O
systems O
and O
word O
alignment O
tools O
. O
We O
remove O
these O
assumptions O
and O
study O
cross B-TaskName
- I-TaskName
lingual I-TaskName
semantic I-TaskName
parsing I-TaskName
as O
a O
zero O
- O
shot O
problem O
, O
without O
parallel O
data O
( O
i.e. O
, O
utterance O
- O
logical O
form O
pairs O
) O
for O
new O
languages O
. O
We O
propose O
a O
multi B-MethodName
- I-MethodName
task I-MethodName
encoderdecoder I-MethodName
model O
to O
transfer O
parsing O
knowledge O
to O
additional O
languages O
using O
only O
Englishlogical O
form O
paired O
data O
and O
in O
- O
domain O
natural O
language O
corpora O
in O
each O
new O
language O
. O
Our O
model O
encourages O
language B-MethodName
- I-MethodName
agnostic I-MethodName
encodings I-MethodName
by O
jointly O
optimizing O
for O
logical O
- O
form O
generation O
with O
auxiliary O
objectives O
designed O
for O
cross O
- O
lingual O
latent O
representation O
alignment O
. O
Our O
parser O
performs O
significantly O
above O
translation O
- O
based O
baselines O
and O
, O
in O
some O
cases O
, O
competes O
with O
the O
supervised O
upper O
- O
bound O
. O
1 O

Introduction O

Executable O
semantic O
parsing O
maps O
a O
natural O
language O
utterance O
to O
a O
logical O
form O
( O
LF O
) O
for O
execution O
in O
some O
knowledge O
base O
to O
return O
a O
denotation O
. O
The O
parsing O
task O
renders O
an O
utterance O
as O
a O
semantically O
identical O
, O
but O
machine O
- O
interpretable O
, O
expression O
grounded O
in O
a O
denotation O
. O
The O
transduction O
between O
natural O
and O
formal O
languages O
has O
allowed O
semantic O
parsers O
to O
become O
critical O
infrastructure O
in O
building O
human O
- O
computer O
interfaces O
for O
question O
answering O
, O
( O
Berant O
et O
al O
. O
, O
2013 O
; O
Liang O
, O
2016 O
; O
Kollar O
et O
al O
. O
, O
2018 O
) O
, O
dialog O
systems O
( O
Artzi O
and O
Zettlemoyer O
, O
2011 O
) O
, O
and O
robotics O
( O
Dukes O
, O
2014 O
) O
. O

Recent O
advances O
in O
semantic B-TaskName
parsing I-TaskName
have O
improved O
accuracy O
for O
neural O
parsers O
( O
Jia O
and O
Liang O
, O
2016 O
; O
Dong O
and O
Lapata O
, O
2016 O
; O
Wang O
et O
al O
. O
, O
2020a O
) O
and O
examined O
their O
generalization O
capabilities O
with O
new O
dataset O
challenges O
( O
Zhong O
et O
al O
. O
, O
2017 O
; O
Yu O
1 O
Our O
code O
and O
data O
are O
available O
at O
github.com O
/ O
tomsherborne O
/ O
zx-parse O
. O
The O
encoder O
generates O
a O
representation O
of O
the O
English O
utterance O
( O
blue O
points O
) O
to O
condition O
upon O
during O
decoding O
. O
Producing O
the O
same O
logical O
form O
from O
the O
equivalent O
Chinese O
utterance O
requires O
a O
similar O
encoding O
. O
However O
, O
without O
alignment O
, O
the O
representation O
may O
partially O
match O
( O
purple O
points O
) O
or O
not O
at O
all O
( O
red O
points O
) O
, O
leading O
the O
decoder O
to O
generate O
an O
inaccurate O
, O
ill O
- O
formed O
query O
. O
et O
al O
. O
, O
2018 O
) O
, O
in O
addition O
to O
considering O
languages O
other O
than O
English O
( O
Duong O
et O
al O
. O
, O
2017 O
; O
inter O
alia O
. O
) O
. O

Prior O
work O
largely O
assumes O
that O
utterance O
- O
logical O
form O
training O
data O
is O
parallel O
in O
all O
languages O
( O
Jie O
and O
Lu O
, O
2014 O
) O
, O
or O
must O
be O
created O
with O
human O
translation O
( O
Susanto O
and O
Lu O
, O
2017a O
) O
. O
This O
entry O
barrier O
to O
localization O
for O
new O
languages O
has O
motivated O
the O
exploration O
of O
machine B-TaskName
translation I-TaskName
( O
MT B-TaskName
) O
as O
an O
economical O
alternative O
( O
Sherborne O
et O
al O
. O
, O
2020 O
; O
Moradshahi O
et O
al O
. O
, O
2020 O
) O
. O
However O
, O
MT B-TaskName
can O
introduce O
performance O
- O
limiting O
artifacts O
and O
struggle O
to O
accurately O
model O
native O
speakers O
( O
Riley O
et O
al O
. O
, O
2020 O
) O
. O
Additionally O
, O
high O
- O
quality O
machine B-TaskName
translation I-TaskName
is O
less O
viable O
for O
lower O
resource O
languages O
, O
further O
limiting O
the O
appeal O
of O
MT B-TaskName
- O
based O
approaches O
. O

In O
this O
work O
, O
we O
propose O
a O
new O
approach O
for O
zero B-MethodName
- I-MethodName
shot I-MethodName
executable I-MethodName
semantic I-MethodName
parsing I-MethodName
. O
Our O
method O
maximizes O
the O
success O
of O
cross O
- O
lingual O
transfer O
for O
a O
parser O
, O
trained O
on O
English O
paired O
data O
( O
EN O
→ O
LF O
) O
, O
to O
accurately O
generate O
logical O
forms O
from O
new O
languages O
( O
X O
→ O
LF O
) O
. O
Our O
goal O
is O
to O
parse O
utterances O
in O
a O
new O
language O
, O
l O
, O
without O
observing O
paired O
training O
data O
for O
this O
language O
, O
suitable O
machine B-TaskName
translation I-TaskName
, O
or O
bilingual O
dictionaries O
between O
l O
and O
English O
. O
Our O
critical O
dependencies O
are O
a O
pre O
- O
trained O
language O
model O
and O
utterancelogical O
form O
paired O
data O
for O
a O
source O
language O
( O
i.e. O
, O
English O
) O
. O
Aside O
from O
the O
zero O
- O
shot O
problem O
which O
is O
hard O
on O
its O
own O
( O
since O
paired O
data O
is O
not O
available O
for O
new O
languages O
) O
, O
our O
semantic B-TaskName
parsing I-TaskName
challenge O
is O
further O
compounded O
with O
the O
difficulties O
inherent O
to O
structured O
prediction O
and O
the O
deficiency O
of O
copying O
strategies O
without O
gold O
tokenlevel O
alignment O
( O
Zhu O
et O
al O
. O
, O
2020 O
) O
. O

We O
conceptualize O
cross B-TaskName
- I-TaskName
lingual I-TaskName
semantic I-TaskName
parsing I-TaskName
as O
a O
latent O
representation O
alignment O
problem O
. O
As O
illustrated O
in O
Figure O
1 O
, O
we O
wish O
to O
encode O
different O
languages O
to O
an O
overlapping O
latent O
space O
for O
the O
decoder O
to O
have O
any O
chance O
at O
generating O
accurate O
logical O
forms O
. O
To O
achieve O
this O
, O
we O
train O
a O
decoder O
, O
conditioned O
upon O
encodings O
from O
a O
source O
language O
( O
e.g. O
, O
English O
) O
, O
to O
generate O
logical O
forms O
and O
simultaneously O
train O
encodings O
of O
a O
new O
language O
( O
e.g. O
, O
Chinese O
) O
to O
be O
maximally O
similar O
to O
English O
. O
We O
hypothesize O
that O
if O
latent O
representations O
are O
aligned O
from O
a O
language O
- O
agnostic O
encoder O
, O
one O
can O
generate O
accurate O
logical O
forms O
from O
a O
new O
language O
without O
semantic B-TaskName
parsing I-TaskName
training O
data O
and O
thus O
eliminate O
the O
errors O
outlined O
in O
Figure O
1 O
. O

Our O
approach O
adopts O
a O
multi O
- O
task O
learning O
paradigm O
and O
trains O
a O
parser O
with O
auxiliary O
objectives O
, O
optimized O
to O
converge O
representations O
of O
additional O
new O
languages O
. O
We O
encourage O
languageagnostic O
representations O
by O
jointly O
optimizing O
for O
generating O
logical O
forms O
, O
reconstructing O
natural O
language O
, O
and O
promoting O
language O
invariance O
. O
Our O
intuition O
is O
that O
auxiliary O
losses O
can O
be O
exploited O
to O
induce O
similarity O
in O
a O
multi O
- O
lingual O
latent O
space O
. O
The O
effect O
of O
such O
alignment O
is O
that O
a O
decoder O
, O
trained O
only O
on O
English O
, O
can O
recognize O
an O
encoding O
from O
another O
language O
and O
generate O
the O
relevant O
logical O
form O
. O
Similar O
multi O
- O
task O
approaches O
have O
been O
successful O
in O
spoken O
- O
language O
understanding O
( O
van O
der O
Goot O
et O
al O
. O
, O
2021 O
) O
, O
text O
simplification O
( O
Mallinson O
et O
al O
. O
, O
2020 O
; O
Zhao O
et O
al O
. O
, O
2020b O
) O
, O
dependency O
parsing O
( O
Ahmad O
et O
al O
. O
, O
2019b O
) O
, O
and O
machine O
translation O
( O
Arivazhagan O
et O
al O
. O
, O
2019 O
) O
. O
This O
work O
, O
to O
our O
knowledge O
is O
the O
first O
attempt O
to O
devise O
auxiliary O
objectives O
for O
executable O
semantic B-TaskName
parsing I-TaskName
as O
a O
zero O
- O
shot O
task O
. O
Our O
framework O
and O
hypothesis O
are O
also O
sufficiently O
flexible O
for O
application O
in O
additional O
zero O
- O
shot O
sequence O
transduction O
tasks O
. O

Our O
motivation O
is O
to O
improve O
parsing O
for O
non O
- O
English O
languages O
with O
maximal O
resource O
efficiency O
and O
minimal O
external O
dependencies O
beyond O
native O
- O
speaker O
utterances O
. O
We O
, O
therefore O
, O
induce O
a O
shared O
multilingual O
space O
without O
resorting O
to O
machine O
translation O
( O
Sherborne O
et O
al O
. O
, O
2020 O
; O
Moradshahi O
et O
al O
. O
, O
2020 O
) O
and O
argue O
that O
our O
approach O
is O
superior O
because O
it O
( O
a O
) O
nullifies O
the O
introduction O
of O
translation O
or O
word O
alignment O
errors O
and O
( O
b O
) O
scales O
to O
low O
- O
resource O
languages O
without O
reliable O
MT B-TaskName
. O
Experimental O
results O
on O
Overnight O
( O
Wang O
et O
al O
. O
, O
2015 O
; O
Sherborne O
et O
al O
. O
, O
2020 O
) O
and O
a O
new O
executable O
version O
of O
MultiATIS++ B-DatasetName
show O
that O
our O
parser O
generates O
more O
accurate O
logical O
forms O
with O
a O
minimized O
cross O
- O
lingual O
transfer O
penalty O
from O
English O
to O
French O
( O
FR O
) O
, O
Portuguese O
( O
PT O
) O
, O
Spanish O
( O
ES O
) O
, O
German O
( O
DE O
) O
, O
Chinese O
( O
ZH O
) O
, O
Hindi O
( O
HI O
) O
, O
and O
Turkish O
( O
TR O
) O
. O

Related O
Work O

Cross O
- O
lingual O
Modeling O
This O
area O
has O
recently O
gained O
increased O
interest O
across O
several O
natural O
language O
understanding O
settings O
( O
Zhao O
et O
al O
. O
, O
2020a O
; O
Nooralahzadeh O
et O
al O
. O
, O
2020 O
) O
with O
benchmarks O
such O
as O
XGLUE O
( O
Liang O
et O
al O
. O
, O
2020 O
) O
and O
XTREME O
( O
Hu O
et O
al O
. O
, O
2020 O
) O
allowing O
to O
study O
classification O
and O
generation O
tasks O
for O
multiple O
languages O
. O
Crosslingual O
approaches O
have O
also O
been O
developed O
for O
dependency O
parsing O
( O
Tiedemann O
et O
al O
. O
, O
2014 O
; O
Schuster O
et O
al O
. O
, O
2019 O
) O
, O
sentence O
simplification O
( O
Mallinson O
et O
al O
. O
, O
2020 O
) O
, O
and O
spoken O
- O
language O
understanding O
( O
SLU O
; O
He O
et O
al O
. O
, O
2013 O
; O
Upadhyay O
et O
al O
. O
, O
2018 O
) O
. O

Pre O
- O
training O
has O
shown O
to O
be O
widely O
beneficial O
for O
a O
wide O
range O
of O
cross O
- O
lingual O
models O
Conneau O
et O
al O
. O
, O
2020a O
) O
. O
By O
virtue O
of O
being O
trained O
on O
massive O
corpora O
, O
these O
models O
purportedly O
learn O
an O
overlapping O
cross O
- O
lingual O
latent O
space O
( O
Conneau O
et O
al O
. O
, O
2020b O
) O
but O
have O
also O
been O
identified O
as O
under O
- O
trained O
for O
some O
tasks O
( O
Li O
et O
al O
. O
, O
2021 O
) O
, O
shown O
poor O
zero O
- O
shot O
performance O
, O
especially O
for O
languages O
dissimilar O
to O
English O
( O
Pires O
et O
al O
. O
, O
2019 O
) O
, O
and O
high O
variance O
( O
Keung O
et O
al O
. O
, O
2020 O
) O
. O

Semantic B-TaskName
Parsing I-TaskName
Most O
previous O
work O
( O
Lu O
, O
2014 O
; O
Susanto O
and O
Lu O
, O
2017b O
, O
a O
) O
has O
focused O
on O
multilingual O
semantic O
parsing O
, O
i.e. O
, O
learning O
from O
multiple O
natural O
languages O
in O
parallel O
, O
largely O
affirming O
the O
benefit O
of O
" O
high O
- O
resource O
" O
multilingual O
data O
and O
multi O
- O
language O
ensemble O
training O
( O
Jie O
and O
Lu O
, O
2014 O
) O
. O
Shao O
et O
al O
. O
( O
2020 O
) O
further O
improved O
cross O
- O
lingual O
similarity O
with O
adversarial O
language O
identification O
across O
such O
ensembled O
training O
data O
. O
Code O
- O
switching O
in O
multilingual O
parsing O
has O
also O
been O
explored O
through O
mixed O
- O
language O
training O
datasets O
( O
Duong O
et O
al O
. O
, O
2017 O
; O
Einolghozati O
et O
al O
. O
, O
2021 O
) O
. O
To O
adapt O
a O
parser O
to O
new O
languages O
, O
machine O
translation O
has O
been O
used O
as O
a O
reasonable O
proxy O
for O
in O
- O
language O
data O
( O
Sherborne O
et O
al O
. O
, O
2020 O
; O
Moradshahi O
et O
al O
. O
, O
2020 O
) O
. O
However O
, O
machine O
translation O
, O
in O
either O
direction O
can O
introduce O
limiting O
artifacts O
( O
Artetxe O
et O
al O
. O
, O
2020 O
) O
with O
poor O
generalization O
due O
to O
how O
" O
translationese O
" O
training O
data O
diverges O
from O
gold O
test O
utterances O
( O
Riley O
et O
al O
. O
, O
2020 O
) O
. O

Zero B-MethodName
- I-MethodName
shot I-MethodName
parsing I-MethodName
has O
primarily O
focused O
on O
' O
cross O
- O
domain O
' O
challenges O
to O
improve O
generalization O
across O
varying O
query O
structures O
and O
lexicons O
( O
Herzig O
and O
Berant O
, O
2018 O
; O
Givoli O
and O
Reichart O
, O
2019 O
) O
or O
different O
databases O
( O
Zhong O
et O
al O
. O
, O
2020 O
; O
Suhr O
et O
al O
. O
, O
2020 O
; O
Yu O
et O
al O
. O
, O
2018 O
) O
. O
The O
combination O
of O
zero B-MethodName
- I-MethodName
shot I-MethodName
parsing I-MethodName
with O
cross O
- O
lingual O
modeling O
has O
also O
been O
examined O
for O
the O
UCCA O
formalism O
( O
Hershcovich O
et O
al O
. O
, O
2019 O
) O
and O
for O
task O
- O
oriented O
dialogue O
systems O
( O
see O
below O
) O
. O

Dialog O
Modeling O
Cross O
- O
lingual O
transfer O
has O
been O
studied O
in O
the O
context O
of O
goal O
- O
oriented O
dialog O
for O
the O
spoken O
language O
understanding O
( O
SLU O
) O
tasks O
of O
intent O
classification O
and O
slot O
labeling O
( O
i.e. O
, O
parsing O
an O
utterance O
into O
a O
semantic O
frame O
identifying O
the O
user O
's O
intent O
and O
its O
arguments O
) O
. O
Recently O
released O
multilingual O
datasets O
like O
MultiATIS++ B-DatasetName
and O
MTOP O
( O
Li O
et O
al O
. O
, O
2021 O
) O
have O
facilitated O
the O
study O
of O
zero O
- O
shot O
transfer O
through O
the O
combination O
of O
pre O
- O
training O
, O
machine O
translation O
, O
and O
word O
alignment O
( O
to O
project O
annotations O
between O
languages O
) O
. O
Recent O
work O
in O
this O
setting O
( O
Zhu O
et O
al O
. O
, O
2020 O
; O
Li O
et O
al O
. O
, O
2021 O
; O
Krishnan O
et O
al O
. O
, O
2021 O
; O
Nicosia O
et O
al O
. O
, O
2021 O
) O
identifies O
a O
penalty O
for O
cross O
- O
lingual O
transfer O
that O
neither O
pre O
- O
training O
nor O
machine B-TaskName
translation I-TaskName
can O
fully O
overcome O
. O

Problem O
Formulation O

The O
primary O
challenge O
for O
cross B-TaskName
- I-TaskName
lingual I-TaskName
parsing I-TaskName
is O
learning O
parameters O
that O
can O
parse O
an O
utterance O
, O
x O
, O
from O
an O
unseen O
test O
language O
to O
an O
accurate O
logical O
form O
( O
LF O
) O
. O
Typically O
, O
a O
parser O
trained O
on O
language O
l O
, O
or O
multiple O
languages O
{ O
l O
1 O
, O
. O
. O
. O
, O
l O
N O
} O
, O
is O
only O
capable O
for O
these O
languages O
and O
performs O
poorly O
outside O
this O
set O
. O
For O
a O
new O
language O
, O
prior O
approaches O
require O
parallel O
datasets O
and O
models O
( O
Jie O
and O
Lu O
, O
2014 O
; O
Haas O
and O
Riezler O
, O
2016 O
; O
Duong O
et O
al O
. O
, O
2017 O
) O
. O

In O
our O
work O
, O
zero B-MethodName
- I-MethodName
shot I-MethodName
parsing I-MethodName
refers O
to O
parsing O
utterances O
in O
new O
languages O
without O
paired O
data O
during O
training O
, O
For O
some O
language O
, O
l O
, O
there O
exists O
no O
pairing O
of O
x O
l O
to O
a O
logical O
form O
, O
y O
, O
except O
for O
English O
. O
2 O
This O
setting O
also O
excludes O
" O
silver O
- O
standard O
" O
training O
pairs O
created O
using O
machine O
- O
translation O
. O
As O
these O
models O
have O
ultimately O
observed O
some O
form O
of O
utterance O
- O
logical O
form O
pairs O
for O
each O
new O
language O
, O
we O
do O
not O
consider O
such O
approaches O
here O
and O
refer O
to O
Sherborne O
et O
al O
. O
( O
2020 O
) O
as O
an O
example O
of O
using O
MT B-TaskName
for O
this O
task O
. O

It O
might O
be O
tempting O
to O
approach O
this O
problem O
as O
a O
case O
of O
fine O
- O
tuning O
a O
pre O
- O
trained O
( O
English O
) O
decoder O
for O
LF B-TaskName
generation I-TaskName
. O
Problematically O
, O
the O
output O
target O
is O
expressed O
in O
a O
formally O
defined O
language O
( O
e.g. O
, O
SQL O
or O
λ−DCS O
) O
which O
models O
the O
semantics O
of O
questions O
very O
differently O
to O
natural O
language O
( O
e.g. O
, O
without O
presumption O
or O
co O
- O
operation O
; O
Kaplan O
1978 O
) O
. O
Formal O
languages O
( O
Kamp O
and O
Reyle O
, O
1993 O
) O
additionally O
present O
artifacts O
which O
render O
fine O
- O
tuning O
challenging O
such O
as O
unfamiliar O
syntax O
( O
e.g. O
, O
table O
aliases O
or O
explicit O
recursion O
) O
and O
long O
output O
sequences O
. O
In O
practice O
, O
we O
observed O
finetuning O
leads O
to O
poor O
performance O
( O
e.g. O
, O
< O
1 O
% O
accuracy O
on O
all O
languages O
) O
, O
with O
the O
model O
insisting O
on O
hallucinating O
natural O
language O
. O
This O
is O
seemingly O
at O
odds O
with O
adjacent O
work O
in O
dialog O
modeling O
, O
which O
has O
found O
pre O
- O
trained O
decoders O
to O
be O
beneficial O
( O
Li O
et O
al O
. O
, O
2021 O
) O
. O
However O
, O
SLU O
requires O
learning O
a O
lightweight O
label O
vocabulary O
compared O
to O
the O
200 O
+ O
tokens O
required O
in O
LFs O
. O
Additionally O
, O
SLU O
typically O
maintains O
output O
sequences O
of O
similar O
size O
to O
natural O
language O
inputs O
( O
with O
tightly O
coupled O
syntactic O
compositionality O
between O
the O
two O
) O
, O
whereas O
the O
syntactic O
and O
structural O
demands O
of O
LF O
generation O
are O
largely O
divorced O
from O
the O
input O
utterance O
. O

In O
our O
solution O
, O
the O
model O
is O
trained O
to O
parse O
from O
utterance O
- O
logical O
forms O
pairs O
only O
in O
English O
. O
Other O
languages O
are O
incorporated O
using O
auxiliary O
objectives O
and O
data O
detailed O
in O
Section O
4 O
. O
We O
explore O
the O
hypothesis O
that O
an O
overlapping O
multi O
- O
lingual O
latent O
space O
can O
be O
learned O
through O
auxiliary O
objectives O
in O
tandem O
with O
logical O
form O
generation O
( O
see O
Figure O
2 O
) O
. O
Our O
intuition O
is O
that O
introducing O
these O
additional O
losses O
minimizes O
cross O
- O
lingual O
variance O
in O
latent O
encoding O
space O
by O
optimizing O
for O
language O
- O
agnostic O
representations O
with O
high O
similarity O
to O
the O
source O
language O
( O
i.e. O
, O
English O
) O
. O
Our O
approach O
minimizes O
the O
cross O
- O
lingual O
transfer O
penalty O
such O
that O
the O
zeroshot O
parser O
predicts O
logical O
forms O
from O
test O
inputs O
regardless O
of O
utterance O
language O
. O
By O
framing O
the O
cross O
- O
lingual O
parsing O
task O
as O
a O
latent O
representation O
alignment O
challenge O
, O
we O
explore O
a O
possible O
upper O
bound O
of O
parsing O
accuracy O
without O
errors O
from O
external O
dependencies O
. O
Section O
6 O
demonstrates O
that O
our O
zero O
- O
shot O
model O
, O
using O
only O
English O
paired O
data O
and O
a O
small O
additional O
corpus O
, O
can O
generate O
accurate O
logical O
forms O
above O
translation O
baselines O
to O
compete O
with O
fully O
supervised O
in O
- O
language O
training O
. O

Our O
Zero O
- O
shot O
Model O
: O
ZX B-MethodName
- I-MethodName
PARSE I-MethodName

We O
adopt O
a O
multi B-MethodName
- I-MethodName
task I-MethodName
sequence I-MethodName
- I-MethodName
to I-MethodName
- I-MethodName
sequence I-MethodName
model O
( O
Luong O
et O
al O
. O
, O
2016 O
) O
which O
combines O
logical B-TaskName
form I-TaskName
generation I-TaskName
with O
two O
auxiliary O
objectives O
. O
The O
first O
is O
a O
language O
identification O
discriminator O
and O
the O
second O
is O
a O
reconstruction O
or O
translation O
decoder O
. O
An O
overview O
of O
our O
semantic O
parser O
is O
given O
in O
Figure O
2 O
; O
we O
describe O
each O
component O
below O
. O

Generating B-TaskName
Logical I-TaskName
Forms I-TaskName
Predicting B-TaskName
logical I-TaskName
forms I-TaskName
is O
the O
primary O
objective O
for O
our O
model O
. O
Given O
an O
utterance O
x O
= O
( O
x O
1 O
, O
x O
2 O
, O
. O
. O
. O
, O
x O
T O
) O
, O
we O
wish O
to O
generate O
logical O
form O
y O
= O
( O
y O
1 O
, O
y O
2 O
, O
. O
. O
. O
, O
y O
M O
) O
representing O
the O
same O
meaning O
in O
a O
machine O
- O
executable O
language O
. O
We O
model O
this O
transduction O
task O
using O
an O
encoder O
- O
decoder O
neural O
network O
( O
Sutskever O
et O
al O
. O
, O
2014 O
) O
based O
upon O
the O
Transformer B-MethodName
architecture O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
. O

The O
sequence O
x O
is O
encoded O
to O
a O
latent O
representation O
z O
= O
( O
z O
1 O
, O
z O
2 O
, O
. O
. O
. O
, O
z O
T O
) O
through O
Equation O
( O
1 O
) O
using O
a O
stacked O
self O
- O
attention O
Transformer B-MethodName
encoder O
, O
E O
, O
with O
weights O
θ O
E O
. O

z O
= O
E O
( O
x|θ O
E O
) O
( O
1 O
) O
p O
( O
y|x O
) O
= O
M O
i=0 O
p O
( O
y O
i O
|y O
< O
i O
, O
x O
) O
( O
2 O
) O
p O
( O
y O
i O
|y O
< O
i O
, O
x O
) O
= O
soft O
( O
D O
LF O
( O
y O
< O
i O
|z O
, O
θ O
D O
LF O
) O
) O
( O
3 O
) O
L O
LF O
= O
− O
( O
x O
, O
y O
) O
∈S O
LF O
log O
p O
( O
y|x O
) O
( O
4 O
) O

The O
conditional O
probability O
of O
the O
output O
sequence O
y O
is O
expressed O
in O
Equation O
( O
2 O
) O
as O
each O
token O
y O
i O
is O
autoregressively O
generated O
based O
upon O
z O
and O
prior O
outputs O
, O
y O
< O
i O
. O
Equation O
( O
3 O
) O
models O
distribution O
p O
( O
y O
i O
|y O
< O
i O
, O
x O
) O
using O
a O
Transformer O
decoder O
for O
logical O
forms O
, O
D O
LF O
, O
with O
associated O
weights O
θ O
D O
LF O
where O
soft O
is O
the O
softmax O
function O
. O

We O
predict O
an O
output O
, O
ŷ O
, O
for O
semantic O
parsing O
dataset O
S O
LF O
= O
{ O
x O
n O
, O
y O
n O
} O
N O
n=0 O
, O
through O
the O
encoder O
and O
logical O
form O
decoder O
, O
{ O
E O
, O
D O
LF O
} O
. O
Equation O
( O
4 O
) O
describes O
the O
loss O
objective O
minimizing O
the O
crossentropy O
between O
y O
andŷ O
. O

Language B-TaskName
Prediction I-TaskName
Our O
first O
additional O
objective O
encourages O
language O
- O
agnostic O
representations O
by O
reducing O
the O
discriminability O
of O
the O
source O
language O
, O
l O
, O
from O
z. O
Equation O
( O
5 O
) O
defines O
a O
Language B-TaskName
Prediction I-TaskName
( O
LP B-TaskName
) O
network O
to O
predict O
l O
from O
z O
using O
a O
linear O
classifier O
over O
L O
training O
languages O
: O

LP B-TaskName
( O
x O
) O
= O
W O
i O
x O
+ O
b O
i O
( O
5 O
) O

where O
W O
i O
∈ O
R O
L×|z| O
and O
b O
i O
∈ O
R O
L O
are O
a O
weight O
and O
bias O
respectively O
. O
We O
follow O
the O
best O
model O
from O
Ahmad O
et O
al O
. O
( O
2019b O
) O
. O
Equation O
( O
6 O
) O
describes O
the O
conditional O
model O
for O
the O
output O
distribution O
where O
a O
language O
label O
is O
predicted O
using O
the O
time O
- O
average O
of O
the O
input O
encoding O
z O
of O
length O
T O
: O

p O
( O
l|x O
) O
= O
soft O
LP B-TaskName
1 O
T O
t O
z O
t O
( O
6 O
) O

Finally O
, O
Equation O
( O
7 O
) O
describes O
the O
objective O
function O
for O
the O
LP B-TaskName
network O
: O

L O
LP B-TaskName
= O
− O
x O
log O
p O
( O
l|x O
) O
( O
7 O
) O

However O
, O
we O
reverse O
this O
gradient O
in O
the O
backward O
pass O
before O
the O
LP B-TaskName
network O
, O
to O
encourage O
the O
encoder O
to O
produce O
language O
invariant O
representations O
( O
Ganin O
et O
al O
. O
, O
2016 O
) O
. O
The O
LP B-TaskName
network O
is O
optimized O
to O
discriminate O
the O
source O
language O
from O
z O
, O
but O
the O
encoder O
is O
now O
optimized O
adversarially O
against O
this O
objective O
. O
Our O
intuition O
is O
that O
discouraging O
language O
discriminability O
in O
z O
encourages O
latent O
representation O
similarity O
across O
languages O
, O
and O
therefore O
reduces O
the O
penalty O
for O
cross O
- O
lingual O
transfer O
. O

Generating B-TaskName
Natural I-TaskName
Language I-TaskName
The O
final O
objective O
acts O
towards O
both O
regularization O
and O
crosslingual O
similarity O
. O
Motivated O
by O
domain O
- O
adaptive O
pre O
- O
training O
( O
Gururangan O
et O
al O
. O
, O
2020 O
) O
, O
we O
further O
adapt O
the O
encoder O
towards O
question O
- O
style O
utterances O
from O
native O
speakers O
of O
each O
test O
language O
lacking O
task O
- O
specific O
training O
data O
. O
We O
add O
an O
additional O
Transformer B-MethodName
decoder O
optimized O
to O
reconstruct O
a O
noisy O
input O
from O
latent O
representation O
z O
, O
in O
Equation O
( O
1 O
) O
. O
Utterance O
, O
x O
, O
is O
input O
to O
the O
encoder O
, O
E O
, O
and O
a O
separate O
decoder O
, O
D O
NL O
, O
then O
reconstructs O
x O
from O
z. O
We O
follow O
the O
denoising O
objective O
from O
and O
replace O
x O
with O
noised O
inputx O
= O
N O
( O
x O
) O
with O
noising O
function O
N. O
The O
output O
probability O
of O
reconstruction O
is O
given O
in O
Equation O
( O
9 O
) O
with O
each O
token O
predicted O
through O
Equation O
( O
10 O
) O
using O
decoder O
, O
D O
NL O
, O
with O
weights O
θ O
D O
NL O
: O

z O
= O
E O
( O
x|θ O
E O
) O
( O
8 O
) O

p O
( O
x|x O
) O
= O
T O
i=0 O
p O
( O
x O
i O
|x O
< O
i O
, O
x O
) O
( O
9 O
) O
p O
( O
x O
i O
|x O
< O
i O
, O
x O
) O
=soft O
( O
D O
NL O
( O
x O
< O
i O
|ẑ O
, O
θ O
D O
NL O
) O
) O
( O
10 O
) O

The O
auxiliary O
objectives O
are O
trained O
using O
both O
the O
utterances O
from O
S O
LF O
and O
monolingual O
data O
, O

S O
NL O
= O
{ O
{ O
x O
n O
} O
N O
n=0 O
} O
L O
l=0 O

, O
in O
L O
languages O
( O
see O
Section O
5 O
) O
. O
Submodel O
, O
{ O
E O
, O
D O
NL O
} O
, O
predicts O
the O
reconstruction O
of O
x O
fromx O
with O
the O
following O
objective O
: O

L O
NL O
= O
− O
x O
log O
p O
( O
x|x O
) O
( O
11 O
) O

In O
the O
form O
described O
above O
, O
this O
objective O
requires O
only O
unlabeled O
, O
monolingual O
utterances O
in O
each O
target O
language O
. O
However O
, O
we O
can O
also O
augment O
it O
with O
a O
translation O
component O
to O
exploit O
natural O
language O
bi O
- O
text O
between O
the O
new O
language O
and O
English O
( O
e.g. O
, O

S O
NL O
= O
{ O
{ O
x O
n O
EN O
, O
x O
n O
l O
} O
N O
n=0 O
} O
L O
l=0 O

) O
to O
further O
promote O
cross O
- O
lingual O
similarity O
. O
According O
to O
some O
sampling O
factor O
τ O
, O
we O
randomly O
choose O
whether O
to O
reconstruct O
an O
utterance O
( O
as O
above O
) O
or O
translate O
to O
the O
parallel O
English O
utterance O
( O
i.e. O
, O
replace O
x O
in O
Equation O
( O
11 O
) O
with O
x O
EN O
) O
. O

Combined O
Model O

The O
combined O
model O
uses O
a O
single O
encoder O
, O
E O
, O
and O
the O
three O
objective O
decoders O
{ O
D O
LF O
, O
D O
NL O
, O
LP O
} O
( O
see O
Figure O
2 O
) O
. O
During O
training O
, O
an O
English O
query O
is O
encoded O
and O
input O
to O
all O
three O
objectives O
to O
express O
output O
loss O
as O
L O
LF O
+ O
L O
NL O
+ O
L O
LP O
. O
For O
new O
languages O
without O
( O
x O
, O
y O
) O
pairs O
, O
the O
utterance O
is O
encoded O
and O
input O
only O
to O
the O
auxiliary O
objectives O
for O
a O
combined O
loss O
as O
L O
NL O
+ O
L O
LP O
. O
During O
inference O
, O
an O
utterance O
is O
encoded O
and O
always O
input O
to O
D O
LF O
to O
predict O
a O
logical O
form O
, O
ŷ O
, O
regardless O
of O
test O
language O
, O
l. O
During O
the O
backward O
pass O
, O
each O
output O
loss O
back O
- O
propagates O
the O
gradient O
signal O
from O
the O
respective O
objective O
function O
. O
For O
the O
encoder O
, O
these O
signals O
are O
combined O
as O
: O

∂L O
∂θ O
E O
= O
∂L O
LF O
∂θ O
E O
− O
λα O
LP O
∂L O
LP O
∂θ O
E O
+ O
α O
NL O
∂L O
NL O
∂θ O
E O
( O
12 O
) O
λ O
= O
2 O
1 O
+ O
e O
−γp O
− O
1 O
( O
13 O

) O

where O
α O
{ O
LP O
, O
NL O
} O
are O
loss O
weightings O
for O
auxiliary O
objectives O
and O
λ O
is O
the O
reversed O
gradient O
scheduling O
parameter O
from O
Ganin O
et O
al O
. O
( O
2016 O
) O
. O
The O
λ O
value O
increments O
with O
training O
progress O
p O
, O
scaled O
by O
γ O
, O
according O
to O
Equation O
( O
13 O
) O
, O
to O
limit O
the O
impact O
of O
noisy O
predictions O
during O
early O
training O
. O

We O
expect O
that O
the O
parser O
will O
adapt O
and O
recognize O
an O
encoding O
from O
an O
unfamiliar O
language O
through O
our O
joint O
training O
process O
, O
and O
successfully O
connect O
new O
language O
representations O
to O
the O
logical O
- O
form O
decoder O
at O
test O
time O
. O
This O
sequenceto B-MethodName
- I-MethodName
sequence I-MethodName
approach O
is O
highly O
flexible O
and O
may O
be O
useful O
for O
zero O
- O
shot O
approaches O
to O
additional O
generation B-TaskName
tasks O
( O
e.g. O
, O
paraphrasing O
) O
. O

Experimental O
Setup O

Semantic B-TaskName
Parsing I-TaskName
Datasets O
Our O
experiments O
examine O
whether O
our O
zero O
- O
shot O
approach O
generalizes O
across O
languages O
and O
domains O
. O
We O
evaluate O
performance O
on O
a O
new O
version O
of O
the O
ATIS B-DatasetName
dataset O
of O
travel O
queries O
( O
Hemphill O
et O
al O
. O
, O
1990 O
; O
Dahl O
et O
al O
. O
, O
1994 O
) O
. O
We O
align O
existing O
English O
utterances O
and O
SQL O
logical O
forms O
from O
Iyer O
et O
al O
. O
( O
2017 O
) O
to O
the O
multi O
- O
lingual O
utterances O
from O
the O
MultiATIS++ B-DatasetName
dataset O
for O
spoken O
language O
understanding O
. O
This O
alignment O
adds O
executable O
SQL O
queries O
to O
utterances O
in O
Chinese O
( O
ZH O
) O
, O
German O
( O
DE O
) O
, O
French O
( O
FR O
) O
, O
Spanish O
( O
ES O
) O
, O
and O
Portuguese O
( O
PT O
) O
. O
We O
use O
the O
same O
4,473 B-HyperparameterValue
/ I-HyperparameterValue
493 I-HyperparameterValue
/ I-HyperparameterValue
448 I-HyperparameterValue
dataset O
split O
for O
training B-HyperparameterName
/ I-HyperparameterName
validation I-HyperparameterName
/ I-HyperparameterName
test I-HyperparameterName
as O
Kwiatkowski O
et O
al O
. O
( O
2011 O
) O
. O
We O
also O
add O
to O
the O
test O
set O
Hindi O
( O
HI O
) O
and O
Turkish O
( O
TR O
) O
utterances O
from O
Upadhyay O
et O
al O
. O
( O
2018 O
) O
. O
3 O
We O
can O
now O
predict O
SQL O
from O
the O
ATIS B-DatasetName
test O
questions O
in O
eight O
natural O
languages O
. O
The O
Multi B-DatasetName
- I-DatasetName
ATIS++ I-DatasetName
Japanese O
set O
was O
excluded O
as O
the O
utterance O
alignment O
to O
this O
language O
was O
not O
recoverable O
. O

We O
also O
examine O
Overnight B-DatasetName
( O
Wang O
et O
al O
. O
, O
2015 O
) O
, O
an O
eight O
- O
domain O
dataset O
covering O
Basketball O
, O
Blocks O
, O
Calendar O
, O
Housing O
, O
Publications O
, O
Recipes O
, O
Restaurants O
, O
and O
Social O
Network O
domains O
. O
Overnight B-DatasetName
comprises O
13,682 O
English O
utterances O
paired O
with O
λ−DCS O
logical O
forms O
, O
executable O
in O
SEMPRE O
( O
Berant O
et O
al O
. O
, O
2013 O
) O
We O
measure O
performance O
with O
denotation O
accuracy O
as O
all O
inferred O
logical O
forms O
are O
executable O
in O
some O
knowledge O
base O
. O
This O
metric O
compares O
the O
retrieved O
denotation O
from O
the O
prediction O
, O
ŷ O
, O
to O
that O
from O
executing O
the O
gold O
- O
standard O
logical O
form O
. O
Dataset O
sizes O
are O
outlined O
in O
Appendix O
A O
. O

Natural O
Language O
Data O
For O
the O
reconstruction B-TaskName
objective O
, O
we O
used O
the O
MKQA B-DatasetName
corpus O
( O
Longpre O
et O
al O
. O
, O
2020 O
) O
, O
a O
multi O
- O
lingual O
translation O
of O
10,000 O
samples O
from O
NaturalQuestions O
( O
Kwiatkowski O
et O
al O
. O
, O
2019 O
) O
. O
This O
is O
suitable O
for O
our O
auxiliary O
objective O
as O
the O
utterances O
are O
native O
- O
speaker O
question O
surface O
forms O
, O
matching O
our O
test O
set O
while O
varying O
in O
subject O
. O
MKQA B-DatasetName
is O
also O
balanced O
across O
new O
languages O
to O
limit O
overexposure O
bias O
to O
one O
new O
language O
. O
For O
bi O
- O
text O
, O
we O
use O
the O
original O
English O
and O
the O
professionally O
translated O
question O
as O
a O
pair O
. O

We O
also O
report O
experiments O
using O
a O
sample O
of O
crawled O
data O
from O
ParaCrawl B-DatasetName
7.1 O
( O
Bañón O
et O
al O
. O
, O
2020 O
) O
. O
The O
sample O
comprises O
10,000 O
web O
scraped O
sentences O
paired O
with O
equivalent O
English O
to O
form O
bitext O
. O
Note O
that O
these O
samples O
are O
mostly O
declarative O
sentences O
and O
as O
such O
do O
not O
match O
the O
surface O
form O
of O
our O
test O
inputs O
( O
i.e. O
, O
questions O
) O
and O
are O
also O
not O
parallel O
between O
sampled O
languages O
. O
We O
contrast O
this O
to O
MKQA B-DatasetName
to O
examine O
how O
the O
style O
of O
natural O
language O
data O
influences O
performance O
. O

For O
ATIS B-DatasetName
experiments O
, O
we O
use O
60,000 O
utterances O
from O
each O
source O
in O
languages O
with O
training O
data O
( O
EN O
, O
FR O
, O
PT O
, O
ES O
, O
DE O
, O
ZH O
) O
. O
For O
Overnight O
, O
we O
use O
3 O
Misalignment O
between O
ATIS B-DatasetName
versions O
result O
in O
the O
test O
sets O
containing O
442 O
and O
381 O
utterances O
for O
HI O
and O
TR O
respectively O
. O

30,000 O
utterances O
in O
EN O
, O
DE O
, O
and O
ZH O
. O

Model O
Configuration O

The O
implementation O
of O
ZX B-MethodName
- I-MethodName
PARSE I-MethodName
( O
see O
Section O
4 O
) O
largely O
follows O
parameter O
settings O
from O
for O
Transformer O
encoder O
and O
decoder O
layers O
( O
see O
Appendix O
A O
for O
details O
on O
model O
configuration O
) O
. O
ZX B-MethodName
- I-MethodName
PARSE I-MethodName
requires O
an O
encoder O
model O
to O
generate O
multi O
- O
lingual O
latent O
representations O
for O
all O
objectives O
. O
Our O
main O
results O
use O
only O
the O
encoder O
component O
of O
mBART50 B-MethodName
( O
Tang O
et O
al O
. O
, O
2020 O
) O
and O
we O
present O
experiments O
using O
other O
pre O
- O
trained O
models O
in O
Appendix O
B. O
We O
use O
all O
pre O
- O
trained O
encoder O
layers O
and O
append O
one O
additional O
learnable O
layer O
. O
All O
decoders O
are O
randomly O
initialized O
six O
- O
layer O
stacks O
. O
Early O
experiments O
found O
this O
approach O
superior O
to O
any O
pretrained O
decoder O
initialization O
. O

The O
language O
predictor O
follows O
from O
Ahmad O
et O
al O
. O
( O
2019b O
) O
as O
a O
single O
linear O
classification O
layer O
mapping O
from O
1,024 O
inputs O
to O
L O
output O
languages O
. O
Earlier O
findings O
supported O
that O
if O
the O
LP B-TaskName
network O
is O
larger O
, O
then O
the O
reversed O
gradient O
signal O
is O
too O
strong O
and O
therefore O
less O
useful O
as O
the O
LP B-TaskName
network O
can O
memorize O
the O
language O
. O

Comparison O
Models O
We O
primarily O
compare O
to O
a O
" O
Translate B-MethodName
- I-MethodName
Test I-MethodName
" O
back O
- O
translation O
baseline O
wherein O
the O
new O
language O
test O
set O
is O
translated O
to O
English O
using O
Google O
Translate O
( O
Wu O
et O
al O
. O
, O
2016 O
) O
and O
input O
to O
a O
reference O
sequence O
- O
to O
- O
sequence O
model O
trained O
on O
English O
. O
We O
also O
compare O
to O
" O
Translate B-MethodName
- I-MethodName
Train I-MethodName
" O
, O
where O
we O
use O
MT B-TaskName
from O
English O
to O
generate O
a O
proxy O
dataset O
in O
each O
new O
language O
( O
e.g. O
, O
French O
, O
Portuguese O
, O
Spanish O
, O
German O
, O
Chinese O
, O
Hindi O
and O
Turkish O
) O
to O
train O
a O
monolingual O
parser O
. O
We O
consider O
improving O
upon O
these O
" O
minimum O
effort O
" O
baselines O
as O
a O
lower O
bound O
for O
justifying O
our O
approach O
. O

Additionally O
, O
we O
compare O
to O
an O
upper O
- O
bound O
monolingual O
model O
trained O
on O
professional O
translations O
of O
the O
new O
languages O
. O
We O
report O
results O
on O
MultiATIS++ B-DatasetName
for O
FR O
, O
PT O
, O
ES O
, O
DE O
, O
and O
ZH O
( O
professional O
translations O
are O
not O
available O
for O
Overnight O
training O
data O
) O
. O
This O
is O
the O
" O
maximum O
effort O
" O
strategy O
that O
we O
desire O
to O
avoid O
. O
Parameters O
for O
these O
reference O
systems O
match O
those O
outlined O
above O
e.g. O
, O
mBART50 O
encoder O
to O
logical O
form O
decoder O
. O

Results O

Our O
results O
are O
outlined O
to O
answer O
four O
core O
questions O
, O
with O
additional O
ablations O
in O
Appendix O
B. O
Our O
findings O
support O
the O
hypothesis O
that O
we O
can O
minimize O
the O
cross O
- O
lingual O
transfer O
penalty O
by O
im- O
( O
Dahl O
et O
al O
. O
, O
1994 O
) O
and O
Overnight B-DatasetName
( O
eight O
- O
domain O
average O
; O
Wang O
et O
al O
. O
, O
2015 O
) O
for O
supervised O
monolingual O
upper O
- O
bound O
, O
Translate B-MethodName
- I-MethodName
Test I-MethodName
, O
and O
our O
best O
ZX B-MethodName
- I-MethodName
PARSE I-MethodName
model O
. O
Results O
for O
English O
( O
EN O
) O
, O
French O
( O
FR O
) O
, O
Portuguese O
( O
PT O
) O
, O
Spanish O
( O
ES O
) O
, O
German O
( O
DE O
) O
, O
Chinese O
( O
ZH O
) O
, O
Hindi O
( O
HI O
) O
and O
Turkish O
( O
TR O
) O
ranked O
by O
similarity O
to O
English O
( O
Ahmad O
et O
al O
. O
, O
2019a O
) O
. O
Best O
results O
compared O
to O
baselines O
are O
bolded O
. O
proving O
latent O
alignment O
with O
auxiliary O
objectives O
. O
We O
also O
examine O
the O
latent O
space O
directly O
and O
find O
ZX B-MethodName
- I-MethodName
PARSE I-MethodName
learns O
more O
similar O
representations O
between O
languages O
. O
Our O
parser O
achieves O
state O
- O
of O
- O
theart O
zero O
- O
shot O
results O
for O
all O
non O
- O
English O
languages O
in O
the O
MultiATIS++ B-DatasetName
and O
Overnight B-DatasetName
benchmarks O
. O

Better O
than O
Translation B-TaskName
? O
We O
compare O
between O
ZX B-MethodName
- I-MethodName
PARSE I-MethodName
and O
the O
upper O
- O
and O
lower O
- O
bounds O
in O
Table O
1 O
. O
Our O
multi O
- O
task O
approach O
significantly O
improves O
upon O
" O
Translate B-MethodName
- I-MethodName
Test I-MethodName
" O
for O
all O
languages O
included O
within O
the O
auxiliary O
objectives O
( O
p O
< O
0.01 O
) O
. O
For O
ATIS B-DatasetName
, O
we O
find O
that O
" O
Translate B-MethodName
- I-MethodName
Train I-MethodName
" O
performs O
below O
" O
Translate B-MethodName
- I-MethodName
Test I-MethodName
" O
for O
languages O
similar O
to O
English O
( O
FR O
, O
ES O
, O
PT O
) O
but O
worse O
for O
more O
distant O
languages O
( O
DE O
, O
ZH O
) O
. O
ZX B-MethodName
- I-MethodName
PARSE I-MethodName
performance O
improves O
on O
" O
Translate B-MethodName
- I-MethodName
Train I-MethodName
" O
for O
all O
languages O
included O
in O
reconstruction O
( O
EN O
, O
FR O
, O
PT O
, O
ES O
, O
DE O
, O
ZH O
) O
, O
however O
, O
the O
general O
cross O
- O
lingual O
improvement O
insufficiently O
extends O
to O
additional O
languages O
( O
HI O
, O
TR O
) O
to O
perform O
above O
baselines O
. O

Within O
ZX B-MethodName
- I-MethodName
PARSE I-MethodName
, O
French O
and O
German O
demonstrate O
the O
strongest O
zero O
- O
shot O
accuracy B-MetricName
-+2.4 B-DatasetName
% I-DatasetName
and O
+2.7 B-MetricValue
% I-MetricValue
above O
the O
monolingual O
upper O
bound O
for O
ATIS B-DatasetName
. O
We O
do O
not O
observe O
similar O
improvement O
for O
Portuguese O
or O
Spanish O
despite O
their O
similarity O
to O
English O
. O
This O
may O
be O
a O
result O
of O
German O
and O
French O
dominating O
the O
pre O
- O
training O
corpora O
com O
- O
pared O
to O
other O
new O
languages O
. O
( O
Tang O
et O
al O
. O
, O
2020 O
, O
their O
Table O
6 O
) O
. O

Our O
model O
demonstrates O
similar O
significant O
improvement O
for O
Overnight B-DatasetName
( O
p O
< O
0.01 O
) O
, O
however O
, O
we O
find O
lesser O
gain O
compared O
to O
ATIS B-DatasetName
. O
This O
may O
be O
a O
consequence O
of O
the O
compounded O
challenge O
of O
evaluating O
eight O
varied O
domains O
of O
complex O
linguistic O
constructs O
. O
Here O
, O
we O
find O
that O
" O
Translate B-MethodName
- I-MethodName
Train I-MethodName
" O
is O
a O
stronger O
approach O
than O
" O
Translate B-MethodName
- I-MethodName
Test I-MethodName
" O
, O
which O
may O
be O
a O
consequence O
of O
machine B-TaskName
- I-TaskName
translation I-TaskName
direction O
. O
Our O
best O
approach O
on O
German O
still O
improves O
above O
" O
Translate B-MethodName
- I-MethodName
Train I-MethodName
" O
( O
+4.0 B-MetricValue
% I-MetricValue
) O
, O
however O
, O
we O
find O
performance O
on O
Chinese O
to O
be O
only O
marginally O
improved O
by O
comparison O
( O
+0.6 B-MetricValue
% I-MetricValue
) O
. O
We O
also O
observe O
some O
contrast O
in O
ZX B-MethodName
- I-MethodName
PARSE I-MethodName
performance O
related O
to O
orthographic O
similarity O
to O
English O
. O
Parsing O
accuracy B-MetricName
on O
Overnight B-DatasetName
in O
German O
is O
+6.2 B-MetricValue
% I-MetricValue
above O
Chinese O
, O
with O
a O
similar O
+9.1 B-MetricValue
% I-MetricValue
gap O
between O
these O
same O
languages O
for O
ATIS B-DatasetName
. O

Which O
Objective O
Matters O
? O
Ablations O
to O
the O
model O
are O
shown O
in O
Table O
2 O
, O
identifying O
the O
contributions O
of O
different O
objectives O
. O
Model O
( O
a O
) O
shows O
that O
without O
auxiliary O
objectives O
, O
performance O
in O
new O
languages O
is O
generally O
below O
Translate B-MethodName
- I-MethodName
Test I-MethodName
. O
This O
is O
unsurprising O
, O
as O
this O
approach O
uses O
only O
pre O
- O
trained O
cross O
- O
lingual O
information O
without O
additional O
effort O
to O
improve O
similarity O
. O
Such O
efforts O
are O
incorporated O
in O
Model O
( O
b O
) O
using O
the O
additional O
reconstruction O
decoder O
. O
Even O
without O
the O
LP B-TaskName
loss O
, O
domain O
targeted O
adaptation O
( O
with O
translation O
) O
improves O
cross B-TaskName
- I-TaskName
lingual I-TaskName
parsing I-TaskName
by O
an O
average O
across O
new O
languages O
of O
+9.3 B-MetricValue
% I-MetricValue
for O
ATIS B-DatasetName
and O
+2.9 B-MetricValue
% I-MetricValue
for O
Overnight B-DatasetName
. O
Notably O
, O
we O
identified O
an O
optimal O
ratio O
of O
translation O
to O
reconstruction O
of O
50 O
% O
( O
i.e. O
, O
τ O
= O
0.5 O
) O
. O
This O
suggests O
that O
both O
monolingual O
utterances O
( O
for O
domain O
- O
adaptive O
tuning O
) O
and O
bi O
- O
text O
( O
for O
translation B-TaskName
) O
contribute O
to O
the O
utility O
of O
our O
method O
beyond O
reliance O
on O
one O
technique O
. O

Evaluating O
the O
LP B-DatasetName
objective O
within O
Model O
( O
c O
) O
and O
( O
d O
) O
, O
we O
find O
the O
reversed O
gradient O
successfully O
reduces O
language O
discriminability O
. O
For O
Model O
( O
d O
) O
, O
language O
prediction O
accuracy B-MetricName
during O
training O
peaks O
at O
93 B-MetricValue
% I-MetricValue
after O
2 B-MetricValue
% I-MetricValue
progress O
and O
subsequently O
decreases O
to O
< O
8 B-MetricValue
% I-MetricValue
beyond O
10 B-MetricValue
% I-MetricValue
of O
training O
. O
Language B-TaskName
prediction I-TaskName
accuracy B-MetricName
for O
the O
test O
set O
is O
7.2 B-MetricValue
% I-MetricValue
. O
We O
observe O
a O
similar O
trend O
for O
Model O
( O
c O
) O
. O
Comparing O
individual O
objectives O
, O
we O
find O
the O
addition O
of O
the O
language O
predictor O
alone O
less O
helpful O
than O
the O
reconstruction O
decoder O
. O
Comparing O
Model O
( O
a O
) O
and O
( O
c O
) O
, O
we O
observe O
a O
smaller O
average O
improvement O
on O
new O
languages O
of O
+4.3 B-MetricValue
% I-MetricValue
for O
ATIS B-DatasetName
and O
+1.8 B-MetricValue
% I-MetricValue
for O
Overnight B-DatasetName
. O
This O
suggests O
adaptation O
towards O
specific O
surface O
form O
patterns O
can O
be O
more O
effective O
here O
than O
modeling O
languages O
as O
discrete O
labels O
. O

Considering O
the O
combination O
of O
objectives O
in O
Model O
( O
d O
) O
, O
we O
identify O
cumulative O
benefit O
to O
parsing O
with O
both O
objectives O
. O
Compared O
to O
Model O
( O
a O
) O
, O
the O
full O
model O
improves O
by O
an O
average O
of O
+16.3 B-MetricValue
% I-MetricValue
for O
ATIS B-DatasetName
and O
+9.9 B-MetricValue
% I-MetricValue
for O
Overnight B-DatasetName
across O
new O
languages O
. O
Our O
findings O
support O
our O
claim O
that O
latent O
cross O
- O
lingual O
similarity O
can O
be O
improved O
using O
auxiliary O
objectives O
and O
we O
specifically O
identify O
that O
a O
combination O
of O
approaches O
yields O
superior O
parsing O
. O
We O
suggest O
that O
this O
combination O
benefits O
from O
constructive O
interference O
, O
as O
the O
language B-TaskName
prediction I-TaskName
loss O
promotes O
invariance O
in O
tandem O
with O
multilingual B-TaskName
generation I-TaskName
tasks O
adapting O
the O
encoder O
to O
improve O
modeling O
the O
surface O
form O
( O
e.g. O
, O
questions O
from O
native O
speakers O
) O
of O
the O
new O
language O
test O
data O
. O

Additional O
objectives O
also O
improve O
parsing B-TaskName
for O
Hindi O
and O
Turkish O
despite O
neither O
being O
included O
within O
auxiliary O
training O
data O
( O
see O
HI O
and O
TR O
columns O
in O
Table O
3 O
) O
. O
By O
adapting O
our O
latent O
representation O
to O
encourage O
similarity O
, O
we O
improve O
parsing O
accuracy B-MetricName
for O
two O
typologically O
diverse O
languages O
without O
explicit O
guidance O
. O
To O
further O
examine O
this O
, O
we O
visualize O
the O
MultiATIS++ B-DatasetName
test O
set O
in O
Figure O
3 O
from O
ZX B-MethodName
- I-MethodName
PARSE I-MethodName
compared O
to O
mBART50 B-MethodName
. O
Quantitatively O
, O
we O
find O
the O
average O
cosine B-MetricName
distance I-MetricName
between O
the O
sentence O
- O
mean O
of O
parallel O
utterances O
reduces O
from O
0.58 B-MetricValue
to O
0.47 B-MetricValue
. O
Similarly O
, O
the O
average O
tokenlevel O
symmetric O
Hausdorff B-MetricName
distance I-MetricName
( O
Taha O
and O
Hanbury O
, O
2015 O
) O
between O
languages O
reduces O
from O
0.72 B-MetricValue
to O
0.41 B-MetricValue
. O
This O
further O
supports O
that O
we O
learn O
more O
similar O
representations O
and O
our O
method O
has O
wider O
utility O
beyond O
explicitly O
targeted O
languages O
. O

Does O
Language O
Style O
Matter O
? O

In O
Table O
3 O
we O
examine O
whether O
our O
auxiliary O
objectives O
are O
influenced O
by O
the O
style O
of O
natural O
language O
corpora O
for O
reconstruction O
. O
We O
find O
the O
use O
of O
questions O
positively O
improves O
performance O
compared O
to O
crawled O
sentences O
. O
Using O
questions O
either O
as O
monolingual O
utterances O
( O
i.e. O
, O
no O
translation O
in O
D O
NL O
) O
or O
with O
as O
a O
bi O
- O
text O
sample O
( O
i.e. O
, O
reconstruction O
and O
translation O
in O
D O
NL O
) O
improves O
above O
the O
Translate B-MethodName
- I-MethodName
Test I-MethodName
baseline O
. O
We O
observe O
modest O
improvements O
with O
ParaCrawl B-DatasetName
, O
especially O
when O
introducing O
bitext O
into O
D O
NL O
, O
but O
this O
is O
less O
consistent O
across O
languages O
. O
Overall O
, O
our O
results O
suggest O
that O
ZX B-MethodName
- I-MethodName
PARSE I-MethodName
is O
robust O
even O
when O
question O
- O
style O
data O
is O
unavailable O
but O
can O
be O
particularly O
effective O
when O
adapting O
towards O
both O
new O
languages O
and O
domains O
. O
We O
also O
examined O
the O
influence O
of O
language O
family O
on O
performance O
( O
see O
Appendix O
B O
) O
and O
found O
that O
best O
performance O
utilizes O
a O
linguistically O
varied O
ensemble O
of O
languages O
. O
model O
and O
simplest O
approach O
( O
Model O
( O
a O
) O
in O
Table O
2 O
) O
, O
we O
find O
more O
well O
- O
formed O
logical O
forms O
account O
for O
the O
largest O
improvement O
( O
32.5 B-MetricValue
% I-MetricValue
fewer O
ill O
- O
formed O
SQL O
queries O
for O
ATIS B-DatasetName
and O
35.2 B-MetricValue
% I-MetricValue
fewer O
ill O
- O
formed O
λ O
- O
DCS O
queries O
for O
Overnight B-DatasetName
) O
. O
This O
supports O
our O
notion O
in O
Figure O
1 O
that O
better O
latent O
alignment O
can O
minimize O
cross O
- O
lingual O
penalty O
. O
However O
, O
improved O
structure O
prediction O
is O
insufficient O
to O
solve O
this O
task O
on O
its O
own O
; O
58.7 B-MetricValue
% I-MetricValue
of O
remaining O
errors B-MetricName
in O
the O
best O
model O
are O
due O
to O
mishandled O
entities O
with O
the O
highest O
entity O
errors O
for O
Chinese O
( O
60.2 B-MetricValue
% I-MetricValue
) O
and O
lowest O
for O
French O
( O
36.7 B-MetricValue
% I-MetricValue
) O
. O
This O
suggests O
that O
aligning O
entities O
across O
languages O
might O
be O
necessary O
for O
further O
improvement O
. O

Conclusion O

We O
presented O
a O
multi O
- O
task O
model O
for O
zero O
- O
shot O
cross B-TaskName
- I-TaskName
lingual I-TaskName
semantic I-TaskName
parsing I-TaskName
which O
combines O
logical O
form O
generation O
with O
auxiliary O
objectives O
that O
require O
only O
modest O
natural O
language O
corpora O
for O
localization O
. O
Through O
aligning O
latent O
representations O
, O
ZX B-MethodName
- I-MethodName
PARSE I-MethodName
minimizes O
the O
error O
from O
cross B-TaskName
- I-TaskName
lingual I-TaskName
transfer I-TaskName
and O
improves O
accuracy B-MetricName
across O
languages O
unseen O
during O
training O
. O

Although O
we O
focused O
exclusively O
on O
executable O
semantic B-TaskName
parsing I-TaskName
, O
our O
approach O
is O
general O
and O
potentially O
relevant O
for O
linguistically O
motivated O
frameworks O
such O
as O
Abstract O
Meaning O
Representation O
( O
Banarescu O
et O
al O
. O
, O
2013 O
; O
Damonte O
and O
Cohen O
, O
2018 O
) O
or O
Discourse O
Representation O
Theory O
( O
Kamp O
and O
Reyle O
, O
1993 O
; O
Evang O
and O
Bos O
, O
2016 O
) O
. O
In O
the O
future O
, O
we O
will O
investigate O
a O
few O
- O
shot O
scenario O
and O
study O
sample O
efficient O
cross O
- O
lingual O
transfer O
by O
explicitly O
promoting O
generalization O
using O
techniques O
such O
as O
meta O
- O
learning O
( O
Finn O
et O
al O
. O
, O
2017 O
) O
. O

Ethics O
Statement O

A O
key O
limitation O
of O
our O
work O
is O
the O
limited O
coverage O
of O
eight O
higher O
- O
resource O
languages O
. O
As O
such O
, O
we O
are O
unable O
to O
test O
our O
approach O
in O
a O
genuinely O
lowresource O
scenario O
. O
We O
must O
also O
consider O
the O
risk O
of O
over O
- O
generalization O
to O
dominant O
dialects O
within O
each O
language O
as O
we O
lack O
an O
evaluation O
of O
additional O
dialects O
( O
e.g. O
our O
English O
dataset O
is O
representative O
of O
American O
English O
but O
not O
Indian O
English O
) O
. O
We O
hope O
that O
such O
issues O
can O
be O
addressed O
with O
additional O
data O
collection O
. O

Our O
training O
requirements O
are O
detailed O
in O
Appendix O
A. O
We O
hope O
our O
work O
contributes O
to O
further O
usage O
and O
development O
of O
singular O
multilingual O
models O
as O
opposed O
to O
learning O
N O
monolingual O
models O
for O
N O
languages O
. O
( O
Dahl O
et O
al O
. O
, O
1994 O
; O
Upadhyay O
et O
al O
. O
, O
2018 O
; O
and O
Overnight O
( O
Wang O
et O
al O
. O
, O
2015 O
; O
Sherborne O
et O
al O
. O
, O
2020 O
) O
. O
Each O
example O
is O
an O
utterance O
paired O
with O
a O
logical O
form O
. O
Table O
5 O
: O
Pretrained O
model O
configurations O
and O
configuration O
for O
the O
trainable O
components O
of O
ZX B-MethodName
- I-MethodName
PARSE I-MethodName
( O
e.g. O
, O
the O
objectives O
) O
. O
All O
models O
use O
a O
hidden B-HyperparameterName
dimension I-HyperparameterName
of O
1,024 B-HyperparameterValue
, O
a O
feed B-HyperparameterName
- I-HyperparameterName
forward I-HyperparameterName
hidden I-HyperparameterName
projection I-HyperparameterName
of O
4,096 B-HyperparameterValue
and O
16 B-HyperparameterName
heads B-HyperparameterValue
per O
multi O
- O
head O
attention O
layer O
. O
For O
natural O
language O
, O
all O
models O
use O
byte O
- O
level O
BPE O
tokenization O
( O
Wang O
et O
al O
. O
, O
2020b O
) O
and O
logical O
forms O
are O
tokenized O
using O
whitespace O
. O

A O
Experimental O
Setup O

Zero O
- O
shot O
Model O
Configuration O
The O
encoder O
, O
E O
, O
decoders O
, O
{ O
D O
LF O
, O
D O
NL O
} O
, O
and O
embedding O
matrices O
all O
use O
a O
dimension B-HyperparameterName
size O
of O
1,024 B-HyperparameterValue
with O
the O
self B-HyperparameterName
- I-HyperparameterName
attention I-HyperparameterName
projection I-HyperparameterName
of O
4,096 B-HyperparameterValue
and O
16 B-HyperparameterValue
heads B-HyperparameterName
per O
layer O
. O
Both O
decoders O
are O
6 O
- O
layer O
stacks O
. O

Weights O
were O
initialized O
by O
sampling O
from O
normal O
distribution O
N O
( O
0 O
, O
0.02 O
) O
. O
The O
language O
prediction O
network O
is O
a O
two O
- O
layer O
feed O
- O
forward O
network O
projecting O
from O
z O
to O
1,024 B-HyperparameterValue
hidden B-HyperparameterName
units I-HyperparameterName
then O
to O
|L| O
for O
L O
languages O
. O
L B-HyperparameterName
is O
six B-HyperparameterValue
for O
experiments O
on O
ATIS B-DatasetName
and O
three B-HyperparameterValue
for O
experiments O
on O
Overnight B-DatasetName
. O

Configurations O
for O
models O
used O
in O
this O
work O
are O
reported O
in O
Table O
5 O
with O
similar O
details O
for O
the O
objective O
components O
of O
ZX B-MethodName
- I-MethodName
PARSE I-MethodName
. O
Initial O
experiments O
examined O
XLM O
- O
R O
- O
base O
, O
which O
is O
12 O
layers O
opposed O
to O
24 O
, O
however O
, O
performance O
was O
significantly O
worse O
and O
, O
therefore O
, O
this O
model O
was O
not O
considered O
further O
. O
Experiments O
reported O
in O
Section O
6 O
all O
use O
mBART50 B-MethodName
as O
the O
pre O
- O
trained O
encoder O
as O
all O
other O
pre O
- O
trained O
models O
performed O
significantly O
worse O
( O
see O
Appendix O
B O
) O
. O
In O
all O
our O
experiments O
, O
we O
found O
that O
a O
randomly O
initialized O
decoder O
was O
superior O
to O
using O
pre O
- O
trained O
weights O
. O

A O
complete O
outline O
of O
dataset O
partitions O
per O
language O
is O
shown O
in O
Table O
4 O
for O
both O
datasets O
. O
ZX B-MethodName
- I-MethodName
PARSE I-MethodName
uses O
only O
English O
training O
and O
validation O
data O
and O
tests O
on O
all O
additional O
languages O
. O
We O
did O
not O
use O
multi O
- O
lingual O
validation O
data O
as O
recommended O
in O
Keung O
et O
al O
. O
( O
2020 O
) O
as O
this O
approach O
did O
not O
prove O
critically O
beneficial O
in O
early O
experiments O
and O
doing O
so O
would O
explode O
the O
data O
requirements O
for O
a O
multi O
- O
lingual O
system O
. O

Experimental O
Setting O

The O
system O
was O
trained O
using O
the O
Adam O
optimizer O
( O
Kingma O
and O
Ba O
, O
2015 O
) O
with O
a O
learning B-HyperparameterName
rate I-HyperparameterName
of O
1 B-HyperparameterValue
× I-HyperparameterValue
10 I-HyperparameterValue
−4 I-HyperparameterValue
, O
and O
a O
weight B-HyperparameterName
decay I-HyperparameterName
factor O
of O
0.1 B-HyperparameterValue
. O
We O
use O
a O
" O
Noam O
" O
schedule O
for O
the O
learning O
rate O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
with O
a O
warmup B-HyperparameterName
of O
5,000 B-HyperparameterValue
steps I-HyperparameterValue
. O
For O
pre O
- O
trained O
components O
, O
we O
fine O
- O
tune O
XLM B-MethodName
- I-MethodName
R I-MethodName
and O
mBART B-MethodName
encoders O
with O
learning B-HyperparameterName
rate I-HyperparameterName
1 B-HyperparameterValue
× I-HyperparameterValue
10 I-HyperparameterValue
−5 I-HyperparameterValue
but O
freeze O
the O
encoder O
when O
using O
mBART50 B-MethodName
. O
Loss B-HyperparameterName
weighting I-HyperparameterName
values O
for O
α O
{ O
LP O
, O
NL O
} O
were O
empirically O
optimized O
to O
{ O
0.33 B-HyperparameterValue
, O
0.1 B-HyperparameterValue
} O
respectively O
from O
a O
range O
{ O
0.5 O
, O
0.33 O
, O
0.1 O
, O
0.05 O
, O
0.01 O
, O
0.005 O
, O
0.001 O
} O
. O
Batches B-HyperparameterName
during O
training O
were O
size O
50 B-HyperparameterValue
and O
homogeneously O
sampled O
from O
either O
S O
LF O
or O
S O
NL O
, O
with O
an O
epoch O
consuming O
one O
pass O
over O
both O
. O
Models O
were O
trained O
for O
a O
maximum O
of O
100 B-HyperparameterValue
epochs B-HyperparameterName
with O
early O
stopping O
. O
Model O
selection O
and O
hyper O
- O
parameters O
were O
tuned O
on O
the O
S O
LF O
validation O
set O
in O
English O
e.g. O
, O
validation O
only O
evaluates O
performance O
on O
logical O
- O
form O
generation O
and O
not O
additional O
objectives O
. O
Test O
predictions O
were O
generated O
using O
beam O
search O
with O
5 O
hypotheses O
. O

For O
the O
reconstruction O
noising O
function O
, O
we O
use O
token O
masking O
to O
randomly O
replace O
u O
tokens O
in O
x O
with O
" O
< O
mask O
> O
" O
where O
u O
is O
sampled O
from O
U O
( O
0 O
, O
v O
) O
. O
We O
found O
v B-HyperparameterName
= O
3 B-HyperparameterValue
as O
the O
empirically O
optimal O
maximum O
tokens O
to O
mask O
in O
an O
input O
. O
Similarly O
, O
we O
found O
γ B-HyperparameterName
= O
40 B-HyperparameterValue
optimal O
for O
the O
language O
prediction O
loss O
and O
τ B-HyperparameterName
= O
0.5 B-HyperparameterValue
as O
the O
optimal O
sampling O
factor O
for O
translation O
versus O
reconstruction O
. O
This O
value O
of O
τ B-HyperparameterName
corresponds O
to O
using O
half O
the O
reconstruction O
data O
as O
mono O
- O
lingual O
utterances O
and O
half O
as O
bi O
- O
text O
paired O
with O
English O
. O

Reproducibility O
All O
models O
were O
implemented O
using O
AllenNLP B-MethodName
( O
Gardner O
et O
al O
. O
, O
2018 O
) O
and O
Py B-MethodName
- I-MethodName
Torch I-MethodName
( O
Paszke O
et O
al O
. O
, O
2019 O
) O
, O
using O
pre O
- O
trained O
models O
from O
HuggingFace B-MethodName
( O
Wolf O
et O
al O
. O
, O
2019 O
) O
. O
Each O
model O
is O
trained O
on O
1 O
NVIDIA O
RTX3090 O
GPU O
in O
a O
cluster O
configuration O
, O
with O
no O
model O
requiring O
over O
24 O
hours O
to O
complete O
training O
. O
Hyperparameters O
were O
chosen O
by O
training O
a O
reference O
model O
for O
parsing O
English O
utterances O
and O
selecting O
the O
system O
with O
minimum O
validation O
loss O
. O
Our O
optimization O
grid O
- O
search O
explored O
: O
{ O
6 B-HyperparameterValue
, O
9 B-HyperparameterValue
, O
12 B-HyperparameterValue
} O
decoder B-HyperparameterName
layers I-HyperparameterName
; O
freezing O
or O
unfreezing O
the O
pre O
- O
trained O
encoder O
; O
{ O
0 B-HyperparameterValue
, O
1 B-HyperparameterValue
, O
2 B-HyperparameterValue
} O
additional B-HyperparameterName
encoder I-HyperparameterName
layers I-HyperparameterName
ap O
- O
pended O
to O
the O
pre O
- O
trained O
encoder O
; O
learning B-HyperparameterName
rates I-HyperparameterName
of O
1 B-HyperparameterValue
× I-HyperparameterValue
10 I-HyperparameterValue
{ O
−3 B-HyperparameterValue
, O
−4 B-HyperparameterValue
, O
−5 B-HyperparameterValue
} O
and O
a O
weight B-HyperparameterName
decay I-HyperparameterName
factor O
of O
{ O
0 B-HyperparameterValue
, O
0.1 B-HyperparameterValue
, O
0.01 B-HyperparameterValue
} O
. O
Optimal O
parameters O
in O
these O
early O
tests O
were O
carried O
through O
for O
all O
additional O
models O
. O

Additionally O
, O
we O
optimized O
hyper O
- O
parameters O
for O
auxiliary O
objectives O
through O
linear O
search O
with O
all O
other O
factors O
fixed O
. O
The O
upper O
limit O
, O
v B-HyperparameterName
, O
for O
the O
number B-HyperparameterName
of I-HyperparameterName
tokens I-HyperparameterName
to I-HyperparameterName
mask I-HyperparameterName
during O
reconstruction O
, O
U O
( O
0 O
, O
v O
) O
, O
was O
optimized O
from O
integers O
1 B-HyperparameterValue
- O
6 B-HyperparameterValue
. O
The O
MKQA B-DatasetName
dataset O
used O
for O
auxiliary O
tasks O
contains O
shorter O
sentences O
than O
prior O
work O
using O
masking O
, O
such O
as O
, O
and O
we O
observed O
that O
high O
levels O
of O
masking O
ultimately O
destroys O
the O
input O
sentence O
and O
handicaps O
the O
overall O
task O
. O
τ B-HyperparameterName
was O
optimized O
between O
values O
of O
0.0 B-HyperparameterValue
( O
e.g. O
ignore O
bi O
- O
text O
) O
to O
1.0 B-HyperparameterValue
( O
e.g. O
all O
data O
is O
used O
as O
bi O
- O
text O
) O
in O
increments O
of O
0.1 O
. O
Finally O
, O
we O
optimize O
the O
γ B-HyperparameterName
parameter O
within O
Equation O
7 O
between O
{ O
0 B-HyperparameterValue
, O
5 B-HyperparameterValue
, O
10 B-HyperparameterValue
, O
20 B-HyperparameterValue
, O
40 B-HyperparameterValue
, O
50 B-HyperparameterValue
, O
100 B-HyperparameterValue
} O
on O
an O
approximately O
logarithmic O
scale O
. O
The O
optimal O
value O
of O
γ B-HyperparameterName
= O
40 B-HyperparameterValue
results O
in O
loss O
L O
LF O
reaching O
99 O
% O
of O
the O
maximum O
value O
at O
approximately O
13.6 O
% O
of O
training O
progress O
. O

B O
Additional O
Results O

We O
extend O
the O
results O
in O
Section O
6 O
to O
include O
additional O
ablations O
for O
all O
pre O
- O
trained O
models O
. O
Table O
6 O
details O
all O
results O
for O
ATIS B-DatasetName
across O
eight O
test O
languages O
. O
Additionally O
, O
complete O
results O
across O
all O
domains O
in O
Overnight B-DatasetName
are O
reported O
for O
English O
in O
Table O
7 O
, O
German O
in O
Table O
8 O
, O
and O
Chinese O
in O
Table O
9 O
. O
Table O
3 O
, O
comparing O
between O
reconstruction O
data O
sources O
, O
is O
expanded O
on O
for O
Overnight B-DatasetName
in O
Table O
10 O
. O
Finally O
, O
we O
present O
additional O
ablations O
to O
our O
model O
considering O
reconstruction O
language O
families O
in O
Table O
11 O
and O
12 O
. O
Which O
Pre O
- O
trained O
Encoder O
? O
Our O
full O
results O
using O
three O
different O
pre O
- O
trained O
encoders O
are O
outlined O
in O
Tables O
6 O
- O
9 O
. O
Our O
experiments O
identify O
mBART B-MethodName
as O
the O
weakest O
pre O
- O
trained O
model O
, O
reporting O
the O
lowest O
accuracies O
for O
all O
ATIS B-DatasetName
test O
languages O
. O
ZX B-MethodName
- I-MethodName
PARSE I-MethodName
using O
XLM B-MethodName
- I-MethodName
R I-MethodName
generally O
improved O
upon O
mBART B-MethodName
for O
ATIS B-DatasetName
but O
proved O
worse O
for O
Overnight B-DatasetName
. O
As O
XLM B-MethodName
- I-MethodName
R I-MethodName
is O
not O
pre O
- O
trained O
for O
sequence O
- O
to O
- O
sequence O
tasks O
, O
this O
result O
suggests O
this O
model O
could O
be O
poorer O
at O
representing O
input O
content O
in O
more O
complex O
queries O
. O
Despite O
being O
half O
the O
size O
of O
XLM B-MethodName
- I-MethodName
R I-MethodName
, O
mBART50 B-MethodName
is O
the O
only O
pre O
- O
trained O
encoder O
able O
to O
perform O
competitively O
across O
all O
languages O
. O
Despite O
lower O
performance O
with O
different O
pre O
- O
trained O
models O
, O
we O
identify O
that O
introducing O
additional O
objectives O
yields O
improved O
accuracy B-MetricName
in O
most O
cases O
. O
Similar O
to O
our O
results O
using O
mBART50 B-MethodName
, O
we O
find O
that O
combining O
tasks O
is O
the O
optimal O
strategy O
for O
ZX B-MethodName
- I-MethodName
PARSE I-MethodName
using O
either O
XLM B-MethodName
- I-MethodName
R I-MethodName
or O
mBART B-MethodName
as O
an O
encoder O
. O

We O
additionally O
explored O
if O
pre O
- O
training O
is O
required O
for O
our O
approach O
by O
training O
a O
comparable O
model O
from O
scratch O
. O
While O
performance O
on O
English O
was O
similar O
to O
our O
best O
results O
, O
we O
found O
that O
cross O
- O
lingual O
transfer O
was O
extremely O
poor O
and O
these O
results O
are O
omitted O
due O
to O
negligible O
accuracies B-MetricName
( O
< O
2 B-HyperparameterValue
% I-HyperparameterValue
) O
for O
non O
- O
English O
languages O
. O
Overall O
, O
this O
suggests O
that O
our O
methodology O
is O
optimal O
when O
aligning O
an O
existing O
multi O
- O
lingual O
latent O
space O
rather O
than O
inducing O
a O
multi O
- O
lingual O
latent O
space O
from O
scratch O
. O

Ablations O
of O
Reconstruction O
Language O
Data O

We O
present O
ablations O
to O
our O
main O
experiments O
examining O
the O
influence O
of O
language O
similarity O
in O
reconstruction O
data O
for O
ATIS B-DatasetName
in O
Table O
11 O
and O
for O
Overnight B-DatasetName
in O
Table O
12 O
. O
Similar O
to O
our O
results O
for O
Hindi O
and O
Turkish O
in O
Table O
2 O
, O
we O
find O
that O
using O
our O
auxiliary O
objectives O
in O
our O
model O
improves O
overall O
cross O
- O
lingual O
alignment O
in O
languages O
that O
we O
did O
not O
intentionally O
target O
with O
reconstruction O
data O
. O

In O
our O
first O
case O
, O
we O
consider O
omitting O
the O
Romance O
genus O
languages O
( O
French O
, O
Spanish O
, O
Portuguese O
) O
from O
the O
reconstruction O
corpus O
for O
experiments O
on O
ATIS B-DatasetName
. O
The O
observed O
reduction O
in O
performance O
across O
all O
languages O
is O
likely O
a O
consequence O
of O
reduced O
training O
data O
leading O
to O
weaker O
crosslingual O
alignment O
. O
Notably O
, O
this O
drop O
is O
largest O
for O
French O
( O
−11.2 B-MetricValue
% I-MetricValue
) O
and O
Spanish O
( O
−7.1 B-MetricValue
% I-MetricValue
) O
. O
In O
contrast O
, O
the O
smallest O
reduction O
is O
for O
Chinese O
( O
−3.9 B-MetricValue
% I-MetricValue
) O
and O
English O
( O
−2.8 B-MetricValue
% I-MetricValue
) O
. O
We O
additionally O
examine O
the O
effect O
of O
omitting O
the O
only O
Sino O
- O
Tibetan O
language O
( O
Chinese O
) O
from O
experiments O
on O
both O
ATIS O
and O
Overnight O
. O
While O
we O
observe O
a O
similar O
overall O
reduction O
in O
performance O
here O
-our O
notable O
finding O
is O
a O
larger O
reduction O
in O
parsing O
accuracy O
for O
Chinese O
across O
both O
ATIS B-DatasetName
( O
−17.0 B-MetricValue
% I-MetricValue
) O
and O
Overnight B-DatasetName
( O
−11.1 B-MetricValue
% I-MetricValue
) O
. O
Without O
a O
similar O
language O
to O
Chinese O
( O
in O
the O
same O
family O
or O
with O
a O
similar O
orthography O
) O
in O
this O
experiment O
, O
we O
suggest O
there O
is O
little O
to O
" O
support O
" O
better O
cross O
- O
lingual O
alignment O
for O
Chinese O
relative O
to O
others O
. O
This O
contrasts O
with O
the O
performance O
drop O
for O
Romance O
languages O
, O
which O
are O
still O
relatively O
similar O
to O
English O
and O
German O
. O

Overall O
, O
these O
ablations O
support O
that O
both O
variety O
and O
similarity O
are O
important O
for O
considering O
language O
data O
for O
auxiliary O
objectives O
. O
Performance O
on O
omitted O
languages O
can O
improve O
from O
a O
base- O
( O
Wang O
et O
al O
. O
, O
2015 O
) O
( O
Wang O
et O
al O
. O
, O
2015 O
) O
( O
Wang O
et O
al O
. O
, O
2015 O
) O
compared O
across O
reconstruction O
data O
usage O
for O
English O
, O
German O
and O
Chinese O
. O
We O
compare O
between O
MKQA B-DatasetName
( O
Longpre O
et O
al O
. O
, O
2020 O
) O
and O
ParaCrawl B-DatasetName
( O
Bañón O
et O
al O
. O
, O
2020 O
) O
with O
additional O
contrast O
between O
using O
reconstruction O
data O
as O
monolingual O
utterances O
( O
e.g. O
τ B-HyperparameterName
= O
0.0 B-HyperparameterValue
) O
or O
with O
some O
proportion O
as O
bi O
- O
text O
where O
the O
target O
sequence O
is O
replaced O
with O
the O
parallel O
English O
utterance O
( O
e.g. O
τ B-HyperparameterName
= O
0.5 B-HyperparameterValue
) O
. O
Domains O
are O
Basketball O
, O
Blocks O
, O
Calendar O
, O
Housing O
, O
Publications O
, O
Recipes O
, O
Restaurants O
and O
Social O
Network O
. O
Best O
results O
for O
each O
language O
are O
bolded O
. O

Acknowledgements O

We O
thank O
the O
anonymous O
reviewers O
for O
their O
feedback O
and O
Bailin O
Wang O
, O
Kate O
McCurdy O
, O
and O
Rui O
Zhang O
for O
insightful O
discussion O
. O
The O
authors O
gratefully O
acknowledge O
the O
support O
of O
the O
UK O
Engineering O
and O
Physical O
Sciences O
Research O
Council O
( O
grant O
EP O
/ O
L016427 O
/ O
1 O
; O
Sherborne O
) O
and O
the O
European O
Research O
Council O
( O
award O
number O
681760 O
; O
Lapata O
) O
. O

Causal O
Direction O
of O
Data B-TaskName
Collection I-TaskName
Matters O
: O
Implications O
of O
Causal O
and O
Anticausal O
Learning O
for O
NLP O

The O
principle O
of O
independent O
causal O
mechanisms O
( O
ICM O
) O
states O
that O
generative O
processes O
of O
real O
world O
data O
consist O
of O
independent O
modules O
which O
do O
not O
influence O
or O
inform O
each O
other O
. O
While O
this O
idea O
has O
led O
to O
fruitful O
developments O
in O
the O
field O
of O
causal O
inference O
, O
it O
is O
not O
widely O
- O
known O
in O
the O
NLP O
community O
. O
In O
this O
work O
, O
we O
argue O
that O
the O
causal O
direction O
of O
the O
data B-TaskName
collection I-TaskName
process O
bears O
nontrivial O
implications O
that O
can O
explain O
a O
number O
of O
published O
NLP O
findings O
, O
such O
as O
differences O
in O
semi O
- O
supervised O
learning O
( O
SSL O
) O
and O
domain O
adaptation O
( O
DA O
) O
performance O
across O
different O
settings O
. O
We O
categorize O
common O
NLP O
tasks O
according O
to O
their O
causal O
direction O
and O
empirically O
assay O
the O
validity O
of O
the O
ICM O
principle O
for O
text O
data O
using O
minimum B-MetricName
description I-MetricName
length I-MetricName
. O
We O
conduct O
an O
extensive O
meta O
- O
analysis O
of O
over O
100 O
published O
SSL O
and O
30 O
DA O
studies O
, O
and O
find O
that O
the O
results O
are O
consistent O
with O
our O
expectations O
based O
on O
causal O
insights O
. O
This O
work O
presents O
the O
first O
attempt O
to O
analyze O
the O
ICM O
principle O
in O
NLP O
, O
and O
provides O
constructive O
suggestions O
for O
future O
modeling O
choices O
. O
1 O
* O
Equal O
contribution O
. O
1 O
The O
codes O
are O
at O
https O
: O
/ O
/ O
github.com O
/ O
zhijing-jin O
/ O
icm4nlp O
. O

Introduction O

NLP O
practitioners O
typically O
do O
not O
pay O
great O
attention O
to O
the O
causal O
direction O
of O
the O
data B-TaskName
collection I-TaskName
process O
. O
As O
a O
motivating O
example O
, O
consider O
the O
case O
of O
collecting O
a O
dataset O
to O
train O
a O
machine O
translation O
( O
MT O
) O
model O
to O
translate O
from O
English O
( O
En O
) O
to O
Spanish O
( O
Es O
) O
: O
it O
is O
common O
practice O
to O
mix O
all O
available O
En O
- O
Es O
sentence O
pairs O
together O
and O
train O
the O
model O
on O
the O
entire O
pooled O
data O
set O
( O
Bahdanau O
et O
al O
. O
, O
2015 O
; O
Cho O
et O
al O
. O
, O
2014 O
) O
. O
However O
, O
such O
mixed O
corpora O
actually O
consist O
of O
two O
distinct O
types O
of O
data O
: O
( O
i O
) O
sentences O
that O
originated O
in O
English O
and O
have O
been O
translated O
( O
by O
human O
translators O
) O
into O
Spanish O
( O
En→Es O
) O
; O
and O
( O
ii O
) O
sentences O
that O
originated O
in O
Given O
the O
English O
sentence O
above O
, O
can O
you O
write O
its O
Spanish O
translation O
? O

Prompt O
for O
annotators O

[ O
En O
] O
This O
is O
a O
beautiful O
world O
. O

[ O
Es O
] O
Este O
es O
un O
mundo O
hermoso O
. O

Cause O
: O

Effect O
: O

Annotation O
process O
( O
Noise O
) O
Effect O
= O
CausalMechanism O
( O
Cause O
, O
Noise O
) O

Figure O
1 O
: O
Annotation O
process O
for O
NLP O
data O
: O
the O
random O
variable O
that O
exists O
first O
is O
typically O
the O
cause O
( O
e.g. O
, O
a O
given O
prompt O
) O
, O
and O
the O
one O
generated O
afterwards O
is O
typically O
the O
effect O
( O
e.g. O
, O
the O
annotated O
answer O
) O
. O

Spanish O
and O
have O
subsequently O
been O
translated O
into O
English O
( O
Es→En O
) O
. O
2 O
Intuitively O
, O
these O
two O
subsets O
are O
qualitatively O
different O
, O
and O
an O
increasing O
number O
of O
observations O
by O
the O
NLP O
community O
indeed O
suggests O
that O
they O
exhibit O
different O
properties O
( O
Freitag O
et O
al O
. O
, O
2019 O
; O
Edunov O
et O
al O
. O
, O
2020 O
; O
Riley O
et O
al O
. O
, O
2020 O
; O
Shen O
et O
al O
. O
, O
2021 O
) O
. O
In O
the O
case O
of O
MT O
, O
for O
example O
, O
researchers O
find O
that O
training O
models O
on O
each O
of O
these O
two O
types O
of O
data O
separately O
leads O
to O
different O
test O
performance O
, O
as O
well O
as O
different O
performance O
improvement O
by O
semi O
- O
supervised O
learning O
( O
SSL O
) O
( O
Bogoychev O
and O
Sennrich O
, O
2019 O
; O
Graham O
et O
al O
. O
, O
2020 O
; O
Edunov O
et O
al O
. O
, O
2020 O
) O
. O
Motivated O
by O
this O
observation O
that O
the O
data B-TaskName
collection I-TaskName
process O
seems O
to O
matter O
for O
model O
performance O
, O
in O
this O
work O
, O
we O
provide O
an O
explanation O
of O
this O
phenomenon O
from O
the O
perspective O
of O
causality O
( O
Pearl O
, O
2009 O
; O
Peters O
et O
al O
. O
, O
2017 O
) O
. O

First O
, O
we O
introduce O
the O
notion O
of O
the O
causal O
direction O
for O
a O
given O
NLP O
task O
, O
see O
Fig O
. O
1 O
for O
an O
example O
. O
Throughout O
, O
we O
denote O
the O
input O
of O
a O
learning O
task O
by O
X O
and O
the O
output O
which O
is O
to O
be O
predicted O
by O
Y O
. O
If O
, O
during O
the O
data O
collection O
process O
, O
X O
is O
generated O
first O
, O
and O
then O
Y O
is O
collected O
based O
on O
X O
( O
e.g. O
, O
through O
annotation O
) O
, O
we O
say O
that O
X O
causes O
Y O
, O
and O
denote O
this O
by O
X O
→ O
Y O
. O
If O
, O
on O
the O
other O
hand O
, O
Y O
is O
Figure O
2 O
: O
( O
Top O
) O
A O
causal O
graph O
C O
→ O
E O
, O
where O
C O
is O
the O
cause O
and O
E O
is O
the O
effect O
. O
The O
function O
f O
( O
• O
, O
N O
E O
) O
denotes O
the O
causal O
process O
, O
or O
mechanism O
, O
P O
E|C O
by O
which O
the O
effect O
E O
is O
generated O
from O
C O
and O
unobserved O
noise O
N O
E O
. O
( O
Bottom O
) O
Based O
on O
whether O
the O
direction O
of O
prediction O
aligns O
with O
the O
direction O
of O
causation O
or O
not O
, O
we O
distinguish O
two O
types O
of O
tasks O
: O
( O
i O
) O
causal O
learning O
, O
i.e. O
, O
predicting O
the O
effect O
from O
the O
cause O
; O
and O
( O
ii O
) O
anticausal O
learning O
, O
i.e. O
, O
predicting O
the O
cause O
from O
the O
effect O
. O
generated O
first O
, O
and O
then O
X O
is O
collected O
based O
on O
Y O
, O
we O
say O
that O
Y O
causes O
X O
( O
Y O
→ O
X O
) O
. O
3 O
Based O
on O
whether O
the O
direction O
of O
prediction O
aligns O
with O
the O
causal O
direction O
of O
the O
data O
collection O
process O
or O
not O
, O
Schölkopf O
et O
al O
. O
( O
2012 O
) O
categorize O
these O
types O
of O
tasks O
as O
causal O
learning O
( O
X O
→ O
Y O
) O
, O
or O
anticausal O
learning O
( O
Y O
→ O
X O
) O
, O
respectively O
; O
see O
Fig O
. O
2 O
for O
an O
illustration O
. O
In O
the O
context O
of O
our O
motivating O
MT O
example O
this O
means O
that O
, O
if O
the O
goal O
is O
to O
translate O
from O
English O
( O
X O
= O
En O
) O
into O
Spanish O
( O
Y O
= O
Es O
) O
, O
training O
only O
on O
subset O
( O
i O
) O
of O
the O
data O
consisting O
of O
En→Es O
pairs O
corresponds O
to O
causal O
learning O
( O
X O
→ O
Y O
) O
, O
whereas O
training O
only O
on O
subset O
( O
ii O
) O
consisting O
of O
Es→En O
pairs O
is O
categorised O
as O
anticausal O
learning O
( O
Y O
→ O
X O
) O
. O

Based O
on O
the O
principle O
of O
independent O
causal O
mechanisms O
( O
ICM O
) O
Peters O
et O
al O
. O
, O
2017 O
) O
, O
it O
has O
been O
hypothesized O
that O
the O
causal O
direction O
of O
data B-TaskName
collection I-TaskName
( O
i.e. O
, O
whether O
a O
given O
NLP O
learning O
task O
can O
be O
classified O
as O
causal O
or O
anticausal O
) O
has O
implications O
for O
the O
effectiveness O
of O
commonly O
used O
techniques O
such O
as O
SSL O
and O
domain O
adaptation O
( O
DA O
) O
( O
Schölkopf O
et O
al O
. O
, O
2012 O
) O
. O
We O
will O
argue O
that O
this O
can O
explain O
performance O
differences O
reported O
by O
the O
NLP O
community O
across O
different O
data O
collection O
processes O
and O
tasks O
. O
In O
particular O
, O
we O
make O
the O
following O
contributions O
: O

1 O
. O
We O
categorize O
a O
number O
of O
common O
NLP O
tasks O
according O
to O
the O
causal O
direction O
of O
the O
underlying O
data B-TaskName
collection I-TaskName
process O
( O
§ O
2 O
) O
. O
2 O
. O
We O
review O
the O
ICM O
principle O
and O
its O
implications O
for O
common O
techniques O
of O
using O
unlabelled O
data O
such O
as O
SSL O
and O
DA O
in O
the O
context O
of O
causal O
and O
anticausal O
NLP O
tasks O
( O
§ O
3 O
) O
. O
3 O
. O
We O
empirically O
assay O
the O
validity O
of O
ICM O
for O
NLP O
data O
using O
minimum B-MetricName
description I-MetricName
length I-MetricName
in O
a O
machine O
translation O
setting O
( O
§ O
4 O
) O
. O
4 O
. O
We O
verify O
experimentally O
and O
through O
a O
metastudy O
of O
over O
respectively O
100 O
( O
SSL O
) O
and O
30 O
( O
DA O
) O
published O
findings O
that O
the O
difference O
in O
SSL O
( O
§ O
5 O
) O
and O
domain O
adaptation O
( O
DA O
) O
( O
§ O
6 O
) O
performance O
on O
causal O
vs O
anticausal O
datasets O
reported O
in O
the O
literature O
is O
consistent O
with O
what O
is O
predicted O
by O
the O
ICM O
principle O
. O
5 O
. O
We O
make O
suggestions O
on O
how O
to O
use O
findings O
in O
this O
paper O
for O
future O
work O
in O
NLP O
( O
§ O
7 O
) O
. O

Categorization O
of O
Common O
NLP O
Tasks O
into O
Causal O
and O
Anticausal O
Learning O

We O
start O
by O
categorizing O
common O
NLP O
tasks O
which O
use O
an O
input O
variable O
X O
to O
predict O
a O
target O
or O
output O
variable O
Y O
into O
causal O
learning O
( O
X O
→ O
Y O
) O
, O
anticausal O
learning O
( O
Y O
→ O
X O
) O
, O
and O
other O
tasks O
that O
do O
not O
have O
a O
clear O
underlying O
causal O
direction O
, O
or O
which O
typically O
rely O
on O
mixed O
( O
causal O
and O
anticausal O
) O
types O
of O
data O
, O
as O
summarised O
in O
Tab O
. O
1 O
. O
Key O
to O
this O
categorization O
is O
determining O
whether O
the O
input O
X O
corresponds O
to O
the O
cause O
or O
the O
effect O
in O
the O
data B-TaskName
collection I-TaskName
process O
. O
As O
illustrated O
in O
Fig O
. O
1 O
, O
if O
the O
input O
X O
and O
output O
Y O
are O
generated O
at O
two O
different O
time O
steps O
, O
then O
the O
variable O
that O
is O
generated O
first O
is O
typically O
the O
cause O
, O
and O
the O
other O
that O
is O
subsequently O
generated O
is O
typically O
the O
effect O
, O
provided O
it O
is O
generated O
based O
on O
the O
previous O
one O
( O
rather O
than O
, O
say O
, O
on O
a O
common O
confounder O
that O
causes O
both O
variables O
) O
. O
If O
X O
and O
Y O
are O
generated O
jointly O
, O
then O
we O
need O
to O
distinguish O
based O
on O
the O
underlying O
generative O
process O
whether O
one O
of O
the O
two O
variables O
is O
causing O
the O
other O
variable O
. O

Learning O
Effect O
from O
Cause O
( O
Causal O
Learning O
) O
Causal O
( O
X O
→ O
Y O
) O
NLP O
tasks O
typically O
aim O
to O
predict O
a O
post O
- O
hoc O
generated O
human O
annotation O
( O
i.e. O
, O
the O
target O
Y O
is O
the O
effect O
) O
from O
a O
given O
input O
X O
( O
the O
cause O
) O
. O
Examples O
include O
: O
summarization O
( O
article→summary O
) O
where O
the O
goal O
is O
to O
produce O
a O
summary O
Y O
of O
a O
given O
input O
text O
X O
; O
parsing O
and O
tagging O
( O
text→linguists O
' O
annotated O
structure O
) O
where O
the O
goal O
is O
to O
predict O
an O
annotated O
syntactic O
structure O
Y O
of O
a O
given O
input O
sentence O
X O
; O
data O
- O
totext O
generation O
( O
data→description O
) O
where O
the O
goal O
is O
to O
produce O
a O
textual O
description O
Y O
of O
a O
set O
of O
structured O
input O
data O
X O
; O
and O
information O
extraction O
( O
text→entities O
/ O
relations O
/ O
etc O
) O
where O
the O
goal O
is O
to O
extract O
structured O
information O
from O
a O
given O
text O
. O

Learning O
Cause O
from O
Effect O
( O
Anticausal O
Learning O
) O
Anticausal O
( O
Y O
→ O
X O
) O
NLP O
tasks O
typically O
aim O
to O
predict O
or O
infer O
some O
latent O
target O
property O
Y O
such O
as O
an O
unobserved O
prompt O
from O
an O
observed O
input O
X O
which O
takes O
the O
form O
of O
one O
of O
its O
effects O
. O
Typical O
anticausal O
NLP O
learning O
problems O
include O
, O
for O
example O
, O
author O
attribute O
identification O
( O
author O
attribute→text O
) O
where O
the O
goal O
is O
to O
predict O
some O
unobserved O
attribute O
Y O
of O
the O
writer O
of O
a O
given O
text O
snippet O
X O
; O
and O
review O
sentiment O
classification O
( O
sentiment→review O
text O
) O
where O
the O
goal O
is O
to O
predict O
the O
latent O
sentiment O
Y O
that O
caused O
an O
author O
to O
write O
a O
particular O
review O
X O
. O

Other O
/ O
Mixed O
Some O
tasks O
can O
be O
categorized O
as O
either O
causal O
or O
anticausal O
, O
depending O
on O
how O
exactly O
the O
data O
is O
collected O
. O
In O
§ O
1 O
, O
we O
discussed O
the O
example O
of O
MT O
where O
different O
types O
of O
( O
causal O
and O
anticausal O
) O
data O
are O
typically O
mixed O
. O
Another O
example O
is O
the O
task O
of O
intent O
classification O
: O
if O
the O
same O
author O
reveals O
their O
intent O
before O
the O
writing O
( O
i.e. O
, O
intent→text O
) O
, O
it O
can O
be O
viewed O
as O
an O
anticausal O
learning O
task O
; O
if O
, O
on O
the O
other O
hand O
, O
the O
data O
is O
annotated O
by O
other O
people O
who O
are O
not O
the O
original O
author O
( O
i.e. O
, O
text→annotated O
intent O
) O
, O
it O
can O
be O
viewed O
as O
a O
causal O
learning O
task O
. O
A O
similar O
reasoning O
applies O
to O
question O
answering O
and O
generation O
tasks O
which O
respectively O
aim O
to O
provide O
an O
answer O
to O
a O
given O
question O
, O
or O
vice O
versa O
: O
if O
first O
a O
piece O
of O
informative O
text O
is O
selected O
and O
annotators O
are O
then O
asked O
to O
come O
up O
with O
a O
corresponding O
question O
( O
answer→question O
) O
as O
, O
e.g. O
, O
in O
the O
SQuAD O
dataset O
( O
Rajpurkar O
et O
al O
. O
, O
2016 O
) O
, O
then O
question O
answering O
is O
an O
anticausal O
and O
question O
generation O
a O
causal O
learning O
task O
; O
if O
, O
on O
the O
other O
hand O
, O
a O
question O
such O
as O
a O
search O
query O
is O
selected O
first O
and O
subsequently O
an O
answer O
is O
pro O
- O
vided O
( O
question→answer O
) O
as O
, O
e.g. O
, O
in O
the O
Natural O
Questions O
dataset O
( O
Kwiatkowski O
et O
al O
. O
, O
2019 O
) O
, O
then O
question O
answering O
is O
a O
causal O
and O
question O
generation O
an O
anticausal O
learning O
task O
. O
Often O
, O
multiple O
such O
datasets O
are O
combined O
without O
regard O
for O
their O
causal O
direction O
. O

Implications O
of O
ICM O
for O
Causal O
and O
Anticausal O
Learning O
Problems O

Whether O
we O
are O
in O
a O
causal O
or O
anticausal O
learning O
scenario O
has O
important O
implications O
for O
semisupervised O
learning O
( O
SSL O
) O
and O
domain O
adaptation O
( O
DA O
) O
( O
Schölkopf O
et O
al O
. O
, O
2012 O
; O
Sgouritsa O
et O
al O
. O
, O
2015 O
; O
Zhang O
et O
al O
. O
, O
2013Zhang O
et O
al O
. O
, O
, O
2015Gong O
et O
al O
. O
, O
2016 O
; O
von O
Kügelgen O
et O
al O
. O
, O
2019von O
Kügelgen O
et O
al O
. O
, O
, O
2020 O
, O
which O
are O
techniques O
also O
commonly O
used O
in O
NLP O
. O
These O
implications O
are O
derived O
from O
the O
principle O
of O
independent O
causal O
mechanisms O
( O
ICM O
) O
( O
Schölkopf O
et O
al O
. O
, O
2012 O
; O
Lemeire O
and O
Dirkx O
, O
2006 O
) O
which O
states O
that O
" O
the O
causal O
generative O
process O
of O
a O
system O
's O
variables O
is O
composed O
of O
autonomous O
modules O
that O
do O
not O
inform O
or O
influence O
each O
other O
" O
( O
Peters O
et O
al O
. O
, O
2017 O
) O
. O

In O
the O
bivariate O
case O
, O
this O
amount O
to O
a O
type O
of O
independence O
assumption O
between O
the O
distribution O
P O
C O
of O
the O
cause O
C O
, O
and O
the O
causal O
process O
, O
or O
mechanism O
, O
P O
E|C O
that O
generates O
the O
effect O
from O
the O
cause O
. O
For O
example O
, O
for O
a O
question O
answering O
task O
, O
the O
generative O
process O
P O
C O
by O
which O
one O
person O
comes O
up O
with O
a O
question O
C O
is O
" O
independent O
" O
of O
the O
process O
P O
E|C O
by O
which O
another O
person O
produces O
an O
answer O
E O
for O
question O
C. O
4 O
Here O
, O
" O
independent O
" O
is O
not O
meant O
in O
the O
sense O
of O
statistical O
independence O
of O
random O
variables O
, O
but O
rather O
as O
independence O
at O
the O
level O
of O
generative O
processes O
or O
distributions O
in O
the O
sense O
that O
P O
C O
and O
P O
E|C O
do O
not O
share O
information O
( O
the O
person O
asking O
the O
question O
and O
the O
one O
answering O
may O
not O
know O
each O
other O
) O
and O
can O
be O
manipulated O
independently O
of O
each O
other O
( O
we O
can O
swap O
either O
of O
the O
two O
for O
another O
participant O
without O
the O
other O
one O
being O
influenced O
by O
this O
) O
. O
Crucially O
, O
this O
type O
of O
independence O
is O
generally O
violated O
in O
the O
opposite O
, O
i.e. O
, O
anticausal O
, O
direction O
: O
P O
E O
and O
P O
C|E O
may O
share O
information O
and O
change O
dependently O
( O
Daniušis O
et O
al O
. O
, O
2010 O
; O
. O
This O
has O
two O
important O
implications O
for O
common O
learning O
tasks O
( O
Schölkopf O
et O
al O
. O
, O
2012 O
) O
which O
are O
illustrated O
in O
Fig O
. O
3 O
. O
Figure O
3 O
: O
The O
ICM O
principle O
assumes O
that O
the O
generative O
process O
P O
C O
of O
the O
cause O
C O
is O
independent O
of O
the O
causal O
mechanism O
P O
E|C O
: O
the O
two O
distributions O
share O
no O
information O
and O
each O
may O
be O
changed O
or O
manipulated O
without O
affecting O
the O
other O
. O
In O
the O
anticausal O
direction O
, O
on O
the O
other O
hand O
, O
the O
effect O
distribution O
P O
E O
is O
( O
in O
the O
generic O
case O
) O
not O
independent O
of O
the O
inverse O
mechanism O
P O
C|E O
: O
they O
may O
share O
information O
and O
change O
dependently O
. O
( O
Left O
) O
SSL O
, O
which O
aims O
to O
improve O
an O
estimate O
of O
the O
target O
conditional O
P O
Y O
|X O
given O
additional O
unlabelled O
input O
data O
from O
P O
X O
, O
should O
therefore O
not O
help O
for O
causal O
learning O
( O
X O
→ O
Y O
) O
, O
but O
may O
help O
in O
the O
anticausal O
direction O
( O
Y O
→ O
X O
) O
. O
( O
Right O
) O
DA O
, O
which O
aims O
to O
adapt O
a O
model O
of O
P O
Y O
|X O
from O
a O
source O
domain O
to O
a O
target O
domain O
( O
e.g. O
, O
fine O
- O
tuning O
on O
a O
smaller O
dataset O
) O
, O
should O
work O
better O
for O
causal O
learning O
settings O
where O
a O
change O
in O
P O
C O
is O
not O
expected O
to O
lead O
to O
a O
change O
in O
the O
mechanism O
P O
E|C O
, O
whereas O
in O
the O
anticausal O
direction O
P O
E O
and O
P O
C|E O
may O
change O
in O
a O
dependent O
manner O
. O

Implications O
of O
ICM O
for O
SSL O
First O
, O
if O
P O
C O
shares O
no O
information O
with O
P O
E|C O
, O
SSL O
- O
where O
one O
has O
additional O
unlabelled O
input O
data O
from O
P O
X O
and O
aims O
to O
improve O
an O
estimate O
of O
the O
target O
conditional O
P O
Y O
|X O
-should O
not O
work O
in O
the O
causal O
direction O
( O
X O
→ O
Y O
) O
, O
but O
may O
work O
in O
the O
anticausal O
direction O
( O
Y O
→ O
X O
) O
, O
as O
P O
E O
and O
P O
C|E O
may O
share O
information O
. O
Causal O
NLP O
tasks O
should O
thus O
be O
less O
likely O
to O
show O
improvements O
over O
a O
supervised O
baseline O
when O
using O
SSL O
than O
anticausal O
tasks O
. O
Traditionally O
, O
the O
ICM O
principle O
is O
thought O
of O
in O
the O
context O
of O
physical O
processes O
or O
mechanisms O
, O
rather O
than O
social O
or O
linguistic O
ones O
such O
as O
language O
. O
Since O
ICM O
amounts O
to O
an O
independence O
assumption O
that O
- O
while O
well O
motivated O
in O
principlemay O
not O
always O
hold O
in O
practice O
, O
5 O
we O
now O
assay O
its O
validity O
on O
NLP O
data O
. O
Recall O
, O
that O
ICM O
postulates O
a O
type O
of O
independence O
between O
P O
C O
and O
P O
E|C O
. O
One O
way O
to O
formalize O
this O
uses O
Kolmogorov B-MetricName
complexity I-MetricName
K B-MetricName
( I-MetricName
• I-MetricName
) I-MetricName
as O
a O
measure O
of O
algorithmic O
information O
, O
which O
can O
be O
5 O
E.g. O
, O
due O
to O
confounding O
influences O
from O
unobserved O
variables O
, O
or O
mechanisms O
which O
have O
co O
- O
evolved O
to O
be O
dependent O
understood O
as O
the O
length O
of O
the O
shortest O
program O
that O
computes O
a O
particular O
algorithmic O
object O
such O
as O
a O
distribution O
or O
a O
function O
( O
Solomonoff O
, O
1964 O
; O
Kolmogorov O
, O
1965 O
) O
. O
ICM O
then O
reads O

) O
: O
6 O
K B-MetricName
( O
P O
C O
, O
E O
) O
+ O
= O
K B-MetricName
( O
P O
C O
) O
+ O
K B-MetricName
( O
P O
E|C O
) O
+ O
≤ O
K B-MetricName
( O
P O
E O
) O
+ O
K B-MetricName
( O
P O
C|E O
) O
. O
( O
1 O
) O

In O
other O
words O
, O
the O
shortest O
description O
of O
the O
joint O
distribution O
P O
C O
, O
E O
corresponds O
to O
describing O
P O
C O
and O
P O
E|C O
separately O
( O
i.e. O
, O
they O
share O
no O
information O
) O
, O
whereas O
there O
may O
be O
redundant O
( O
shared O
) O
information O
in O
the O
non O
- O
causal O
direction O
such O
that O
a O
separate O
description O
of O
P O
E O
and O
P O
C|E O
will O
generally O
be O
longer O
than O
that O
of O
the O
joint O
distribution O
P O
C O
, O
E O
. O

Estimation O
by O
MDL B-MetricName

Since O
Kolmogorov B-MetricName
complexity I-MetricName
is O
not O
computable O
( O
Li O
et O
al O
. O
, O
2008 O
) O
, O
we O
adopt O
a O
commonly O
used O
proxy O
, O
the O
minimum B-MetricName
description I-MetricName
length I-MetricName
( O
MDL B-MetricName
) O
( O
Grünwald O
, O
2007 O
) O
, O
to O
test O
the O
applicability O
of O
ICM O
for O
NLP O
data O
. O
Given O
an O
input O
, O
such O
as O
a O
collection O
of O
observations O
{ O
( O
c O
i O
, O
e O
i O
) O
} O
n O
i=1 O
∼ O
P O
C O
, O
E O
, O
MDL B-MetricName
returns O
the O
shortest O
codelength O
( O
in O
bits O
) O
needed O
to O
compress O
the O
input O
, O
as O
well O
as O
the O
parameters O
needed O
to O
decompress O
it O
. O
We O
use O
MDL B-MetricName
to O
approximate O
( O
1 O
) O
as O
follows O
: O

MDL B-MetricName
( O
c O
1 O
: O
n O
, O
e O
1 O
: O
n O
) O
= O
MDL B-MetricName
( O
c O
1 O
: O
n O
) O
+ O
MDL B-MetricName
( O
e O
1 O
: O
n O
|c O
1 O
: O
n O
) O
≤ O
MDL B-MetricName
( O
e O
1 O
: O
n O
) O
+ O
MDL B-MetricName
( O
c O
1 O
: O
n O
|e O
1 O
: O
n O
) O
, O
( O
2 O
) O

where O
MDL B-MetricName
( O
•|• O
) O
denotes O
a O
conditional O
compression O
where O
the O
second O
argument O
is O
treated O
as O
" O
free O
parameters O
" O
which O
do O
not O
count O
towards O
the O
compression O
length O
of O
the O
first O
argument O
. O
Eq O
. O
( O
2 O
) O
can O
thus O
6 O
Here O
, O
be O
interpreted O
as O
a O
comparison O
between O
two O
ways O
of O
compressing O
the O
same O
data O
( O
c O
1 O
: O
n O
, O
e O
1 O
: O
n O
) O
: O
either O
we O
first O
compress O
c O
1 O
: O
n O
and O
then O
compress O
e O
1 O
: O
n O
conditional O
on O
c O
1 O
: O
n O
, O
or O
vice O
versa O
. O
According O
to O
the O
ICM O
principle O
, O
the O
first O
way O
should O
tend O
to O
be O
more O
" O
concise O
" O
than O
the O
second O
. O

Calculating O
MDL B-MetricName
Using O
Machine O
Translation O
as O
a O
Case O
Study O

To O
empirically O
assess O
the O
validity O
of O
ICM O
for O
NLP O
data O
using O
MDL B-MetricName
as O
a O
proxy O
, O
we O
turn O
to O
MT O
as O
a O
case O
study O
. O
We O
choose O
MT O
because O
the O
input O
and O
output O
spaces O
of O
MT O
are O
relatively O
symmetric O
, O
as O
opposed O
to O
other O
NLP O
tasks O
such O
as O
text O
classification O
where O
the O
input O
space O
is O
sequences O
, O
but O
the O
output O
space O
is O
a O
small O
set O
of O
labels O
. O
There O
are O
only O
very O
few O
studies O
which O
calculate O
MDL B-MetricName
on O
NLP O
data O
, O
so O
we O
extend O
the O
method O
of O
Voita O
and O
Titov O
( O
2020 O
) O
to O
calculate O
MDL B-MetricName
using O
online O
codes O
( O
Rissanen O
, O
1984 O
) O
for O
deep O
learning O
tasks O
( O
Blier O
and O
Ollivier O
, O
2018 O
) O
. O
Since O
the O
original O
calculation O
method O
for O
MDL B-MetricName
by O
Voita O
and O
Titov O
( O
2020 O
) O
was O
developed O
for O
classification O
, O
we O
extend O
it O
to O
sequence O
- O
to O
- O
sequence O
( O
Seq2Seq O
) O
generation O
. O
Specifically O
, O
given O
a O
translation O
dataset O
D O
= O
{ O
( O
x O
1 O
, O
y O
1 O
) O
, O
. O
. O
. O
, O
( O
x O
n O
, O
y O
n O
) O
} O
of O
n O
pairs O
of O
sentences O
x O
i O
with O
translation O
y O
i O
, O
denote O
the O
size O
of O
the O
vocabulary O
of O
the O
source O
language O
by O
V O
x O
, O
and O
the O
size O
of O
the O
vocabulary O
of O
the O
target O
language O
by O
V O
y O
. O
In O
order O
to O
assess O
whether O
( O
2 O
) O
holds O
, O
we O
need O
to O
calculate O
four O
different O
terms O
: O
two O
marginal O
terms O
MDL O
( O
x O
1 O
: O
n O
) O
and O
MDL O
( O
y O
1 O
: O
n O
) O
, O
and O
two O
conditional O
terms O
MDL B-MetricName
( O
y O
1 O
: O
n O
|x O
1 O
: O
n O
) O
and O
MDL B-MetricName
( O
x O
1 O
: O
n O
|y O
1 O
: O
n O
) O
. O

Codelength B-MetricName
of O
the O
Conditional O
Terms O

To O
calculate O
the O
codelength B-MetricName
of O
the O
two O
conditional O
terms O
, O
we O
extend O
the O
method O
of O
Voita O
and O
Titov O
( O
2020 O
) O
from O
classification O
to O
Seq2Seq O
generation O
. O
Following O
the O
setting O
of O
Voita O
and O
Titov O
( O
2020 O
) O
, O
we O
break O
the O
dataset O
D O
into O
10 O
disjoint O
subsets O
with O
increasing O
sizes O
and O
denote O
the O
end O
index O
of O
each O
subset O
as O
t O
i O
. O
7 O
We O
then O
estimate O
MDL B-MetricName
( O
y O

1 O
: O
n O
|x O
1 O
: O
n O
) O
as O
' O
MDL B-MetricName
( O
y O
1 O
: O
n O
|x O
1 O
: O
n O
) O
= O
t O
1 O
i=1 O
length O
( O
y O
i O
) O
• O
log O
2 O
V O
y O
− O
n−1 O
i=1 O
log O
2 O
p O
θ O
i O
( O
y O
1+t O
i O
: O
t O
i+1 O
|x O
1+t O
i O
: O
t O
i+1 O
) O
, O
( O
3 O
) O

where O
length O
( O
y O
i O
) O
refers O
to O
the O
number O
of O
tokens O
in O
the O
sequence O
y O
i O
, O
θ O
i O
are O
the O
parameters O
of O
a O
translation O
model O
h O
i O
trained O
on O
the O
first O
t O
i O
data O
points O
, O
and O
seq O
idx O
1 O
: O
idx O
2 O
refers O
to O
the O
set O
of O
sequences O
from O
7 O
The O
sizes O
of O
the O
10 O
subsets O
are O
0.1 O
, O
0.2 O
, O
0.4 O
, O
0.8 O
, O
1.6 O
, O
3.2 O
, O
6.25 O
, O
12.5 O
, O
25 O
, O
and O
50 O
percent O
of O
the O
dataset O
size O
, O
respectively O
. O
E.g. O
, O
t1 O
= O
0.1 O
% O
n O
, O
t2 O
= O
( O
0.1 O
% O
+ O
0.2 O
% O
) O
n O
, O
. O
. O
. O
. O
the O
idx O
1 O
-th O
to O
the O
idx O
2 O
-th O
sample O
in O
the O
dataset O
D O
, O
where O
seq O
∈ O
{ O
x O
, O
y O
} O
and O
idx O
i O
∈ O
{ O
1 O
, O
. O
. O
. O
, O
n O
} O
. O

Similarly O
, O
when O
calculating O
MDL B-MetricName
( O
x O
1 O
: O
n O
|y O
1 O
: O
n O
) O
, O
we O
simply O
swap O
the O
roles O
of O
x O
and O
y O
. O

Codelength B-MetricName
of O
the O
Marginal O
Terms O
When O
calculating O
the O
two O
marginal O
terms O
, O
MDL B-MetricName
( O
x O
1 O
: O
n O
) O
and O
MDL B-MetricName
( O
y O
1 O
: O
n O
) O
, O
we O
make O
two O
changes O
from O
the O
above O
calculation O
of O
conditional O
terms O
: O
first O
, O
we O
replace O
the O
translation O
models O
h O
i O
with O
language O
models O
; O
second O
, O
we O
remove O
the O
conditional O
distribution O
. O

That O
is O
, O
we O
calculate O
MDL B-MetricName
( O
x O
1 O
: O
n O
) O
as O

' O
MDL B-MetricName
( O
x O
1 O
: O
n O
) O
= O
t O
1 O
i=1 O
length O
( O
x O
i O
) O
• O
log O
2 O
V O
x O
− O
n−1 O
i=1 O
log O
2 O
p O
θ O
i O
( O
x O
1+t O
i O
: O
t O
i+1 O
) O
, O
( O
4 O
) O

where O
θ O
i O
are O
the O
parameters O
of O
a O
language O
model O
h O
i O
trained O
on O
the O
first O
t O
i O
data O
points O
. O
We O
apply O
the O
same O
method O
to O
calculate O
MDL B-MetricName
( O
y O
1 O
: O
n O
) O
. O

For O
the O
language O
model O
, O
we O
use O
GPT2 B-MethodName
( O
Radford O
et O
al O
. O
, O
2019 O
) O
, O
and O
for O
the O
translation O
model O
, O
we O
use O
the O
Marian B-MethodName
neural O
machine O
translation O
model O
( O
Junczys O
- O
Dowmunt O
et O
al O
. O
, O
2018 O
) O
trained O
on O
the O
OPUS O
Corpus O
( O
Tiedemann O
and O
Nygaard O
, O
2004 O
) O
. O
For O
fair O
comparison O
, O
all O
models O
adopt O
the O
transformer O
architecture O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
, O
and O
have O
roughly O
the O
same O
number O
of O
parameters O
. O
See O
Appendix O
B O
for O
more O
experimental O
details O
. O

CausalMT B-DatasetName
Corpus O

For O
our O
MDL B-MetricName
experiment O
, O
we O
need O
datasets O
for O
which O
the O
causal O
direction O
of O
data O
collection O
is O
known O
, O
i.e. O
, O
for O
which O
we O
have O
ground O
- O
truth O
annotation O
of O
which O
text O
is O
the O
original O
and O
which O
is O
a O
translation O
, O
instead O
of O
a O
mixture O
of O
both O
. O
Since O
existing O
MT O
corpora O
do O
not O
have O
this O
property O
as O
discussed O
in O
§ O
1 O
, O
we O
curate O
our O
own O
corpus O
, O
which O
we O
call O
the O
CausalMT B-DatasetName
corpus O
. O

Specifically O
, O
we O
consider O
the O
existing O
MT O
dataset O
WMT'19 B-DatasetName
, O
8 O
and O
identify O
some O
subsets O
that O
have O
a O
clear O
notion O
of O
causality O
. O
The O
subsets O
we O
use O
are O
the O
EuroParl B-DatasetName
( O
Koehn O
, O
2005 O
) O
translation O
corpora O
. O
9 O
For O
EuroParl B-DatasetName
, O
each O
text O
has O
meta O
information O
such O
as O
the O
speaker O
's O
language O
; O
for O
Global B-DatasetName
Voices I-DatasetName
, O
each O
text O
has O
meta O
information O
about O
whether O
it O
is O
translated O
or O
not O
. O
We O
regard O
text O
that O
is O
in O
the O
same O
language O
as O
the O
speaker O
's O
native O
language O
in O
EuroParl B-DatasetName
( O
and O
non O
- O
translated O
text O
in O
Global B-DatasetName
Voices I-DatasetName
) O
as O
the O
original O
( O
i.e. O
, O
the O
cause O
) O
. O

We O
then O
retrieve O
a O
corresponding O
effect O
by O
using O
the O
cause O
text O
to O
match O
the O
parallel O
pairs O
in O
the O
processed O
dataset O
. O
In O
this O
way O
, O
we O
compile O
six O
translation O
datasets O
with O
clear O
causal O
direction O
as O
summarized O
in O
Tab O
. O
2 O
. O
For O
each O
dataset O
, O
we O
use O
1 O
K O
samples O
each O
as O
test O
and O
validation O
sets O
, O
and O
use O
the O
rest O
for O
training O
. O

Results O

The O
results O
of O
our O
MDL B-MetricName
experiment O
on O
the O
six O
CausalMT B-DatasetName
datasets O
are O
summarised O
in O
Tab O
. O
3 O
. O
If O
ICM O
holds O
, O
we O
expect O
the O
sum O
of O
codelengths B-MetricName
to O
be O
smaller O
for O
the O
causal O
direction O
than O
for O
the O
anticausal O
one O
, O
see O
( O
2 O
) O
. O
As O
can O
be O
seen O
from O
the O
last O
column O
, O
this O
is O
the O
case O
for O
five O
out O
of O
the O
six O
datasets O
. O
For O
example O
, O
on O
one O
of O
the O
largest O
datasets O
( O
En→Es O
) O
, O
the O
MDL B-MetricName
difference O
is O
346 O
kbits O
. O
10 O
Comparing O
the O
dataset O
sizes O
in O
Tab O
. O
2 O
and O
results O
in O
Tab O
. O
3 O
, O
we O
observe O
that O
the O
absolute O
MDL B-MetricName
values O
are O
roughly O
proportional O
to O
dataset O
size O
, O
but O
other O
factors O
such O
as O
language O
and O
task O
complexity O
also O
play O
a O
role O
. O
This O
is O
inherent O
to O
the O
nature O
of O
MDL B-MetricName
being O
the O
sum O
of O
codelengths B-MetricName
of O
the O
model O
and O
of O
the O
data O
given O
the O
model O
. O
Since O
we O
use O
equally O
- O
sized O
datasets O
for O
each O
language O
pair O
in O
the O
CausalMT B-DatasetName
corpus O
( O
i.e. O
, O
in O
both O
the O
X O
→ O
Y O
and O
Y O
→ O
X O
directions O
, O
see O
Tab O
. O
2 O
) O
, O
numbers O
for O
the O
same O
language O
pair O
in O
Tab O
. O
3 O
, O
including O
the O
most O
important O
column O
" O
MDL B-MetricName
( O
X O
) O
+MDL B-MetricName
( O
Y|X O
) O
vs. O
MDL B-MetricName
( O
Y O
) O
+MDL B-MetricName
( O
X|Y O
) O
" O
, O
form O
a O
valid O
comparison O
. O
That O
is O
, O
En O
& O
Es O
experiments O
are O
comparable O
within O
themselves O
, O
so O
are O
the O
other O
language O
pairs O
. O
For O
some O
of O
the O
smaller O
differences O
in O
the O
last O
column O
in O
Tab O
. O
3 O
, O
and O
, O
in O
particular O
the O
reversed O
inequality O
in O
row O
4 O
, O
a O
potential O
explanation O
may O
be O
the O
relatively O
small O
dataset O
size O
, O
as O
well O
as O
the O
fact O
that O
text O
data O
may O
be O
confounded O
( O
e.g. O
, O
through O
shared O
grammar O
and O
semantics O
) O
. O

SSL O
for O
Causal O
vs. O
Anticausal O
Models O

In O
semi O
- O
supervised O
learning O
( O
SSL O
) O
, O
we O
are O
given O
a O
typically O
- O
small O
set O
of O
k O
labeled O
observations O
D O
L O
= O
{ O
( O
x O
1 O
, O
y O
1 O
) O
, O
. O
. O
. O
, O
( O
x O
k O
, O
y O
k O
) O
} O
, O
and O
a O
typicallylarge O
set O
of O
m O
unlabeled O
observations O
of O
the O
input O

D O
U O
= O
{ O
x O
( O
u O
) O
1 O
, O
. O
. O
. O
, O
x O
( O
u O
) O

m O
} O
. O
SSL O
then O
aims O
to O
use O
the O
additional O
information O
about O
the O
input O
distribution O
P O
X O
from O
the O
unlabeled O
dataset O
D O
U O
to O
improve O
a O
model O
of O
P O
Y O
|X O
learned O
on O
the O
labeled O
dataset O
D O
L O
. O

As O
explained O
in O
§ O
3 O
, O
SSL O
should O
only O
work O
for O
anticausal O
( O
or O
confounded O
) O
learning O
tasks O
, O
according O
to O
the O
ICM O
principle O
. O
Schölkopf O
et O
al O
. O
( O
2012 O
) O
have O
observed O
this O
trend O
on O
a O
number O
of O
classification O
and O
regression O
tasks O
on O
small O
- O
scale O
numerical O
inputs O
, O
such O
as O
predicting O
Boston O
housing O
prices O
from O
quantifiable O
neighborhood O
features O
( O
causal O
learning O
) O
, O
or O
breast O
cancer O
from O
lab O
statistics O
( O
anticausal O
learning O
) O
. O
However O
, O
there O
exist O
no O
studies O
investigating O
the O
implications O
of O
ICM O
for O
SSL O
on O
NLP O
data O
, O
which O
is O
of O
a O
more O
complex O
nature O
due O
to O
the O
high O
dimensionality O
of O
the O
input O
and O
output O
spaces O
, O
as O
well O
as O
potentially O
large O
confounding O
. O
In O
the O
following O
, O
we O
use O
a O
sequence O
- O
to O
- O
sequence O
decipherment O
experiment O
( O
§ O
5.1 O
) O
and O
a O
meta O
- O
study O
of O
existing O
literature O
( O
§ O
5.2 O
) O
to O
showcase O
that O
the O
same O
phenomenon O
also O
occurs O
in O
NLP O
. O

Decipherment O
Experiment O

To O
have O
control O
over O
causal O
direction O
of O
the O
data O
collection O
process O
, O
we O
use O
a O
synthetic O
decipherment O
dataset O
to O
test O
the O
difference O
in O
SSL O
improvement O
between O
causal O
and O
anticausal O
learning O
tasks O
. O

Dataset O
We O
create O
a O
synthetic O
dataset O
of O
encrypted O
sequences O
. O
Specifically O
, O
we O
( O
i O
) O
adopt O
a O
monolingual O
English O
corpus O
( O
for O
which O
we O
use O
the O
English O
corpus O
of O
the O
En→Es O
in O
the O
CausalMT B-DatasetName
dataset O
, O
for O
convenience O
) O
, O
( O
ii O
) O
apply O
the O
ROT13 O
encryption O
algorithm O
( O
Schneier O
, O
1996 O
) O
to O
obtain O
the O
encrypted O
corpus O
, O
and O
then O
( O
iii O
) O
apply O
noise O
on O
the O
corpus O
that O
is O
chosen O
to O
be O
the O
effect O
corpus O
. O

In O
the O
encryption O
step O
( O
ii O
) O
, O
for O
each O
English O
sentence O
x O
, O
its O
encryption O
ROT13 O
( O
x O
) O
replaces O
each O
letter O
with O
the O
13th O
letter O
after O
it O
in O
the O
alphabet O
, O
e.g. O
, O
" O
A O
" O
→ O
" O
N O
, O
" O
" O
B O
" O
→ O
" O
O. O
" O
Note O
that O
we O
choose O
ROT13 O
due O
to O
its O
invertibility O
, O
since O
ROT13 O
( O
ROT13 O
( O
x O
) O
) O
= O
x. O
Therefore O
, O
without O
any O
noises O
, O
the O
corpus O
of O
English O
and O
the O
corpus O
of O
encrypted O
sequences O
by O
ROT13 O
are O
symmetric O
. O

In O
the O
noising O
step O
( O
iii O
) O
, O
we O
apply O
noise O
either O
to O
the O
English O
text O
or O
to O
the O
ciphertext O
, O
thus O
creating O
two O
datasets O
Cipher→En B-DatasetName
, O
and O
En→Cipher B-DatasetName
, O
respectively O
. O
When O
applying O
noise O
to O
a O
sequence O
, O
we O
use O
the O
implementation O
of O
the O
Fairseq O
library O
. O
11 O
Namely O
, O
we O
mask O
some O
random O
words O
in O
the O
sequence O
( O
word O
masking O
) O
, O
permute O
a O
part O
of O
the O
sequence O
( O
permuted O
noise O
) O
, O
randomly O
shift O
the O
endings O
of O
the O
sequence O
to O
the O
beginning O
( O
rolling O
noise O
) O
, O
and O
insert O
some O
random O
characters O
or O
masks O
to O
the O
sequence O
( O
insertion O
noise O
) O
. O
We O
set O
the O
probability O
of O
all O
noises O
to O
p O
= O
5 O
% O
. O

Results O
For O
each O
of O
the O
two O
datasets O
En→Cipher B-DatasetName
and O
Cipher→En B-DatasetName
, O
we O
perform O
SSL O
in O
the O
causal O
and O
anticausal O
direction O
by O
either O
treating O
the O
input O
X O
as O
the O
cause O
and O
the O
target O
Y O
as O
the O
effect O
, O
or O
vice O
versa O
. O
Specifically O
, O
we O
use O
a O
standard O
Transformer O
architecture O
for O
the O
supervised O
model O
, O
and O
for O
SSL O
, O
we O
multitask O
the O
translation O
task O
with O
an O
additional O
denoising O
autoencoder O
( O
Vincent O
et O
al O
. O
, O
2008 O
) O
using O
the O
Fairseq O
Python O
package O
. O
The O
results O
are O
shown O
in O
Tab O
. O
4 O
. O
It O
can O
be O
seen O
that O
in O
both O
cases O
, O
anticausal O
models O
show O
a O
substantially O
larger O
SSL O
improvement O
than O
causal O
models O
. O

We O
also O
note O
that O
there O
is O
a O
substantial O
gap O
in O
the O
supervised O
performance O
between O
causal O
and O
anticausal O
learning O
tasks O
on O
the O
same O
underlying O
data O
. O
This O
is O
also O
expected O
as O
causal O
learning O
is O
typically O
easier O
than O
anticausal O
learning O
since O
it O
corresponds O
to O
learning O
the O
" O
natural O
" O
forward O
function O
, O
or O
causal O
mechanism O
, O
while O
anticausal O
learning O
cor-11 O
Link O
to O
the O
Fairseq O
implementation O
. O
responds O
to O
learning O
the O
less O
natural O
, O
non O
- O
causal O
inverse O
mechanism O
. O

SSL O
Improvements O
in O
Existing O
Work O

After O
verifying O
the O
different O
behaviour O
in O
SSL O
improvement O
predicted O
by O
the O
ICM O
principle O
on O
the O
decipherment O
experiment O
, O
we O
conduct O
an O
extensive O
meta O
- O
study O
to O
survey O
whether O
this O
trend O
is O
also O
reflected O
in O
published O
NLP O
findings O
. O
To O
this O
end O
, O
we O
consider O
a O
diverse O
set O
of O
tasks O
, O
and O
SSL O
methods O
. O
The O
tasks O
covered O
in O
our O
meta O
- O
study O
include O
machine O
translation O
, O
summarization O
, O
parsing O
, O
tagging O
, O
information O
extraction O
, O
review O
sentiment O
classification O
, O
text O
category O
classification O
, O
word O
sense O
disambiguation O
, O
and O
chunking O
. O
The O
SSL O
methods O
include O
self O
- O
training O
, O
co O
- O
training O
( O
Blum O
andMitchell O
, O
1998 O
) O
, O
tri O
- O
training O
( O
Zhou O
andLi O
, O
2005 O
) O
, O
transductive O
support O
vector O
machines O
( O
Joachims O
, O
1999 O
) O
, O
expectation O
maximization O
( O
Nigam O
et O
al O
. O
, O
2006 O
) O
, O
multitasking O
with O
language O
modeling O
( O
Dai O
and O
Le O
, O
2015 O
) O
, O
multitasking O
with O
sentence O
reordering O
( O
as O
used O
in O
Zhang O
and O
Zong O
( O
2016 O
) O
) O
, O
and O
crossview O
training O
( O
Clark O
et O
al O
. O
, O
2018 O
) O
. O
Further O
details O
on O
our O
meta O
study O
are O
explained O
in O
Appendix O
A O
. O

We O
covered O
55 O
instances O
of O
causal O
learning O
and O
50 O
instances O
of O
anticausal O
learning O
. O
A O
summary O
of O
the O
trends O
of O
causal O
SSL O
and O
anticausal O
SSL O
are O
listed O
in O
Tab O
. O
5 O
. O
Echoing O
with O
the O
implications O
of O
ICM O
stated O
in O
§ O
3 O
, O
for O
causal O
learning O
tasks O
, O
the O
average O
improvement O
by O
SSL O
is O
only O
very O
small O
, O
0.04 B-MetricValue
% I-MetricValue
. O
In O
contrast O
, O
the O
anticausal O
SSL O
improvement O
is O
larger O
, O
1.70 B-MetricValue
% I-MetricValue
on O
average O
. O
We O
use O
Welch O
's O
t O
- O
test O
( O
Welch O
, O
1947 O
) O
to O
assess O
whether O
the O
difference O
in O
mean O
between O
the O
two O
distributions O
of O
SSL O
improvment O
( O
with O
unequal O
variance O
) O
is O
significant O
and O
obtain O
a O
p O
- O
value O
of O
0.011 B-MetricValue
. O

DA O
for O
Causal O
vs. O
Anticausal O
Models O

We O
also O
consider O
a O
supervised O
domain O
adaptation O
( O
DA O
) O
setting O
in O
which O
the O
goal O
is O
to O
adapt O
a O
model O
trained O
on O
a O
large O
labeled O
data O
set O
from O
a O
source O
domain O
, O
to O
a O
potentially O
different O
target O
domain O
from O
which O
we O
only O
have O
a O
a O
small O
labeled O
data O
set O
. O
As O
explained O
in O
§ O
3 O
, O
DA O
should O
only O
work O
well O
for O
causal O
learning O
, O
but O
not O
necessarily O
for O
anticausal O
learning O
, O
according O
to O
the O
ICM O
principle O
. O
Similar O
to O
the O
meta O
- O
study O
on O
SSL O
, O
we O
also O
review O
existing O
NLP O
literature O
on O
DA O
. O
We O
focus O
on O
DA O
improvement O
, O
i.e. O
, O
the O
performance O
gain O
of O
using O
DA O
over O
an O
unadapted O
baseline O
that O
only O
learns O
from O
the O
source O
data O
and O
is O
tested O
on O
the O
target O
domain O
. O
Since O
the O
number O
of O
studies O
on O
DA O
that O
we O
can O
find O
is O
smaller O
than O
for O
SSL O
, O
we O
cover O
22 O
instances O
of O
DA O
on O
causal O
tasks O
, O
and O
11 O
instances O
of O
DA O
on O
anticausal O
tasks O
. O

The O
results O
are O
summarised O
in O
Tab O
. O
6 O
. O
We O
find O
that O
the O
observations O
again O
echo O
with O
our O
expectations O
( O
according O
to O
ICM O
) O
that O
DA O
should O
work O
better O
for O
causal O
, O
than O
for O
anticausal O
learning O
tasks O
. O
Again O
, O
we O
use O
Welch O
's O
t O
- O
test O
( O
Welch O
, O
1947 O
) O
to O
verify O
that O
the O
DA O
improvements O
of O
causal O
learning O
and O
anticausal O
learning O
are O
statistically O
different O
, O
and O
obtain O
a O
p O
- O
value O
of O
0.023 B-MetricValue
. O

How O
to O
Use O
the O
Findings O
in O
this O
Study O

Data O
Collection O
Practice O
in O
NLP O
Due O
to O
the O
different O
implications O
of O
causal O
and O
anticausal O
learning O
tasks O
, O
we O
strongly O
suggest O
annotating O
the O
causal O
direction O
when O
collecting O
new O
NLP O
data O
. O
One O
way O
to O
do O
this O
is O
to O
only O
collect O
data O
from O
one O
causal O
direction O
and O
to O
mention O
this O
in O
the O
meta O
information O
. O
For O
example O
, O
summarization O
data O
collected O
from O
the O
TL O
; O
DR O
of O
scientific O
papers O
SciTldr O
( O
Cachola O
et O
al O
. O
, O
2020 O
) O
should O
be O
causal O
, O
as O
the O
TL O
; O
DR O
summaries O
on O
OpenReview O
( O
some O
from O
authors O
when O
submitting O
the O
paper O
, O
others O
derived O
from O
the O
beginning O
of O
peer O
reviews O
) O
were O
likely O
composed O
after O
the O
original O
papers O
or O
reviews O
were O
written O
. O
Alternatively O
, O
one O
may O
allow O
mixed O
corpora O
, O
but O
label O
the O
causal O
direction O
for O
each O
( O
x O
, O
y O
) O
pair O
, O
e.g. O
, O
which O
is O
the O
original O
vs. O
translated O
text O
in O
a O
translation O
pair O
. O
Since O
more O
data O
often O
leads O
to O
better O
model O
performance O
, O
it O
is O
common O
to O
mix O
data O
from O
both O
causal O
directions O
, O
e.g. O
, O
training O
on O
both O
En→Es O
and O
Es→En O
data O
. O
Annotating O
the O
causal O
direction O
for O
each O
pair O
allows O
future O
users O
of O
the O
dataset O
to O
potentially O
handle O
the O
causal O
and O
anticausal O
parts O
of O
the O
data O
differently O
. O

Causality O
- O
Aware O
Modeling O
When O
building O
NLP O
models O
, O
the O
causal O
direction O
provides O
additional O
information O
that O
can O
potentially O
be O
built O
into O
the O
model O
. O
In O
the O
MT O
case O
, O
since O
causal O
and O
anticausal O
learning O
can O
lead O
to O
different O
performance O
( O
Ni O
et O
al O
. O
, O
2021 O
) O
, O
one O
way O
to O
take O
advantage O
of O
the O
known O
causal O
direction O
is O
to O
add O
a O
prefix O
such O
as O
" O
[ O
Modeling O
- O
Effect O
- O
to O
- O
Cause O
] O
" O
to O
the O
original O
input O
, O
so O
that O
the O
model O
can O
learn O
from O
causally O
- O
annotated O
input O
- O
output O
pairs O
. O
For O
example O
, O
Riley O
et O
al O
. O
( O
2020 O
) O
use O
labels O
of O
the O
causal O
direction O
to O
elicit O
different O
behavior O
at O
inference O
time O
. O
Another O
option O
is O
to O
carefully O
design O
a O
combination O
of O
different O
modeling O
techniques O
, O
such O
as O
limiting O
self O
- O
training O
( O
a O
method O
for O
SSL O
) O
only O
to O
the O
anticausal O
direction O
and O
allowing O
back O
- O
translation O
in O
both O
directions O
, O
as O
preliminarily O
explored O
by O
Shen O
et O
al O
. O
( O
2021 O
) O
. O

Causal O
Discovery O
Suppose O
that O
we O
are O
given O
measurements O
of O
two O
types O
of O
NLP O
data O
X O
and O
Y O
( O
e.g. O
, O
text O
, O
parse O
tree O
, O
intent O
type O
) O
whose O
collection O
process O
is O
unknown O
, O
i.e. O
, O
which O
is O
the O
cause O
and O
which O
the O
effect O
. O
One O
key O
finding O
of O
our O
study O
is O
that O
there O
is O
typically O
a O
causal O
footprint O
of O
the O
data O
collection O
process O
which O
manifests O
itself O
, O
e.g. O
, O
when O
computing O
the O
description O
length O
in O
different O
directions O
( O
§ O
4 O
) O
or O
when O
performing O
SSL O
( O
§ O
5 O
) O
or O
DA O
( O
§ O
6 O
) O
. O
Based O
on O
which O
direction O
has O
the O
shorter O
MDL B-MetricName
, O
or O
allows O
better O
SSL O
or O
DA O
, O
we O
can O
thus O
infer O
one O
causal O
direction O
over O
the O
other O
. O

Prediction O
of O
SSL O
and O
DA O
Effectiveness O
Being O
able O
to O
predict O
the O
effectiveness O
of O
SSL O
or O
DA O
for O
a O
given O
NLP O
task O
can O
be O
very O
useful O
, O
e.g. O
, O
to O
set O
the O
weights O
in O
an O
ensemble O
of O
different O
models O
( O
Søgaard O
, O
2013 O
) O
. O
While O
predicting O
SSL O
performance O
has O
previously O
been O
studied O
from O
a O
non O
- O
causal O
perspective O
( O
Nigam O
and O
Ghani O
, O
2000 O
; O
Asch O
and O
Daelemans O
, O
2016 O
) O
, O
our O
findings O
suggest O
that O
a O
simple O
qualitative O
description O
of O
the O
data O
collection O
process O
in O
terms O
of O
its O
causal O
direction O
( O
as O
summarised O
for O
the O
most O
common O
NLP O
tasks O
in O
Tab O
. O
1 O
) O
can O
also O
be O
surprisingly O
effective O
to O
evaluate O
whether O
SSL O
or O
DA O
should O
be O
expected O
to O
work O
well O
. O

Limitations O
and O
Future O
Work O

We O
note O
that O
ICM O
- O
when O
taken O
strictly O
- O
is O
an O
idealized O
assumption O
that O
may O
be O
violated O
and O
thus O
may O
not O
hold O
exactly O
for O
a O
given O
real O
- O
world O
data O
set O
, O
e.g. O
, O
due O
to O
confounding O
, O
i.e. O
, O
when O
both O
variables O
are O
influenced O
by O
a O
third O
, O
unobserved O
variable O
. O

In O
this O
case O
, O
one O
may O
observe O
less O
of O
a O
difference O
between O
causal O
and O
anticausal O
learning O
tasks O
. O

We O
also O
note O
that O
, O
while O
we O
have O
made O
an O
effort O
to O
classify O
different O
NLP O
tasks O
as O
typically O
causal O
or O
anticausal O
, O
our O
categorization O
should O
not O
be O
ap O
- O
plied O
blindly O
without O
regard O
for O
the O
specific O
generative O
process O
at O
hand O
: O
deviations O
are O
possible O
as O
explained O
in O
the O
Mixed O
/ O
Other O
category O
. O

Another O
limitation O
is O
that O
the O
SSL O
and O
DA O
settings O
considered O
in O
this O
paper O
are O
only O
a O
subset O
of O
the O
various O
settings O
that O
exist O
in O
NLP O
. O
Our O
study O
does O
not O
cover O
, O
for O
example O
, O
SSL O
that O
uses O
additional O
output O
data O
( O
e.g. O
, O
Jean O
et O
al O
. O
( O
2015 O
) O
; O
Gülçehre O
et O
al O
. O
( O
2015 O
) O
; O
Sennrich O
and O
Zhang O
( O
2019 O
) O
) O
, O
or O
unsupervised O
DA O
( O
as O
reviewed O
by O
Ramponi O
and O
Plank O
( O
2020 O
) O
) O
. O
In O
addition O
, O
in O
our O
meta O
- O
study O
of O
published O
SSL O
and O
DA O
findings O
, O
the O
improvements O
of O
causal O
vs. O
anticausal O
learning O
might O
be O
amplified O
by O
the O
scale O
of O
research O
efforts O
on O
different O
tasks O
and O
potentially O
suffer O
from O
selection O
bias O
. O

Finally O
, O
we O
remark O
that O
, O
in O
the O
present O
work O
, O
we O
have O
focused O
on O
bivariate O
prediction O
tasks O
with O
an O
input O
X O
and O
output O
Y O
. O
Future O
work O
may O
also O
apply O
ICM O
- O
based O
reasoning O
to O
more O
complex O
NLP O
settings O
, O
for O
example O
, O
by O
( O
i O
) O
incorporating O
additional O
( O
sequential O
/ O
temporal O
) O
structure O
of O
the O
data O
( O
e.g. O
, O
for O
MT O
or O
language O
modeling O
) O
or O
( O
ii O
) O
considering O
settings O
in O
which O
the O
input O
X O
consists O
of O
both O
cause O
X O
CAU O
and O
effect O
X O
EFF O
features O
of O
the O
target O
Y O
( O
von O
Kügelgen O
et O
al O
. O
, O
2019Kügelgen O
et O
al O
. O
, O
, O
2020 O
. O

Related O
Work O

NLP O
and O
Causality O
Existing O
work O
on O
NLP O
and O
causality O
mainly O
focuses O
on O
the O
extracting O
text O
features O
for O
causal O
inference O
. O
Researchers O
first O
propose O
a O
causal O
graph O
based O
on O
domain O
knowledge O
, O
and O
then O
use O
text O
features O
to O
represent O
some O
elements O
in O
the O
causal O
graph O
, O
e.g. O
, O
the O
cause O
( O
Egami O
et O
al O
. O
, O
2018 O
) O
, O
effect O
Grimmer O
, O
2016 O
) O
, O
andconfounders O
( O
Roberts O
et O
al O
. O
, O
2020 O
; O
Veitch O
et O
al O
. O
, O
2020 O
; O
Keith O
et O
al O
. O
, O
2020 O
) O
. O
Another O
line O
of O
work O
mines O
causal O
relations O
among O
events O
from O
textual O
expressions O
, O
and O
uses O
them O
to O
perform O
relation O
extraction O
( O
Do O
et O
al O
. O
, O
2011 O
; O
Mirza O
and O
Tonelli O
, O
2014 O
; O
Dunietz O
et O
al O
. O
, O
2017 O
; O
Hosseini O
et O
al O
. O
, O
2021 O
) O
, O
question O
answering O
( O
Oh O
et O
al O
. O
, O
2016 O
) O
, O
or O
commonsense O
reasoning O
Bosselut O
et O
al O
. O
, O
2019 O
) O
. O
For O
a O
recent O
survey O
, O
we O
refer O
to O
Feder O
et O
al O
. O
( O
2021 O
) O
. O

Usage O
of O
MDL B-MetricName
in O
NLP O
Although O
MDL B-MetricName
has O
been O
used O
for O
causal O
discovery O
for O
low O
- O
dimensional O
data O
( O
Budhathoki O
and O
Vreeken O
, O
2017 O
; O
Mian O
et O
al O
. O
, O
2021 O
; O
, O
only O
very O
few O
studies O
adopt O
MDL B-MetricName
on O
high O
- O
dimensional O
NLP O
data O
. O
Most O
existing O
uses O
of O
MDL B-MetricName
on O
NLP O
are O
for O
probing O
and O
interpretability O
: O
e.g. O
, O
Voita O
and O
Titov O
( O
2020 O
) O
use O
it O
for O
probing O
of O
a O
small O
Bayesian O
model O
and O
network O
pruning O
, O
based O
on O
the O
method O
proposed O
by O
Blier O
and O
Ollivier O
( O
2018 O
) O
to O
calculate O
MDL B-MetricName
for O
deep O
learning O
. O
We O
are O
not O
aware O
of O
existing O
work O
using O
MDL B-MetricName
for O
causal O
discovery O
, O
or O
to O
verify O
causal O
concepts O
such O
as O
ICM O
in O
the O
context O
of O
NLP O
. O

Existing O
Discussions O
on O
SSL O
and O
DA O
in O
NLP O
SSL O
and O
DA O
has O
long O
been O
used O
in O
NLP O
, O
as O
reviewed O
by O
Søgaard O
( O
2013 O
) O
and O
Ramponi O
and O
Plank O
( O
2020 O
) O
. O
However O
, O
there O
have O
been O
a O
number O
of O
studies O
that O
report O
negative O
results O
for O
SSL O
Steedman O
et O
al O
. O
, O
2003 O
; O
Reichart O
and O
Rappoport O
, O
2007 O
; O
Abney O
, O
2007 O
; O
Spreyer O
and O
Kuhn O
, O
2009 O
; O
Søgaard O
and O
Rishøj O
, O
2010 O
) O
and O
DA O
( O
Plank O
et O
al O
. O
, O
2014 O
) O
. O
Our O
works O
constitutes O
the O
first O
explanation O
of O
the O
ineffectiveness O
of O
SSL O
and O
DA O
on O
certain O
NLP O
tasks O
from O
the O
perspective O
of O
causal O
and O
anticausal O
learning O
. O

Conclusion O

This O
work O
presents O
the O
first O
effort O
to O
use O
causal O
concepts O
such O
as O
the O
ICM O
principle O
and O
the O
distinction O
between O
causal O
and O
anticausal O
learning O
to O
shed O
light O
on O
some O
commonly O
observed O
trends O
in O
NLP O
. O
Specifically O
, O
we O
provide O
an O
explanation O
of O
observed O
differences O
in O
SSL O
( O
Tabs O
. O
4 O
and O
5 O
) O
and O
DA O
( O
Tab O
. O
6 O
) O
performance O
on O
a O
number O
of O
NLP O
tasks O
: O
DA O
tends O
to O
work O
better O
for O
causal O
learning O
tasks O
, O
whereas O
SSL O
typically O
only O
works O
for O
anticausal O
learning O
tasks O
, O
as O
predicted O
by O
the O
ICM O
principle O
. O
These O
insights O
, O
together O
with O
our O
categorization O
of O
common O
NLP O
tasks O
( O
Tab O
. O
1 O
) O
into O
causal O
and O
anticausal O
learning O
, O
may O
prove O
useful O
for O
future O
NLP O
efforts O
. O
Moreover O
, O
we O
empirically O
confirm O
using O
MDL B-MetricName
that O
the O
description O
of O
data O
is O
typically O
shorter O
in O
the O
causal O
than O
in O
the O
anticausal O
direction O
( O
Tab O
. O
3 O
) O
, O
suggesting O
that O
a O
causal O
footprint O
can O
also O
be O
observed O
for O
text O
data O
. O
This O
has O
interesting O
potential O
implications O
for O
discovering O
causal O
relations O
between O
different O
types O
of O
NLP O
data O
. O

Ethical O
Considerations O

Use O
of O
Data O
This O
paper O
uses O
two O
types O
of O
data O
, O
a O
subset O
of O
existing O
machine O
translation O
dataset O
, O
and O
synthetic O
decipherment O
data O
. O
As O
far O
as O
we O
are O
concerned O
, O
there O
are O
no O
sensitive O
issues O
such O
as O
privacy O
regarding O
the O
data O
usage O
. O

Potential O
Stakeholders O
This O
research O
focuses O
on O
meta O
properties O
of O
two O
commonly O
applied O
methodologies O
, O
SSL O
and O
DA O
in O
NLP O
. O
Although O
this O
research O
is O
not O
directly O
connected O
to O
specific O
applications O
in O
society O
, O
the O
usage O
of O
this O
study O
can O
benefit O
future O
research O
in O
SSL O
and O
DA O
. O

Acknowledgements O

We O
thank O
Simon O
Buchholz O
for O
helpful O
discussions O
, O
Nasim O
Rahaman O
, O
Shehzaad O
Dhuliawala O
, O
Yifan O
Hou O
, O
Tiago O
Pimentel O
, O
and O
the O
anonymous O
reviewers O
for O
feedback O
on O
the O
manuscript O
, O
and O
Di O
Jin O
for O
helping O
with O
computational O
resources O
. O
This O
work O
was O
supported O
by O
the O
German O
Federal O
Ministry O
of O
Education O
and O
Research O
( O
BMBF O
) O
: O
Tübingen O
AI O
Center O
, O
FKZ O
: O
01IS18039B O
, O
and O
by O
the O
Machine O
Learning O
Cluster O
of O
Excellence O
, O
EXC O
number O
2064 O
/ O
1 O
-Project O
number O
390727645 O
. O

A O
Meta O
Study O
Settings O
of O
SSL O
and O
DA O

For O
the O
meta O
study O
of O
SSL O
, O
we O
covered O
but O
are O
not O
limited O
to O
all O
relevant O
papers O
cited O
by O
the O
review O
on O
NLP O
SSL O
by O
Søgaard O
( O
2013 O
) O
. O
We O
went O
through O
the O
leaderboard O
of O
many O
NLP O
tasks O
and O
covered O
the O
SSL O
papers O
listed O
on O
the O
leaderboards O
. O
The O
papers O
covered O
by O
our O
meta O
study O
are O
available O
on O
our O
GitHub O
. O

For O
supervised O
DA O
, O
we O
searched O
papers O
with O
the O
keyword O
domain O
adaptation O
and O
task O
names O
from O
a O
wide O
range O
of O
tasks O
that O
use O
supervised O
DA O
. O

Note O
that O
for O
fair O
comparison O
, O
we O
do O
not O
consider O
papers O
without O
a O
comparable O
supervised O
baseline O
corresponding O
to O
the O
SSL O
, O
or O
a O
comparable O
unadapted O
baseline O
corresponding O
to O
the O
DA O
. O
We O
do O
not O
consider O
MT O
DA O
which O
tackles O
the O
out O
- O
ofvocabulary O
( O
OOV O
) O
problem O
because O
P O
( O
E|C O
) O
may O
be O
different O
for O
OOV O
( O
Habash O
, O
2008 O
; O
III O
and O
Jagarlamudi O
, O
2011 O
) O
. O

B O
Experimental O
Details O
of O
Minimum B-MetricName
Description I-MetricName
Length I-MetricName

We O
calculate O
the O
MDL B-MetricName
( O
X O
) O
and O
MDL B-MetricName
( O
Y O
) O
by O
a O
language O
model O
, O
and O
obtain O
MDL B-MetricName
( O
X|Y O
) O
and O
MDL B-MetricName
( O
Y|X O
) O
using O
translation O
models O
. O
For O
language O
model O
, O
we O
use O
the O
autoregressive O
GPT2 B-MethodName
( O
Radford O
et O
al O
. O
, O
2019 O
) O
, O
and O
for O
the O
translation O
model O
, O
we O
the O
Marian B-MethodName
Neural O
Machine O
Translation O
model O
( O
Junczys O
- O
Dowmunt O
et O
al O
. O
, O
2018 O
) O
trained O
on O
the O
OPUS O
Corpus O
( O
Tiedemann O
and O
Nygaard O
, O
2004 O
) O
. O
Both O
these O
models O
use O
the O
layers O
from O
the O
transformer O
model O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
. O
The O
autoregressive O
language O
model O
consists O
only O
of O
decoder O
layers O
, O
whereas O
the O
translation O
model O
used O
six O
encoder O
and O
six O
decoder O
layers B-HyperparameterName
. O
Both O
of O
these O
models O
have O
roughly O
the O
same O
number O
of O
parameters O
. O
We O
used O
the O
huggingface O
implementation O
( O
Wolf O
et O
al O
. O
, O
2020 O
) O
of O
these O
models O
for O
their O
respective O
set O
of O
languages O
. O

How O
Gender O
Debiasing O
Affects O
Internal O
Model O
Representations O
, O
and O
Why O
It O
Matters O

Common O
studies O
of O
gender O
bias O
in O
NLP O
focus O
either O
on O
extrinsic O
bias O
measured O
by O
model O
performance O
on O
a O
downstream O
task O
or O
on O
intrinsic O
bias O
found O
in O
models O
' O
internal O
representations O
. O
However O
, O
the O
relationship O
between O
extrinsic O
and O
intrinsic O
bias O
is O
relatively O
unknown O
. O
In O
this O
work O
, O
we O
illuminate O
this O
relationship O
by O
measuring O
both O
quantities O
together O
: O
we O
debias O
a O
model O
during O
downstream O
fine O
- O
tuning O
, O
which O
reduces O
extrinsic O
bias O
, O
and O
measure O
the O
effect O
on O
intrinsic O
bias O
, O
which O
is O
operationalized O
as O
bias O
extractability O
with O
information O
- O
theoretic O
probing O
. O
Through O
experiments O
on O
two O
tasks O
and O
multiple O
bias O
metrics O
, O
we O
show O
that O
our O
intrinsic O
bias O
metric O
is O
a O
better O
indicator O
of O
debiasing O
than O
( O
a O
contextual O
adaptation O
of O
) O
the O
standard O
WEAT O
metric O
, O
and O
can O
also O
expose O
cases O
of O
superficial O
debiasing O
. O
Our O
framework O
provides O
a O
comprehensive O
perspective O
on O
bias O
in O
NLP O
models O
, O
which O
can O
be O
applied O
to O
deploy O
NLP O
systems O
in O
a O
more O
informed O
manner O
. O
1 O
* O
Supported O
by O
the O
Viterbi O
Fellowship O
in O
the O
Center O
for O
Computer O
Engineering O
at O
the O
Technion O
. O

Introduction O

Efforts O
to O
identify O
and O
mitigate O
gender O
bias O
in O
Natural O
Language O
Processing O
( O
NLP O
) O
systems O
typically O
target O
one O
of O
two O
notions O
of O
bias O
. O
Extrinsic O
evaluation O
methods O
and O
debiasing O
techniques O
focus O
on O
the O
bias O
reflected O
in O
a O
downstream O
task O
Zhao O
et O
al O
. O
, O
2018 O
) O
, O
while O
intrinsic O
methods O
focus O
on O
a O
model O
's O
internal O
representations O
, O
such O
as O
word O
or O
sentence O
embedding O
geometry O
( O
Caliskan O
et O
al O
. O
, O
2017 O
; O
Bolukbasi O
et O
al O
. O
, O
2016 O
; O
Guo O
and O
Caliskan O
, O
2021 O
) O
. O
Despite O
an O
abundance O
of O
evidence O
pointing O
towards O
gender O
bias O
in O
pretrained O
language O
models O
( O
LMs O
) O
, O
the O
extent O
of O
harm O
caused O
by O
these O
biases O
is O
not O
clear O
when O
it O
is O
not O
reflected O
in O
a O
specific O
downstream O
task O
( O
Barocas O
Figure O
1 O
: O
Our O
proposed O
framework O
. O
Black O
arrows O
mark O
forward O
passes O
, O
red O
arrows O
mark O
things O
we O
measure O
. O
We O
first O
( O
a O
) O
train O
a O
model O
on O
a O
downstream O
task O
, O
then O
( O
b O
) O
train O
another O
model O
on O
the O
same O
task O
using O
a O
debiased O
dataset O
, O
and O
finally O
( O
c O
) O
measure O
intrinsic O
bias O
in O
both O
models O
and O
compare O
. O
Kate O
Crawford O
, O
2017 O
; O
Blodgett O
et O
al O
. O
, O
2020 O
; O
Bommasani O
et O
al O
. O
, O
2021 O
) O
. O
For O
instance O
, O
while O
the O
word O
embedding O
proximity O
of O
" O
doctor O
" O
to O
" O
man O
" O
and O
" O
nurse O
" O
to O
" O
woman O
" O
is O
intuitively O
normatively O
wrong O
, O
it O
is O
not O
clear O
when O
such O
phenomena O
would O
lead O
to O
downstream O
predictions O
manifesting O
in O
social O
biases O
. O
Recently O
, O
Goldfarb O
- O
Tarrant O
et O
al O
. O
( O
2021 O
) O
have O
shown O
that O
debiasing O
static O
embeddings O
intrinsically O
is O
not O
correlated O
with O
extrinsic O
gender O
bias O
measures O
, O
but O
the O
nature O
of O
the O
reverse O
relationship O
is O
unknown O
: O
how O
are O
extrinsic O
interventions O
reflected O
in O
intrinsic O
representations O
? O
Furthermore O
, O
Gonen O
and O
Goldberg O
( O
2019a O
) O
demonstrated O
that O
a O
number O
of O
intrinsic O
debiasing O
methods O
applied O
to O
static O
embeddings O
only O
partially O
remove O
the O
bias O
and O
that O
most O
of O
it O
is O
still O
hidden O
within O
the O
embed O
- O
ding O
. O
Complementing O
their O
view O
, O
we O
examine O
extrinsic O
debiasing O
methods O
, O
as O
well O
as O
demonstrate O
the O
possible O
harm O
this O
could O
cause O
. O
Contrary O
to O
their O
conclusion O
, O
we O
do O
not O
claim O
that O
these O
debiasing O
methods O
should O
not O
be O
trusted O
, O
as O
long O
as O
they O
are O
utilized O
with O
care O
. O

Our O
goal O
is O
to O
gain O
a O
better O
understanding O
of O
the O
relationship O
between O
a O
model O
's O
internal O
representations O
and O
its O
extrinsic O
gender O
bias O
by O
examining O
the O
effects O
of O
various O
debiasing O
methods O
on O
the O
model O
's O
representations O
. O
Specifically O
, O
we O
fine O
- O
tune O
models O
with O
and O
without O
gender O
debiasing O
strategies O
, O
evaluate O
their O
external O
bias O
using O
various O
bias O
metrics O
, O
and O
measure O
intrinsic O
bias O
in O
the O
representations O
. O
We O
operationalize O
intrinsic O
bias O
via O
two O
metrics O
: O
First O
, O
we O
use O
CEAT B-MetricName
( O
Guo O
and O
Caliskan O
, O
2021 O
) O
, O
a O
contextual O
adaptation O
of O
the O
widely O
used O
intrinsic O
bias O
metric O
WEAT O
( O
Caliskan O
et O
al O
. O
, O
2017 O
) O
. O
Second O
, O
we O
propose O
to O
use O
an O
information O
- O
theoretic O
probe O
to O
quantify O
the O
degree O
to O
which O
gender O
can O
be O
extracted O
from O
the O
internal O
model O
representations O
. O
Then O
, O
we O
examine O
how O
these O
intrinsic O
metrics O
correlate O
with O
a O
variety O
of O
extrinsic O
bias O
metrics O
that O
we O
measure O
on O
the O
model O
's O
downstream O
performance O
. O
Our O
approach O
is O
visualised O
in O
Figure O
1 O
. O

We O
perform O
extensive O
experiments O
on O
two O
downstream O
tasks O
( O
occupation B-TaskName
prediction I-TaskName
and O
coreference B-TaskName
resolution I-TaskName
) O
; O
several O
debiasing O
strategies O
that O
involve O
alterations O
to O
the O
training O
dataset O
( O
such O
as O
removing O
names O
and O
gender O
indicators O
, O
or O
balancing O
the O
data O
by O
oversampling O
or O
downsampling O
) O
; O
and O
a O
multitude O
of O
extrinsic O
bias O
metrics O
. O
Our O
analysis O
reveals O
new O
insights O
into O
the O
way O
language O
models O
encode O
and O
use O
information O
on O
gender O
: O

• O
The O
effect O
of O
debiasing O
on O
internal O
representations O
is O
reflected O
in O
gender O
extractability O
, O
while O
not O
always O
in O
CEAT O
. O
Thus O
, O
gender O
extractability O
is O
a O
more O
reliable O
indicator O
of O
gender O
bias O
in O
NLP O
models O
. O

• O
In O
cases O
of O
high O
gender O
extractability O
but O
low O
extrinsic O
bias O
metrics O
, O
the O
debiasing O
is O
superficial O
, O
and O
the O
internal O
representations O
are O
a O
good O
indicator O
for O
this O
: O
The O
bias O
is O
still O
present O
in O
internal O
representations O
and O
can O
be O
restored O
by O
retraining O
the O
classification O
layer O
. O
Therefore O
, O
our O
proposed O
measuring O
method O
can O
help O
in O
detecting O
such O
cases O
before O
deploying O
the O
model O
. O

• O
The O
two O
tasks O
show O
different O
patterns O
of O
correlation O
between O
intrinsic O
and O
extrinsic O
bias O
. O

The O
coreference B-TaskName
task O
exhibits O
a O
high O
correlation O
. O
The O
occupation B-TaskName
prediction I-TaskName
task O
exhibits O
a O
lower O
correlation O
, O
but O
it O
increases O
after O
retraining O
( O
a O
case O
of O
superficial O
debiasing O
) O
. O
Gender O
extractability O
shows O
higher O
correlations O
with O
extrinsic O
metrics O
than O
CEAT O
, O
increasing O
the O
confidence O
in O
this O
metric O
as O
a O
reliable O
measure O
for O
gender O
bias O
in O
NLP O
models O
. O

Methodology O

In O
this O
study O
, O
we O
investigate O
the O
relationship O
between O
extrinsic O
bias O
metrics O
of O
a O
task O
and O
a O
model O
's O
internal O
representations O
, O
under O
various O
debiasing O
conditions O
, O
for O
two O
datasets O
in O
English O
. O
We O
perform O
extrinsic O
debiasing O
, O
evaluate O
various O
extrinsic O
and O
intrinsic O
bias O
metrics O
before O
and O
after O
debiasing O
, O
and O
examine O
correlations O
. O

Dataset O
. O
Let O
D O
= O
{ O
X O
, O
Y O
, O
Z O
} O
be O
a O
dataset O
consisting O
of O
input O
data O
X O
, O
labels O
Y O
and O
protected O
attributes O
Z. O
2 O
This O
work O
focuses O
on O
gender O
as O
the O
protected O
attribute O
z. O
In O
all O
definitions O
, O
F O
and O
M O
indicate O
female O
and O
male O
gender O
, O
respectively O
, O
as O
the O
value O
of O
the O
protected O
attribute O
z O
. O

Trained O
Model O
. O
The O
model O
is O
optimized O
to O
solve O
the O
downstream O
task O
posed O
by O
the O
dataset O
. O
It O
can O
be O
formalized O
as O
f O
• O
g O
: O
X O
→ O
R O
|Y| O
, O
where O
g O
( O
• O
) O
is O
the O
feature O
extractor O
, O
implemented O
by O
a O
language O
model O
, O
e.g. O
, O
RoBERTa B-MethodName
( O
Liu O
et O
al O
. O
, O
2019 O
) O
, O
f O
( O
• O
) O
is O
the O
classification O
function O
, O
and O
Y O
is O
the O
set O
of O
the O
possible O
labels O
for O
the O
task O
. O

Bias O
Metrics O

Each O
bias O
evaluation O
method O
described O
in O
the O
literature O
can O
be O
categorized O
as O
extrinsic O
or O
intrinsic O
. O
In O
all O
definitions O
, O
r O
indicates O
the O
model O
's O
output O
probabilities O
. O

Extrinsic O
Metrics O

Extrinsic O
methods O
involve O
measuring O
the O
bias O
of O
a O
model O
solving O
a O
downstream O
problem O
. O
The O
extrinsic O
metric O
is O
a O
function O
: O

E O
( O
X O
, O
Y O
, O
R O
, O
Z O
) O
∈ O
R O

The O
output O
represents O
the O
quantity O
of O
bias O
measured O
; O
the O
further O
from O
0 O
the O
number O
is O
, O
the O
larger O
the O
bias O
is O
. O
Our O
analysis O
comprises O
a O
wide O
range O
of O
extrinsic O
metrics O
, O
including O
some O
that O
have O
been O
measured O
in O
the O
past O
on O
the O
analyzed O
tasks O
( O
Zhao O
et O
al O
. O
, O
2018 O
; O
Ravfogel O
et O
al O
. O
, O
2020 O
; O
Goldfarb O
- O
Tarrant O
et O
al O
. O
, O
2021 O
) O
and O
some O
that O
have O
never O
been O
measured O
before O
, O
and O
shows O
our O
results O
apply O
to O
many O
of O
them O
. O
For O
illustration O
, O
we O
will O
consider O
occupation O
prediction O
, O
a O
common O
task O
in O
research O
on O
gender O
bias O
Ravfogel O
et O
al O
. O
, O
2020 O
; O
. O
The O
input O
x O
is O
a O
biography O
and O
the O
prediction O
y O
is O
the O
profession O
of O
the O
person O
described O
in O
it O
. O
The O
protected O
attribute O
z O
is O
the O
gender O
of O
that O
person O
. O

Performance O
gap O
. O
This O
is O
the O
difference O
in O
performance O
metric O
for O
two O
different O
groups O
, O
for O
instance O
two O
groups O
of O
binary O
genders O
, O
or O
a O
group O
of O
pro O
- O
stereotypical O
and O
a O
group O
of O
anti O
- O
stereotypical O
examples O
. O
We O
measure O
the O
following O
metrics O
: O
True B-MetricName
Positive I-MetricName
Rate I-MetricName
( O
TPR B-MetricName
) O
, O
False B-MetricName
Positive I-MetricName
Rate I-MetricName
( O
FPR B-MetricName
) O
, O
and O
Precision B-MetricName
. O
In O
occupation B-TaskName
prediction I-TaskName
, O
for O
instance O
, O
the O
TPR B-MetricName
gap O
for O
each O
profession O
y O
expresses O
the O
difference O
in O
the O
percentage O
of O
women O
and O
men O
whose O
profession O
is O
y O
and O
are O
correctly O
classified O
as O
such O
. O
We O
also O
measure O
F1 O
of O
three O
standard O
clustering O
metrics O
for O
coreference O
resolution O
. O
Each O
such O
performance O
gap O
captures O
a O
different O
facet O
of O
gender O
bias O
, O
and O
one O
might O
be O
more O
interested O
in O
one O
of O
the O
metrics O
depending O
on O
the O
application O
. O
We O
compute O
two O
types O
of O
performance O
gap O
metrics O
: O
( O
1 O
) O
the O
sum B-MetricName
of I-MetricName
absolute I-MetricName
gap I-MetricName
values O
over O
all O
classes O
; O
( O
2 O
) O
the O
Pearson B-MetricName
correlation I-MetricName
between O
the O
performance O
gap O
for O
a O
class O
and O
the O
percentage O
of O
women O
in O
that O
class O
. O
For O
instance O
, O
if O
y O
is O
a O
profession O
, O
we O
measure O
the O
correlation O
between O
performance O
gaps O
and O
percentages O
of O
women O
in O
each O
profession O
. O
3 O
The O
two O
metrics O
are O
closely O
related O
but O
answer O
slightly O
different O
questions O
: O
the O
sum O
quantifies O
how O
a O
model O
behaves O
differently O
on O
different O
genders O
, O
and O
the O
correlation O
shows O
the O
relation O
of O
model O
behaviour O
to O
social O
biases O
( O
in O
the O
world O
or O
the O
data O
) O
without O
regard O
to O
actual O
gap O
size O
. O

Statistical O
metrics O
. O
For O
breadth O
of O
analysis O
, O
we O
examine O
three O
additional O
statistical O
metrics O
( O
Barocas O
et O
al O
. O
, O
2019 O
) O
, O
which O
correspond O
to O
different O
notions O
of O
bias O
. O
All O
three O
are O
measured O
as O
differences O
( O
d O
) O
between O
two O
probability O
distributions O
, O
and O
we O
then O
obtain O
a O
single O
bias O
quantity O
per O
metric O
by O
summing O
all O
computed O
distances O
. O

• O
Independence B-MetricName
: O
d O
P O
( O
r|z O
= O
z O
) O
, O
P O
( O
r O
) O
∀z O
∈ O
{ O
F O
, O
M O
} O
. O
For O
instance O
, O
we O
measure O
the O
difference O
between O
the O
distribution O
of O
model O
's O
predictions O
on O
women O
and O
the O
distribution O
of O
all O
predictions O
. O
Independence O
is O
stronger O
as O
the O
prediction O
r O
is O
less O
correlated O
with O
the O
protected O
attribute O
z. O
It O
is O
measured O
with O
no O
relation O
to O
the O
gold O
labels O
. O

• O
Separation B-MetricName
: O
d O
P O
( O
r|y O
= O
y O
, O
z O
= O
z O
) O
, O
P O
( O
r|y O
= O
y O
) O
∀y O
∈ O
Y O
, O
z O
∈ O
{ O
F O
, O
M O
} O
. O
For O
instance O
, O
we O
measure O
the O
difference O
between O
the O
distribution O
of O
a O
model O
's O
predictions O
on O
women O
who O
are O
teachers O
and O
the O
distribution O
of O
predictions O
on O
all O
teachers O
. O
It O
encapsulates O
the O
TPR O
and O
FPR O
gaps O
discussed O
previously O
, O
and O
can O
be O
seen O
as O
a O
more O
general O
metric O
. O

• O
Sufficiency B-MetricName
: O
d O
P O
( O
y|r O
= O
r O
, O
z O
= O
z O
) O
, O
P O
( O
y|r O
= O
r O
) O
. O
For O
instance O
, O
we O
measure O
the O
difference O
between O
the O
distribution O
of O
gold O
labels O
on O
women O
classified O
as O
teachers O
by O
the O
model O
and O
the O
distribution O
of O
gold O
labels O
on O
all O
individuals O
classified O
as O
teachers O
by O
the O
model O
. O
Sufficiency O
relates O
to O
the O
concept O
of O
calibration O
in O
classification O
. O
A O
difference O
in O
the O
classifier O
's O
scores O
for O
men O
and O
for O
women O
indicates O
that O
it O
might O
be O
penalizing O
or O
over O
- O
promoting O
one O
of O
the O
genders O
. O

Intrinsic O
Metrics O

Intrinsic O
methods O
are O
applied O
to O
the O
representation O
obtained O
from O
the O
feature O
extractor O
. O
These O
methods O
are O
independent O
of O
any O
downstream O
task O
. O
The O
intrinsic O
metric O
is O
a O
function O
: O

I O
( O
g O
( O
X O
) O
, O
Z O
) O
∈ O
R O
Compression B-MetricName
. O

Our O
main O
intrinsic O
metric O
is O
the O
compression B-MetricName
of O
gender O
information O
evaluated O
by O
a O
minimum O
description O
length O
( O
MDL O
) O
probing O
classifier O
( O
Voita O
and O
Titov O
, O
2020 O
) O
, O
trained O
to O
predict O
gender O
from O
the O
model O
's O
representations O
. O
Probing O
classifiers O
are O
widely O
used O
for O
predicting O
various O
properties O
of O
interest O
from O
frozen O
model O
representations O
( O
Belinkov O
and O
Glass O
, O
2019 O
) O
. O
MDL O
probes O
were O
proposed O
because O
a O
probe O
's O
accuracy O
may O
be O
misleading O
due O
to O
memorization O
and O
other O
issues O
( O
Hewitt O
and O
Liang O
, O
2019 O
; O
Belinkov O
, O
2021 O
) O
. O
We O
use O
the O
MDL O
online O
code O
, O
where O
the O
probe O
is O
trained O
in O
timesteps O
, O
on O
increasing O
subsets O
of O
the O
training O
set O
, O
then O
evaluated O
against O
the O
rest O
of O
it O
. O
Higher O
compression O
indicates O
greater O
gender O
extractability O
. O

CEAT B-MetricName
. O
We O
also O
measure O
CEAT B-MetricName
( O
Guo O
and O
Caliskan O
, O
2021 O
) O
, O
which O
is O
a O
contextualized O
version O
of O
WEAT O
( O
Caliskan O
et O
al O
. O
, O
2017 O
) O
, O
a O
widely O
used O
bias O
metric O
for O
static O
word O
embeddings O
. O
WEAT O
defines O
sets O
X O
and O
Y O
of O
target O
words O
, O
and O
sets O
A O
and O
B O
of O
attribute O
words O
. O
For O
instance O
, O
A O
and O
B O
contain O
males O
and O
females O
names O
, O
while O
X O
and O
Y O
contain O
career O
and O
family O
related O
words O
, O
respectively O
. O
The O
bias O
is O
operationalized O
as O
the O
geometric O
proximity O
between O
the O
target O
and O
attribute O
word O
embeddings O
, O
and O
is O
quantified O
in O
CEAT B-MetricName
by O
the O
Combined O
Effect O
Size O
( O
CES O
) O
and O
a O
p O
- O
value O
for O
the O
null O
hypothesis O
of O
having O
no O
biased O
associations O
. O
For O
more O
information O
on O
CEAT B-MetricName
refer O
to O
Appendix O
A.4.3 O
. O

Debiasing O
Techniques O

We O
debias O
models O
by O
modifying O
the O
downstream O
task O
's O
training O
data O
before O
fine O
- O
tuning O
. O
Scrubbing O
( O
De O
- O
Arteaga O
et O
al O
. O
, O
2019 O
) O
removes O
first O
names O
and O
gender O
- O
specific O
terms O
( O
" O
he O
" O
, O
" O
she O
" O
, O
" O
husband O
" O
, O
" O
wife O
" O
, O
" O
Mr O
" O
, O
" O
Mrs O
" O
, O
etc O
. O
) O
. O
Balancing O
subsamples O
or O
oversamples O
examples O
such O
that O
each O
gender O
is O
equally O
represented O
in O
the O
resulting O
dataset O
w.r.t O
each O
label O
. O
Anonymization O
( O
Zhao O
et O
al O
. O
, O
2018 O
) O
removes O
named O
entities O
. O
Counterfactual O
Augmentation O
( O
Zhao O
et O
al O
. O
, O
2018 O
) O
involves O
replacing O
male O
entities O
in O
an O
example O
with O
female O
entities O
, O
and O
adding O
the O
modified O
example O
to O
the O
training O
set O
. O
As O
some O
of O
these O
are O
dataset O
/ O
task O
- O
specific O
, O
we O
give O
more O
details O
in O
the O
following O
section O
. O

Experiments O

In O
each O
experiment O
, O
we O
fine O
- O
tune O
a O
model O
for O
a O
downstream O
task O
. O
For O
training O
, O
we O
use O
either O
the O
original O
dataset O
or O
a O
dataset O
debiased O
with O
one O
of O
the O
methods O
from O
Section O
2.2 O
. O
Figure O
2 O
presents O
examples O
of O
debiasing O
methods O
for O
the O
two O
downstream O
tasks O
. O
We O
measure O
two O
intrinsic O
metrics O
by O
probing O
that O
model O
's O
inner O
representations O
for O
gender O
extractability O
( O
as O
measured O
by O
MDL O
) O
and O
by O
CEAT O
, O
and O
test O
various O
extrinsic O
metrics O
. O
The O
relation O
between O
one O
intrinsic O
and O
one O
extrinsic O
metric O
becomes O
one O
data O
point O
, O
and O
we O
repeat O
over O
many O
random O
seeds O
( O
for O
both O
the O
model O
and O
the O
probe O
) O
. O
Further O
implementation O
details O
are O
in O
appendix O
A O
. O

Occupation B-TaskName
Prediction I-TaskName

The O
task O
of O
occupation B-TaskName
prediction I-TaskName
is O
to O
predict O
a O
person O
's O
occupations O
( O
from O
a O
closed O
set O
) O
, O
based O
on O
their O
biography O
. O
We O
use O
the O
Bias O
in O
Bios O
dataset O
. O
Regardless O
of O
the O
training O
method O
, O
the O
test O
set O
is O
subsampled O
such O
that O
each O
profession O
has O
equal O
gender O
representation O
. O

Britney O
currently O
works O
on O
CNN O
's O
newest O
primetime O
show O
. O
She O
has O
also O
written O
for O
the O
New O
York O
Times O
. O

_ O
currently O
works O
on O
CNN O
's O
newest O
primetime O
show O
. O
_ O
has O
also O
written O
for O
the O
New O
York O
Times O
. O

Scrubbing O

My O
sister O
is O
taking O
a O
painting O
class O
this O
summer O
, O
so O
she O
has O
been O
sharing O
the O
latest O
lesson O
with O
me O
. O

My O
brother O
is O
taking O
a O
painting O
class O
this O
summer O
, O
so O
he O
has O
been O
sharing O
the O
latest O
lesson O
with O
me O
. O

Counterfactual O
augmentation O
Occupation O
Classification O
Coreference B-TaskName
Resolution I-TaskName
Original O
dataset O

Original O
dataset O
Model O
. O
Our O
main O
model O
is O
a O
RoBERTa B-MethodName
model O
( O
Liu O
et O
al O
. O
, O
2019 O
) O
topped O
with O
a O
linear O
classifier O
, O
which O
receives O
the O
[ O
CLS O
] O
token O
embedding O
as O
input O
and O
generates O
a O
probability O
distribution O
over O
the O
professions O
. O
In O
addition O
, O
we O
experiment O
with O
training O
a O
baseline O
classifier O
layer O
on O
top O
of O
a O
frozen O
, O
non O
- O
finetuned O
RoBERTa B-MethodName
. O
We O
also O
replicate O
our O
RoBERTa B-MethodName
experiments O
with O
a O
DeBERTa B-MethodName
model O
( O
He O
et O
al O
. O
, O
2020 O
) O
, O
to O
verify O
that O
our O
results O
are O
are O
not O
model O
specific O
and O
hold O
more O
broadly O
. O

Debiasing O
Techniques O
. O
Following O
De O
- O
Arteaga O
et O
al O
. O
( O
2019 O
) O
we O
experiment O
with O
scrubbing O
the O
training O
dataset O
. O
Figure O
2 O
shows O
an O
example O
biography O
snippet O
and O
its O
scrubbed O
version O
. O
We O
also O
conduct O
balancing O
( O
per O
profession O
, O
subsampling O
and O
oversampling O
to O
ensure O
an O
equal O
number O
of O
males O
and O
females O
per O
profession O
) O
, O
which O
has O
not O
previously O
been O
used O
on O
this O
dataset O
and O
task O
. O

Metrics O
. O
We O
measure O
all O
bias O
metrics O
from O
Section O
2.1 O
except O
for O
F1 O
. O

Probing O
. O
The O
probing O
dataset O
for O
this O
task O
is O
the O
test O
set O
, O
and O
the O
gender O
label O
of O
a O
single O
biography O
is O
the O
gender O
of O
the O
person O
described O
in O
it O
. O
We O
probe O
the O
[ O
CLS O
] O
token O
representation O
of O
the O
biography O
. O
In O
addition O
to O
the O
models O
described O
above O
, O
we O
measure O
baseline O
extractability O
of O
gender O
information O
from O
a O
randomly O
initialized O
RoBERTa B-MethodName
model O
. O

Coreference O
Resolution O

The O
task O
of O
coreference B-TaskName
resolution I-TaskName
is O
to O
find O
all O
textual O
expressions O
referring O
to O
the O
same O
real O
- O
world O
entities O
. O
We O
train O
on O
Ontonotes O
5.0 O
( O
Weischedel O
et O
al O
. O
, O
2013 O
) O
and O
test O
on O
the O
Winobias O
challenge O
dataset O
( O
Zhao O
et O
al O
. O
, O
2018 O
) O
. O
Winobias O
consists O
of O
sentence O
pairs O
, O
pro O
- O
and O
anti O
- O
stereotypical O
variants O
, O
with O
individuals O
referred O
to O
by O
their O
profession O
. O
For O
example O
, O
" O
The O
physician O
hired O
the O
secretary O
be- O
cause O
he O
/ O
she O
was O
busy O
. O
" O
is O
pro O
/ O
anti O
- O
stereotypical O
, O
based O
on O
US O
labor O
statistics O
. O
4 O
A O
coreference O
system O
is O
measured O
by O
the O
performance O
gap O
between O
the O
pro O
- O
and O
anti O
- O
stereotypical O
subsets O
. O

Model O
. O
We O
use O
the O
model O
presented O
in O
Lee O
et O
al O
. O
( O
2018a O
) O
with O
RoBERTa B-MethodName
as O
a O
feature O
extractor O
. O

Debiasing O
Techniques O
. O
Following O
Zhao O
et O
al O
. O
( O
2018 O
) O
, O
we O
apply O
anonymization O
( O
denoted O
as O
Anon O
) O
and O
counterfactual O
augmentation O
( O
CA O
) O
on O
the O
training O
set O
. O
These O
techniques O
were O
used O
jointly O
in O
previous O
work O
; O
we O
examine O
each O
individually O
as O
well O
. O

Metrics O
. O
Following O
Zhao O
et O
al O
. O
( O
2018 O
) O
, O
we O
measure O
the O
F1 O
difference O
between O
anti O
- O
and O
prostereotypical O
examples O
. O
5 O
We O
also O
interpret O
the O
task O
as O
a O
classification O
problem O
, O
and O
measure O
all O
metrics O
from O
Section O
2.1 O
. O
For O
more O
details O
refer O
to O
Appendix O
A.4.2 O
. O

Probing O
. O
We O
probe O
the O
representation O
of O
a O
profession O
word O
as O
extracted O
from O
Winobias O
sentences O
, O

Results O

Tables O
1a O
and O
1b O
present O
intrinsic O
and O
extrinsic O
metrics O
for O
RoBERTa B-MethodName
models O
on O
the O
occupation O
prediction O
and O
coreference O
resolution O
tasks O
, O
respectively O
. O
We O
present O
a O
representative O
subset O
of O
the O
measured O
metrics O
that O
demonstrate O
the O
observed O
phenomena O
; O
full O
results O
are O
found O
in O
Appendix O
B. O
The O
DeBERTa B-MethodName
model O
results O
are O
consistent O
with O
the O
RoBERTa B-MethodName
model O
trends O
. O

Compression O
Reflects O
Debiasing O
Effects O

As O
shown O
in O
the O
tables O
, O
compression O
captures O
differences O
in O
models O
that O
were O
debiased O
differently O
. O
CEAT B-MetricName
, O
however O
, O
can O
not O
differentiate O
between O
occupation O
prediction O
models O
. O
For O
example O
, O
in O
occupation O
prediction O
( O
Table O
1a O
) O
the O
compression O
rate O
varies O
significantly O
between O
a O
non O
- O
debiased O
and O
a O
debiased O
model O
via O
scrubbing O
and O
oversampling O
, O
while O
CEAT B-MetricName
detects O
no O
difference O
between O
the O
models O
. O
In O
coreference O
resolution O
( O
Table O
1b O
) O
, O
both O
compression O
and O
CEAT B-MetricName
are O
able O
to O
identify O
differences O
between O
the O
non O
- O
debiased O
model O
and O
the O
others O
, O
such O
as O
CA O
, O
which O
has O
both O
a O
lower O
compression O
and O
CEAT B-MetricName
effect O
. O
But O
the O
CEAT B-MetricName
effect O
sizes O
are O
small O
( O
below O
0.5 O
) O
, O
which O
implies O
no O
bias O
, O
in O
contrast O
to O
the O
extrinsic O
metrics O
. O

High O
Gender O
Extractability O
Implies O
Superficial O
Debiasing O

Extrinsic O
and O
intrinsic O
effects O
of O
debiasing O
. O
In O
occupation O
classification O
( O
1a O
and O
1b O
also O
compare O
extrinsic O
metrics O
before O
and O
after O
retraining O
. O
All O
models O
show O
bias O
restoration O
, O
due O
to O
the O
classification O
layer O
being O
trained O
on O
the O
biased O
dataset O
. O
6 O
The O
amount O
of O
bias O
restored O
varies O
between O
models O
in O
a O
way O
that O
is O
predictable O
by O
the O
compression B-MetricName
metric O
. O

In O
the O
occupation B-TaskName
prediction I-TaskName
task O
, O
comparing O
Before O
and O
After O
numbers O
in O
Table O
1a O
, O
the O
model O
fine O
- O
tuned O
using O
a O
scrubbed O
dataset O
- O
which O
has O
the O
lowest O
compression O
rate O
- O
displays O
the O
least O
bias O
restoration O
, O
confirming O
that O
the O
LM O
absorbed O
the O
process O
of O
debiasing O
. O
The O
model O
fine O
- O
tuned O
on O
subsampled O
data O
has O
higher O
extrinsic O
bias O
after O
retraining O
. O
Hence O
, O
the O
debiasing O
was O
primarily O
cosmetic O
, O
and O
the O
representations O
within O
the O
LM O
were O
not O
debiased O
. O
The O
model O
fine O
- O
tuned O
on O
oversampled O
data O
- O
which O
has O
the O
highest O
compression O
- O
has O
the O
highest O
extrinsic O
bias O
( O
except O
for O
FPR O
) O
, O
even O
though O
this O
was O
not O
true O
before O
retraining O
. O

In O
coreference B-TaskName
resolution I-TaskName
, O
comparing O
Before O
and O
After O
numbers O
in O
Table O
1b O
, O
models O
with O
the O
least O
extrinsic O
bias O
( O
CA O
and O
CA+Anon O
) O
are O
also O
least O
biased O
after O
retraining O
. O
Compression B-MetricName
rate O
predicted O
this O
; O
these O
models O
also O
had O
lower O
compression B-MetricName
rates O
than O
non O
- O
debiased O
models O
. O
Interestingly O
, O
the O
model O
fine O
- O
tuned O
with O
an O
anonymized O
dataset O
is O
the O
most O
biased O
after O
retraining O
, O
consistent O
with O
its O
high O
compression B-MetricName
rate O
relative O
to O
the O
other O
models O
. O
As O
with O
subsampling O
and O
oversampling O
in O
occupation B-TaskName
prediction I-TaskName
, O
anonymization O
's O
( O
lack O
of O
) O
effect O
on O
extrinsic O
metrics O
was O
cosmetic O
( O
compare O
None O
and O
Anon O
in O
Before O
block O
, O
Table O
1b O
) O
. O
Anonymization O
actually O
had O
a O
biasing O
effect O
on O
the O
LM O
, O
which O
was O
realized O
after O
retraining O
. O

We O
conclude O
that O
compression B-MetricName
rate O
is O
a O
useful O
indicator O
of O
superficial O
debiasing O
, O
and O
can O
potentially O
be O
used O
to O
verify O
and O
gain O
confidence O
in O
attempts O
to O
debias O
an O
NLP O
model O
, O
especially O
when O
there O
is O
little O
or O
no O
testing O
data O
. O
retraining O
. O
In O
occupation B-TaskName
prediction I-TaskName
, O
certain O
extrinsic O
metrics O
have O
a O
weak O
correlation O
with O
compression O
rate O
, O
while O
others O
do O
not O
. O
Except O
one O
metric O
( O
FPR B-MetricName
gap O
sum O
) O
, O
the O
compression O
rate O
and O
the O
extrinsic O
metric O
correlate O
more O
after O
retraining O
. O
Figure O
3 O
illustrates O
this O
for O
TPR B-MetricName
- O
gap O
( O
Pearson B-MetricName
) O
. O
The O
increase O
is O
due O
to O
superficial O
debiasing O
, O
especially O
by O
subsampling O
data O
, O
which O
prior O
to O
retraining O
had O
low O
extrinsic O
metrics O
and O
relatively O
high O
intrinsic O
metrics O
. O
This O
shows O
that O
correlation O
between O
extrinsic O
metrics O
and O
compression O
rate O
for O
certain O
metrics O
is O
stronger O
than O
it O
appeared O
before O
retraining O
. O
It O
is O
unsurprising O
that O
CEAT B-MetricName
does O
not O
correlate O
with O
any O
extrinsic O
metrics O
, O
since O
CEAT B-MetricName
could O
not O
distinguish O
between O
different O
models O
. O

Correlation O
between O
Extrinsic O
and O
Intrinsic O
Metrics O

Coreference B-TaskName
resolution I-TaskName
shows O
stronger O
correlations O
between O
compression O
rate O
and O
extrinsic O
met O
- O
rics O
, O
but O
low O
correlations O
between O
Pearson B-MetricName
metrics O
. O
We O
further O
discuss O
cases O
of O
no O
correlation O
in O
appendix O
D. O
Correlations O
decrease O
after O
retraining O
, O
but O
metrics O
that O
were O
highly O
correlated O
remain O
so O
( O
> O
0.7 O
after O
retraining O
) O
. O
The O
correlations O
are O
visualized O
for O
F1 O
difference O
metrics O
in O
Figure O
4 O
. O
CEAT B-MetricName
and O
extrinsic O
metrics O
correlate O
much O
less O
than O
compression O
rate O
( O
Table O
2 O
) O
. O
Our O
results O
are O
in O
line O
with O
those O
of O
Goldfarb O
- O
Tarrant O
et O
al O
. O
( O
2021 O
) O
, O
who O
found O
a O
lack O
of O
correlation O
between O
extrinsic O
metrics O
and O
WEAT O
, O
the O
static O
- O
embedded O
version O
of O
CEAT B-MetricName
. O

Given O
that O
recent O
work O
( O
Goldfarb O
- O
Tarrant O
et O
al O
. O
, O
2021 O
; O
Cao O
et O
al O
. O
, O
2022 O
) O
questions O
the O
validity O
of O
intrinsic O
metrics O
as O
a O
reliable O
indicator O
for O
gender O
bias O
, O
the O
compression B-MetricName
rate O
provides O
a O
reliable O
alternative O
to O
current O
intrinsic O
metrics O
, O
by O
offering O
correlation O
to O
many O
extrinsic O
bias O
metrics O
. O

Related O
Work O

There O
are O
few O
studies O
that O
examine O
both O
intrinsic O
and O
extrinsic O
metrics O
. O
Previous O
work O
by O
Goldfarb O
- O
Tarrant O
et O
al O
. O
( O
2021 O
) O
showed O
that O
debiasing O
static O
embeddings O
intrinsically O
is O
not O
correlated O
with O
extrinsic O
bias O
, O
challenging O
the O
assumption O
that O
intrinsic O
metrics O
are O
predictive O
of O
bias O
. O
We O
examine O
the O
other O
direction O
, O
exploring O
how O
extrinsic O
debiasing O
affects O
intrinsic O
metrics O
. O
We O
also O
extend O
beyond O
their O
work O
to O
contextualized O
embeddings O
, O
a O
wider O
range O
of O
extrinsic O
metrics O
, O
and O
a O
new O
, O
more O
effective O
intrinsic O
metric O
based O
on O
information O
- O
theoretic O
probing O
. O
A O
contemporary O
work O
by O
Cao O
et O
al O
. O
( O
2022 O
) O
measured O
the O
correlations O
between O
intrinsic O
and O
extrinsic O
metrics O
in O
contextualized O
settings O
across O
different O
language O
models O
. O
In O
contrast O
, O
our O
work O
examines O
the O
correlations O
across O
different O
versions O
of O
the O
same O
language O
model O
by O
fine O
- O
tuning O
it O
using O
various O
debiasing O
techniques O
. O
Studies O
that O
inspect O
extrinsic O
metrics O
include O
either O
a O
challenge O
dataset O
curated O
to O
expose O
differences O
in O
model O
behavior O
by O
gender O
, O
or O
a O
test O
dataset O
labelled O
by O
gender O
. O
Among O
these O
datasets O
are O
Winobias O
( O
Zhao O
et O
al O
. O
, O
2018 O
) O
, O
Winogender O
( O
Rudinger O
et O
al O
. O
, O
2018 O
) O
and O
GAP O
( O
Webster O
et O
al O
. O
, O
2018 O
) O
for O
coreference O
resolution O
, O
WinoMT O
( O
Stanovsky O
et O
al O
. O
, O
2019 O
) O
for O
machine O
translation O
, O
EEC O
( O
Kiritchenko O
and O
Mohammad O
, O
2018 O
) O
for O
sentiment O
analysis O
, O
BOLD O
( O
Dhamala O
et O
al O
. O
, O
2021 O
) O
for O
language O
generation O
, O
gendered O
NLI O
( O
Sharma O
et O
al O
. O
, O
2020 O
) O
for O
natural O
language O
inference O
and O
Bias O
in O
Bios O
for O
occupation O
prediction O
. O

Studies O
that O
measure O
gender O
bias O
intrinsically O
in O
static O
word O
or O
sentence O
embeddings O
measure O
characteristics O
of O
the O
geometry O
, O
such O
as O
the O
prox O
- O
imity O
between O
female O
- O
and O
male O
- O
related O
words O
to O
stereotypical O
words O
, O
or O
how O
embeddings O
cluster O
or O
relate O
to O
a O
gender O
subspace O
( O
Bolukbasi O
et O
al O
. O
, O
2016 O
; O
Caliskan O
et O
al O
. O
, O
2017 O
; O
Gonen O
and O
Goldberg O
, O
2019b O
; O
Ethayarajh O
et O
al O
. O
, O
2019 O
) O
. O
However O
, O
metrics O
and O
debiasing O
methods O
for O
static O
embeddings O
do O
not O
apply O
directly O
to O
contextualized O
ones O
. O
Several O
studies O
use O
sentence O
templates O
to O
adapt O
to O
contextual O
embeddings O
( O
May O
et O
al O
. O
, O
2019 O
; O
Kurita O
et O
al O
. O
, O
2019 O
; O
Tan O
and O
Celis O
, O
2019 O
) O
. O
This O
templated O
approach O
is O
difficult O
to O
scale O
, O
and O
lacks O
the O
range O
of O
representations O
that O
a O
contextual O
embedding O
offers O
. O
Other O
work O
extracts O
embedding O
representations O
of O
words O
from O
natural O
corpora O
( O
Zhao O
et O
al O
. O
, O
2019 O
; O
Guo O
and O
Caliskan O
, O
2021 O
; O
Basta O
et O
al O
. O
, O
2019 O
) O
. O
These O
studies O
often O
adapt O
the O
WEAT O
method O
( O
Caliskan O
et O
al O
. O
, O
2017 O
) O
, O
which O
measures O
embedding O
geometry O
. O
None O
measure O
the O
effect O
of O
the O
presumably O
found O
" O
bias O
" O
on O
a O
downstream O
task O
. O

There O
is O
a O
growing O
conversation O
in O
the O
field O
( O
Barocas O
et O
al O
. O
, O
2017 O
; O
Kate O
Crawford O
, O
2017 O
; O
Blodgett O
et O
al O
. O
, O
2020 O
; O
Bommasani O
et O
al O
. O
, O
2021 O
) O
about O
the O
importance O
of O
articulating O
the O
harms O
of O
measured O
bias O
. O
In O
general O
, O
extrinsic O
metrics O
have O
clear O
, O
interpretable O
impacts O
for O
which O
harm O
can O
be O
defined O
. O
Intrinsic O
metrics O
have O
an O
unclear O
effect O
. O
Without O
evidence O
from O
a O
concrete O
downstream O
task O
, O
a O
found O
intrinsic O
bias O
is O
only O
theoretically O
harmful O
. O
Our O
work O
is O
a O
step O
towards O
understanding O
whether O
intrinsic O
metrics O
provide O
valuable O
insights O
about O
bias O
in O
a O
model O
. O

Discussion O
and O
Conclusions O

This O
study O
examined O
whether O
bias O
in O
internal O
representations O
is O
related O
to O
extrinsic O
bias O
. O
We O
designed O
a O
new O
framework O
in O
which O
we O
debias O
a O
model O
on O
a O
downstream O
task O
, O
and O
measure O
its O
intrinsic O
bias O
. O
We O
found O
that O
gender O
extractability O
from O
internal O
representations O
, O
measured O
by O
compression O
rate O
via O
MDL O
probing O
, O
reflects O
bias O
in O
a O
model O
. O
Compression O
was O
much O
more O
reliable O
than O
an O
alternative O
intrinsic O
metric O
for O
contextualised O
representations O
, O
CEAT B-MetricName
. O
Compression B-MetricName
correlated O
well O
- O
to O
varying O
degrees O
- O
with O
many O
extrinsic O
metrics O
. O
We O
thus O
encourage O
NLP O
practitioners O
to O
use O
compression O
as O
an O
intrinsic O
indicator O
for O
gender O
bias O
in O
NLP O
models O
. O
When O
comparing O
two O
alternative O
models O
, O
a O
lower O
compression B-MetricName
rate O
provides O
confidence O
in O
a O
model O
's O
superiority O
in O
terms O
of O
gender O
bias O
. O
The O
relative O
success O
of O
compression B-MetricName
over O
CEAT B-MetricName
may O
be O
because O
the O
compression B-MetricName
rate O
was O
calculated O
on O
the O
same O
dataset O
as O
the O
extrinsic O
metrics O
, O
whereas O
CEAT B-MetricName
was O
measured O
on O
a O
different O
dataset O
not O
necessarily O
aligned O
with O
a O
specific O
downstream O
task O
. O
The O
use O
of O
a O
non O
- O
task O
- O
aligned O
dataset O
is O
a O
common O
strategy O
among O
other O
intrinsic O
metrics O
( O
May O
et O
al O
. O
, O
2019 O
; O
Kurita O
et O
al O
. O
, O
2019 O
; O
Basta O
et O
al O
. O
, O
2021 O
) O
. O
Another O
possible O
explanation O
is O
that O
compression B-MetricName
rate O
measures O
a O
more O
focused O
concept O
, O
namely O
the O
gender O
information O
within O
the O
internal O
representations O
. O
CEAT B-MetricName
measures O
proximity O
among O
embeddings O
of O
general O
terms O
that O
may O
include O
other O
social O
contexts O
that O
do O
not O
directly O
relate O
to O
gender O
( O
e.g. O
a O
female O
term O
like O
' O
lady O
' O
or O
' O
Sarah O
' O
contains O
information O
about O
not O
just O
gender O
but O
class O
, O
culture O
, O
formality O
, O
etc O
, O
and O
it O
can O
be O
hard O
to O
isolate O
just O
one O
of O
these O
from O
the O
rest O
) O
. O

Our O
results O
show O
that O
when O
a O
debiasing O
method O
reduces O
extrinsic O
metrics O
but O
not O
compression B-MetricName
, O
it O
indicates O
that O
the O
language O
model O
remains O
biased O
. O
When O
such O
superficial O
debiasing O
occurs O
, O
the O
debiased O
language O
model O
may O
be O
reapplied O
to O
another O
task O
, O
as O
in O
Jin O
et O
al O
. O
( O
2021 O
) O
, O
resulting O
in O
unexpected O
biases O
and O
nullifying O
the O
supposed O
debiasing O
. O
Our O
findings O
suggest O
that O
practitioners O
of O
NLP O
should O
take O
special O
care O
when O
adopting O
previously O
debiased O
models O
and O
inspect O
them O
carefully O
, O
perhaps O
using O
our O
framework O
. O
Our O
results O
differ O
from O
those O
of O
Mendelson O
and O
Belinkov O
( O
2021a O
) O
, O
who O
found O
that O
the O
debiasing O
increases O
bias O
extractability O
as O
measured O
by O
compression B-MetricName
rate O
. O
However O
, O
they O
studied O
different O
, O
non O
- O
social O
biases O
, O
that O
arise O
from O
spurious O
or O
unintended O
correlations O
in O
training O
datasets O
( O
often O
called O
dataset O
biases O
) O
. O
In O
our O
case O
, O
some O
debiasing O
strategies O
increase O
intrinsic O
bias O
while O
others O
decrease O
it O
. O
Future O
work O
could O
investigate O
why O
debiasing O
affects O
extractability O
differently O
for O
these O
two O
types O
of O
biases O
. O

Our O
work O
also O
highlighted O
the O
importance O
of O
the O
classification B-HyperparameterName
layer I-HyperparameterName
. O
Using O
a O
debiased O
objective O
, O
such O
as O
a O
balanced O
dataset O
, O
the O
classification B-HyperparameterName
layer I-HyperparameterName
can O
provide O
significant O
debiasing O
. O
This O
holds O
even O
if O
the O
internal O
representations O
are O
biased O
and O
the O
classifier O
is O
a O
single O
linear O
layer O
, O
as O
shown O
in O
the O
occupation O
prediction O
task O
. O
Bias O
stems O
in O
part O
from O
internal O
LM O
bias O
and O
in O
part O
from O
classification O
bias O
. O
Practitioners O
should O
focus O
their O
efforts O
on O
both O
parts O
when O
attempting O
to O
debias O
a O
model O
. O

We O
used O
a O
broader O
set O
of O
extrinsic O
metrics O
than O
is O
typically O
used O
, O
and O
found O
that O
the O
bias O
metrics O
behaved O
differently O
: O
some O
decreased O
more O
than O
others O
after O
debiasing O
, O
and O
they O
correlated O
differently O
with O
compression O
rate O
. O
Debiasing O
efforts O
may O
not O
be O
fully O
understood O
by O
testing O
only O
a O
few O
extrinsic O
metrics O
. O
However O
, O
compression B-MetricName
as O
an O
intrinsic O
bias O
metric O
can O
indicate O
meaningful O
debiasing O
of O
internal O
model O
representations O
even O
when O
not O
all O
metrics O
are O
easily O
measurable O
, O
since O
it O
correlates O
well O
with O
many O
extrinsic O
metrics O
. O

A O
major O
limitation O
of O
this O
study O
is O
the O
use O
of O
gender O
as O
a O
binary O
variable O
, O
which O
is O
trans O
- O
exclusive O
. O
Cao O
and O
Daumé O
III O
( O
2020 O
) O
made O
the O
first O
steps O
towards O
inclusive O
gender O
bias O
evaluation O
in O
NLP O
, O
revealing O
that O
coreference B-TaskName
systems O
fail O
on O
genderinclusive O
text O
. O
Further O
work O
is O
required O
to O
adjust O
our O
framework O
to O
non O
- O
binary O
genders O
, O
potentially O
revealing O
insights O
about O
the O
poor O
performance O
of O
NLP O
systems O
in O
that O
area O
. O

A O
Implementation O
Details O

We O
used O
RoBERTa B-MethodName
in O
all O
models O
( O
base O
size O
, O
120 O
M O
parameters O
) O
. O
We O
use O
following O
random B-HyperparameterName
seeds I-HyperparameterName
in O
all O
repeated O
experiments O
: O
0 B-HyperparameterValue
, O
5,11,26,42,46,50,63,83,90 B-HyperparameterValue
. O
Our O
code O
was O
implemented O
mainly O
using O
the O
Python O
libraries O
Pytorch O
( O
Paszke O
et O
al O
. O
, O
2019 O
) O
, O
Transformers O
( O
Wolf O
et O
al O
. O
, O
2020 O
) O
, O
Sklearn O
( O
Pedregosa O
et O
al O
. O
, O
2011 O
) O
, O
and O
the O
experiments O
were O
logged O
using O
Wandb O
( O
Biewald O
, O
2020 O
) O
. O

A.1 O
Occupation B-TaskName
Classification I-TaskName

We O
fine O
- O
tuned O
a O
RoBERTa B-MethodName
- O
base O
model O
with O
a O
linear B-HyperparameterName
classification I-HyperparameterName
layer I-HyperparameterName
on O
top O
. O
Training O
was O
done O
for O
10 B-HyperparameterValue
epochs B-HyperparameterName
at O
a O
learning B-HyperparameterName
rate I-HyperparameterName
of O
5e-5 B-HyperparameterValue
, O
batch B-HyperparameterName
size I-HyperparameterName
of O
64 B-HyperparameterValue
. O
The O
input O
to O
RoBERTa B-MethodName
was O
the O
biography O
tokens O
, O
which O
is O
limited O
to O
the O
first O
128 O
tokens O
. O
The O
resulting O
[ O
CLS O
] O
token O
embedding O
is O
fed O
to O
the O
classifier O
to O
predict O
the O
occupation O
. O
The O
probing O
task O
involves O
using O
the O
same O
[ O
CLS O
] O
token O
and O
training O
the O
probing O
classifier O
to O
predict O
the O
gender O
of O
the O
person O
in O
the O
biography O
. O
The O
experiments O
without O
fine O
- O
tuning O
included O
either O
a O
pre O
- O
trained O
or O
a O
previously O
fine O
- O
tuned O
RoBERTa B-MethodName
. O
We O
first O
extracted O
the O
pre O
- O
trained O
RoBERTa B-MethodName
's O
embeddings O
of O
tokens O
from O
the O
[ O
CLS O
] O
and O
then O
trained O
a O
linear O
classifier O
on O
them O
. O
The O
learning B-HyperparameterName
rate I-HyperparameterName
was O
0.001 B-HyperparameterValue
and O
the O
batch B-HyperparameterName
size I-HyperparameterName
was O
64 B-HyperparameterValue
. O
We O
trained O
the O
classification B-HyperparameterName
layer I-HyperparameterName
with O
pre O
- O
trained O
RoBERTa B-MethodName
on O
300 B-HyperparameterValue
epochs B-HyperparameterName
, O
but O
with O
fine O
- O
tuned O
RoBERTa B-MethodName
, O
10 B-HyperparameterValue
epochs B-HyperparameterName
were O
sufficient O
. O
For O
all O
training O
processes O
, O
the O
epoch B-HyperparameterName
with O
the O
greatest O
validation O
accuracy O
was O
saved O
. O
Finetuning O
took O
7 O
hours O
on O
a O
GeForce O
RTX O
2080 O
Ti O
GPU O
. O
Bias O
in O
Bios B-DatasetName
contains O
almost O
400k O
biographies O
, O
and O
we O
obtain O
validation O
( O
10 B-HyperparameterValue
% I-HyperparameterValue
) O
and O
test O
set O
( O
25 B-HyperparameterValue
% I-HyperparameterValue
) O
by O
splitting O
with O
Scikit O
- O
learn O
's O
( O
Pedregosa O
et O
al O
. O
, O
2011 O
) O
test_train_split B-HyperparameterName
with O
our O
random B-HyperparameterName
seeds I-HyperparameterName
. O

A.2 O
Coreference B-TaskName
Resolution I-TaskName

We O
use O
the O
implementation O
of O
Xu O
and O
Choi O
( O
2020 O
) O
, O
a O
model O
that O
was O
introduced O
by O
Lee O
et O
al O
. O
( O
2018b O
) O
and O
has O
been O
adopted O
by O
many O
coreference O
resolution O
models O
. O
Coreference B-TaskName
resolution I-TaskName
is O
the O
process O
of O
clustering O
different O
mentions O
in O
a O
text O
that O
refer O
to O
the O
same O
real O
- O
world O
entities O
. O
The O
task O
is O
solved O
by O
detecting O
mentions O
through O
text O
spans O
and O
then O
predicting O
for O
each O
pair O
of O
spans O
if O
they O
represent O
the O
same O
entity O
. O
The O
span O
representations O
were O
extracted O
with O
a O
RoBERTa B-MethodName
model O
, O
which O
is O
fine O
- O
tuned O
throughout O
the O
training O
process O
, O
except O
in O
the O
retraining O
experiment O
. O
Fine O
- O
tuning O
took O
3 O
hours O
on O
an O
NVIDIA O
RTX O
A6000 O
GPU O
. O
Ontonotes B-DatasetName
5.0 O
has O
625k O
sentences O
and O
we O
use O
the O
standard O
validation B-HyperparameterName
and I-HyperparameterName
test I-HyperparameterName
splits I-HyperparameterName
. O

A.3 O
Probing O
Classifier O

We O
use O
the O
MDL O
probe O
( O
Voita O
and O
Titov O
, O
2020 O
) O
implementation O
by O
Mendelson O
and O
Belinkov O
( O
2021b O
) O
. O
In O
all O
experiments O
, O
we O
use O
a O
linear O
probe O
and O
train O
it O
with O
a O
batch B-HyperparameterName
size I-HyperparameterName
of O
16 B-HyperparameterValue
and O
a O
learning B-HyperparameterName
rate I-HyperparameterName
of O
1e-3 B-HyperparameterValue
. O
The O
timestamps O
used O
, O
meaning O
the O
accumulating O
fractions O
of O
data O
that O
the O
probe O
is O
trained O
on O
, O
are O
2.0 O
% O
, O
3.0 O
% O
, O
4.4 O
% O
, O
6.5 O
% O
, O
9.5 O
% O
, O
14.0 O
% O
, O
21.0 O
% O
, O
31.0 O
% O
, O
45.7 O
% O
, O
67.6 O
% O
, O
100 O
% O
. O

A.4 O
Metrics O

A.4.1 O
Fairness O
- O
Based O
Metrics O
Implementation O

All O
three O
statistical O
fairness O
metrics O
measure O
the O
difference O
between O
two O
probability O
distributions O
, O
where O
this O
difference O
describes O
a O
notion O
of O
bias O
. O
We O
calculate O
Independence B-MetricName
and O
Separation B-MetricName
via O
Kullback O
- O
Leibler O
( O
KL O
) O
divergence O
, O
using O
the O
Al O
- O
lenNLP O
implementation O
( O
https O
: O
/ O
/ O
github.com O
/ O
allenai O
/ O
allennlp O
) O
. O
We O
calculate O
Sufficiency B-MetricName
via O
Wasserstein O
distance O
instead O
, O
which O
is O
motivated O
by O
Kwegyir O
- O
Aggrey O
et O
al O
. O
( O
2021 O
) O
. O
In O
this O
case O
, O
we O
can O
not O
use O
KL O
divergence O
, O
since O
there O
are O
some O
classes O
that O
do O
not O
occur O
in O
model O
predictions O
for O
both O
male O
and O
female O
genders O
. O
This O
causes O
the O
probability O
distributions O
to O
not O
have O
the O
same O
support O
, O
and O
KL O
divergence O
is O
unbounded O
. O
Wasserstein O
distance O
lacks O
the O
requirement O
for O
equal O
support O
. O

A.4.2 O
Classification O
Metrics O
Interpretation O
in O
Winobias O

Winobias B-DatasetName
datasets O
contain O
pairs O
of O
stereotypical O
and O
anti O
- O
stereotypical O
sentences O
. O
The O
stereotypes O
are O
derived O
from O
the O
US O
labor O
statistics O
( O
for O
instance O
, O
a O
profession O
with O
a O
majority O
of O
males O
is O
stereotypically O
male O
) O
. O
Since O
coreference O
resolution O
is O
viewed O
as O
a O
clustering O
problem O
, O
it O
is O
usually O
measured O
via O
clustering O
evaluation O
metrics O
. O
Coreference B-TaskName
resolution I-TaskName
is O
commonly O
measured O
as O
the O
average O
F1 O
score O
of O
these O
, O
and O
the O
same O
is O
true O
for O
Winobias B-DatasetName
. O
Nevertheless O
, O
coreference B-TaskName
resolution I-TaskName
is O
accomplished O
by O
making O
a O
prediction O
for O
each O
pair O
of O
mentions O
, O
so O
it O
can O
be O
seen O
as O
a O
classification O
task O
. O
Winobias B-DatasetName
can O
be O
viewed O
as O
a O
simpler O
task O
than O
general O
coreference O
resolution O
, O
as O
it O
contains O
exactly O
two O
mentions O
of O
professions O
and O
one O
pronoun O
, O
which O
refers O
to O
exactly O
one O
profession O
. O
Therefore O
, O
we O
reframe O
it O
as O
a O
classification O
problem O
. O
In O
a O
Winobias B-DatasetName
sentence O
with O
two O
professions O
x O
and O
y O
, O
as O
well O
as O
a O
pronoun O
p O
, O
where O
p O
is O
referring O
to O
x O
, O
a O
true O
positive O
would O
be O
to O
cluster O
x O
and O
p O
together O
, O
while O
a O
false O
positive O
would O
be O
to O
cluster O
y O
and O
p O
together O
. O
Our O
classification O
metrics O
are O
derived O
based O
on O
these O
definitions O
. O
For O
instance O
, O
the O
TPR B-MetricName
gap O
for O
profession O
" O
teacher O
" O
, O
which O
is O
a O
stereotypical O
female O
occupation O
, O
is O
the O
TPR B-MetricName
rate O
on O
pro O
- O
stereotypical O
sentences O
( O
with O
a O
female O
pronoun O
) O
minus O
the O
TPR B-MetricName
rate O
on O
anti O
- O
stereotypical O
sentences O
( O
with O
a O
male O
pronoun O
) O
. O

A.4.3 O
CEAT B-MetricName

The O
Word O
Embedding O
Association O
Test O
( O
WEAT O
) O
developed O
by O
( O
Caliskan O
et O
al O
. O
, O
2017 O
) O
is O
a O
method O
for O
evaluating O
bias O
in O
static O
word O
embeddings O
. O
The O
test O
is O
defined O
as O
follows O
: O
given O
two O
sets O
of O
target O
words O
X O
, O
Y O
( O
e.g. O
, O
' O
executive O
' O
, O
' O
management O
' O
, O
' O
professional O
' O
and O
' O
home O
' O
, O
' O
parents O
' O
, O
' O
children O
' O
) O
and O
two O
sets O
of O
attribute O
words O
( O
e.g. O
, O
male O
names O
and O
female O
names O
) O
, O
and O
using O
⃗ O
w O
to O
represent O
the O
word O
embedding O
for O
word O
w O
, O
the O
effect O
size O
is O
: O
In O
essence O
, O
the O
effect O
size O
measures O
how O
different O
are O
the O
distances O
between O
the O
embedding O
vectors O
of O
each O
target O
group O
and O
the O
attribute O
groups O
. O
Specifically O
, O
if O
s O
( O
x O
, O
A O
, O
B O
) O
> O
0 O
, O
⃗ O

x O
is O
more O
similar O
to O
attribute O
words O
B O
and O
vice O
versa O
. O
For O
instance O
, O
a O
larger O
effect O
size O
is O
observed O
if O
target O
words O
X O
are O
more O
similar O
to O
attribute O
words O
A O
and O
target O
words O
Y O
are O
more O
similar O
to O
attribute O
words O
B. O
|ES| O
> O
0.5 O
and O
|ES| O
> O
0.8 O
are O
considered O
medium O
and O
large O
effect O
sizes O
, O
respectively O
( O
Rice O
and O
Harris O
, O
2005 O
) O
. O
The O
null O
hypothesis O
holds O
that O
there O
is O
no O
difference O
between O
the O
two O
sets O
of O
target O
words O
in O
terms O
of O
their O
relative O
similarity O
to O
the O
two O
sets O
of O
attribute O
words O
, O
indicating O
that O
there O
are O
no O
biased O
associations O
. O
Statistical O
significance B-MetricName
is O
defined O
by O
the O
p O
- O
value O
of O
WEAT O
, O
which O
reflects O
the O
probability O
of O
observing O
the O
effect O
size O
under O
the O
null O
hypothesis O
. O

Since O
a O
word O
can O
take O
on O
a O
great O
variety O
of O
vector O
representations O
in O
a O
contextual O
setting O
, O
ES O
varies O
according O
to O
the O
sentences O
used O
to O
extract O
word O
representation O
. O
Thus O
, O
to O
adopt O
WEAT O
to O
contextualized O
representations O
, O
the O
Combined O
Effect O
Size O
( O
CES O
) O
( O
Guo O
and O
Caliskan O
, O
2021 O
) O
is O
derived O
as O
the O
distribution O
of O
WEAT O
effect O
sizes O
over O
many O
possible O
contextual O
word O
representations O
: O

CES O
( O
X O
, O
Y O
, O
A O
, O
B O
) O
= O
N O
i=1 O
v O
i O
ES O
i O
N O
i=1 O
v O
i O

where O
ES O
i O
denotes O
the O
WEAT O
effect O
size O
of O
the O
i'th O
choice O
of O
word O
representations O
from O
a O
large O
corpus O
, O
and O
v O
i O
is O
the O
inverse O
of O
the O
sum O
of O
in O
- O
sample O
variance O
V O
i O
and O
between O
- O
sample O
variance O
in O
the O
distribution O
of O
random O
- O
effects O
. O
As O
in O
Guo O
and O
Caliskan O
( O
2021 O
) O
, O
the O
representation O
for O
each O
word O
is O
derived O
from O
10,000 O
random O
sentences O
extracted O
from O
a O
corpus O
of O
Reddit O
comments O
. O

The O
combined O
effect O
size O
of O
each O
of O
the O
models O
is O
examined O
on O
WEAT B-DatasetName
stimulus I-DatasetName
6 I-DatasetName
, O
which O
contains O
target O
words O
of O
career O
/ O
family O
and O
attribute O
words O
of O
male O
/ O
female O
names O
. O
This O
was O
the O
only O
one O
that O
detected O
bias O
on O
a O
pre O
- O
trained O
RoBERTa B-MethodName
( O
CES B-MetricName
close O
to O
0.5 B-MetricValue
and O
p O
< O
0.05 O
) O
. O
The O
points O
that O
we O
kept O
in O
our O
analysis O
are O
those O
where O
p O
< O
0.05 O
, O
which O
make O
up O
90 O
% O
of O
the O
points O
in O
occupation O
prediction O
and O
95 O
% O
of O
the O
points O
in O
coreference B-TaskName
resolution I-TaskName
. O

B O
Full O
Results O

In O
this O
section O
we O
provide O
the O
full O
results O
of O
a O
RoBERTa B-MethodName
model O
trained O
on O
the O
downstream O
task O
. O

Table O
3 O
presents O
results O
for O
the O
occupation B-TaskName
prediction I-TaskName
task O
after O
fine O
- O
tuning O
, O
Table O
4 O
presents O
the O
retrained O
model O
results O
. O

Figure O
5 O
illustrates O
the O
correlations O
between O
extrinsic O
metrics O
and O
compression O
rate O
before O
and O
after O
retraining O
. O

Table O
5 O
presents O
the O
complete O
results O
for O
the O
occupation O
prediction O
task O
of O
the O
model O
trained O
without O
fine O
- O
tuning O
, O
meaning O
that O
the O
RoBERTa B-MethodName
model O
is O
the O
pretrained O
version O
from O
Liu O
et O
al O
. O
( O
2019 O
) O
and O
only O
the O
classification O
layer O
was O
updated O
. O
Subsampling O
the O
dataset O
has O
significant O
debiasing O
effects O
, O
which O
suggests O
that O
this O
debiasing O
method O
can O
achieve O
low O
extrinsic O
bias O
even O
when O
internal O
bias O
exists O
. O
The O
Pearson B-MetricName
correlation I-MetricName
on O
precision O
exhibits O
a O
different O
behavior O
. O
It O
makes O
sense O
nonetheless O
: O
precision O
is O
computed O
as O
T O
P O
\ O
( O
T O
P O
+ O
F O
P O
) O
. O
A O
biased O
model O
will O
assign O
more O
examples O
of O
a O
specific O
profession O
to O
a O
specific O
gender O
( O
which O
aligns O
with O
the O
percentage O
of O
biographies O
of O
this O
profession O
with O
this O
gender O
on O
the O
training O
set O
) O
, O
increasing O
both O
T O
P O
and O
F O
P O
and O
decreasing O
precision O
. O
The O
results O
on O
the O
coreference O
resolution O
task O
align O
with O
the O
results O
of O
occupation O
prediction O
. O

Table O
6 O
presents O
the O
results O
using O
a O
DeBERTa B-MethodName
model O
( O
He O
et O
al O
. O
, O
2020 O
) O
for O
the O
occupation B-TaskName
classification I-TaskName
task O
. O
The O
trends O
are O
similar O
to O
those O
of O
RoBERTa B-MethodName
, O
with O
the O
same O
metrics O
showing O
an O
increase O
, O
no O
change O
, O
or O
decrease O
in O
correlation O
after O
re O
- O
training O
, O
suggesting O
a O
general O
trend O
in O
the O
behavior O
of O
these O
metrics O
in O
relation O
to O
internal O
model O
representations O
. O

Table O
7 O
displays O
the O
results O
on O
a O
finetuned O
model O
for O
the O
coreference O
resolution O
task O
and O
Table O
8 O
displays O
the O
retraining O
results O
. O

Figure O
6 O
shows O
the O
correlations O
between O
compression B-MetricName
rate O
and O
extrinsic O
metrics O
before O
and O
after O
the O
retraining O
. O
C O
Why O
is O
scrubbing O
not O
as O
effective O
as O
subsampling O
? O

The O
debiasing O
method O
of O
subsampling O
significantly O
reduced O
external O
biases O
in O
the O
occupation O
prediction O
task O
. O
Although O
compression B-MetricName
rates O
show O
that O
scrubbing O
reduced O
more O
gender O
information O
, O
subsampling O
outperforms O
it O
as O
a O
debiasing O
method O
. O
We O
find O
that O
in O
spite O
of O
the O
scrubbing O
, O
a O
probe O
is O
able O
to O
correctly O
identify O
the O
gender O
from O
an O
internal O
representation O
with O
68.8 B-MetricValue
% I-MetricValue
accuracy B-MetricName
compared O
to O
90.7 B-MetricValue
% I-MetricValue
on O
the O
original O
, O
non O
- O
scrubbed O
data O
. O
This O
means O
that O
although O
the O
scrubbing O
process O
reduces O
extrinsic O
bias O
significantly O
, O
gender O
information O
is O
still O
embedded O
in O
the O
[ O
CLS O
] O
token O
embeddings O
. O

To O
investigate O
the O
source O
of O
gender O
information O
after O
scrubbing O
, O
we O
use O
logistic B-MethodName
regression I-MethodName
( O
LR B-MethodName
) O
model O
to O
predict O
the O
gender O
from O
the O
Bag O
- O
of O
- O
Words O
of O
the O
scrubbed O
biographies O
. O
We O
perform O
an O
iterative O
process O
for O
automatic O
extra O
scrubbing O
: O
in O
each O
iteration O
we O
( O
1 O
) O
train O
a O
LR O
model O
for O
gender O
prediction O
( O
2 O
) O
scrub O
the O
n O
most O
significant O
words O
for O
each O
gender O
according O
to O
the O
LR O
weights O
. O
The O
most O
relevant O
words O
among O
5 O
seeds O
of O
training O
with O
n=10 O
words O
scrubbed O
per O
iteration O
are O
displayed O
in O
Table O
9 O
. O
The O
model O
learns O
indirect O
correlations O
to O
gender O
in O
the O
absence O
of O
explicit O
gendered O
words O
. O
Because O
the O
significant O
words O
are O
related O
to O
male O
- O
or O
femaledominated O
professions O
, O
we O
conducted O
the O
process O
on O
a O
specific O
profession O
. O
Table O
10 O
presents O
the O
most O
significant O
words O
for O
biographies O
of O
nurses O
. O
There O
are O
differences O
in O
wording O
even O
between O
females O
and O
males O
in O
the O
same O
profession O
. O
The O
results O
of O
this O
study O
are O
in O
line O
with O
the O
results O
of O
other O
studies O
that O
have O
been O
conducted O
on O
the O
way O
biographies O
are O
written O
for O
men O
and O
women O
( O
Wagner O
et O
al O
. O
, O
2016 O
; O
Sun O
and O
Peng O
, O
2021 O
) O
. O

Subsampling O
is O
therefore O
more O
effective O
even O
when O
gender O
information O
is O
present O
since O
it O
prevents O
the O
model O
from O
learning O
correlations O
between O
gender O
information O
and O
a O
profession O
whereas O
scrubbing O
only O
attempts O
to O
remove O
gender O
indicators O
without O
removing O
correlations O
. O
On O
the O
other O
hand O
, O
it O
is O
possible O
that O
oversampling O
is O
less O
effective O
for O
debiasing O
since O
seeing O
more O
non O
- O
unique O
examples O
an O
unrepresented O
group O
encourages O
learning O
correlations O
. O

D O
A O
closer O
look O
into O
no O
- O
correlation O
cases O
D.1 O
Occupation B-TaskName
Prediction I-TaskName

Although O
compression B-MetricName
has O
the O
ability O
to O
identify O
bias O
in O
most O
cases O
, O
some O
metrics O
still O
show O
little O
or O
no O
correlation O
with O
compression O
rate O
. O
These O
results O
suggest O
that O
gender O
information O
comprises O
only O
one O
facet O
of O
embedded O
bias O
in O
the O
representations O
. O
Other O
factors O
that O
may O
influence O
these O
metrics O
are O
not O
considered O
or O
measured O
, O
such O
as O
the O
connection O
between O
a O
name O
and O
a O
profession O
. O

For O
example O
, O
as O
can O
be O
see O
in O
Tables O
3 O
and O
4 O
, O
LMs O
finetuned O
on O
subsampled O
data O
have O
the O
largest O
FPR B-MetricName
gaps O
after O
retraining O
, O
despite O
being O
the O
least O
biased O
before O
retraining O
, O
while O
those O
finetuned O
on O
oversampled O
data O
have O
the O
next O
- O
to O
- O
lowest O
FPR B-MetricName
gaps O
after O
retraining O
. O
The O
information O
encoded O
in O
the O
internal O
representations O
may O
have O
been O
encoded O
in O
a O
manner O
that O
allowed O
the O
classification O
layer O
to O
exhibit O
a O
smaller O
FPR B-MetricName
gap O
when O
trained O
on O
a O
balanced O
dataset O
. O
However O
, O
when O
the O
classification O
Figure O
7 O
: O
Occupation O
prediction O
: O
Before O
( O
left O
) O
and O
after O
( O
right O
) O
plots O
of O
compression O
rate O
versus O
Pearson B-MetricName
metrics O
as O
computed O
from O
real O
- O
world O
statistics O
( O
as O
opposed O
to O
statistics O
derived O
from O
the O
training O
dataset O
) O
. O
This O
shows O
the O
unrealiability O
of O
using O
real O
world O
statistics O
to O
draw O
conclusions O
, O
as O
they O
may O
not O
be O
reflected O
in O
the O
data O
. O

Acknowledgements O

This O
research O
was O
supported O
by O
the O
ISRAEL O
SCI O
- O
ENCE O
FOUNDATION O
( O
grant O
No O
. O
448 O
/ O
20 O
) O
and O
by O
an O
Azrieli O
Foundation O
Early O
Career O
Faculty O
Fellowship O
. O
We O
also O
thank O
Kate O
McCurdy O
and O
Andreas O
Grivas O
for O
comments O
on O
early O
drafts O
, O
the O
members O
of O
the O
Technion O
CS O
NLP O
group O
for O
their O
valuable O
feedback O
, O
and O
the O
anonymous O
reviewers O
for O
their O
useful O
suggestions O
. O

Debiasing O
Strategy O

Metric O

None O
Oversampling O
Subsampling O
Scrubbing O
Compression B-MetricName
4.121 O
± O
1.238 O
8.522 O
* O
± O
2.354 O
3.568 O
± O
1.516 O
1.699 O
* O
± O
0.138 O
Accuracy B-MetricName
0.861 O
± O
0.005 O
0.852 O
* O
± O
0.004 O
0.861 O
± O
0.003 O
0.851 O
* O
± O
0.003 O
TPR B-MetricName
gap O
( O
P O
) O
0.763 O
± O
0.071 O
0.729 O
± O
0.067 O
0.319 O
* O
± O
0.114 O
0.704 O
* O
± O
0.068 O
TPR O
gap O
( O
S O
) O
2.391 O
± O
0.257 O
2.145 O
* O
± O
0.220 O
1.598 O
* O
± O
0.273 O
2.019 O
* O
± O
0.262 O
FPR B-MetricName
gap O
( O
P O
) O
0.591 O
± O
0.052 O
0.491 O
* O
± O
0.059 O
0.087 O
* O
± O
0.094 O
0.552 O
± O
0.063 O
FPR B-MetricName
gap O
( O
S O
) O
0.075 O
± O
0.010 O
0.085 O
* O
± O
0.011 O
0.030 O
* O
± O
0.006 O
0.057 O
* O
± O
0.007 O
Precision B-MetricName
gap O
( O
P O
) O

-0.880 O
± O
0.031 O
-0.855 O
± O
0.115 O
-0.299 O
* O
± O
0.215 O
-0.815 O
* O
± O
0.040 O
Precision B-MetricName
gap O
( O
S O
) O

3.621 O
± O
0.337 O
3.401 O
± O
0.667 O
1.549 O
* O
± O
0.229 O
2.590 O
* O
± O
0.279 O
Independence B-MetricName
gap O
( O
S O
) O
0.009 O
± O
0.002 O
0.008 O
± O
0.002 O
0.001 O
* O
± O
0.000 O
0.005 O
* O
± O
0.001 O
Separation B-MetricName
gap O
( O
S O
) O
0.327 O
± O
0.051 O
0.305 O
± O
0.030 O
0.204 O
* O
± O
0.032 O
0.296 O
± O
0.053 O
Sufficiency B-MetricName
gap O
( O
S O
) O
9.451 O
± O
1.945 O
8.324 O
* O
± O
1.537 O
1.218 O
* O
± O
0.330 O
4.930 O
* O
± O
0.927 O
layer O
was O
retrained O
on O
biased O
training O
data O
, O
it O
used O
the O
same O
features O
to O
make O
biased O
predictions O
. O

D.2 O
Coreference B-TaskName
Resolution I-TaskName

The O
cases O
where O
there O
is O
no O
correlation O
between O
our O
intrinsic O
metric O
and O
an O
extrinsic O
metric O
are O
the O
cases O
where O
the O
metric O
is O
based O
on O
Pearson B-MetricName
correlation O
. O
Unlike O
occupation B-TaskName
prediction I-TaskName
, O
coreference B-TaskName
resolution I-TaskName
seems O
to O
exhibit O
no O
correlation O
between O
those O
metrics O
and O
compression B-MetricName
rate O
. O
These O
metrics O
are O
computed O
as O
the O
Pearson B-MetricName
correlation O
between O
a O
performance O
gap O
for O
a O
specific O
profession O
and O
the O
percentage O
of O
women O
in O
that O
profession O
, O
however O
the O
percentages O
are O
computed O
differently O
in O
each O
task O
: O
in O
occupation B-TaskName
prediction I-TaskName
, O
the O
percentages O
are O
computed O
from O
the O
train O
set O
, O
focusing O
on O
the O
representation O
each O
gender O
has O
in O
the O
data O
. O
In O
Winobias B-DatasetName
, O
the O
percentages O
are O
taken O
from O
the O
US O
labor O
statistics O
, O
and O
are O
unrelated O
to O
the O
training O
dataset O
statistics O
. O
We O
note O
that O
the O
two O
statistics O
can O
be O
different O
-the O
real O
- O
world O
representation O
of O
women O
in O
a O
profession O
does O
not O
have O
to O
be O
equal O
to O
their O
representation O
in O
written O
text O
( O
Suresh O
and O
Guttag O
, O
2021 O
) O
. O
We O
thus O
decided O
to O
test O
what O
happens O
if O
we O
change O
the O
statistics O
used O
in O
Winobias B-DatasetName
to O
dataset O
statistics O
, O
but O
Ontonotes B-DatasetName
5.0 O
has O
very O
little O
representation O
to O
each O
profession O
and O
the O
statistics O
extracted O
from O
it O
would O
not O
be O
reliable O
. O
We O
thus O
took O
a O
different O
approach O
and O
computed O
the O
Pearson B-MetricName
correlations O
for O
occupation B-TaskName
prediction I-TaskName
with O
real O
world O
statistics O
instead O
of O
dataset O
statistics O
. O
To O
do O
this O
, O
we O
mapped O
the O
professions O
appearing O
in O
this O
dataset O
to O
professions O
from O
the O
US O
labor O
statistics O
, O
and O
dropped O
those O
who O
could O
no O
be O
mapped O
( O
6 O
out O
of O
29 O
of O
the O
professions O
which O
is O
21.4 O
% O
) O
. O
We O
then O
repeated O
all O
experiments O
on O
the O
Pearson B-MetricName
metrics O
using O
these O
statistics O
. O
Figure O
7 O
shows O
the O
results O
. O
Correlations O
are O
very O
different O
when O
computed O
with O
respect O
to O
real O
- O
world O
statistics O
. O
TPR B-MetricName
- O
gap O
has O
no O
correlation O
at O
all O
although O
it O
had O
with O
training O
data O
statistics O
, O
the O
correlation O
for O
FPR B-MetricName
- O
gap O
after O
retraining O
exists O
but O
is O
negative O
, O
and O
the O
correlation O
with O
precision B-MetricName
- O
gap O
does O
not O
exist O
after O
retraining O
. O
We O
thus O
conclude O
that O
the O
Pearson B-MetricName
metrics O
are O
less O
reliable O
as O
they O
are O
heavily O
dependent O
on O
the O
statistics O
with O
respect O
to O
which O
they O
are O
calculated O
. O

Rethinking O
the O
Role O
of O
Scale O
for O
In O
- O
Context O
Learning O
: O
An O
Interpretability O
- O
based O
Case O
Study O
at O
66 O
Billion O
Scale O

Language O
models O
have O
been O
shown O
to O
perform O
better O
with O
an O
increase O
in O
scale O
on O
a O
wide O
variety O
of O
tasks O
via O
the O
in O
- O
context O
learning O
paradigm O
. O
In O
this O
paper O
, O
we O
investigate O
the O
hypothesis O
that O
the O
ability O
of O
a O
large O
language O
model O
to O
in O
- O
context O
learn O
- O
perform O
a O
task O
is O
not O
uniformly O
spread O
across O
all O
of O
its O
underlying O
components O
. O
Using O
a O
66 B-MethodName
billion I-MethodName
parameter I-MethodName
language I-MethodName
model I-MethodName
( O
OPT-66B B-MethodName
) O
across O
a O
diverse O
set O
of O
14 O
downstream O
tasks O
, O
we O
find O
this O
is O
indeed O
the O
case O
: O
∼70 O
% O
of O
the O
attention O
heads O
and O
∼20 O
% O
of O
the O
feed O
forward O
networks O
can O
be O
removed O
with O
minimal O
decline O
in O
task O
performance O
. O
We O
find O
substantial O
overlap O
in O
the O
set O
of O
attention O
heads O
( O
un O
) O
important O
for O
incontext O
learning O
across O
tasks O
and O
number O
of O
in O
- O
context O
examples O
. O
We O
also O
address O
our O
hypothesis O
through O
a O
task O
- O
agnostic O
lens O
, O
finding O
that O
a O
small O
set O
of O
attention O
heads O
in O
OPT-66B B-MethodName
score O
highly O
on O
their O
ability O
to O
perform O
primitive O
induction O
operations O
associated O
with O
incontext O
learning O
, O
namely O
, O
prefix O
matching O
and O
copying O
. O
These O
induction O
heads O
overlap O
with O
task O
- O
specific O
important O
heads O
, O
reinforcing O
arguments O
by O
Olsson O
et O
al O
. O
( O
2022 O
) O
regarding O
induction O
head O
generality O
to O
more O
sophisticated O
behaviors O
associated O
with O
in O
- O
context O
learning O
. O
Overall O
, O
our O
study O
provides O
several O
insights O
that O
indicate O
large O
language O
models O
may O
be O
undertrained O
for O
in O
- O
context O
learning O
and O
opens O
up O
questions O
on O
how O
to O
pre O
- O
train O
language O
models O
to O
more O
effectively O
perform O
in O
- O
context O
learning O
. O

Introduction O

In O
recent O
years O
, O
large O
language O
models O
( O
LLMs O
) O
Rae O
et O
al O
. O
, O
2021 O
; O
Lieber O
et O
al O
. O
, O
2021 O
; O
Black O
et O
al O
. O
, O
2022 O
; O
Zhang O
et O
al O
. O
, O
2022 O
; O
Chowdhery O
et O
al O
. O
, O
2022 O
; O
Hoffmann O
et O
al O
. O
, O
2022 O
; O
Smith O
et O
al O
. O
, O
2022 O
) O
based O
on O
the O
Transformer O
architecture O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
pre O
- O
trained O
using O
self O
- O
supervision O
on O
web O
- O
scale O
textual O
corpora O
have O
revolutionized O
the O
field O
of O
natural O
language O
processing O
( O
NLP O
) O
. O
At O
larger O
scales O
, O
these O
models O
demonstrate O
remarkable O
emergent O
( O
Wei O
et O
al O
. O
, O
2022 O
) O
prowess O
in O
performing O
a O
wide O
variety O
of O
tasks O
without O
any O
form O
of O
fine O
- O
tuning O
, O
via O
the O
zero O
/ O
few O
- O
shot O
incontext O
learning O
paradigm O
. O
How O
in O
- O
context O
learning O
works O
has O
been O
an O
open O
question O
since O
its O
advent O
and O
recent O
studies O
( O
Xie O
et O
al O
. O
, O
2021 O
; O
Garg O
et O
al O
. O
, O
2022 O
; O
Olsson O
et O
al O
. O
, O
2022 O
; O
Min O
et O
al O
. O
, O
2022b O
) O
have O
begun O
scratching O
the O
surface O
toward O
better O
understanding O
the O
paradigm O
. O
In O
this O
paper O
, O
we O
empirically O
address O
the O
following O
key O
question O
: O
Are O
all O
LLM O
components O
really O
needed O
to O
perform O
in O
- O
context O
learning O
? O

The O
first O
way O
we O
address O
the O
aforementioned O
question O
is O
through O
the O
lens O
of O
task O
- O
specific O
importance B-MetricName
scores O
and O
structured O
pruning O
( O
Li O
et O
al O
. O
, O
2016 O
; O
Molchanov O
et O
al O
. O
, O
2016 O
; O
Anwar O
et O
al O
. O
, O
2017 O
) O
of O
components O
underlying O
modern O
LLMs O
, O
which O
are O
primarily O
stacks O
composed O
of O
multiple O
highdimensional O
self O
- O
attention O
blocks O
that O
form O
multiheaded O
attention O
and O
densely O
activated O
feed O
forward O
networks O
( O
FFNs O
) O
. O
We O
pick O
the O
Open B-MethodName
Pretrained I-MethodName
Transformer I-MethodName
( O
OPT B-MethodName
) O
( O
Zhang O
et O
al O
. O
, O
2022 O
) O
model O
with O
66B O
parameters O
for O
our O
analyses O
, O
which O
yield O
several O
surprising O
observations O
. O
We O
find O
that O
important O
attention O
heads O
are O
primarily O
clustered O
in O
the O
intermediate O
layers O
and O
important O
FFNs O
are O
primarily O
in O
later O
layers O
of O
the O
model O
( O
§ O
4 O
) O
. O
We O
find O
that O
the O
ability O
to O
perform O
zero O
/ O
few O
- O
shot O
in O
- O
context O
learning O
on O
a O
variety O
of O
14 O
NLP O
datasets O
/ O
tasks O
stays O
nearly O
intact O
when O
up O
to O
70 O
% O
( O
∼15.7B O
parameters O
in O
OPT-66B B-MethodName
) O
of O
the O
attention O
heads O
are O
removed O
( O
§ O
5.1 O
) O
. O
The O
attention O
heads O
that O
are O
( O
un O
) O
important O
for O
in O
- O
context O
learning O
also O
seem O
to O
overlap O
across O
tasks O
( O
§ O
6.1 O
) O
and O
shots O
( O
§ O
6.2 O
) O
, O
and O
pruning O
attention O
heads O
based O
on O
a O
" O
universal O
" O
importance O
order O
computed O
using O
all O
14 O
datasets O
generalizes O
to O
varying O
degrees O
on O
out O
- O
of O
- O
distribution O
datasets O
( O
§ O
6.1.2 O
) O
. O
These O
observations O
indicate O
that O
a O
common O
taskagnostic O
subset O
of O
the O
attention O
heads O
are O
responsible O
for O
in O
- O
context O
learning O
. O
We O
also O
find O
that O
only O
up O
to O
20 O
% O
of O
the O
FFNs O
( O
∼8.5B O
parameters O
) O
can O
be O
removed O
with O
minimal O
decline O
in O
zero O
/ O
few O
- O
shot O
in O
- O
context O
learning O
performance O
( O
§ O
5.2 O
) O
, O
indicating O
the O
importance O
of O
FFNs O
toward O
in O
- O
context O
learning O
. O

The O
second O
way O
we O
address O
the O
aforementioned O
question O
is O
by O
quantifying O
the O
capacity O
of O
all O
attention O
heads O
in O
OPT-66B B-MethodName
to O
perform O
a O
subset O
of O
task O
- O
agnostic O
primitive O
operations O
associated O
with O
in O
- O
context O
learning O
, O
namely O
, O
prefix B-TaskName
matching I-TaskName
and O
copying B-TaskName
: O
explicitly O
searching O
for O
a O
prior O
occurrence O
of O
the O
current O
token O
in O
- O
context O
and O
copying O
over O
its O
suffix O
. O
Elhage O
et O
al O
. O
( O
2021 O
) O
and O
Olsson O
et O
al O
. O
( O
2022 O
) O
developed O
a O
mathematical O
framework O
to O
reverse O
- O
engineer O
a O
Transformer O
and O
also O
find O
such O
heads O
, O
termed O
induction O
heads O
, O
and O
explored O
the O
hypothesis O
that O
such O
heads O
drive O
in O
- O
context O
learning O
with O
model O
sizes O
up O
to O
13B O
parameters O
in O
a O
mostly O
task O
- O
agnostic O
fashion O
. O
Using O
this O
framework O
, O
we O
compute O
task O
- O
agnostic O
scores O
for O
prefix B-TaskName
matching I-TaskName
and O
copying B-TaskName
for O
each O
attention O
head O
and O
find O
that O
a O
small O
set O
of O
heads O
in O
OPT-66B B-MethodName
have O
nontrivial O
scores O
for O
both O
primitives O
( O
§ O
6.3 O
) O
. O
Qualitative O
inspection O
and O
quantitative O
analyses O
show O
that O
these O
heads O
overlap O
( O
to O
varying O
degrees O
) O
with O
the O
ones O
identified O
earlier O
to O
be O
important O
for O
in O
- O
context O
learning O
via O
our O
set O
of O
14 O
NLP O
datasets O
/ O
tasks O
, O
indicating O
that O
induction O
heads O
are O
capable O
of O
more O
sophisticated O
behaviors O
associated O
with O
in O
- O
context O
learning O
such O
as O
latent O
concept O
matching O
but O
are O
not O
the O
only O
heads O
with O
such O
capabilities O
( O
§ O
6.3.1 O
) O
. O

Overall O
, O
our O
study O
provides O
several O
insights O
about O
in O
- O
context O
learning O
at O
massive O
scale O
using O
both O
task O
- O
specific O
and O
task O
- O
agnostic O
settings O
. O
In O
a O
world O
of O
ever O
increasing O
language O
model O
sizes O
, O
we O
believe O
these O
insights O
serve O
as O
a O
strong O
foundation O
for O
researchers O
and O
practitioners O
in O
language O
modeling O
to O
build O
and O
leverage O
compact O
language O
models O
that O
can O
also O
demonstrate O
emergent O
abilities O
. O

Background O
& O
Methods O

In O
this O
section O
, O
we O
establish O
notation O
and O
methods O
with O
the O
Open B-MethodName
Pre I-MethodName
- I-MethodName
trained I-MethodName
Transformer I-MethodName
( O
OPT B-MethodName
) O
( O
Zhang O
et O
al O
. O
, O
2022 O
) O
model O
used O
for O
our O
study O
, O
provide O
background O
on O
in O
- O
context O
learning O
and O
the O
mathematical O
formulation O
of O
induction O
heads O
by O
Olsson O
et O
al O
. O
( O
2022 O
) O
that O
we O
build O
on O
, O
and O
describe O
our O
adaptation O
of O
oracle O
and O
gradient O
- O
based O
importance B-MetricName
score O
formulations O
for O
in O
- O
context O
learning O
. O

Open B-MethodName
Pre I-MethodName
- I-MethodName
trained I-MethodName
Transformer I-MethodName
( O
OPT B-MethodName
) O

OPT B-MethodName
is O
a O
suite O
of O
language O
models O
of O
varying O
sizes O
aimed O
at O
serving O
as O
open O
replicas O
of O
GPT-3 O
. O
The O
largest O
openly O
accessible O
model O
from O
this O
suite O
is O
OPT-66B B-MethodName
with O
66 O
billion O
parameters O
. O

Architecture O
: O
Consider O
a O
tokenized O
input O
sentence O
to O
OPT B-MethodName
, O
X O
∈ O
R O
N O
×de B-HyperparameterName
, O
where O
N O
is O
the O
number O
of O
tokens O
in O
the O
sentence O
and O
d B-HyperparameterName
e I-HyperparameterName
is O
the O
embedding B-HyperparameterName
dimension I-HyperparameterName
. O
The O
input O
is O
processed O
by O
multiple O
decoder O
layers O
consisting O
of O
multi O
- O
headed O
attention O
( O
MHA O
) O
blocks O
, O
layer O
norm O
( O
LN O
) O
and O
feed O
forward O
networks O
( O
FFN O
) O
, O
followed O
by O
a O
linear O
layer O
to O
produce O
logits O
over O
the O
vocabulary O
. O
The O
decoder O
layers O
can O
be O
formally O
expressed O
as O
follows O
: O

t O
( O
ℓ+1 O
) O
= O
z O
ℓ O
+ O
MHA O
ℓ O
( O
LN O
ℓ O
( O
z O
ℓ O
) O
) O

( O
1 O
) O

z O
( O
ℓ+1 O
) O
= O
t O
( O
ℓ+1 O
) O
+ O
FFN O
ℓ O
( O
t O
( O
ℓ+1 O
) O
) O
( O
2 O
) O

where O
z O
1 O
= O
X O
, O
and O
( O
1 O
) O
& O
( O
2 O
) O
are O
the O
residual O
connections O
corresponding O
to O
the O
MHA O
and O
FFN O
in O
layer O
ℓ O
> O
= O
1 O
respectively O
. O
OPT-66B B-MethodName
was O
pre O
- O
trained O
with O
a O
maximum B-HyperparameterValue
sequence I-HyperparameterValue
length I-HyperparameterValue
of O
2048 B-HyperparameterValue
and O
embedding B-HyperparameterName
dimension I-HyperparameterName
d B-HyperparameterName
e I-HyperparameterName
= O
9216 B-HyperparameterValue
. O

MHA O
: O

In O
an O
MHA O
block O
, O
H B-HyperparameterName
attention B-HyperparameterName
heads I-HyperparameterName
are O
applied O
in O
parallel O
to O
the O
input O
and O
their O
outputs O
are O
concatenated O
. O
In O
OPT-66B B-MethodName
, O
there O
are O
H B-HyperparameterName
= O
72 B-HyperparameterValue
attention B-HyperparameterName
heads I-HyperparameterName
of O
dimension B-HyperparameterName
d B-HyperparameterName
h I-HyperparameterName
= O
128 B-HyperparameterValue
in O
every O
layer O
ℓ. O
An O
individual O
attention O
head O
h O
in O
layer O
ℓ O
consists O
of O
three O
learnable O
matrices O
, O
W O
h O
k O
, O
W O
h O
q O
, O
W O
h O
v O
∈ O
R O
de×d O
h O
, O
all O
unique O
to O
the O
head O
, O
such O
that O
it O
applies O
selfattention O
A O
h O
( O
. O
) O
on O
the O
input O
, O
where O
d B-HyperparameterName
h I-HyperparameterName
= O
d B-HyperparameterName
e I-HyperparameterName
/ O
H. B-HyperparameterName
Formally O
, O
for O
input O
M O
in O
layer O
ℓ O
: O

MHA O
ℓ O
( O
M O
) O
= O
[ O
A O
1 O
( O
M O
) O
; O
• O
• O
• O
; O
A O
H O
( O
M O
) O
] O
W O
ℓ O
o O
( O
3 O
) O
A O
h O
( O
M O
) O
= O
s O
h O
( O
M O
) O
MW O
h O
v O
( O
4 O
) O
s O
h O
( O
M O
) O
= O
σ O
MW O
h O
q O
( O
W O
h O
k O
) O
T O
M O
T O
√ O
d O
h O
( O
5 O

) O

where O
σ O
is O
the O
softmax O
function O
and O
W O
ℓ O
o O
∈ O
R O
de×de O
is O
a O
learnable O
output O
matrix O
unique O
to O
the O
MHA O
block O
in O
layer O
ℓ. O
To O
ensure O
OPT B-MethodName
is O
auto O
- O
regressive O
, O
the O
output O
of O
s O
h O
( O
. O
) O
is O
masked O
to O
prevent O
the O
dependence O
of O
the O
hidden O
state O
of O
the O
token O
i O
, O
z O
ℓ O
i O
∈ O
R O
de O
, O
on O
future O
tokens O
in O
indices O
{ O
i O
+ O
1 O
, O
. O
. O
. O
, O
N O
} O
. O

To O
remove O
a O
head O
h O
in O
layer O
ℓ O
in O
practice O
, O
we O
set O
A O
h O
( O
M O
) O
to O
be O
the O
zero O
matrix O
in O
Equation O
( O
3 O
) O
. O
This O
implies O
that O
W O
h O
k O
, O
W O
h O
q O
, O
W O
h O
v O
can O
be O
entirely O
removed O
, O
and O
the O
corresponding O
d O
h O
rows O
in O
W O
ℓ O
o O
can O
also O
be O
removed O
. O
In O
total O
, O
there O
are O
4608 B-HyperparameterValue
attention B-HyperparameterName
heads I-HyperparameterName
across O
64 B-HyperparameterValue
layers B-HyperparameterName
in O
OPT-66B B-MethodName
that O
constitute O
21.7B O
of O
the O
total O
66B O
parameters O
. O

FFN O
: O
Each O
layer O
ℓ O
consists O
of O
a O
feed O
forward O
network O
( O
FFN O
) O
parameterized O
by O
a O
high O
- O
dimensional O
projection O
matrix O
, O
W O
ℓ O
1 O
∈ O
R O
de×d O
followed O
by O
a O
low O
- O
dimensional O
projection O
matrix O
, O
W O
ℓ O
2 O
∈ O
R O
d×de B-HyperparameterName
where O
d B-HyperparameterName
= O
36864 B-HyperparameterValue
for O
OPT-66B. B-MethodName
Formally O
, O
for O
input O
M O
in O
layer O
ℓ O
: O

FFN O
ℓ O
( O
M O
) O
= O
ReLU O
( O
LN O
ℓ O
( O
M O
) O
W O
ℓ O
1 O
) O
W O
ℓ O
2 O
( O
6 O
) O

where O
ReLU O
is O
the O
rectified O
linear O
unit O
activation O
function O
and O
LN O
is O
the O
layer O
norm O
. O

To O
remove O
an O
FFN O
in O
layer O
ℓ O
in O
practice O
, O
we O
set O
FFN O
ℓ O
( O
M O
) O
to O
be O
the O
zero O
matrix O
in O
Equation O
( O
6 O
) O
. O
This O
implies O
W O
ℓ O
1 O
, O
W O
ℓ O
2 O
and O
the O
layer O
norm O
LN O
ℓ O
( O
. O
) O
for O
the O
FFN O
can O
be O
entirely O
removed O
. O
In O
total O
, O
FFNs O
constitute O
43.4B O
parameters O
in O
OPT-66B B-MethodName
. O

In O
- O
Context O
Learning O
& O
Induction O
Heads O

With O
increasingly O
larger O
language O
models O
being O
trained O
in O
recent O
years O
, O
a O
new O
paradigm O
of O
learning O
termed O
in O
- O
context O
learning O
has O
become O
popular O
. O
In O
this O
paradigm O
, O
language O
models O
perform O
tasks O
by O
being O
prompted O
to O
generate O
output O
text O
conditioned O
on O
a O
few O
( O
or O
zero O
) O
incontext O
training O
examples O
that O
form O
solved O
" O
inputoutput O
" O
pairs O
for O
the O
task O
along O
with O
a O
query O
input O
. O
Figure O
1 O
illustrates O
the O
paradigm O
for O
the O
task O
of O
identifying O
the O
sound O
that O
an O
animal O
makes O
. O
In O
some O
cases O
, O
tasks O
can O
also O
be O
accompanied O
by O
task O
descriptions O
/ O
templates O
to O
help O
prime O
the O
language O
model O
better O
, O
e.g. O
, O
zero O
- O
shot O
translating O
from O
English O
to O
German O
using O
the O
prompt O
: O
While O
these O
examples O
involve O
learning O
and O
relying O
on O
latent O
concepts O
during O
inference O
, O
few O
- O
shot O
in O
- O
context O
learning O
can O
additionally O
involve O
explicit O
primitive O
interactions O
between O
the O
in O
- O
context O
examples O
. O
For O
example O
, O
with O
the O
prompt O
: O
the O
model O
may O
rely O
on O
prior O
in O
- O
context O
translations O
of O
the O
tokens O
I O
and O
like O
when O
performing O
the O
task O
for O
the O
query O
input O
. O
Olsson O
et O
al O
. O
( O
2022 O
) O
developed O
a O
mathematical O
framework O
toward O
better O
understanding O
such O
mechanics O
, O
starting O
off O
with O
a O
task O
- O
agnostic O
formulation O
of O
in O
- O
context O
learning O
as O
the O
ability O
of O
a O
model O
to O
better O
predict O
tokens O
later O
in O
the O
context O
than O
the O
tokens O
earlier O
. O
They O
define O
a O
set O
of O
task O
- O
agnostic O
primitive O
operations O
that O
reflect O
the O
kind O
of O
interactions O
we O
refer O
to O
in O
the O
above O
example O
, O
namely O
, O
prefix B-TaskName
matching I-TaskName
and O
copying B-TaskName
. O
These O
operations O
are O
defined O
in O
a O
simplistic O
fashion O
on O
a O
repeated O
sequence O
of O
randomly O
generated O
tokens O
: O
explicitly O
searching O
for O
a O
prior O
occurrence O
of O
the O
current O
token O
in O
- O
context O
and O
copying O
over O
its O
suffix O
. O
The O
heads O
that O
are O
capable O
of O
performing O
these O
operations O
are O
termed O
induction O
heads O
. O
Figure O
2 O
depicts O
these O
operations O
for O
a O
repeated O
sequence O
of O
tokens O
. O
While O
these O
operations O
are O
intertwined O
in O
practice O
, O
the O
capacity O
of O
attention O
heads O
to O
independently O
perform O
them O
is O
computed O
with O
the O
scoring O
algorithms O
described O
in O
detail O
in O
Appendix O
A.8 O
. O

Importance B-MetricName
Scores O

Consider O
a O
model O
M O
and O
a O
dataset O
D O
= O
{ O
X O
, O
Y O
} O
, O
where O

X O
= O
{ O
x O
1 O
, O
• O
• O
• O
, O
x O
L O
} O
and O
Y O
= O
{ O
y O
1 O
, O
• O
• O
• O
, O
y O
L O
} O
such O
that O
x O
i O
represents O
a O
prompt O
with O
few O
( O
or O
zero O
) O
in O
- O
context O

training O
examples O
along O
with O
a O
query O
input O
and O
y O
i O
represents O
the O
corresponding O
target O
output O
sequence O
. O
We O
define O
and O
compute O
importance B-MetricName
scores O
for O
model O
components O
using O
such O
datasets O
to O
quantify O
their O
relative O
contributions O
to O
the O
model O
's O
ability O
to O
perform O
in O
- O
context O
learning O
. O

Oracle O

Let O
P O
M O
( O
D O
) O
denote O
a O
dataset O
/ O
task O
- O
specific O
performance O
metric O
, O
e.g. O
, O
accuracy B-MetricName
. O
Given O
dataset O
D O
, O
the O
oracle O
importance B-MetricName
score O
of O
a O
component O
C O
in O
M O
is O
computed O
as O
follows O
: O

IS O
C O
( O
D O
) O
= O
P O
M O
( O
D O
) O
− O
P O
M O
\C O
( O
D O
) O
( O
7 O
) O

where O
M O
\C O
denotes O
the O
resultant O
model O
when O
C O
is O
pruned O
from O
M. O
Clearly O
, O
if O
pruning O
a O
component O
leads O
to O
poor O
model O
performance O
on O
the O
task O
, O
it O
must O
be O
important O
for O
the O
task O
. O
Similarly O
, O
if O
there O
is O
no O
difference O
or O
an O
improvement O
in O
performance O
upon O
pruning O
a O
component O
, O
it O
must O
be O
unimportant O
. O
Computing O
oracle O
importance B-MetricName
scores O
for O
K O
model O
components O
requires O
us O
to O
perform O
O O
( O
K O
) O
evaluations O
for O
each O
dataset O
D O
. O

Gradient O
- O
based O

Given O
dataset O
D O
, O
the O
gradient O
- O
based O
importance B-MetricName
score O
( O
Molchanov O
et O
al O
. O
, O
2016 O
; O
Michel O
et O
al O
. O
, O
2019 O
) O
of O
an O
attention O
head O
h O
captures O
the O
expected O
sensitivity O
of O
the O
model O
to O
h O
and O
is O
computed O
as O
follows O
: O

IS O
h O
( O
D O
) O
= O
E O
( O
x O
, O
y O
) O
A O
h O
( O
[ O
x O
; O
y O
] O
) O
T O
∂L O
( O
y|x O
) O
∂A O
h O
( O
[ O
x O
; O
y O
] O
) O
( O
8 O
) O

where O
; O
is O
the O
concatenation O
operator O
, O
( O
x O
, O
y O
) O
∼ O
D O
such O
that O
x O
is O
a O
sequence O
of O
T O
x O
tokens O
x O
1 O
: O
Tx O
, O
y O
is O
a O
sequence O
of O
T O
y O
tokens O
y O
1 O
: O
Ty O
, O
A O
h O
is O
the O
output O
of O
head O
h O
defined O
in O
( O
4 O
) O
and O
the O
loss O
term O
in O
( O
8 O
) O
is O
computed O
using O
the O
auto O
- O
regressive O
decomposition O
of O
the O
log O
- O
likelihood O
: O

L O
( O
y|x O
) O
= O
− O
1 O
T O
y O
j O
= O
Ty O
j=1 O

log O
( O
p O
( O
y O
j O
|x O
, O
y O
1 O
: O
j−1 O
) O
) O
( O
9 O
) O

These O
importance B-MetricName
scores O
can O
be O
efficiently O
computed O
for O
all O
heads O
by O
simply O
performing O
a O
single O
forward O
and O
backward O
pass O
over O
the O
model O
with O
D O
. O

We O
also O
define O
the O
aggregated O
importance B-MetricName
score O
of O
an O
attention O
head O
on O
a O
set O
of O
datasets O
S O
= O
{ O
D O
1 O
, O
• O
• O
• O
, O
D O
K O
} O
as O
follows O
: O

IS O
h O
( O
S O
) O
= O
E O
D∼S O
[ O
IS O
h O
( O
D O
) O
] O
( O
10 O
) O

3 O
Experimental O
Setup O

We O
conducted O
our O
experiments O
on O
OPT-66B B-MethodName
, O
which O
was O
the O
largest O
publicly O
available O
dense O
decoderonly O
language O
model O
at O
the O
time O
of O
our O
experiments O
. O
We O
efficiently O
compute O
gradient O
- O
based O
importance B-MetricName
scores O
for O
the O
4608 B-HyperparameterValue
attention B-HyperparameterName
heads I-HyperparameterName
and O
oracle O
importance B-MetricName
scores O
for O
the O
64 B-HyperparameterValue
feed B-HyperparameterName
forward I-HyperparameterName
networks I-HyperparameterName
( O
FFNs B-HyperparameterName
) O
in O
OPT-66B. B-MethodName
We O
experiment O
with O
a O
variety O
of O
14 O
NLP O
datasets O
/ O
tasks O
. O
For O
consistency O
in O
the O
evaluation O
metric O
, O
we O
report O
accuracy B-MetricName
on O
all O
tasks O
. O
Our O
choice O
of O
datasets O
and O
metric O
is O
in O
line O
with O
Zhang O
et O
al O
. O
( O
2022 O
) O
. O
The O
datasets O
include O
ARC B-DatasetName
Easy O
and O
Challenge O
and O
OpenBookQA B-DatasetName
( O
Mihaylov O
et O
al O
. O
, O
2018 O
) O
for O
advanced O
question B-TaskName
- I-TaskName
answering I-TaskName
, O
Hel B-DatasetName
- I-DatasetName
laSwag I-DatasetName
( O
Zellers O
et O
al O
. O
, O
2019 O
) O
, O
PIQA B-DatasetName
( O
Bisk O
et O
al O
. O
, O
2020 O
) O
and O
Winogrande B-DatasetName
( O
Sakaguchi O
et O
al O
. O
, O
2021 O
) O
for O
various O
forms O
of O
commonsense B-TaskName
reasoning I-TaskName
, O
and O
the O
following O
datasets O
from O
the O
standard O
SuperGLUE B-DatasetName
benchmark O
( O
Wang O
et O
al O
. O
, O
2019 O
) O
: O
BoolQ B-DatasetName
, O
CB B-DatasetName
, O
COPA B-DatasetName
, O
MultiRC B-DatasetName
, O
ReCoRD B-DatasetName
, O
RTE B-DatasetName
, O
WiC B-DatasetName
, O
and O
WSC B-DatasetName
. O
For O
a O
subset O
of O
experiments O
involving O
evaluation O
of O
outof O
- O
distribution O
generalization O
, O
we O
also O
use O
2 O
additional O
datasets O
: O
MathQA B-DatasetName
( O
Amini O
et O
al O
. O
, O
2019 O
) O
and O
LAMBADA B-DatasetName
( O
Paperno O
et O
al O
. O
, O
2016 O
) O
. O
We O
use O
a O
modified O
version O
of O
the O
lm O
- O
evaluation O
- O
harness O
framework O
( O
Gao O
et O
al O
. O
, O
2021 O
) O
for O
our O
experiments O
. O
The O
default O
framework O
samples O
in O
- O
context O
examples O
at O
random O
, O
which O
we O
use O
without O
modification O
. O

4 O
Importance B-MetricName
Scores O
for O
OPT-66B B-MethodName
Figure O
3 O
depicts O
a O
heatmap O
of O
the O
head O
importance B-MetricName
scores O
averaged O
across O
all O
tasks O
( O
as O
described O
in O
§ O
2.3.2 O
) O
in O
the O
5 O
- O
shot O
setting O
. O
Task O
- O
averaged O
heatmaps O
for O
the O
0 O
- O
shot O
and O
1 O
- O
shot O
settings O
and O
all O
task O
- O
specific O
heatmaps O
are O
provided O
in O
Appendix O
A.1 O
. O
We O
observe O
that O
the O
important O
attention O
heads O
are O
primarily O
clustered O
in O
the O
intermediate O
layers O
of O
OPT-66B B-MethodName
in O
both O
the O
task O
- O
averaged O
and O
taskspecific O
cases O
. O
We O
also O
observe O
overlap O
in O
the O
important O
heads O
across O
the O
different O
zero O
/ O
few O
- O
shot O
settings O
, O
confirmed O
in O
follow O
- O
up O
analysis O
in O
§ O
6.2 O
. O

Attention O
Heads O

Feed O
Forward O
Networks O

We O
compute O
oracle O
importance B-MetricName
scores O
( O
both O
taskspecific O
and O
averaged O
across O
tasks O
) O
for O
each O
FFN O
as O
described O
in O
§ O
2.3.1 O
in O
the O
zero O
/ O
few O
- O
shot O
settings O
. O
1 O
4 O
7 O
10 O
13 O
16 O
19 O
22 O
25 O
28 O
31 O
34 O
37 O
40 O
43 O
46 O
49 O
52 O
55 O
58 O
61 O
We O
observe O
in O
the O
0 O
/ O
1 O
- O
shot O
settings O
that O
the O
removal O
of O
any O
FFN O
in O
the O
early O
( O
1 O
- O
30 O
) O
layers O
of O
OPT-66B B-MethodName
either O
gives O
comparable O
or O
better O
performance O
for O
a O
vast O
majority O
of O
tasks O
. O
In O
the O
5 O
- O
shot O
setting O
however O
, O
both O
the O
early O
and O
later O
layers O
seem O
to O
have O
important O
FFNs O
for O
most O
tasks O
. O
We O
also O
generally O
observe O
high O
variance O
in O
FFN O
importance B-MetricName
scores O
in O
later O
layers O
. O
We O
particularly O
note O
high O
variance O
for O
WSC B-DatasetName
and O
MultiRC B-DatasetName
, O
observing O
that O
removal O
of O
some O
individual O
FFNs O
can O
lead O
to O
absolute O
accuracy B-MetricName
improvements O
/ O
degradation O
of O
up O
to O
20 B-MetricValue
% I-MetricValue
! O
We O
leave O
further O
investigation O
into O
the O
cause O
for O
this O
variance O
for O
future O
work O
. O

Iterative O
Pruning O

We O
now O
assess O
to O
what O
extent O
we O
can O
remove O
multiple O
attention O
heads O
and O
/ O
or O
FFNs O
with O
minimal O
decline O
in O
task O
performance O
. O
For O
each O
task O
in O
each O
( O
0 O
/ O
1 O
/ O
5 O
- O
shot O
) O
in O
- O
context O
learning O
setting O
, O
we O
create O
separate O
rankings O
of O
attention O
heads O
and O
FFNs O
in O
OPT-66B B-MethodName
by O
separately O
sorting O
them O
in O
ascending O
order O
by O
their O
importance B-MetricName
scores O
( O
§ O
4.1 O
and O
§ O
4.2 O
) O
. O
We O
then O
remove O
unimportant O
attention O
heads O
or O
FFNs O
in O
an O
iterative O
fashion O
using O
these O
rankings O
, O
10 O
% O
at O
a O
time O
, O
and O
re O
- O
evaluate O
task O
performance O
after O
each O
removal O
. O
1 O

Removing O
Attention O
Heads O

Figure O
5 O
depicts O
the O
resulting O
task O
- O
specific O
and O
task O
- O
averaged O
accuracy B-MetricName
trends O
in O
the O
5 O
- O
shot O
setting O
. O
Corresponding O
0 O
/ O
1 O
- O
shot O
trends O
are O
depicted O
in O
Appendix O
A.3 O
. O
We O
observe O
that O
the O
average O
accuracy B-MetricName
across O
tasks O
does O
not O
change O
much O
up O
until O
1 O
We O
do O
not O
remove O
attention O
heads O
one O
at O
a O
time O
and O
reevaluate O
given O
the O
number O
of O
heads O
and O
evaluation O
cost O
. O
∼70 O
% O
of O
the O
attention O
heads O
are O
removed O
. O
A O
finegrained O
look O
at O
the O
individual O
tasks O
also O
mostly O
shows O
similar O
trends O
, O
with O
accuracy B-MetricName
staying O
fairly O
intact O
until O
a O
large O
proportion O
of O
the O
heads O
are O
removed O
. O
Some O
oddities O
include O
tasks O
such O
as O
WSC B-DatasetName
and O
CB B-DatasetName
, O
wherein O
we O
see O
that O
the O
0 O
- O
shot O
accuracy B-MetricName
actually O
increases O
after O
removal O
of O
70 O
% O
of O
the O
heads O
. O
Figure O
6 O
depicts O
the O
resulting O
task O
- O
specific O
and O
task O
- O
averaged O
accuracy B-MetricName
trends O
in O
the O
0 O
- O
shot O
setting O
. O
Corresponding O
1 O
/ O
5 O
- O
shot O
trends O
are O
depicted O
in O
Appendix O
A.4 O
. O
We O
observe O
that O
in O
the O
0 O
- O
shot O
setting O
, O
the O
average O
accuracy B-MetricName
across O
tasks O
does O
not O
change O
up O
until O
∼20 O
% O
of O
the O
FFNs O
are O
removed O
. O
For O
some O
tasks O
such O
as O
PIQA B-DatasetName
, O
Winogrande B-DatasetName
and O
RTE B-DatasetName
, O
the O
accuracy B-MetricName
does O
not O
change O
even O
if O
30 O
% O
of O
the O
FFNs O
( O
∼13B O
of O
the O
66B O
parameters O
) O
are O
removed O
. O
We O
also O
observe O
that O
the O
inflection O
point O
after O
which O
we O
observe O
a O
sharp O
decline O
in O
accuracy B-MetricName
changes O
to O
10 B-MetricValue
% I-MetricValue
for O
the O
few O
- O
shot O
settings O
. O
Overall O
, O
these O
observations O
indicate O
that O
FFNs O
play O
a O
critical O
role O
toward O
in O
- O
context O
learning O
. O
We O
now O
investigate O
whether O
the O
inflection O
points O
to O
in O
- O
context O
learning O
performance O
when O
removing O
either O
attention O
heads O
or O
FFNs O
in O
an O
iterative O
fashion O
still O
hold O
when O
removing O
them O
in O
tandem O
. O
Figure O
7 O
depicts O
the O
average O
5 O
- O
shot O
accuracy B-MetricName
of O
all O
tasks O
on O
joint O
iterative O
removal O
of O
attention O
heads O
and O
FFNs O
. O
Corresponding O
0 O
/ O
1 O
- O
shot O
trends O
are O
depicted O
in O
Appendix O
A.5 O
. O
We O
observe O
that O
the O
removal O
of O
70 O
% O
of O
the O
attention O
heads O
( O
∼15.7B O
parameters O
) O
and O
20 O
% O
of O
the O
FFNs O
( O
∼8.5B O
parameters O
) O
leads O
to O
a O
mere O
5 B-MetricValue
% I-MetricValue
absolute O
drop O
in O
the O
average O
0 O
- O
shot O
accuracy B-MetricName
. O
In O
the O
1 O
- O
shot O
setting O
, O
the O
drop O
in O
accuracy B-MetricName
is O
6 B-MetricValue
% I-MetricValue
on O
removing O
70 O
% O
of O
the O
attention O
heads O
and O
10 O
% O
of O
the O
FFNs O
. O
In O
the O
5 O
- O
shot O
setting O
, O
the O
drop O
in O
accuracy B-MetricName
is O
4 B-MetricValue
% I-MetricValue
on O
removing O
60 O
% O
of O
the O
attention O
heads O
and O
20 O
% O
of O
the O
FFNs O
. O
Overall O
, O
these O
new O
inflection O
points O
have O
deviated O
by O
at O
most O
10 B-MetricValue
% I-MetricValue
absolute O
, O
which O
may O
be O
attributed O
to O
the O
interplay O
between O
heads O
and O
FFNs O
. O

Removing O
FFNs O

Combined O
Removal O
of O
Heads O
& O
FFNs O

Detailed O
Analysis O
of O
Attention O
Heads O

In O
this O
section O
, O
we O
perform O
a O
detailed O
analysis O
of O
the O
attention O
heads O
in O
OPT-66B B-MethodName
, O
given O
that O
in O
- O
context O
learning O
is O
auto O
- O
regressive O
in O
nature O
and O
attention O
heads O
explicitly O
encode O
cross O
- O
token O
interactions O
. O
Michel O
et O
al O
. O
( O
2019 O
) O
found O
preliminary O
empirical O
evidence O
of O
the O
existence O
of O
" O
universally O
" O
important O
attention O
heads O
in O
trained O
task O
- O
specific O
Transformer O
and O
BERT O
models O
via O
evaluating O
on O
out O
- O
of O
- O
domain O
test O
sets O
for O
machine O
translation O
and O
natural O
language O
inference O
respectively O
. O
With O
similar O
motivation O
, O
we O
study O
if O
the O
( O
un O
) O
important O
attention O
heads O
identified O
in O
various O
in O
- O
context O
learning O
settings O
for O
OPT-66B B-MethodName
are O
shared O
across O
tasks O
. O

Cross O
- O
Task O
Analysis O

Spearman B-MetricName
's I-MetricName
Rank I-MetricName
Correlation I-MetricName

We O
assess O
overlap O
in O
( O
un O
) O
important O
attention O
heads O
across O
tasks O
by O
sorting O
task O
- O
specific O
head O
importance B-MetricName
scores O
to O
get O
head O
importance B-MetricName
rankings I-MetricName
and O
computing O
the O
Spearman B-MetricName
's I-MetricName
rank I-MetricName
correlation I-MetricName
coefficient I-MetricName
( O
SRCC B-MetricName
) O
between O
the O
rankings O
for O
every O
pair O
of O
tasks O
in O
the O
zero O
- O
shot O
and O
few O
- O
shot O
settings O
. O
We O
also O
sort O
the O
task O
- O
aggregate O
head O
importance B-MetricName
scores O
to O
get O
the O
aggregate O
ranking O
and O
compute O
the O
SRCC B-MetricName
against O
the O
ranking O
for O
every O
constituent O
task O
. O
All O
correlations O
are O
depicted O
in O
Figure O
8 O
for O
the O
5 O
- O
shot O
setting O
and O
Appendix O
A.6 O
for O
the O
0 O
/ O
1 O
- O
shot O
settings O
. O

In O
both O
zero O
and O
few O
- O
shot O
settings O
, O
we O
observe O
statistically O
significant O
( O
p O
< O
0.01 O
) O
positive O
correlations O
in O
the O
head O
importance B-MetricName
rankings I-MetricName
for O
every O
pair O
of O
tasks O
, O
as O
well O
as O
between O
every O
task O
's O
ranking O
and O
the O
aggregate O
ranking O
. O
This O
indicates O
that O
the O
set O
of O
( O
un O
) O
important O
attention O
heads O
are O
clustered O
together O
across O
tasks O
. O
We O
also O
observe O
seemingly O
lower O
magnitude O
SRCC B-MetricName
values O
between O
every O
task O
and O
ReCoRD B-DatasetName
, O
a O
long O
reading B-TaskName
comprehension I-TaskName
task O
which O
requires O
commonsense B-TaskName
reasoning I-TaskName
, O
indicating O
the O
amount O
of O
head O
overlap O
is O
proportionally O
lower O
. O

Generalization O
Trends O

To O
understand O
how O
well O
head O
importance B-MetricName
rankings I-MetricName
generalize O
across O
tasks O
, O
we O
study O
accuracy B-MetricName
trends O
for O
tasks O
when O
pruning O
using O
various O
head O
importance B-MetricName
rankings I-MetricName
. O
We O
study O
two O
sets O
of O
tasks O
. O

The O
first O
set O
of O
tasks O
we O
study O
were O
used O
to O
compute O
the O
aggregate O
ranking O
: O
COPA B-DatasetName
, O
Winogrande B-DatasetName
and O
ReCoRD B-DatasetName
. O
For O
each O
of O
these O
3 O
tasks O
, O
we O
consider O
the O
impact O
of O
pruning O
based O
on O
the O
self O
- O
ranking O
, O
aggregate O
ranking O
and O
the O
rankings O
from O
the O
tasks O
which O
share O
the O
highest O
and O
lowest O
SRCC B-MetricName
with O
them O
. O
Figures O
9a O
, O
9b O
and O
9c O
de O
- O
pict O
the O
accuracy B-MetricName
trends O
for O
these O
3 O
tasks O
in O
the O
5 O
- O
shot O
setting O
. O
Corresponding O
trends O
in O
the O
0 O
/ O
1shot O
settings O
are O
in O
Appendix O
A.7 O
. O
In O
the O
0 O
- O
shot O
setting O
, O
we O
observe O
that O
the O
accuracy B-MetricName
on O
all O
3 O
tasks O
when O
pruning O
using O
the O
rankings O
described O
is O
almost O
unaffected O
up O
to O
the O
50 O
% O
mark O
. O
We O
then O
observe O
a O
sharp O
decline O
in O
accuracy B-MetricName
on O
COPA B-DatasetName
and O
Winogrande B-DatasetName
when O
the O
model O
is O
pruned O
to O
the O
70 O
% O
mark O
using O
the O
ranking O
identified O
via O
ReCoRD B-DatasetName
, O
the O
task O
with O
the O
lowest O
SRCC B-MetricName
( O
0.13 B-MetricValue
) O
with O
both O
COPA B-DatasetName
and O
Winogrande B-DatasetName
. O
This O
indicates O
that O
even O
if O
the O
rankings O
vary O
between O
ReCoRD B-DatasetName
and O
COPA B-DatasetName
/ O
Winogrande B-DatasetName
( O
as O
reflected O
in O
the O
low O
magnitude O
of O
the O
SRCC B-MetricName
score O
) O
, O
the O
set O
of O
attention O
heads O
important O
for O
0 O
- O
shot O
learning O
with O
ReCoRD B-DatasetName
are O
important O
for O
COPA B-DatasetName
/ O
Winogrande B-DatasetName
too O
. O
To O
further O
verify O
this O
, O
we O
calculated O
and O
found O
71 O
% O
and O
76 O
% O
overlap O
between O
the O
top O
30 O
% O
important O
attention O
heads O
for O
ReCoRD B-DatasetName
- O
COPA B-DatasetName
and O
ReCoRD B-DatasetName
- O
Winogrande B-DatasetName
respectively O
. O
Comparing O
the O
zero O
- O
shot O
setting O
against O
the O
few O
- O
shot O
settings O
, O
we O
note O
that O
the O
decline O
/ O
divergence O
in O
accuracy B-MetricName
beyond O
the O
50 O
% O
pruning O
mark O
using O
the O
ReCoRD B-DatasetName
ranking O
is O
less O
sharp O
for O
COPA B-DatasetName
and O
Winogrande B-DatasetName
in O
the O
1 O
- O
shot O
setting O
and O
fades O
away O
in O
the O
5 O
- O
shot O
setting O
, O
indicating O
a O
convergence O
of O
important O
heads O
across O
tasks O
. O

The O
second O
set O
of O
tasks O
we O
study O
are O
unseen O
, O
i.e. O
, O
not O
used O
to O
compute O
the O
aggregate O
ranking O
: O
MathQA B-DatasetName
and O
LAMBADA B-DatasetName
. O
For O
these O
tasks O
, O
we O
analyze O
accuracy B-MetricName
trends O
when O
pruning O
using O
the O
selfranking O
and O
aggregate O
ranking O
. O
Figures O
9d O
and O
9e O
depict O
their O
accuracy B-MetricName
trends O
in O
the O
5 O
- O
shot O
setting O
. O
Corresponding O
trends O
in O
the O
0 O
/ O
1 O
- O
shot O
settings O
are O
in O
Appendix O
A.7 O
. O
As O
expected O
, O
we O
observe O
that O
the O
self O
- O
ranking O
accuracy O
curves O
are O
somewhat O
higher O
than O
the O
aggregate O
ranking O
accuracy O
curves O
in O
general O
across O
both O
tasks O
. O
For O
MathQA B-DatasetName
, O
we O
also O
observe O
that O
the O
absolute O
difference O
in O
accuracy B-MetricName
for O
both O
cases O
is O
within O
1 B-MetricValue
- I-MetricValue
2 I-MetricValue
% I-MetricValue
. O
These O
indicate O
that O
the O
aggregate O
rankings O
generalize O
well O
to O
MathQA B-DatasetName
but O
not O
as O
much O
to O
LAMBADA B-DatasetName
. O

Cross O
- O
Shot O
Analysis O

To O
see O
if O
the O
attention O
heads O
identified O
to O
be O
( O
un O
) O
important O
for O
a O
task O
are O
shared O
across O
the O
different O
zero O
and O
few O
- O
shot O
settings O
, O
we O
compute O
Spearman B-MetricName
's I-MetricName
rank I-MetricName
correlation I-MetricName
coefficient I-MetricName
( O
SRCC B-MetricName
) O
between O
the O
cross O
- O
shot O
head O
importance O
rankings O
for O
each O
task O
and O
compute O
the O
mean O
and O
variance O
across O
all O
14 O
tasks O
. O
We O
observe O
that O
the O
mean O
SRCC B-MetricName
is O
higher O
for O
rankings O
within O
the O
few O
- O
shot O
setting O
( O
0.41 B-MetricValue
for O
1 O
- O
shot O
vs. O
5 O
- O
shot O
) O
than O
for O
rankings O
across O
the O
zero O
and O
few O
- O
shot O
settings O
( O
0.39 B-MetricValue
for O
0 O
- O
shot O
vs. O
1 O
- O
shot O
and O
0.37 B-MetricValue
for O
0 O
- O
shot O
vs. O
5 O
- O
shot O
) O
, O
with O
low O
variance O
( O
0.001 O
) O
and O
p O
- O
value O
< O
0.01 O
. O
This O
matches O
the O
intuition O
that O
a O
similar O
set O
of O
heads O
must O
be O
important O
within O
the O
different O
few O
- O
shot O
settings O
than O
across O
the O
zero O
- O
shot O
and O
any O
of O
the O
few O
- O
shot O
settings O
. O
However O
, O
we O
also O
see O
that O
the O
SRCC B-MetricName
magnitudes O
for O
the O
latter O
are O
not O
very O
far O
off O
. O
In O
totality O
, O
these O
indicate O
non O
- O
trivial O
overlap O
in O
the O
( O
un O
) O
important O
attention O
heads O
for O
tasks O
across O
shots O
. O

Induction O
Heads O
in O
OPT-66B B-MethodName

We O
look O
for O
induction O
heads O
in O
OPT-66B B-MethodName
by O
quantifying O
the O
capacity O
of O
all O
attention O
heads O
to O
perform O
prefix B-TaskName
matching I-TaskName
and O
copying B-TaskName
using O
random O
input O
sequences O
in O
a O
task O
- O
agnostic O
fashion O
, O
following O
the O
definition O
and O
algorithms O
by O
Olsson O
et O
al O
. O
( O
2022 O
) O
discussed O
in O
§ O
2.2 O
and O
Appendix O
A.8 O
. O

Figures O
10a O
and O
10b O
depict O
the O
prefix B-MetricName
matching I-MetricName
and O
copying B-MetricName
score O
heatmaps O
respectively O
for O
OPT-66B. B-MethodName
We O
observe O
that O
a O
small O
subset O
of O
attention O
heads O
in O
OPT-66B B-MethodName
have O
high O
prefix B-MetricName
matching I-MetricName
scores O
, O
located O
in O
the O
upper O
layers O
( O
31 O
+ O
) O
of O
the O
model O
. O
On O
the O
other O
hand O
, O
there O
are O
a O
relatively O
larger O
number O
of O
attention O
heads O
with O
high O
copying B-MetricName
scores O
, O
although O
the O
vast O
majority O
of O
these O
are O
also O
located O
in O
the O
upper O
layers O
( O
41 O
+ O
) O
. O
When O
seen O
in O
conjunction O
, O
these O
observations O
indicate O
that O
there O
is O
a O
sparse O
set O
of O
attention O
heads O
that O
are O
capable O
of O
performing O
both O
primitive O
operations O
and O
thus O
can O
be O
deemed O
plausible O
induction O
heads O
. O

Are O
Induction O
Heads O
Important O
? O

We O
now O
study O
whether O
induction O
heads O
( O
which O
encode O
the O
basic O
in O
- O
context O
learning O
primitives O
of O
explicit O
prefix B-TaskName
matching I-TaskName
and O
copying B-TaskName
) O
overlap O
with O
attention O
heads O
identified O
to O
be O
important O
( O
and O
consequently O
capable O
of O
sophisticated O
and O
latent O
behaviors O
associated O
with O
in O
- O
context O
learning O
) O
for O
our O
chosen O
downstream O
tasks O
. O

A O
qualitative O
comparison O
of O
the O
heatmaps O
in O
Figure O
10 O
against O
the O
heatmaps O
referenced O
in O
§ O
4.1 O
indicates O
that O
induction O
heads O
do O
overlap O
with O
taskaggregated O
important O
attention O
heads O
. O
To O
better O
facilitate O
this O
comparison O
, O
we O
first O
formalize O
the O
total O
capacity O
of O
a O
model O
to O
perform O
prefix B-TaskName
matching I-TaskName
( O
or O
copying B-TaskName
) O
to O
be O
the O
sum O
of O
the O
respective O
scores O
for O
individual O
attention O
heads O
in O
the O
model O
. O
We O
then O
investigate O
how O
much O
of O
this O
capacity O
is O
retained O
when O
attention O
heads O
are O
pruned O
in O
the O
order O
of O
least O
important O
heads O
first O
. O
Figure O
11 O
picts O
this O
comparison O
. O
We O
observe O
that O
much O
of O
the O
total O
prefix B-MetricName
matching I-MetricName
score O
is O
retained O
when O
20 O
% O
of O
the O
least O
important O
heads O
are O
removed O
, O
with O
the O
slope O
of O
decline O
becoming O
sharp O
only O
after O
the O
40 O
% O
pruning O
mark O
. O
This O
indicates O
that O
unimportant O
heads O
also O
have O
low O
prefix B-MetricName
matching I-MetricName
scores O
. O
We O
also O
observe O
that O
the O
prefix B-MetricName
matching I-MetricName
scores O
are O
generally O
higher O
for O
heads O
important O
for O
few O
- O
shot O
in O
- O
context O
learning O
than O
for O
heads O
important O
for O
zero O
- O
shot O
learning O
. O
On O
the O
other O
hand O
, O
we O
observe O
across O
the O
zero O
- O
shot O
and O
few O
- O
shot O
settings O
that O
the O
total O
copying B-MetricName
score O
retained O
on O
pruning O
attention O
heads O
rapidly O
and O
consistently O
declines O
, O
indicating O
that O
even O
unimportant O
heads O
have O
a O
non O
- O
trivial O
capacity O
to O
perform O
copying B-TaskName
. O
When O
seen O
in O
conjunction O
, O
these O
observations O
indicate O
that O
induction O
heads O
in O
OPT-66B B-MethodName
are O
capable O
of O
sophisticated O
behaviors O
associated O
with O
in O
- O
context O
learning O
popular O
downstream O
NLP O
tasks O
and O
reinforce O
the O
induction O
head O
generality O
arguments O
Olsson O
et O
al O
. O
( O
2022 O
) O
make O
in O
the O
context O
of O
smaller O
models O
with O
stylized O
and O
synthetic O
tasks O
. O
We O
also O
provide O
per O
- O
task O
plots O
in O
Appendix O
A.9 O
which O
showcase O
that O
some O
tasks O
rely O
on O
induction O
heads O
more O
than O
other O
tasks O
. O

Related O
Work O

There O
has O
been O
an O
interest O
in O
effectively O
leveraging O
the O
in O
- O
context O
learning O
paradigm O
( O
Zhao O
et O
al O
. O
, O
2021 O
; O
Holtzman O
et O
al O
. O
, O
2021 O
; O
Min O
et O
al O
. O
, O
2022a O
; O
Lu O
et O
al O
. O
, O
2022 O
; O
Rubin O
et O
al O
. O
, O
2022 O
; O
ever O
since O
its O
introduction O
by O
, O
but O
there O
have O
been O
relatively O
fewer O
studies O
toward O
better O
understanding O
the O
paradigm O
itself O
. O
Xie O
et O
al O
. O
( O
2021 O
) O
cast O
in O
- O
context O
learning O
as O
implicit O
Bayesian O
inference O
where O
the O
language O
model O
implicitly O
infers O
a O
shared O
concept O
among O
in O
- O
context O
examples O
when O
making O
a O
prediction O
. O
Min O
et O
al O
. O
( O
2022b O
) O
study O
the O
role O
of O
the O
in O
- O
context O
examples O
themselves O
, O
finding O
that O
the O
ground O
- O
truth O
labels O
are O
not O
needed O
in O
the O
examples O
and O
that O
the O
more O
important O
drivers O
are O
provision O
of O
the O
label O
space O
, O
the O
distribution O
of O
the O
input O
text O
and O
the O
overall O
format O
of O
the O
sequence O
. O
Garg O
et O
al O
. O
( O
2022 O
) O
showcase O
that O
Transformer O
models O
trained O
from O
scratch O
can O
in O
- O
context O
learn O
the O
class O
of O
linear O
functions O
with O
performance O
comparable O
to O
the O
optimal O
least O
squares O
estimator O
even O
under O
distribution O
shifts O
. O
Razeghi O
et O
al O
. O
( O
2022 O
) O
showcase O
that O
incontext O
learning O
performance O
is O
correlated O
strongly O
with O
term O
frequencies O
in O
the O
pre O
- O
training O
corpora O
used O
. O
Olsson O
et O
al O
. O
( O
2022 O
) O
consider O
an O
alternate O
framing O
of O
in O
- O
context O
learning O
as O
the O
ability O
of O
a O
language O
model O
to O
better O
predict O
tokens O
later O
in O
the O
context O
than O
tokens O
earlier O
and O
hypothesize O
the O
existence O
of O
induction O
heads O
that O
are O
responsible O
for O
in O
- O
context O
learning O
. O
Chan O
et O
al O
. O
( O
2022 O
) O
show O
that O
Transformers O
exhibit O
striking O
differences O
in O
generalizing O
from O
in O
- O
context O
vs. O
in O
- O
weights O
information O
. O

Several O
works O
have O
also O
focused O
on O
analyzing O
and O
interpreting O
how O
attention O
works O
. O
Vig O
and O
Belinkov O
( O
2019 O
) O
performed O
a O
study O
on O
GPT-2 O
, O
finding O
that O
attention O
targets O
different O
parts O
of O
speech O
at O
different O
layer O
depths O
and O
aligns O
with O
dependency O
relations O
most O
strongly O
in O
the O
middle O
layers O
. O
Tenney O
et O
al O
. O
( O
2019 O
) O
showcase O
that O
BERT O
encodes O
the O
classical O
NLP O
pipeline O
in O
an O
interpretable O
way O
across O
layers O
. O
There O
are O
works O
relying O
on O
different O
formulations O
for O
head O
importance O
, O
such O
as O
layerwise O
relevance O
propagation O
( O
Voita O
et O
al O
. O
, O
2019 O
) O
, O
gradient O
- O
based O
importance O
and O
oracle O
knock O
- O
off O
importance O
( O
Michel O
et O
al O
. O
, O
2019 O
) O
, O
with O
small O
taskspecific O
trained O
models O
and O
report O
the O
existence O
of O
specialized O
heads O
. O
Given O
the O
recent O
trend O
of O
increasing O
model O
scale O
( O
Lieber O
et O
al O
. O
, O
2021 O
; O
Chowdhery O
et O
al O
. O
, O
2022 O
; O
Smith O
et O
al O
. O
, O
2022 O
; O
Rae O
et O
al O
. O
, O
2021 O
) O
toward O
tuning O
- O
free O
general O
- O
purpose O
language O
models O
that O
exhibit O
emergent O
in O
- O
context O
learning O
abilities O
, O
we O
draw O
and O
build O
on O
prior O
work O
to O
understand O
just O
how O
much O
scale O
is O
really O
needed O
and O
/ O
or O
used O
for O
in O
- O
context O
learning O
downstream O
, O
an O
aspect O
somewhat O
eclipsed O
by O
the O
focus O
on O
the O
pre O
- O
training O
loss O
curve O
in O
scaling O
laws O
( O
Hoffmann O
et O
al O
. O
, O
2022 O
) O
. O
It O
is O
also O
worth O
noting O
that O
some O
of O
our O
empirical O
observations O
rely O
on O
a O
simple O
greedy O
approach O
to O
training O
- O
free O
pruning O
since O
our O
focus O
was O
not O
to O
optimally O
prune O
a O
language O
model O
with O
respect O
to O
performing O
in O
- O
context O
learning O
. O
Li O
et O
al O
. O
( O
2021 O
) O
show O
the O
greedy O
approach O
is O
sub O
- O
optimal O
and O
produces O
under O
- O
estimates O
and O
Halabi O
et O
al O
. O
( O
2022 O
) O
account O
for O
the O
need O
to O
re O
- O
compute O
importance O
scores O
after O
removal O
of O
each O
attention O
head O
or O
FFN O
by O
formulating O
pruning O
as O
weakly O
sub O
- O
modular O
maximization O
. O

Conclusion O
& O
Future O
Work O

In O
this O
paper O
, O
we O
studied O
the O
efficacy O
of O
attention O
heads O
and O
feed O
forward O
networks O
( O
FFNs O
) O
in O
a O
large O
language O
model O
( O
OPT-66B B-MethodName
) O
in O
performing O
in O
- O
context O
learning O
in O
both O
task O
- O
specific O
and O
task O
- O
agnostic O
settings O
. O
We O
observed O
that O
while O
in O
- O
context O
learning O
may O
have O
emerged O
via O
selfsupervised O
pre O
- O
training O
at O
scale O
, O
only O
a O
core O
nucleus O
of O
attention O
heads O
and O
FFNs O
seem O
to O
be O
important O
for O
in O
- O
context O
learning O
across O
a O
wide O
variety O
of O
downstream O
tasks O
. O
We O
observed O
that O
a O
small O
set O
of O
attention O
heads O
have O
the O
capacity O
to O
perform O
task O
- O
agnostic O
primitive O
induction O
operations O
associated O
with O
in O
- O
context O
learning O
, O
namely O
, O
prefix B-TaskName
matching I-TaskName
and O
copying B-TaskName
. O
We O
also O
saw O
that O
these O
induction O
heads O
overlap O
with O
task O
- O
specific O
important O
attention O
heads O
, O
indicating O
that O
induction O
heads O
are O
capable O
of O
more O
sophisticated O
forms O
of O
incontext O
learning O
and O
reinforcing O
arguments O
( O
Olsson O
et O
al O
. O
, O
2022 O
) O
about O
their O
generality O
. O
Overall O
, O
our O
incontext O
learning O
- O
centric O
observations O
complement O
recent O
work O
( O
Hoffmann O
et O
al O
. O
, O
2022 O
) O
in O
indicating O
that O
large O
language O
models O
may O
be O
under O
- O
trained O
and O
motivate O
several O
interesting O
directions O
for O
future O
work O
. O
While O
induction O
heads O
are O
formed O
naturally O
during O
self O
- O
supervised O
pre O
- O
training O
in O
its O
current O
form O
, O
we O
believe O
it O
may O
be O
possible O
to O
increase O
the O
number O
and O
strength O
of O
induction O
heads O
formed O
by O
defining O
auxiliary O
pre O
- O
training O
objectives O
for O
primitives O
like O
prefix B-TaskName
matching I-TaskName
and O
copying B-TaskName
. O
More O
generally O
, O
it O
may O
also O
be O
prudent O
to O
investigate O
and O
improve O
( O
pre- O
) O
training O
regimes O
to O
increase O
the O
number O
of O
important O
model O
components O
to O
in O
- O
context O
learn O
- O
perform O
a O
wide O
variety O
of O
downstream O
tasks O
. O
Multi O
- O
task O
instruction O
- O
tuning O
likely O
belongs O
to O
this O
category O
and O
it O
would O
be O
interesting O
to O
replicate O
our O
study O
with O
now O
increasingly O
accessible O
instructiontuned O
model O
variants O
( O
such O
as O
OPT B-MethodName
's O
instruction O
meta O
- O
learned O
variant O
OPT O
- O
IML O
) O
. O

Limitations O

Our O
work O
is O
a O
comprehensive O
empirical O
study O
of O
a O
popular O
large O
language O
model O
's O
capacity O
to O
perform O
in O
- O
context O
learning O
, O
relying O
on O
both O
task O
- O
specific O
( O
via O
a O
wide O
variety O
of O
challenging O
and O
practically O
relevant O
downstream O
tasks O
) O
and O
task O
- O
agnostic O
( O
via O
looking O
for O
induction O
heads O
) O
analyses O
and O
connecting O
the O
two O
via O
correlation O
/ O
overlap O
investigations O
. O
We O
do O
not O
claim O
a O
causal O
link O
, O
i.e. O
, O
we O
do O
not O
claim O
that O
an O
attention O
head O
that O
acquires O
the O
capacity O
to O
be O
an O
induction O
head O
will O
become O
capable O
of O
more O
sophisticated O
in O
- O
context O
learning O
associated O
with O
our O
downstream O
tasks O
. O
Making O
this O
claim O
will O
require O
a O
more O
deeper O
investigation O
that O
is O
outside O
the O
scope O
of O
this O
paper O
. O
We O
also O
do O
not O
fully O
understand O
why O
most O
attention O
heads O
seem O
to O
be O
unimportant O
for O
in O
- O
context O
learning O
and O
why O
there O
is O
an O
overlap O
in O
( O
un O
) O
important O
attention O
heads O
across O
tasks O
and O
shots O
, O
which O
warrant O
further O
investigation O
. O
Other O
more O
obvious O
limitations O
to O
our O
work O
include O
our O
use O
of O
only O
up O
to O
5 O
in O
- O
context O
examples O
, O
random O
selection O
of O
in O
- O
context O
examples O
for O
a O
query O
input O
and O
our O
choice O
of O
all O
monolingual O
downstream O
tasks O
. O

Impact O
Statement O

The O
findings O
in O
our O
work O
have O
significant O
implications O
for O
the O
design O
, O
development O
and O
deployment O
of O
large O
language O
models O
, O
known O
to O
have O
a O
very O
high O
carbon O
footprint O
as O
well O
as O
training O
and O
inference O
costs O
. O
Having O
identified O
that O
a O
core O
nucleus O
of O
model O
parameters O
seem O
to O
be O
important O
for O
incontext O
learning O
, O
it O
may O
be O
possible O
to O
reduce O
these O
models O
' O
carbon O
footprint O
and O
mitigate O
these O
costs O
. O
Our O
findings O
provide O
architectural O
transparency O
and O
may O
also O
be O
helpful O
in O
identifying O
targeted O
improvements O
for O
downstream O
tasks O
as O
well O
as O
for O
more O
broader O
facets O
such O
as O
bias O
and O
fairness O
. O

A O
Appendix O

A.1 O
Head O
Importance B-MetricName
Scores O
Figure O
12 O
depicts O
the O
attention O
head O
aggregate O
importance B-MetricName
score O
heatmaps O
in O
the O
0 O
- O
shot O
and O
1 O
- O
shot O
settings O
. O
Figures O
14 O
, O
15 O
and O
16 O
depict O
the O
attention O
head O
importance B-MetricName
scores O
for O
each O
task O
in O
the O
0 O
- O
shot O
, O
1 O
- O
shot O
and O
5 O
- O
shot O
settings O
respectively O
. O

A.2 O
FFN O
Importance B-MetricName
Scores O

Figure O
13 O
depicts O
the O
task O
- O
specific O
and O
taskaveraged O
importance B-MetricName
scores O
for O
feed O
forward O
networks O
in O
the O
0 O
- O
shot O
and O
1 O
- O
shot O
settings O
. O

A.3 O
Removing O
Attention O
Heads O

Figure O
17 O
depicts O
the O
task O
- O
specific O
and O
taskaveraged O
accuracy B-MetricName
trends O
on O
iterative O
removal O
of O
attention O
heads O
in O
the O
order O
of O
least O
important O
first O
in O
the O
0 O
- O
shot O
and O
1 O
- O
shot O
settings O
. O

A.4 O
Removing O
FFNs O

Figure O
18 O
depicts O
the O
task O
- O
specific O
and O
taskaveraged O
accuracy B-MetricName
trends O
on O
iterative O
removal O
of O
feed O
forward O
networks O
in O
the O
order O
of O
least O
important O
first O
in O
the O
1 O
- O
shot O
and O
5 O
- O
shot O
settings O
. O

A.5 O
Combined O
Removal O
of O
Heads O
& O
FFNs O

Figure O
19 O
depicts O
the O
average O
accuracy B-MetricName
of O
all O
tasks O
on O
joint O
iterative O
removal O
of O
attention O
heads O
and O
feed O
forward O
networks O
in O
the O
order O
of O
least O
important O
first O
in O
the O
0 O
- O
shot O
and O
1 O
- O
shot O
settings O
. O

A.6 O
Cross O
- O
Task O
Analysis O
: O
Spearman B-MetricName
's I-MetricName
Rank I-MetricName
Correlation I-MetricName

Figure O
20 O
depicts O
the O
Spearman B-MetricName
's I-MetricName
rank I-MetricName
correlation I-MetricName
coefficients I-MetricName
( O
SRCC B-MetricName
) O
between O
the O
attention O
head O
importance O
rankings O
for O
every O
pair O
of O
tasks O
in O
the O
0 O
- O
shot O
and O
1 O
- O
shot O
settings O
. O
It O
also O
depicts O
the O
SRCC B-MetricName
between O
the O
aggregate O
ranking O
and O
the O
ranking O
for O
each O
constituent O
task O
. O

A.7 O
Cross O
- O
Task O
Analysis O
: O
Generalization O
Trends O

Figures O
21 O
and O
22 O
depict O
the O
cross O
- O
task O
head O
importance B-MetricName
ranking I-MetricName
generalization O
plots O
in O
the O
0 O
- O
shot O
and O
1 O
- O
shot O
settings O
. O

A.8 O
Details O
of O
Prefix B-MetricName
Matching I-MetricName
and O
Copying B-MetricName
Scores O

Algorithms O
1 O
and O
2 O
contain O
pseudo O
- O
code O
to O
compute O
prefix B-MetricName
matching I-MetricName
and O
copying B-MetricName
scores O
respectively O
for O
each O
attention O
head O
in O
OPT-66B. B-MethodName
We O
follow O
the O
approach O
described O
by O
Olsson O
et O
al O
. O
( O
2022 O
) O
, O
but O
instead O
of O
computing O
scores O
using O
10 O
sequences O
with O
fixed O
length O
of O
25 O
, O
we O
compute O
these O
scores O
using O
100 B-HyperparameterValue
sequences B-HyperparameterName
with O
varying O
lengths O
to O
account O
for O
OPT-66B B-MethodName
's O
large O
maximum O
sequence O
length O
. O
Each O
FFN O
is O
knocked O
off O
independently O
to O
compute O
these O
scores O
, O
i.e. O
, O
the O
curves O
are O
discrete O
and O
not O
cumulative O
. O

As O
in O
Olsson O
et O
al O
. O
( O
2022 O
) O
, O
we O
exclude O
a O
small O
fraction O
of O
the O
most O
and O
least O
common O
tokens O
from O
the O
model O
's O
vocabulary O
and O
randomly O
sample O
tokens O
for O
these O
sequences O
to O
strip O
out O
the O
effects O
of O
pretraining O
corpora O
memorization O
from O
our O
scores O
and O
inductive O
behavior O
analyses O
. O

For O
prefix B-MetricName
matching I-MetricName
, O
the O
high O
- O
level O
approach O
is O
the O
following O
: O
take O
a O
random O
sequence O
, O
repeat O
it O
4 O
times O
, O
perform O
a O
forward O
pass O
and O
then O
for O
each O
head O
, O
compute O
the O
attention O
pattern O
and O
take O
the O
average O
of O
all O
attention O
pattern O
entries O
attending O
from O
a O
given O
token O
back O
to O
tokens O
that O
succeeded O
the O
same O
token O
in O
earlier O
repeats O
. O

For O
copying B-MetricName
, O
the O
high O
- O
level O
approach O
is O
the O
following O
: O
take O
a O
random O
sequence O
, O
directly O
feed O
the O
sequence O
through O
each O
head O
and O
compute O
the O
contribution O
of O
the O
head O
to O
the O
output O
logits O
, O
and O
then O
measure O
how O
much O
the O
head O
increased O
the O
logit O
of O
the O
maximally O
attended O
to O
token O
over O
increasing O
the O
logits O
of O
other O
attendable O
tokens O
at O
each O
timestep O
. O
Unlike O
Olsson O
et O
al O
. O
( O
2022 O
) O
, O
we O
do O
not O
scale O
the O
raw O
scores O
to O
be O
in O
the O
range O
of O
-1 O
to O
1 O
. O

A.9 O
Importance O
of O
Induction O
Heads O
to O
Each O
Task O

Figures O
23 O
and O
24 O
showcase O
the O
importance O
of O
induction O
heads O
to O
each O
task O
via O
measuring O
the O
percentage O
of O
the O
total O
prefix B-MetricName
matching I-MetricName
and O
copying B-MetricName
capacities O
retained O
as O
a O
function O
of O
percentage O
of O
attention O
heads O
pruned O
, O
where O
heads O
are O
pruned O
based O
on O
each O
task O
's O
head O
importance B-MetricName
ranking I-MetricName
for O
each O
in O
- O
context O
learning O
setting O
( O
zero O
- O
shot O
, O
oneshot O
and O
five O
- O
shot O
) O
in O
the O
order O
of O
least O
important O
first O
. O
A O
small O
initial O
slope O
of O
decline O
implies O
that O
unimportant O
heads O
also O
have O
low O
prefix B-MetricName
matching I-MetricName
or O
copying B-MetricName
scores O
while O
a O
steep O
initial O
slope O
of O
decline O
implies O
unimportant O
heads O
also O
have O
high O
prefix B-MetricName
matching I-MetricName
or O
copying B-MetricName
scores O
. O
We O
observe O
differ O
- O
ences O
in O
the O
slopes O
of O
decline O
across O
different O
tasks O
, O
with O
tasks O
like O
HellaSwag B-DatasetName
and O
ReCoRD B-DatasetName
( O
which O
have O
high O
accuracies B-MetricName
in O
Figure O
5 O
) O
having O
smaller O
initial O
slopes O
than O
a O
task O
like O
OpenBookQA B-DatasetName
( O
which O
has O
relatively O
lower O
accuracy B-MetricName
in O
Figure O
5 O
) O
. O
When O
seen O
in O
conjunction O
, O
these O
plots O
not O
only O
point O
to O
the O
generality O
of O
induction O
heads O
to O
more O
sophisticated O
behaviors O
associated O
with O
in O
- O
context O
learning O
but O
also O
indicate O
that O
some O
tasks O
rely O
on O
induction O
heads O
more O
than O
others O
. O


The O
OPT-66B B-MethodName
model O
is O
open O
- O
sourced O
by O
Meta O
under O
an O
unrestricted O
license O
for O
academic O
research O
. OPVGRU B-MethodName
: O
Generating O
Diverse O
and O
Relevant O
Dialogue O
Responses O
via O
Pseudo O
- O
Variational O
Mechanism O

We O
investigate O
response B-TaskName
generation I-TaskName
for I-TaskName
multiturn I-TaskName
dialogue I-TaskName
in O
generative O
chatbots O
. O
Existing O
generative O
models O
based O
on O
RNNs B-MethodName
( O
Recurrent B-MethodName
Neural I-MethodName
Networks I-MethodName
) O
usually O
employ O
the O
last O
hidden O
state O
to O
summarize O
the O
history O
, O
which O
makes O
models O
unable O
to O
capture O
the O
subtle O
variability O
observed O
in O
different O
dialogues O
and O
can O
not O
distinguish O
the O
differences O
between O
dialogues O
that O
are O
similar O
in O
composition O
. O
In O
this O
paper O
, O
we O
propose O
Pseudo B-MethodName
- I-MethodName
Variational I-MethodName
Gated I-MethodName
Recurrent I-MethodName
Unit I-MethodName
( O
PVGRU B-MethodName
) O
. O
The O
key O
novelty O
of O
PVGRU B-MethodName
is O
a O
recurrent O
summarizing O
variable O
that O
aggregates O
the O
accumulated O
distribution O
variations O
of O
subsequences O
. O
We O
train O
PVGRU B-MethodName
without O
relying O
on O
posterior O
knowledge O
, O
thus O
avoiding O
the O
training O
- O
inference O
inconsistency O
problem O
. O
PVGRU B-MethodName
can O
perceive O
subtle O
semantic O
variability O
through O
summarizing O
variables O
that O
are O
optimized O
by O
two O
objectives O
we O
employ O
for O
training O
: O
distribution O
consistency O
and O
reconstruction O
. O
In O
addition O
, O
we O
build O
a O
Pseudo B-MethodName
- I-MethodName
Variational I-MethodName
Hierarchical I-MethodName
Dialogue I-MethodName
( O
PVHD B-MethodName
) O
model O
based O
on O
PVGRU B-MethodName
. O
Experimental O
results O
demonstrate O
that O
PVGRU B-MethodName
can O
broadly O
improve O
the O
diversity O
and O
relevance O
of O
responses O
on O
two O
benchmark O
datasets O
. O

Introduction O

The O
structure O
of O
natural O
language O
discourse O
is O
complex O
and O
highly O
variable O
( O
Gormley O
and O
Tong O
, O
2015 O
; O
Chung O
et O
al O
. O
, O
2015 O
; O
Nie O
et O
al O
. O
, O
2022 O
) O
; O
this O
is O
especially O
true O
for O
dialogue O
. O
As O
shown O
in O
Figure O
1 O
, O
examples O
( O
a O
) O
and O
( O
b O
) O
have O
the O
same O
dialogue O
history O
but O
they O
end O
with O
different O
responses O
: O
utterances O
u O
a O
6 O
vs. O
u O
b O
6 O
. O
On O
the O
other O
hand O
, O
two O
dialogues O
with O
semantically O
similar O
utterances O
may O
express O
quite O
different O
context O
meanings O
. O
Because O
of O
this O
variability O
, O
there O
is O
no O
simple O
one O
- O
to O
- O
one O
mapping O
between O
dialogue O
context O
and O
response O
. O
The O
mapping O
can O
be O
one O
- O
to O
- O
many O
-as O
in O
Figure O
1 O
, O
i.e. O
, O
different O
responses O
to O
the O
same O
dialogue O
context O
-as O
well O
as O
many O
- O
to O
- O
one O
, O
i.e. O
, O
different O
context O
histories O
requiring O
the O
same O
response O
. O
We O
observe O
that O
the O
distribution O
of O
a O
dialogue O
context O
( O
e.g. O
, O
N O
a O
6 O
and O
N O
b O
6 O
in O
the O
figure O
) O
is O
composed O
of O
the O
distribution O
of O
its O
utterances O
and O
the O
distribution O
of O
each O
utterance O
is O
composed O
of O
the O
distribution O
of O
its O
words O
. O
A O
good O
model O
of O
word O
level O
and O
utterance O
level O
variation O
is O
a O
key O
requirement O
for O
improving O
the O
quality O
of O
responses O
in O
dialogue O
. O

One O
line O
of O
research O
( O
Henderson O
et O
al O
. O
, O
2014 O
; O
Shang O
et O
al O
. O
, O
2015 O
; O
Luo O
et O
al O
. O
, O
2018 O
) O
employs O
recurrent B-MethodName
neural I-MethodName
networks I-MethodName
( O
RNNs B-MethodName
) O
to O
model O
dialogue O
context O
. O
However O
, O
standard O
RNNs B-MethodName
are O
not O
well O
suited O
for O
dialogue O
context O
variability O
( O
Chung O
et O
al O
. O
, O
2015 O
) O
. O
This O
is O
because O
the O
internal O
transition O
structure O
of O
RNNs B-MethodName
is O
deterministic O
. O
Thus O
, O
RNNs B-MethodName
can O
not O
effectively O
model O
randomness O
and O
variability O
in O
dialogue O
context O
( O
Chung O
et O
al O
. O
, O
2015 O
) O
. O

Variational O
mechanism O
has O
been O
shown O
to O
be O
well O
suited O
for O
modeling O
variability O
-from O
both O
theoretical O
and O
practical O
perspectives O
( O
Kingma O
and O
Welling O
, O
2014 O
) O
. O
Methods O
based O
on O
variational O
mechanism O
Gu O
et O
al O
. O
, O
2019 O
; O
Khan O
et O
al O
. O
, O
2020 O
; O
Sun O
et O
al O
. O
, O
2021 O
) O
introduce O
latent O
variables O
into O
RNNs B-MethodName
to O
model O
one O
- O
to O
- O
many O
and O
many O
- O
to O
- O
one O
phenomena O
in O
dialogue O
. O
Although O
these O
approaches O
achieve O
promising O
results O
, O
they O
still O
have O
defects O
. O
First O
, O
these O
methods O
face O
the O
dilemma O
that O
latent O
variables O
may O
vanish O
because O
of O
the O
posterior O
collapse O
issue O
( O
Zhao O
et O
al O
. O
, O
2017 O
( O
Zhao O
et O
al O
. O
, O
, O
2018Shi O
et O
al O
. O
, O
2020 O
) O
. O
Variational O
mechanism O
can O
work O
only O
when O
latent O
variables O
with O
intractable O
posterior O
distributions O
exist O
( O
Kingma O
and O
Welling O
, O
2014 O
) O
. O
Second O
, O
the O
sampled O
latent O
variables O
may O
not O
correctly O
reflect O
the O
relationship O
between O
dialogue O
context O
and O
response O
due O
to O
the O
one O
- O
tomany O
and O
many O
- O
to O
- O
one O
phenomena O
observed O
in O
dialogue O
( O
Sun O
et O
al O
. O
, O
2021 O
) O
. O
Third O
, O
posterior O
knowledge O
is O
employed O
in O
training O
while O
prior O
knowledge O
is O
used O
in O
inference O
; O
this O
causes O
an O
inconsistency O
problem O
between O
training O
and O
inference O
( O
Shang O
et O
al O
. O
, O
2015 O
; O
Zhao O
et O
al O
. O
, O
2017 O
; O
Shi O
et O
al O
. O
, O
2020 O
) O
. O
a O
man O
is O
making O
a O
sandwich O
while O
sitting O
at O
a O
dresser O
he O
gets O
up O
and O
brings O
the O
sandwich O
to O
… O
can O
you O
see O
the O
guy O
? O
yes O
, O
i O
see O
one O
man O
what O
is O
he O
doing O
? O
he O
was O
sitting O
on O
a O
chair O
and O
applying O
jam O
on O
a O
bread O
. O

1 O
: O
To O
tackle O
these O
problems O
, O
we O
propose O
a O
Pseudo B-MethodName
- I-MethodName
Variational I-MethodName
Gated I-MethodName
Recurrent I-MethodName
Unit I-MethodName
( O
PVGRU B-MethodName
) O
component O
based O
on O
pseudo B-MethodName
- I-MethodName
variational I-MethodName
mechanism I-MethodName
. O
PVGRU B-MethodName
introduces O
a O
recurrent O
summarizing O
variable O
into O
the O
GRU B-MethodName
. O
This O
summarizing O
variable O
can O
aggregate O
the O
accumulated O
distribution O
variations O
of O
subsequences O
. O
The O
methods O
based O
on O
PVGRU B-MethodName
can O
model O
the O
subtle O
semantic O
differences O
between O
different O
sequences O
. O
First O
, O
pseudovariational B-MethodName
mechanism I-MethodName
adopts O
the O
idea O
of O
latent O
variables O
but O
does O
not O
adopt O
posterior O
mechanism O
( O
Serban O
et O
al O
. O
, O
2017 O
; O
Zhao O
et O
al O
. O
, O
2017 O
; O
Park O
et O
al O
. O
, O
2018 O
; O
Sun O
et O
al O
. O
, O
2021 O
) O
. O
Therefore O
, O
PVGRU B-MethodName
does O
not O
suffer O
from O
the O
posterior O
collapse O
issue O
( O
Zhao O
et O
al O
. O
, O
2017 O
( O
Zhao O
et O
al O
. O
, O
, O
2018Shi O
et O
al O
. O
, O
2020 O
) O
. O
Second O
, O
we O
design O
consistency O
and O
reconstruction O
objectives O
to O
optimize O
the O
recurrent O
summarizing O
variable O
in O
PV B-MethodName
- I-MethodName
GRU I-MethodName
; O
this O
ensures O
that O
the O
recurrent O
variable O
can O
reflect O
the O
semantics O
of O
dialogue O
context O
on O
both O
the O
word O
level O
and O
the O
utterance O
level O
. O
The O
consistency O
objective O
makes O
the O
distribution O
of O
the O
incremental O
information O
consistent O
with O
the O
corresponding O
input O
at O
each O
time O
step O
. O
Third O
, O
we O
guarantee O
the O
consistency O
between O
training O
and O
inference O
since O
we O
do O
not O
employ O
posterior O
knowledge O
when O
optimizing O
the O
summarizing O
variable O
. O

Our O
proposed O
method O
avoids O
the O
problems O
caused O
by O
variational O
optimization O
and O
can O
model O
the O
diversity O
problem O
in O
dialogue O
. O
For O
instance O
in O
Figure O
1 O
, O
examples O
( O
a O
) O
and O
( O
b O
) O
have O
the O
same O
dialogue O
history O
but O
different O
responses O
. O
N O
a O
6 O
and O
N O
b O
6 O
can O
learn O
the O
distribution O
differences O
caused O
by O
u O
a O
6 O
and O
u O
b O
6 O
. O
Simultaneously O
, O
semantic O
reconstruction O
can O
enhance O
the O
model O
's O
perception O
of O
semantic O
changes O
, O
which O
in O
turn O
can O
strengthen O
the O
distribution O
differences O
caused O
by O
semantic O
changes O
. O
Although O
the O
example O
only O
shows O
diversity O
at O
the O
utterance O
level O
, O
similar O
diversity O
issues O
exist O
at O
the O
word O
level O
. O
Therefore O
, O
we O
build O
a O
Pseudo B-MethodName
- I-MethodName
Variational I-MethodName
Hierarchical I-MethodName
Dialogue I-MethodName
model I-MethodName
( O
PVHD B-MethodName
) O
based O
on O
PVGRU B-MethodName
to O
model O
both O
word O
level O
and O
utterance O
level O
variation O
. O

To O
summarize O
, O
we O
make O
the O
following O
contributions O
: O

• O
We O
analyze O
the O
reasons O
for O
one O
- O
to O
- O
many O
and O
many O
- O
to O
- O
one O
issues O
from O
high O
variability O
of O
dialogue O
corpus O
and O
propose O
PVGRU B-MethodName
with O
a O
recurrent O
summarizing O
variable O
to O
model O
the O
variability O
of O
dialogue O
sequences O
. O
• O
We O
propose O
to O
optimize O
the O
recurrent O
summarizing O
variable O
using O
consistency O
and O
reconstruction O
objectives O
, O
which O
guarantees O
that O
the O
summarizing O
variable O
can O
reflect O
the O
semantics O
of O
the O
dialogue O
context O
and O
maintain O
the O
consistency O
between O
training O
and O
inference O
processes O
. O
• O
We O
propose O
the O
PVHD B-MethodName
model O
based O
on O
PVGRU B-MethodName
. O

PVHD B-MethodName
significantly O
outperforms O
strong O
baselines O
with O
RNN B-MethodName
and O
Transformer B-MethodName
architectures O
on O
two O
benchmark O
datasets O
. O
The O
code O
including O
baselines O
for O
comparison O
is O
available O
on O
Github O
1 O
. O

RELATED O
WORK O

Dialogue B-TaskName
Generation I-TaskName

As O
an O
important O
task O
in O
Natural O
Language O
Processing O
, O
dialogue B-TaskName
generation I-TaskName
systems O
aim O
to O
generate O
fluent O
and O
informative O
responses O
based O
on O
the O
dialogue O
context O
( O
Ke O
et O
al O
. O
, O
2018 O
) O
. O
Early O
dialogue B-TaskName
generation I-TaskName
models O
( O
Henderson O
et O
al O
. O
, O
2014 O
; O
Shang O
et O
al O
. O
, O
2015 O
; O
Luo O
et O
al O
. O
, O
2018 O
) O
usually O
adopt O
the O
simple O
seq2seq B-MethodName
( O
Sutskever O
et O
al O
. O
, O
2014 O
) O
framework O
to O
model O
the O
relationship O
between O
dialogue O
context O
and O
response O
in O
the O
manner O
of O
machine O
translation O
. O
However O
, O
the O
vanilla O
seq2seq B-MethodName
structure O
tends O
to O
generate O
dull O
and O
generic O
responses O
. O
To O
generate O
informative O
responses O
, O
hierarchical O
structures O
Song O
et O
al O
. O
, O
2021 O
; O
Liu O
et O
al O
. O
, O
2022 O
) O
and O
pre O
- O
training O
techniques O
( O
Radford O
et O
al O
. O
, O
2019 O
; O
Lewis O
et O
al O
. O
, O
2020 O
; O
Zhang O
et O
al O
. O
, O
2020 O
) O
are O
employed O
to O
capture O
the O
hierarchical O
dependencies O
of O
dialogue O
context O
. O
The O
results O
of O
these O
methods O
do O
not O
meet O
expectations O
( O
Wei O
et O
al O
. O
, O
2019 O
) O
. O

The O
main O
reason O
is O
that O
there O
are O
one O
- O
to O
- O
many O
and O
many O
- O
to O
- O
one O
relationships O
between O
dialogue O
context O
and O
responses O
. O
Modeling O
the O
multimapping O
relationship O
is O
crucial O
for O
improving O
the O
quality O
of O
the O
dialog B-TaskName
generation I-TaskName
. O
In O
this O
paper O
, O
we O
propose O
a O
PVGRU B-MethodName
component O
by O
introducing O
recurrent O
summarizing O
variables O
into O
GRU O
, O
which O
can O
model O
the O
varieties O
of O
dialogue O
context O
. O

Variational O
Mechanism O

Variational O
mechanisms O
enable O
efficient O
working O
in O
directed O
probabilistic O
models O
when O
latent O
variables O
with O
intractable O
posterior O
distributions O
existing O
( O
Kingma O
and O
Welling O
, O
2014 O
) O
. O
Variational O
mechanisms O
can O
learn O
the O
latent O
relationship O
between O
dialogue O
context O
and O
responses O
by O
introducing O
latent O
variables O
. O
Most O
existing O
methods O
( O
Serban O
et O
al O
. O
, O
2017 O
; O
Zhao O
et O
al O
. O
, O
2017 O
; O
Bao O
et O
al O
. O
, O
2020 O
) O
based O
on O
variational O
mechanisms O
employ O
prior O
to O
approximate O
true O
posterior O
probability O
. O
These O
methods O
not O
only O
encounter O
the O
problem O
of O
posterior O
collapse O
issue O
but O
also O
the O
problem O
of O
inconsistency O
between O
training O
and O
inference O
( O
Zhao O
et O
al O
. O
, O
2018 O
; O
Shi O
et O
al O
. O
, O
2020 O
) O
. O
In O
this O
paper O
, O
we O
employ O
consistency O
and O
reconstruction O
objectives O
to O
optimize O
the O
summarizing O
variable O
different O
from O
variational O
mechanism O
, O
which O
can O
model O
the O
multi O
- O
mapping O
phenomena O
in O
dialogues O
. O

Preliminary O

In O
this O
paper O
, O
we O
employ O
GRU B-MethodName
( O
Gated B-MethodName
Recurrent I-MethodName
Unit I-MethodName
) O
( O
Cho O
et O
al O
. O
, O
2014 O
) O
as O
the O
implementation O
of O
recurrent B-MethodName
neural I-MethodName
network I-MethodName
( O
RNN B-MethodName
) O
. O
The O
reset O
gate O
r O
t O
is O
computed O
by O
: O

rt O
= O
σ O
( O
Wrxt O
+ O
Urht−1 O
) O
( O
1 O
) O

where O
σ O
is O
the O
logistic O
sigmoid O
function O
. O
x O
t O
represents O
the O
input O
at O
time O
step O
t O
and O
h O
t−1 O
denotes O
the O
hidden O
state O
at O
time O
step O
t-1 O
. O
W O
r O
and O
U O
r O
are O
parameter O
matrices O
which O
are O
learned O
. O
Similarly O
, O
the O
updated O
gate O
z O
t O
is O
defined O
as O
: O

zt O
= O
σ O
( O
Wzxt O
+ O
Uzht−1 O
) O
( O
2 O
) O

The O
hidden O
state O
h O
t O
at O
the O
time O
step O
t O
is O
then O
computed O
by O
: O

ht O
= O
ztht−1 O
+ O
( O
1 O
− O
zt O
) O
ht O
( O
3 O
) O
ht O
= O
ϕ O
( O
W O
xt O
+ O
U O
( O
rt O
⊙ O
ht−1 O
) O
) O
( O
4 O
) O

where O
ϕ O
( O
• O
) O
is O
the O
tanh O
function O
, O
W O
and O
U O
are O
weight O
matrices O
which O
are O
learned O
. O
GRU B-MethodName
is O
considered O
as O
a O
classic O
implementation O
of O
RNN B-MethodName
, O
which O
is O
widely O
employed O
in O
generative O
tasks O
. O
2 O
( O
a O
) O
, O
PVGRU B-MethodName
introduces O
a O
recurrent O
summarizing O
variable O
v O
based O
on O
GRU B-MethodName
. O
The O
recurrent O
summarizing O
variable O
v O
is O
obtained O
based O
on O
the O
incremental O
information O
of O
hidden O
state O
h O
and O
the O
previous O
state O
of O
summarizing O
variable O
. O
Specially O
, O
the O
summarizing O
variable O
v O
0 O
is O
initialized O
with O
standard O
Gaussian O
distribution O
( O
i.e. O
, O
Figure O
3 O
( O
a O
) O
) O
. O
We O
assume O
the O
input O
is O
x O
t O
at O
the O
time O
step O
t O
, O
the O
reset O
gate O
r O
t O
is O
rewrited O
as O
: O

rt O
= O
σ O
( O
Wrxt O
+ O
Urht−1 O
+ O
Vrvt−1 O
) O
( O
5 O
) O

where O
W O
r O
, O
U O
r O
and O
V O
r O
are O
parameter O
matrices O
, O
and O
v O
t−1 O
is O
the O
previous O
summarizing O
variable O
state O
. O

Similarly O
, O
the O
update O
gate O
z O
t O
is O
computed O
by O
: O
We O
introduce O
a O
gate O
g O
t O
for O
summarizing O
variable O
factor O
, O
which O
is O
defined O
as O
follows O
: O

zt O
= O
σ O
( O
Wzxt O
+ O
Uzht−1 O
+ O
Vzvt−1 O
) O
( O
6 O
) O
1- O
tanh O
1- O
RE O
sam O
1- O
ℎ O
ℎ O
−1 O
−1 O
ℎ O
−1 O
ℎ O
( O
0,1 O
) O
0 O
1 O
2 O
… O
1 O
2 O
Encoder O
PVGRU B-MethodName
… O
( O
0,1 O
) O
0 O
… O
1 O
2 O
Context O
PVGRU B-MethodName
… O
1 O
2 O
… O
ℎ O
1 O
ℎ O
2 O
ℎ O
0 O
… O
1 O
2 O
… O
Decoder O
ℎ O
0 O
0 O
1 O
1 O
2 O
+1 O
( O
a O
) O
( O
b O
) O

b O
) O
Recurrence O
( O
a O
) O
Init O
( O
0,1 O
) O
0 O
1 O
ℎ O
0 O
ℎ O
1 O
( O
c O
) O
Training O
ℎ O
−1 O
ℎ O
−1 O
RE O
≈ O
ℎ O
−1 O
ℎ O
−1 O
+1 O

gt O
= O
σ O
( O
Wgxt O
+ O
Ught−1 O
+ O
Vgvt−1 O
) O
( O
7 O
) O

The O
updated O
gate O
of O
summarizing O
factor O
controls O
how O
much O
information O
from O
the O
previous O
variable O
will O
carry O
over O
to O
the O
current O
summarizing O
variable O
state O
. O
Under O
the O
effect O
of O
g O
t O
, O
theh O
t O
follows O
the O
equation O
: O

ht O
= O
ϕ O
( O
W O
xt O
+ O
U O
( O
rt O
⊙ O
ht−1 O
) O
+ O
V O
( O
gt O
⊙ O
vt−1 O
) O
) O
( O
8 O
) O

Then O
the O
PVGRU B-MethodName
updates O
its O
hidden O
state O
h O
t O
using O
the O
same O
recurrence O
equation O
as O
GRU B-MethodName
. O
The O
summarizing O
variable O
v O
t O
at O
the O
time O
step O
t O
is O
defined O
as O
: O
ṽ O

where O
φ O
( O
• O
) O
represents O
a O
nonlinear O
neural O
network O
approximator O
andṽ O
t O
denotes O
the O
variations O
between O
time O
t O
and O
time O
t O
− O
1 O
. O
The O
variations O
across O
subsequent O
up O
to O
time O
t O
is O
defined O
as O
: O

vt O
= O
gt O
⊙ṽt O
+ O
( O
1 O
− O
gt O
) O
⊙ O
vt−1 O
( O
10 O
) O

Figure O
3 O
( O
b O
) O
demonstrates O
the O
schematic O
diagram O
of O
the O
recurrent O
process O
of O
PVGRU B-MethodName
described O
above O
. O
We O
can O
observe O
that O
PVGRU B-MethodName
does O
not O
adopt O
posterior O
knowledge O
, O
which O
can O
guarantee O
the O
consistency O
between O
training O
and O
inference O
. O

Optimization O
Summarizing O
Variable O

Based O
on O
but O
different O
from O
traditional O
variational O
mechanism O
, O
we O
design O
the O
consistency O
and O
reconstruction O
objectives O
to O
optimize O
the O
summarizing O
variable O
. O
The O
consistency O
objective O
ensures O
that O
the O
distribution O
of O
the O
information O
increment O
of O
hidden O
state O
at O
each O
time O
step O
is O
consistent O
with O
the O
input O
. O

For O
example O
, O
we O
will O
keep O
the O
distribution O
of O
information O
increment O
h O
t O
− O
h O
t−1 O
at O
time O
t O
consistent O
with O
x O
t O
. O
The O
consistency O
objective O
function O
at O
time O
step O
t O
is O
denoted O
as O
: O

ℓ O
t O
c O
= O
KL O
( O
p O
( O
xt O
) O
||p O
( O
ht O
− O
ht−1 O
) O
) O
= O
KL O
( O
p O
( O
xt O
) O
||ṽt O
) O
( O
11 O
) O

where O
KL O
( O
• O
) O
represents O
Kullback O
- O
Leibler O
divergence O
( O
Barz O
et O
al O
. O
, O
2018 O
) O
and O
p O
( O
• O
) O
represents O
the O
distribution O
of O
the O
vector O
. O
We O
employ O
" O
sam O
" O
to O
represent O
this O
process O
of O
distribution O
sampling O
in O
Figure O
2 O
( O
a O
) O
. O
The O
reconstruction O
optimization O
objective O
ensures O
that O
the O
summarizing O
variable O
can O
correctly O
reflect O
the O
semantic O
of O
the O
dialogue O
context O
from O
the O
whole O
perspective O
, O
which O
requires O
PVGRU B-MethodName
reconstructs O
the O
sequence O
information O
from O
the O
accumulated O
distribution O
variable O
. O
The O
reconstruction O
loss O
at O
time O
step O
t O
is O
described O
as O
: O

ℓ O
t O
r O
( O
vt O
, O
ht O
) O
= O
1 O
2 O
|f O
( O
vt O
) O
− O
ht| O
, O
|vt O
− O
ht| O
≤ O
δ O
δ|f O
( O
vt O
) O
− O
ht| O
− O
1 O
2 O
δ O
2 O
, O
|vt O
− O
ht| O
> O
δ O
( O
12 O

) O

where O
f O
( O
• O
) O
stands O
for O
decoder O
using O
MLP O
, O
δ O
is O
a O
hyperparameter O
and O
| O
• O
| O
represents O
the O
absolute O
value O
. O
We O
employ O
" O
RE O
" O
to O
represent O
the O
reconstruction O
process O
in O
Figure O
2 O
( O
a O
) O
. O
Figure O
3 O
( O
c O
) O
demonstrates O
the O
schematic O
diagram O
of O
optimizing O
summarizing O
variable O
. O
Reconstruction O
and O
consistency O
objectives O
ensure O
that O
summarizing O
variable O
can O
correctly O
reflect O
the O
semantics O
of O
the O
dialogue O
context O
. O

Hierarchical O
Pseudo O
- O
variational O
Model O

As O
shown O
in O
Figure O
1 O
, O
the O
dialogues O
contain O
word O
- O
level O
and O
sentence O
- O
level O
variability O
. O
We O
follow O
previous O
studies O
( O
Serban O
et O
al O
. O
, O
, O
2017Huang O
et O
al O
. O
, O
2021 O
) O

, O
u O
2 O
, O
... O
, O
u O
m O
} O
to O
utterance O
vec- O
tors O
{ O
h O
u O
1 O
, O
h O
u O
2 O
, O
... O
, O
h O
u O
m O
} O
. O

At O
the O
same O
time O
, O
v O
t O
records O
the O
accumulated O
distribution O
variations O
of O
the O
subsequence O
at O
time O
step O
t. O
The O
context O
PVGRU B-MethodName
takes O
charge O
of O
capturing O
the O
utterance O
- O
level O
variabilities O
. O
The O
last O
hidden O
state O
of O
the O
context O
PVGRU B-MethodName
represents O
a O
summary O
of O
the O
dialogue O
. O
The O
last O
summarizing O
variable O
state O
of O
the O
context O
PVGRU B-MethodName
stands O
for O
the O
distribution O
of O
dialogue O
. O
The O
decoder O
PVGRU B-MethodName
takes O
the O
last O
states O
of O
context O
PVGRU B-MethodName
and O
produces O
a O
probability O
distribution O
over O
the O
tokens O
in O
the O
response O
{ O
y O
1 O
, O
y O
2 O
, O
... O
, O
y O
n O
} O
. O
The O
generation O
process O
of O
training O
and O
inference O
can O
be O
formally O
described O
as O
: O

p O
( O
y O
≤T O
, O
v O
≤n O
) O
= O
n O
t=1 O
p O
( O
yt|y O
< O
t O
, O
v O
< O
t O
) O
( O
13 O
) O

The O
log O
- O
likelihood O
loss O
of O
predicting O
reponse O
is O
formalized O
as O
: O

ℓ O
t O
ll O
= O
logp O
( O
yt|y O
< O
t O
, O
v O
< O
t O
) O
( O
14 O
) O

The O
total O
loss O
can O
be O
written O
as O
: O

ℓ O
total O
= O
E O
T O
t=1 O
( O
ℓ O
t O
ll O
+ O
ℓ O
t O
r O
+ O
ℓ O
t O
c O
) O
( O
15 O
) O

5 O
Experiments O

For O
descriptions O
of O
the O
datasets O
, O
please O
refer O
to O
the O
Appendix O
A.1 O
. O
Please O
refer O
to O
Appendix O
A.2 O
for O
implementation O
details O
. O
In O
Appendix O
A.5 O
we O
show O
the O
ablation O
results O
of O
two O
objective O
functions O
, O
showing O
the O
effectiveness O
of O
the O
objective O
functions O
. O
In O
order O
to O
evaluate O
the O
effectiveness O
of O
experimental O
results O
, O
we O
performed O
a O
significance O
test O
in O
Appendix O
A.6 O
. O
We O
can O
observe O
that O
the O
pvalues O
of O
PVHD B-MethodName
are O
less O
than O
0.05 O
compared O
with O
other O
models O
. O
In O
addition O
, O
we O
present O
case O
studies O
in O
Appendix O
A.7 O
and O
discuss O
model O
limitations O
in O
Appendix O
7 O
, O
respectively O
. O

Baselines O

The O
automatic O
evaluation O
metrics O
is O
employed O
to O
verify O
the O
generality O
of O
PVGRU B-MethodName
, O
we O
select O
the O
following O
RNN B-MethodName
- O
based O
dialogue O
generation O
models O
as O
baselines O
: O
seq2seq B-MethodName
: O
sequence B-MethodName
- I-MethodName
to I-MethodName
- I-MethodName
sequence I-MethodName
model O
GRU B-MethodName
- O
based O
with O
attention O
mechanisms O
( O
Bahdanau O
et O
al O
. O
, O
2015 O
) O
. O
HRED B-MethodName
: O
hierarchical B-MethodName
recurrent I-MethodName
encoder I-MethodName
- I-MethodName
decoder I-MethodName
on O
recurrent B-MethodName
neural I-MethodName
network I-MethodName
( O
Serban O
et O
al O
. O
, O
2016 O
) O
for O
dialogue B-TaskName
generation I-TaskName
. O
HRAN B-MethodName
: O
hierarchical B-MethodName
recurrent I-MethodName
neural I-MethodName
network I-MethodName
dialogue I-MethodName
generation I-MethodName
model I-MethodName
based I-MethodName
on I-MethodName
attentiom I-MethodName
mechanism I-MethodName
( O
Xing O
et O
al O
. O
, O
2018 O
) O
. O
CSG B-MethodName
: O
hierarchical B-MethodName
recurrent I-MethodName
neural I-MethodName
network I-MethodName
model I-MethodName
using I-MethodName
static I-MethodName
attention I-MethodName
for I-MethodName
contextsensitive I-MethodName
generation I-MethodName
of I-MethodName
dialogue I-MethodName
responses I-MethodName
( O
Zhang O
et O
al O
. O
, O
2018 O
) O
. O

To O
evaluate O
the O
performance O
of O
the O
PVHD B-MethodName
, O
we O
choose O
dialogue O
generation O
model O
based O
on O
variational O
mechanism O
as O
baselines O
: O
HVRNN B-MethodName
: O
VRNN B-MethodName
( O
Variational B-MethodName
Recurrent I-MethodName
Neural I-MethodName
Network I-MethodName
) O
( O
Chung O
et O
al O
. O
, O
2015 O
) O
is O
a O
recurrent O
version O
of O
the O
VAE O
. O
We O
combine O
VRNN B-MethodName
( O
Chung O
et O
al O
. O
, O
2015 O
) O
and O
HRED B-MethodName
to O
construct O
the O
HVRNN B-MethodName
. O
CVAE B-MethodName
: O
hierarchical O
dialogue O
generation O
model O
based O
on O
conditional B-MethodName
variational I-MethodName
autoencoders I-MethodName
( O
Zhao O
et O
al O
. O
, O
2017 O
) O
. O
We O
implement O
CVAE B-MethodName
with O
bag O
- O
of O
- O
word O
loss O
and O
KL O
annealing O
technique O
. O
VAD B-MethodName
: O
hierarchical O
dialogue O
generation O
model O
introducing O
a O
series O
of O
latent O
variables O
( O
Du O
et O
al O
. O
, O
2018 O
) O
. O
VHCR B-MethodName
: O
hierarchical O
dialogue O
generation O
model O
using O
global O
and O
local O
latent O
variables O
( O
Park O
et O
al O
. O
, O
2018 O
) O
. O
SepaCVAE B-MethodName
: O
self B-MethodName
- I-MethodName
separated I-MethodName
conditional I-MethodName
variational I-MethodName
autoencoder I-MethodName
introducing O
group O
information O
to O
regularize O
the O
latent O
variables O
( O
Sun O
et O
al O
. O
, O
2021 O
) O
. O
SVT B-MethodName
: O
sequential B-MethodName
variational I-MethodName
transformer I-MethodName
augmenting O
deocder O
with O
a O
sequence O
of O
fine O
- O
grained O
latent O
variables O
( O
Lin O
et O
al O
. O
, O
2020 O
) O
. O
GVT B-MethodName
: O
global B-MethodName
variational I-MethodName
transformer I-MethodName
modeling O
the O
discourselevel O
diversity O
with O
a O
global O
latent O
variable O
( O
Lin O
et O
al O
. O
, O
2020 O
et O
al O
. O
, O
2020 O
) O
. O
Different O
from O
original O
implementation O
, O
we O
do O
not O
use O
knowledge O
on O
the O
DSTC7 O
- O
AVSD O
. O
DialogVED B-MethodName
: O
a O
pre O
- O
trained O
latent O
variable O
encoder O
- O
decoder O
model O
for O
dialog O
response O
generation O
( O
Chen O
et O
al O
. O
, O
2022 O
) O
. O
We O
initialize O
the O
model O
with O
the O
large O
version O
of O
DialogVED B-MethodName
. O

Automatic O
& O
Human B-MetricName
Evaluation I-MetricName

Please O
refer O
to O
Appendix O
A.3 O
and O
Appendix O
A.4 O
for O
details O
of O
automatic B-MetricName
evaluation I-MetricName
metrics O
. O
Some O
differences O
from O
previous O
works O
are O
emphasized O
here O
. O
We O
employ O
improved O
versions O
of O
BLEU B-MetricName
and O
ROUGE B-MetricName
- I-MetricName
L I-MetricName
, O
which O
can O
better O
correlate O
n O
- O
gram O
overlap O
with O
human O
judgment O
by O
weighting O
the O
relevant O
n O
- O
gram O
compared O
with O
original O
BLEU B-MetricName
( O
Chen O
and O
Cherry O
, O
2014 O
) O
. O
Although O
using O
the O
improved O
versions O
of O
BLEU B-MetricName
and O
ROUGE B-MetricName
- I-MetricName
L I-MetricName
will O
result O
in O
lower O
literal O
values O
on O
the O
corresponding O
metrics O
, O
this O
does O
not O
affect O
the O
fairness O
of O
the O
comparison O
. O
We O
adopt O
the O
implementation O
of O
distinct-1 O
/ O
2 O
metrics O
following O
previous O
study O
( O
Bahuleyan O
et O
al O
. O
, O
2018 O
) O
. O

The O
source O
code O
for O
the O
evaluation O
method O
can O
be O
found O
on O
the O
anonymous O
GitHub O
. O

Generality O
of O
PVGRU B-MethodName

Table O
1 O
reports O
the O
automatic B-MetricName
evaluation I-MetricName
performance O
comparison O
of O
the O
models O
using O
GRU B-MethodName
and O
PVGRU B-MethodName
. O
We O
can O
observe O
that O
the O
performance O
of O
the O
models O
based O
on O
PVGRU B-MethodName
is O
higher O
than O
that O
based O
on O
GRU B-MethodName
. O
Specifically O
, O
on O
DailyDialog B-DatasetName
dataset O
, O
the O
average O
performance O
of O
models O
based O
on O
PVGRU B-MethodName
is O
0.63 B-MetricValue
% I-MetricValue
to O
16.35 B-MetricValue
% I-MetricValue
higher O
on O
PPL B-MetricName
, O
1.40 B-MetricValue
% I-MetricValue
to O
1.92 B-MetricValue
% I-MetricValue
higher O
on O
BLEU-1 B-MetricName
, O
1.08 B-MetricValue
% I-MetricValue
to O
2.02 B-MetricValue
% I-MetricValue
higher O
on O
Rouge B-MetricName
- I-MetricName
L I-MetricName
, O
1.10 B-MetricValue
% I-MetricValue
to O
2.33 B-MetricValue
% I-MetricValue
higher O
on O
Dist-1 B-MetricName
and O
1.36 B-MetricValue
% I-MetricValue
to O
1.62 B-MetricValue
% I-MetricValue
higher O
on O
average B-MetricName
embedding I-MetricName
compared O
with O
models O
based O
on O
GRU B-MethodName
. O
On O
DSTC7 B-DatasetName
- I-DatasetName
AVSD I-DatasetName
dataset O
, O
the O
performance O
of O
models O
based O
on O
PVGRU B-MethodName
is O
0.45 B-MetricValue
% I-MetricValue
to O
5.47 B-MetricValue
% I-MetricValue
higher O
on O
PPL B-MetricName
, O
1.14 B-MetricValue
% I-MetricValue
to O
2.57 B-MetricValue
% I-MetricValue
higher O
on O
BLEU-1 B-MetricName
, O
1.38 B-MetricValue
% I-MetricValue
to O
2.7 B-MetricValue
% I-MetricValue
higher O
on O
Rouge B-MetricName
- I-MetricName
L I-MetricName
, O
0.69 B-MetricValue
% I-MetricValue
to O
2.06 B-MetricValue
% I-MetricValue
higher O
on O
Dist-1 B-MetricName
and O
0.69 B-MetricValue
% I-MetricValue
to O
2.69 B-MetricValue
% I-MetricValue
higher O
on O
average B-MetricName
embedding I-MetricName
compared O
with O
models O
based O
on O
GRU B-MethodName
. O
GRU B-MethodName
introduces O
a O
recurrent O
summarizing O
variable O
, O
which O
records O
the O
accumulated O
distribution O
variations O
of O
sequences O
. O
The O
recurrent O
summarizing O
variable O
brings O
randomness O
to O
the O
internal O
transition O
structure O
of O
PVGRU B-MethodName
, O
which O
makes O
model O
perceive O
the O
subtle O
semantic O
variability O
. O

( O
a O
) O
( O
b O
) O
( O
c O
) O
( O
d O
) O

Automatic B-MetricName
Evaluation I-MetricName
Results O
& O
Analysis O

Table O
2 O
reports O
the O
results O
of O
automatic B-MetricName
evaluation I-MetricName
of O
PVHD B-MethodName
and O
other O
baselines O
on O
DailyDialog B-DatasetName
and O
DSTC7 B-DatasetName
- I-DatasetName
AVSD I-DatasetName
datasets O
. O
Compared O
to O
RNNbased B-MethodName
baselines O
based O
on O
variational O
mechanism O
, O
PVHD B-MethodName
enjoys O
an O
advantage O
in O
performance O
. O
On O
DailyDialog B-DatasetName
datasets O
, O
the O
performance O
of O
PVHD B-MethodName
is O
1.16 B-MetricValue
% I-MetricValue
higher O
on O
BLEU-1 B-MetricName
, O
0.45 B-MetricValue
% I-MetricValue
higher O
on O
Rouge B-MetricName
- I-MetricName
L I-MetricName
, O
1.01 B-MetricValue
% I-MetricValue
higher O
on O
Dist-1 B-MetricName
and O
2.22 B-MetricValue
% I-MetricValue
higher O
on O
average B-MetricName
embedding I-MetricName
compared O
to O
HVRNN B-MethodName
. O
As O
compared O
to O
the O
classic O
variational O
mechanism O
models O
CVAE B-MethodName
, O
VAD B-MethodName
and O
VHCR B-MethodName
, O
PVHD B-MethodName
has O
a O
advantage O
of O
0.02 B-MetricValue
% I-MetricValue
to O
22.75 B-MetricValue
% I-MetricValue
on O
PPL B-MetricName
, O
1.87 B-MetricValue
% I-MetricValue
to O
6.88 B-MetricValue
% I-MetricValue
higher O
on O
BLEU-1 B-MetricName
, O
1.48 B-MetricValue
% I-MetricValue
to O
3.25 B-MetricValue
% I-MetricValue
higher O
on O
Dist-1 B-MetricName
, O
0.43 B-MetricValue
% I-MetricValue
to O
13.37 B-MetricValue
% I-MetricValue
higher O
on O
Dist-2 B-MetricName
and O
0.80 B-MetricValue
% I-MetricValue
to O
2.76 O
% O
higher O
on O
average B-MetricName
embedding I-MetricName
. O
We O
can O
observe O
similar O
results O
on O
DSTC7 B-DatasetName
- I-DatasetName
AVSD I-DatasetName
. O
PVHD B-MethodName
enjoys O
the O
advantage O
of O
1.3 B-MetricValue
% I-MetricValue
to O
18.22 B-MetricValue
% I-MetricValue
on O
PPL B-MetricName
, O
3.00 B-MetricValue
% I-MetricValue
to O
3.40 B-MetricValue
% I-MetricValue
higher O
on O
BLEU-1 B-MetricName
, O
0.54 B-MetricValue
% I-MetricValue
to O
1.19 B-MetricValue
% I-MetricValue
higher O
on O
Dist-1 B-MetricName
, O
1.31 B-MetricValue
% I-MetricValue
to O
5.76 B-MetricValue
% I-MetricValue
higher O
on O
Dist-2 B-MetricName
and O
0.11 B-MetricValue
% I-MetricValue
to O
2.22 B-MetricValue
% I-MetricValue
higher O
on O
average B-MetricName
embedding I-MetricName
compared O
with O
these O
classic O
variational O
mechanism O
models O
. O
The O
main O
reason O
for O
the O
unimpressive O
performance O
of O
RNN B-MethodName
- O
based O
baselines O
is O
that O
these O
models O
suffer O
from O
latent O
variables O
vanishing O
observed O
in O
experiments O
. O
As O
shown O
in O
Figure O
4 O
, O
the O
Kullback O
- O
Leibler O
term O
of O
these O
models O
losses O
close O
to O
zero O
means O
that O
variational O
posterior O
distribution O
closely O
matches O
the O
prior O
for O
a O
subset O
of O
latent O
variables O
, O
indicating O
that O
failure O
of O
the O
variational O
mechanism O
( O
Lucas O
et O
al O
. O
, O
2019 O
) O
. O
The O
performance O
of O
SepaCVAE B-MethodName
is O
unimpressive O
. O
In O
fact O
, O
the O
performance O
of O
SepaCVAE B-MethodName
depends O
on O
the O
quality O
of O
context O
grouping O
( O
referring O
to O
dialogue O
augmentation O
in O
original O
paper O
( O
Sun O
et O
al O
. O
, O
2021 O
) O
) O
. O
Sepa B-MethodName
- I-MethodName
CVAE I-MethodName
will O
degenerate O
to O
CVAE B-MethodName
model O
if O
context O
grouping O
fails O
to O
work O
well O
, O
and O
even O
which O
will O
introduce O
wrong O
grouping O
noise O
information O
result O
- O
ing O
in O
degrade O
performance O
. O
As O
shown O
in O
Figure O
4 O
, O
the O
Kullback O
- O
Leibler O
term O
of O
SepaCVAE B-MethodName
losses O
is O
at O
a O
high O
level O
, O
which O
demonstrates O
that O
the O
prior O
for O
a O
subset O
of O
latent O
variables O
can O
not O
approximate O
variational O
posterior O
distribution O
. O

Compared O
with O
Transformer B-MethodName
- O
based O
baselines O
, O
PVHD B-MethodName
still O
enjoys O
an O
advantage O
on O
most O
metrics O
, O
especially O
the O
distinct B-MetricName
metric O
. O
GVT B-MethodName
introduces O
latent O
variables O
between O
the O
whole O
dialogue O
history O
and O
response O
, O
which O
faces O
the O
problem O
of O
latent O
variables O
vanishing O
. O
SVT B-MethodName
introduces O
a O
sequence O
of O
latent O
variables O
into O
the O
decoder O
to O
model O
the O
diversity O
of O
responses O
. O
But O
it O
is O
debatable O
whether O
latent O
variables O
will O
destroy O
the O
fragile O
sequence O
perception O
ability O
of O
the O
transformer O
, O
which O
will O
greatly O
reduce O
the O
quality O
of O
the O
responses O
. O
Training O
the O
transformer B-MethodName
from O
scratch O
instead O
of O
using O
a O
pretrained O
model O
is O
another O
reason O
for O
the O
inferior O
performance O
of O
SVT B-MethodName
and O
GVT B-MethodName
. O
Compared O
to O
DialogVED B-MethodName
and O
PLATO B-MethodName
, O
PVHD B-MethodName
achieves O
the O
best O
performance O
on O
most O
metrics O
. O
The O
main O
reason O
is O
that O
pseudo O
- O
variational O
approaches O
do O
not O
depend O
on O
posteriors O
distribution O
avoiding O
optimization O
problems O
and O
the O
recurrent O
summarizing O
variable O
can O
model O
the O
diversity O
of O
sequences O
. O
Overall O
, O
PVHD B-MethodName
has O
the O
most O
obvious O
advantages O
in O
diversity O
, O
which O
demonstrates O
the O
effectiveness O
of O
the O
recurrent O
summarizing O
variable O
. O
Another O
reason O
is O
that O
Transformer B-MethodName
- O
based O
baselines O
including O
SVT B-MethodName
, O
GVT B-MethodName
, O
PLATO B-MethodName
and O
DialogVED B-MethodName
connect O
all O
the O
dialogue O
history O
utterances O
into O
a O
consecutive O
sequence O
. O
They O
can O
only O
model O
the O
diversity O
between O
entire O
dialogue O
histories O
and O
responses O
. O
Coarse O
- O
grained O
modeling O
is O
the O
reason O
for O
poor O
model O
performance O
. O

Although O
transformers B-MethodName
are O
popular O
for O
generation O
task O
, O
our O
research O
is O
still O
meritorious O
. O
First O
, O
transformer B-MethodName
models O
usually O
require O
pre O
- O
training O
on O
large O
- O
scale O
corpus O
while O
RNN B-MethodName
- O
based O
models O
usually O
do O
not O
have O
such O
limitations O
. O
It O
is O
debatable O
whether O
transformer B-MethodName
models O
training O
from O
scratch O
under O
conditions O
where O
pre O
- O
training O
language O
models O
are O
unavaliable O
can O
achieve O
the O
desired O
performance O
if O
downstream O
task O
does O
not O
have O
enough O
corpus O
. O
Second O
, O
the O
parameter O
amount O
of O
the O
RNNbased B-MethodName
model O
is O
usually O
smaller O
than O
that O
of O
the O
transformer O
- O
based O
model O
. O
The O
parameter O
sizes O
of O
PVHD B-MethodName
on O
the O
DailyDialog B-DatasetName
and O
DSTC7 B-DatasetName
- I-DatasetName
AVSD I-DatasetName
are O
29 O
M O
and O
21 O
M O
, O
respectively O
. O
The O
number O
of O
parameters O
for O
PLATO B-MethodName
and O
DialogVED B-MethodName
is O
132 O
M O
and O
1143 O
M O
on O
two O
datasets O
, O
respectively O
. O
Compared O
to O
PLATO B-MethodName
and O
DialogVED B-MethodName
, O
the O
average O
number O
of O
parameters O
of O
PVHD B-MethodName
is O
5.28x O
and O
45.72x O
smaller O
, O
respectively O
. O

Human B-MetricName
Evaluation I-MetricName
Results O
& O
Analysis O

We O
conduct O
human B-MetricName
evaluation I-MetricName
to O
further O
confirm O
the O
effectiveness O
of O
the O
PVHD B-MethodName
. O
To O
evaluate O
the O
consistency O
of O
the O
results O
assessed O
by O
annotators O
, O
we O
employ O
Pearson B-MetricName
's I-MetricName
correlation I-MetricName
coefficient I-MetricName
( O
Sedgwick O
, O
2012 O
) O
. O
This O
coefficient O
is O
0.35 B-MetricValue
on O
diversity B-MetricName
, O
0.65 B-MetricValue
on O
relevance B-MetricName
, O
and O
0.75 B-MetricValue
on O
fluency B-MetricName
, O
with O
p O
< O
0.0001 O
and O
below O
0.001 O
, O
which O
demonstrates O
high O
correlation O
and O
agreement O
. O
The O
results O
of O
the O
human B-MetricName
evaluation I-MetricName
are O
shown O
in O
Table O
3 O
. O
Compared O
to O
RNN B-MethodName
- O
based O
baselines O
, O
PVHD B-MethodName
has O
a O
significant O
advantage O
in O
relevance B-MetricName
and O
diversity B-MetricName
. O
Specifically O
, O
PVHD B-MethodName
enjoys O
the O
advantage O
of O
11.40 B-MetricValue
% I-MetricValue
on O
diversity B-MetricName
and O
16.00 B-MetricValue
% I-MetricValue
on O
relevance B-MetricName
compared O
to O
SepaCVAE B-MethodName
on O
DailyDialog B-DatasetName
. O
On O
DSTC7 B-DatasetName
- I-DatasetName
AVSD I-DatasetName
, O
PVHD B-MethodName
has O
a O
advantage O
of O
10.50 B-MetricValue
% I-MetricValue
on O
diversity B-MetricName
and O
73.00 B-MetricValue
% I-MetricValue
on O
relevance B-MetricName
compared O
to O
SepaCVAE B-MethodName
. O
Compared O
to O
transformer B-MethodName
- O
based O
baselines O
, O
although O
PVHD B-MethodName
is O
sub O
- O
optimal O
in O
some O
metrics O
, O
it O
enjoys O
the O
advantage O
in O
most O
metrics O
, O
especially O
diversity B-MetricName
. O
In O
terms O
of O
fluency B-MetricName
, O
PVHD B-MethodName
is O
only O
1.00 B-MetricValue
% I-MetricValue
lower O
than O
HVRNN B-MethodName
and O
is O
much O
better O
that O
other O
baselines O
on O
DailyDialog B-DatasetName
. O
However O
, O
the O
fluency B-MetricName
of O
PVHD B-MethodName
is O
26.50 B-MetricValue
% I-MetricValue
lower O
compared O
with O
HVRNN B-MethodName
and O
8.00 B-MetricValue
% I-MetricValue
lower O
compared O
with O
VHCR B-MethodName
on O
DSTC7 B-DatasetName
- I-DatasetName
AVSD I-DatasetName
. O
We O
argue O
that O
introducing O
a O
recurrent O
summary O
variable O
in O
the O
decoder O
increases O
the O
randomness O
of O
word O
generation O
, O
which O
will O
promote O
the O
diversity B-MetricName
of O
the O
responses O
with O
a O
side O
effect O
of O
fluency B-MetricName
reduction O
. O

Effectiveness O
of O
Summarizing O
Variables O

We O
further O
analyze O
the O
effectiveness O
of O
PVHD B-MethodName
on O
summarizing O
variables O
. O
Figure O
5 O
demonstrates O
the O
visualization O
of O
word O
- O
level O
and O
utterance O
- O
level O
summarizing O
variables O
on O
test O
set O
of O
DailyDialog B-DatasetName
and O
DSTC7 B-DatasetName
- I-DatasetName
AVSD I-DatasetName
datasets O
. O
We O
can O
observe O
that O
both O
datasets O
exhibit O
high O
variability O
characteristic O
on O
word O
- O
level O
and O
utterance O
- O
level O
. O
Specifically O
, O
the O
summarizing O
variables O
on O
word O
- O
level O
show O
obvious O
categorical O
features O
, O
which O
indicates O
that O
a O
subsequence O
may O
have O
multiple O
suitable O
candidate O
words O
. O
Moreover O
, O
the O
summarizing O
variables O
on O
utterancelevel O
also O
exhibit O
impressive O
categorical O
features O
, O
which O
confirms O
that O
there O
is O
a O
one O
- O
to O
- O
many O
issue O
in O
the O
dialogue O
. O
These O
phenomena O
make O
dialogue O
generation O
different O
from O
machine O
translation O
where O
unique O
semantic O
mapping O
exists O
between O
source O
and O
target O
language O
. O

Conclusion O

We O
analyze O
the O
reasons O
for O
one O
- O
to O
- O
many O
and O
manyto O
- O
one O
issues O
from O
high O
variability O
of O
dialogue O
. O
We O
build O
PVHD B-MethodName
based O
on O
proposed O
PVGRU B-MethodName
component O
to O
model O
the O
word O
- O
level O
and O
utterance O
- O
level O
variation O
in O
dialogue O
for O
generating O
relevant O
and O
diverse O
responses O
. O
The O
results O
demonstrate O
that O
PVHD B-MethodName
even O
outperforms O
pre O
- O
trained O
language O
models O
on O
diversity O
metrics O
. O

Limitations O

Although O
our O
work O
can O
effectively O
model O
the O
variability O
issue O
in O
dialogue O
, O
we O
acknowledge O
some O
limitations O
of O
our O
study O
. O
Firstly O
, O
our O
study O
can O
work O
well O
on O
the O
approaches O
based O
on O
RNN B-MethodName
, O
but O
can O
not O
be O
employed O
to O
sequence O
models O
based O
on O
Transformer B-MethodName
, O
which O
limits O
the O
generality O
of O
our O
approach O
. O
The O
reasons O
we O
analyze O
are O
as O
follows O
. O

Transformer B-MethodName
is O
not O
a O
good O
architecture O
for O
finegrained O
diversity B-MetricName
. O
The O
diversity B-MetricName
of O
dialogue O
includes O
three O
granularities O
of O
discourse O
level O
, O
utterance O
level O
and O
word O
level O
. O
To O
model O
diversity B-MetricName
, O
models O
will O
be O
required O
to O
utilize O
the O
representation O
at O
time O
t O
and O
the O
relationship O
between O
the O
representation O
at O
time O
t O
and O
time O
t+1 O
to O
determine O
the O
representation O
at O
time O
t+1 O
. O
Relationships O
are O
computed O
step O
by O
step O
. O
If O
we O
only O
consider O
discourse O
- O
level O
diversity B-MetricName
, O
our O
approach O
and O
variational O
mechanisms O
are O
easily O
transferable O
to O
Transformer B-MethodName
architectures O
. O
Because O
we O
can O
use O
the O
Transformer B-MethodName
model O
to O
encode O
the O
entire O
historical O
dialogue O
sequence O
. O
Latent O
variables O
or O
summarizing O
variables O
only O
exist O
between O
the O
entire O
historical O
sequence O
and O
the O
responses O
. O
This O
will O
not O
destroy O
the O
parallel O
structure O
of O
the O
Transformer O
. O
if O
we O
employ O
a O
Transformer B-MethodName
to O
model O
diversity B-MetricName
at O
the O
utterance O
and O
word O
granularity O
, O
this O
will O
seriously O
damage O
the O
parallelism O
of O
the O
Transformer B-MethodName
. O

There O
are O
great O
limitations O
in O
the O
variational O
transformer B-MethodName
models O
. O
The O
transformer B-MethodName
and O
variational O
thinking O
is O
not O
a O
good O
match O
, O
which O
leads O
to O
less O
relevant O
research O
. O
The O
Transformer B-MethodName
baselines O
we O
compared O
in O
the O
manuscript O
( O
i.e. O
SVT B-MethodName
, O
GVT B-MethodName
, O
PLATO B-MethodName
and O
DialogVED B-MethodName
) O
cover O
most O
of O
the O
current O
transformer B-MethodName
models O
that O
combine O
variations O
. O
Although O
SVT B-MethodName
, O
GVT B-MethodName
, O
PLATO B-MethodName
and O
DialogVED B-MethodName
incorporate O
variational O
ideas O
, O
these O
models O
connect O
all O
the O
dialogue O
history O
utterances O
into O
a O
consecutive O
sequence O
. O
It O
is O
inadvisable O
to O
model O
the O
finegrained O
diversity B-MetricName
relationship O
in O
a O
parallel O
structure O
. O

Secondly O
, O
although O
our O
methods O
can O
improve O
the O
diversity B-MetricName
and O
relevence O
of O
responses O
, O
there O
are O
still O
gaps O
in O
fluency B-MetricName
compared O
with O
other O
baselines O
. O
Gold O
: O
honey O
, O
cross O
my O
heart O
, O
i'v O
never O
looked O
at O
another O
woman O
since O
the O
first O
day O
i O
set O
my O
eyes O
on O
you O
! O
believe O
me O
, O
that O
's O
the O
truth O
. O
SVT B-MethodName
: O
Honey O
, O
cross O
my O
heart O
, O
I'v O
never O
looked O
at O
another O
woman O
since O
the O
truth O
. O
GVT B-MethodName
: O
I'v O
never O
looked O
at O
another O
woman O
since O
the O
first O
day O
UNK O
. O
SepaCVAE B-MethodName
: O
i O
know O
how O
you O
really O
well O
. O
DialogVED B-MethodName
: O
i O
'm O
sorry O
, O
but O
i O
ca O
n't O
let O
you O
do O
that O
. O
PVHD B-MethodName
: O
actually O
, O
i'v O
not O
looked O
at O
another O
woman O
at O
all O
because O
the O
first O
day O
i O
set O
my O
eyes O
on O
you O
! O
believe O
me O
. O

Context O
: O
he O
turns O
his O
cellphone O
light O
on O
to O
help O
him O
see O
to O
screw O
the O
knob O
back O
into O
the O
dresser O
drawer O
. O

does O
he O
end O
up O
fixing O
it O
correctly O
? O
yes O
, O
he O
screws O
the O
knob O
back O
on O
correctly O
. O
does O
he O
take O
anything O
our O
of O
the O
drawer O
? O
no O
he O
does O
not O
open O
the O
drawer O
, O
only O
fixes O
it O
. O
Gold O
: O
he O
interacts O
with O
the O
computers O
after O
fixing O
the O
knob O
he O
simply O
stands O
up O
and O
begins O
to O
leave O
the O
room O
. O
SVT B-MethodName
: O
he O
appears O
to O
be O
carrying O
something O
. O
GVT B-MethodName
: O
no O
, O
he O
does O
not O
go O
to O
the O
computer O
. O
SepaCVAE B-MethodName
: O
no O
, O
he O
does O
not O
move O
from O
his O
computer O
. O
DialogVED B-MethodName
: O
no O
, O
he O
does O
not O
touch O
the O
computer O
. O
PVHD B-MethodName
: O
no O
, O
he O
does O
not O
interact O
with O
the O
computer O
at O
all O
. O

A.2 O
Implementation O
Details O

We O
implement O
our O
model O
and O
baselines O
using O
Tensorflow O
2 O
and O
train O
baselines O
on O
a O
server O
with O
RTX O
8000 O
GPU O
( O
48 O
G O
) O
. O
The O
dimension B-HyperparameterName
of I-HyperparameterName
word I-HyperparameterName
embeddings I-HyperparameterName
is O
set O
512 B-HyperparameterValue
. O
We O
consider O
at O
most O
10 O
turns O
of O
dialogue O
context O
and O
50 O
words O
for O
each O
utterance O
. O
The O
encoder O
adopts O
bidirectional O
structure O
and O
the O
decoder O
uses O
unidirectional O
structure O
. O

A.3 O
Automatic B-MetricName
Evaluation I-MetricName
Metrics O

We O
employ O
both O
automatic O
and O
human B-MetricName
evaluations I-MetricName
to O
assess O
the O
performance O
of O
compared O
methods O
. O
The O
automatic B-MetricName
evaluation I-MetricName
mainly O
includes O
the O
following O
metrics O
: O
BLEU B-MetricName
( O
Yang O
et O
al O
. O
, O
2018 O
) O
evaluates O
the O
n O
- O
gram O
co O
- O
occurrence O
between O
generated O
response O
and O
target O
response O
. O
ROUGE B-MetricName
- I-MetricName
L I-MetricName
( O
Yang O
et O
al O
. O
, O
2018 O
) O
evaluates O
the O
overlap O
of O
the O
longest O
common O
subsequences O
between O
generated O
response O
and O
the O
target O
response O
. O
Distinct-1 B-MetricName
/ I-MetricName
2 I-MetricName
( O
Li O
et O
al O
. O
, O
2016 O
) O
measures O
the O
generated O
response O
diversity O
, O
which O
is O
defined O
as O
the O
number O
of O
distinct O
uni O
- O
grams O
/ O
bi O
- O
grams O
divided O
by O
the O
total O
amount O
of O
generated O
words O
. O
PPL B-MetricName
( O
Perplexity O
) O
evaluates O
the O
confidence O
of O
the O
generated O
response O
. O
The O
lower O
PPL B-MetricName
score O
, O
the O
higher O
confidence O
for O
generating O
responses O
. O
Embedding B-MetricName
- I-MetricName
based I-MetricName
metrics O
( O
Average O
, O
Exterma O
and O
Greedy O
) O
measure O
the O
semantic O
relevance O
between O
generated O
response O
and O
target O
response O
( O
Liu O
et O
al O
. O
, O
2016 O
; O
Sedoc O
et O
al O
. O
, O
2019 O
; O
Xu O
et O
al O
. O
, O
2018b O
) O
. O

A.4 O
Human B-MetricName
Evaluation I-MetricName

Following O
the O
work O
of O
( O
Sun O
et O
al O
. O
, O
2021 O
; O
Li O
et O
al O
. O
, O
2017a O
; O
Xu O
et O
al O
. O
, O
2018a O
) O
, O
we O
divide O
six O
crowdsourced O
graduate O
students O
into O
two O
groups O
to O
evaluate O
the O
quality O
of O
generated O
responses O
for O
100 O
randomly O
sampled O
input O
contexts O
, O
respectively O
. O
We O
request O
annotators O
to O
rank O
the O
generated O
responses O
with O
respect O
to O
three O
aspects O
: O
fluency B-MetricName
, O
diversity B-MetricName
, O
and O
relevance B-MetricName
. O
Fluency B-MetricName
measures O
whether O
the O
generated O
responses O
are O
smooth O
or O
grammatically O
correct O
. O
Diversity B-MetricName
evaluates O
whether O
the O
generated O
responses O
are O
informative O
, O
rather O
than O
generic O
and O
repeated O
information O
. O
Relevance B-MetricName
evaluates O
whether O
the O
generated O
responses O
are O
relevant O
to O
the O
dialogue O
context O
. O
The O
average O
scores O
of O
the O
two O
groups O
is O
taken O
as O
the O
final O
score O
. O

A.5 O
Ablation O
Study O

We O
conduct O
ablation O
experiments O
on O
the O
proposed O
loss O
modules O
. O
Table O
4 O
reports O
the O
results O
of O
the O
ablation O
experiments O
of O
PVHD B-MethodName
on O
DailyDialog B-DatasetName
and O
DSTC7 B-DatasetName
- I-DatasetName
AVSD I-DatasetName
. O
-RE O
removes O
the O
reconstruction O
loss O
. O
-CO O
removes O
the O
consistency O
loss O
. O
The O
results O
demonstrate O
that O
our O
optimization O
objectives O
are O
effective O
. O
We O
can O
observe O
that O
the O
reconstruction O
loss O
can O
improve O
the O
BLEU-1 B-MetricName
/ I-MetricName
2 I-MetricName
and O
Rouge B-MetricName
- I-MetricName
L. I-MetricName
The O
consistency O
loss O
can O
improve O
Dist-1 B-MetricName
/ I-MetricName
2 I-MetricName
metrics O
at O
the O
the O
expense O
of O
BLEU-1 B-MetricName
/ I-MetricName
2 I-MetricName
and O
Rouge B-MetricName
- I-MetricName
L I-MetricName
metrics O
. O
We O
believe O
that O
the O
consistency O
loss O
can O
ensure O
the O
consistency O
between O
the O
incremental O
information O
and O
the O
input O
at O
each O
time O
step O
. O
There O
may O
be O
multiple O
candidate O
tokens O
following O
the O
same O
distribution O
, O
which O
increases O
the O
diversity B-MetricName
of O
generated O
responses O
. O
The O
reconstruction O
loss O
can O
make O
the O
summarizing O
variable O
recording O
the O
accumulated O
distribution O
of O
subsequence O
reflect O
the O
semantic O
information O
of O
dialogue O
context O
correctly O
, O
which O
will O
reduce O
the O
randomness O
of O
the O
generation O
process O
by O
limiting O
candidates O
that O
do O
not O
conform O
to O
sequence O
semantics O
. O

A.6 O
Significance O
Testing O

To O
evaluate O
the O
reliability O
of O
the O
PVHD B-MethodName
results O
, O
we O
performe O
multiple O
significance O
tests O
. O
Table O
6 O
( O
in O
Appendix O
A O
) O
reports O
the O
results O
of O
the O
significance O
test O
for O
automatic B-MetricName
evaluation I-MetricName
. O
We O
can O
observe O
that O
the O
p O
- O
values O
of O
PVHD B-MethodName
are O
less O
than O
0.05 O
compared O
with O
other O
models O
. O
Although O
the O
results O
of O
PVHD B-MethodName
is O
not O
optimal O
in O
some O
metrics O
, O
the O
significance O
test O
demonstrates O
that O
results O
of O
PVHD B-MethodName
are O
statistically O
significantly O
different O
from O
other O
models O
. O
In O
other O
words O
, O
the O
performance O
advantage O
of O
PVHD B-MethodName
is O
statistically O
reliable O
and O
not O
an O
accident O
caused O
by O
random O
factors O
. O

3307 O

A.7 O
Case O
Study O

To O
further O
dissect O
the O
quality O
of O
PVHD B-MethodName
, O
several O
examples O
of O
generated O
responses O
are O
provided O
in O
Table O
5 O
. O
Although O
DialogVED B-MethodName
, O
SVT B-MethodName
, O
GVT B-MethodName
can O
generate O
relevant O
responses O
, O
PVHD B-MethodName
can O
produce O
higher O
quality O
responses O
in O
comparison O
. O
Specifically O
, O
for O
the O
first O
example O
, O
the O
responses O
generated O
by O
other O
models O
are O
contextual O
except O
for O
Sepa B-MethodName
- I-MethodName
CVAE I-MethodName
. O
The O
response O
generated O
by O
DialogVED B-MethodName
is O
more O
diffuse O
than O
gold O
response O
, O
but O
response O
generated O
by O
PVHD B-MethodName
is O
more O
informative O
and O
possesses O
a O
different O
sentence O
pattern O
and O
different O
wording O
than O
gold O
response O
to O
some O
extent O
. O
We O
can O
observe O
the O
similar O
case O
for O
the O
second O
example O
. O
We O
believe O
that O
this O
is O
mainly O
due O
to O
the O
capture O
of O
variability O
of O
corpus O
by O
summarizing O
variable O
, O
which O
enables O
the O
model O
to O
identify O
similar O
sentence O
patterns O
and O
words O
, O
and O
generate O
diverse O
responses O
. O

Acknowledgement O

We O
would O
like O
to O
thank O
the O
reviewers O
for O
their O
constructive O
comments O
. O
The O
project O
is O
supported O
by O
the O
National O
Natural O
Science O
Foundation O
of O
China O
( O
62272092,62172086 O
) O
and O
the O
European O
Research O
Council O
( O
grant O
# O
740516 O
) O
. O
The O
project O
is O
also O
supported O
by O
the O
Fundamental O
Research O
Funds O
for O
the O
Central O
Universities O
of O
China O
under O
Grant O
No O
. O
N2116008 O
and O
China O
Scholarship O
Council O
. O

AUTOSUMM B-TaskName
: O
Automatic B-TaskName
Model I-TaskName
Creation I-TaskName
for I-TaskName
Text I-TaskName
Summarization I-TaskName

Recent O
efforts O
to O
develop O
deep O
learning O
models O
for O
text O
generation O
tasks O
such O
as O
extractive O
and O
abstractive O
summarization O
have O
resulted O
in O
state O
- O
of O
- O
the O
- O
art O
performances O
on O
various O
datasets O
. O
However O
, O
obtaining O
the O
best O
model O
configuration O
for O
a O
given O
dataset O
requires O
an O
extensive O
knowledge O
of O
deep O
learning O
specifics O
like O
model O
architecture O
, O
tuning O
parameters O
etc O
. O
, O
and O
is O
often O
extremely O
challenging O
for O
a O
non O
- O
expert O
. O
In O
this O
paper O
, O
we O
propose O
methods O
to O
automatically O
create O
deep O
learning O
models O
for O
the O
tasks O
of O
extractive O
and O
abstractive O
text O
summarization O
. O
Based O
on O
the O
recent O
advances O
in O
Automated O
Machine O
Learning O
and O
the O
success O
of O
large O
language O
models O
such O
as O
BERT O
and O
GPT-2 O
in O
encoding O
knowledge O
, O
we O
use O
a O
combination O
of O
Neural O
Architecture O
Search O
( O
NAS O
) O
and O
Knowledge O
Distillation O
( O
KD O
) O
techniques O
to O
perform O
model O
search O
and O
compression O
using O
the O
vast O
knowledge O
provided O
by O
these O
language O
models O
to O
develop O
smaller O
, O
customized O
models O
for O
any O
given O
dataset O
. O
We O
present O
extensive O
empirical O
results O
to O
illustrate O
the O
effectiveness O
of O
our O
model O
creation O
methods O
in O
terms O
of O
inference O
time O
and O
model O
size O
, O
while O
achieving O
near O
state O
- O
of O
- O
the O
- O
art O
performances O
in O
terms O
of O
accuracy O
across O
a O
range O
of O
datasets O
. O

Introduction O

Machine O
learning O
algorithms O
, O
particularly O
, O
deep O
learning O
techniques O
have O
led O
to O
the O
simplification O
of O
several O
computationally O
expensive O
tasks O
. O
However O
, O
training O
and O
optimizing O
these O
models O
for O
different O
tasks O
demand O
the O
experienced O
engineering O
resources O
and O
require O
expertise O
, O
making O
it O
difficult O
for O
non O
- O
experts O
. O
Automated O
Machine O
Learning O
is O
a O
strategy O
to O
automate O
this O
pipeline O
for O
model O
creation O
including O
automated O
generation O
of O
the O
model O
itself O
. O
* O
Work O
done O
while O
authors O
were O
at O
Adobe O
Research O
. O

In O
the O
case O
of O
Natural O
Language O
Processing O
and O
Text O
analysis O
, O
the O
advent O
of O
large O
language O
models O
such O
as O
BERT O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
, O
GPT2 O
( O
Radford O
et O
al O
. O
, O
2019 O
) O
, O
and O
more O
recently O
GPT3 O
( O
Brown O
et O
al O
. O
, O
2020 O
) O
have O
created O
resources O
that O
can O
be O
exploited O
for O
the O
creation O
of O
robust O
models O
for O
several O
downstream O
NLP O
tasks O
. O
However O
, O
the O
need O
for O
ML O
expertise O
creates O
a O
bottleneck O
. O
Further O
, O
these O
deep O
learning O
models O
have O
thousands O
of O
parameters B-MetricName
and O
need O
fairly O
large O
datasets O
and O
computational B-MetricName
resources I-MetricName
for O
training O
. O

We O
focus O
on O
providing O
algorithms O
for O
autogeneration B-TaskName
of I-TaskName
ML I-TaskName
models I-TaskName
for I-TaskName
complex I-TaskName
NLP I-TaskName
tasks I-TaskName
such I-TaskName
as I-TaskName
extraction I-TaskName
and I-TaskName
generation I-TaskName
, O
making O
them O
accessible O
to O
non O
experts O
. O
Our O
proposed O
approaches O
feed O
off O
the O
knowledge O
available O
in O
large O
pretrained O
models O
to O
auto O
- O
generate O
new O
, O
smaller O
, O
customized O
models O
for O
a O
custom O
dataset O
. O
Specifically O
, O
the O
major O
contributions O
are O
as O
follows O
. O

Figure O
1 O
shows O
the O
overview O
of O
our O
model O
creation O
framework O
. O
The O
input O
is O
the O
dataset O
and O
task O
specifications O
( O
summary O
type O
, O
size O
) O
and O
the O
output O
is O
a O
custom O
trained O
summarization O
model O
, O
which O
can O
be O
further O
used O
to O
create O
text O
summaries O
. O
In O
this O
paper O
, O
we O
generate O
models O
for O
both O
extractive O
and O
abstractive O
summarization O
tasks O
, O
with O
the O
former O
being O
a O
binary O
classification O
task O
to O
extract O
summary O
sentences O
from O
the O
input O
, O
while O
the O
latter O
aims O
to O
generate O
summaries O
containing O
novel O
words O
and O
phrases O
that O
may O
not O
be O
present O
in O
the O
input O
text O
. O
Our O
proposed O
approaches O
distills O
knowledge O
from O
a O
language O
- O
model O
based O
teacher O
network O
to O
generate O
an O
encoder O
- O
decoder O
- O
based O
child O
model O
. O
We O
present O
two O
algorithms O
that O
aid O
in O
auto O
- O
creation O
of O
different O
types O
of O
resulting O
' O
child O
' O
models O
- O
( O
1 O
) O
a O
model O
with O
convolutional O
and O
recurrent O
units O
and O
( O
2 O
) O
a O
mini O
- O
transformer O
based O
model O
. O
The O
first O
is O
achieved O
by O
our O
approach O
AUTOSUMM B-TaskName
- I-TaskName
CREATE I-TaskName
and O
the O
second O
using O
AUTOSUMM B-TaskName
- I-TaskName
DISTILL I-TaskName
, O
which O
are O
detailed O
as O
follows O
. O
Here O
, O
we O
combine O
knowledge O
distillation O
with O
neural O
architecture O
search O
to O
auto O
- O
create O
an O
encoderdecoder O
based O
summarization O
model O
. O
The O
stages O
in O
this O
method O
include O
: O

AutoSumm B-TaskName
- I-TaskName
Create I-TaskName

1 O
. O
Task O
- O
specific O
knowledge O
distillation O
: O
We O
leverage O
knowledge O
from O
a O
transformer O
- O
based O
BERT O
model O
( O
teacher O
) O
fine O
- O
tuned O
for O
extractive O
and O
abstractive O
summarization O
( O
Liu O
and O
Lapata O
, O
2019b O
) O
on O
the O
given O
task O
- O
specific O
( O
summarization O
) O
dataset O
. O
The O
predictions O
from O
the O
teacher O
model O
are O
used O
for O
distillation O
, O
i.e. O
, O
the O
sentences O
classification O
scores O
for O
extractive O
and O
probability O
distributions O
over O
the O
vocabulary O
for O
abstractive O
are O
augmented O
to O
the O
ground O
truth O
. O
A O
Knowledge O
Distillation O
( O
L O
KD O
) O
loss O
is O
included O
to O
perform O
informed O
search O
on O
the O
child O
models O
, O
ensuring O
that O
they O
mimic O
the O
performance O
of O
the O
teacher O
. O
In O
extractive O
summarization O
, O
L O
KD O
is O
the O
MSE O
loss O
between O
soft O
labels O
from O
augmented O
data O
and O
the O
scored O
predicted O
by O
the O
child O
model O
. O

L O
KD O
= O
n O
i=1 O
( O
y O
teacher O
i O
− O
y O
pred O
i O
) O
2 O

( O
1 O
) O

In O
abstractive O
summarization O
, O
L O
KD O
is O
calculated O
at O
each O
time O
step O
t O
using O
soft O
labels O
P O
teacher O
( O
y O
t O
) O
from O
teacher O
model O
and O
the O
predicted O
labels O
P O
pred O
( O
y O
t O
) O
from O
child O
model O
over O
vocab O
V O
as O
follows O
: O

L O
KD O
= O
t O
w∈V O

P O
teacher O
( O
y O
t O
= O
w|y O
1 O
: O
t−1 O
, O
X O
) O
. O
log O
( O
P O
pred O
( O
y O
t O
= O
w|y O
1 O
: O
t−1 O
, O
X O
) O
) O

2 O
. O
Neural O
Architectural O
Search O
: O
Augmented O
dataset O
, O
along O
with O
a O
small O
labelled O
custom O
dataset O
, O
is O
used O
to O
train O
the O
NAS O
module O
, O
which O
searches O
for O
the O
right O
combination O
of O
cells O
that O
result O
in O
the O
child O
model O
most O
suited O
for O
the O
summarization O
task O
. O

In O
our O
approach O
, O
we O
use O
NAS O
to O
search O
the O
encoder O
space O
while O
using O
a O
predefined O
( O
task O
- O
specific O
) O
decoder O
. O
The O
key O
components O
of O
this O
module O
are O
: O
Search O
space O
. O
Following O
, O
we O
define O
macro O
search O
space O
, O
such O
that O
the O
model O
can O
be O
represented O
by O
a O
directed O
acyclic O
graph O
( O
DAG O
) O
with O
nodes O
representing O
a O
layer O
from O
the O
search O
space O
and O
edges O
representing O
the O
directionality O
of O
flow O
of O
information O
. O
The O
search O
space O
has O
4 O
key O
cell O
types O
-CNN O
( O
kernel O
sizes O
1,3,5,7 O
) O
, O
RNN O
( O
bidirectional O
GRU O
) O
, O
Pooling O
layers O
( O
avg O
. O
pool O
and O
max O
. O
pool O
with O
stride O
1 O
and O
uniform O
padding O
) O
, O
and O
Multi O
- O
head O
self O
- O
attention O
( O
8 O
heads O
, O
no O
positional O
embeddings O
) O
. O
We O
constrain O
the O
search O
space O
by O
( O
1 O
) O
defining O
the O
number O
of O
skip O
connections O
allowed O
, O
( O
2 O
) O
limiting O
the O
maximum O
number O
of O
layers O
in O
the O
child O
architecture O
, O
l O
( O
in O
our O
case O
l O
∈ O
1,5,10,18,20 O
) O
, O
and O
( O
3 O
) O
defining O
the O
cells O
allowed O
in O
the O
new O
architecture O
. O
These O
constraints O
define O
the O
exhaustive O
list O
of O
possibilities O
for O
the O
NAS O
algorithm O
. O

Search O
algorithm O
: O
We O
implement O
ENAS B-HyperparameterName
( O
Pham O
et O
al O
. O
, O
2018 O
) O
, O
a O
reinforcement O
learning O
( O
RL O
) O
based O
algorithm O
used O
for O
several O
NAS O
implementations O
( O
Zoph O
and O
Le O
, O
2017 O
) O
. O
It O
consists O
of O
an O
RNN O
controller O
network O
, O
that O
samples O
a O
model O
architecture O
from O
the O
search O
space O
and O
an O
RL O
reward O
to O
nudge O
this O
controller O
towards O
generating O
an O
optimal O
architecture O
. O

Pre O
- O
defined O
Model O
Specifications O
: O
As O
stated O
earlier O
, O
we O
auto O
- O
create O
the O
encoder O
layers O
in O
the O
model O
but O
predefine O
the O
task O
- O
specific O
decoder O
. O
For O
extractive O
summarization O
, O
the O
decoder O
is O
a O
scorer O
function O
with O
sigmoid O
activation O
, O
which O
takes O
in O
the O
text O
representations O
learnt O
from O
the O
encoder O
and O
scores O
each O
sentence O
on O
a O
scale O
of O
( O
0,1 O
] O
. O
The O
sentences O
with O
the O
high O
scores O
are O
chosen O
as O
the O
final O
summary O
based O
on O
the O
summary O
size O
specified O
. O
For O
abstractive O
summarization O
, O
a O
recurrent O
neural O
network O
is O
used O
as O
the O
decoder O
. O
The O
input O
is O
the O
text O
representation O
from O
the O
encoder O
and O
the O
output O
is O
a O
generated O
summary O
( O
generated O
in O
auto O
- O
regressive O
manner O
, O
by O
decoding O
a O
word O
at O
every O
time O
step O
) O
. O

Loss O
: O
The O
architectures O
are O
trained O
with O
a O
crossentropy B-HyperparameterName
loss I-HyperparameterName
at O
sentence O
level O
for O
extractive O
and O
vocab O
level O
for O
abstractive O
as O
follows O
: O

Ext O
( O
L O
CE O
) O
= O
n O
i=1 O
( O
p O
gt O
( O
y O
i O
) O
.log O
( O
y O
child O
i O
) O
( O
3 O
) O
Abs O
( O
L O
CE O
) O
= O
t O
w∈V O
P O
gt O
( O
y O
t O
= O
w O
) O
. O
log O
( O
P O
pred O
( O
y O
t O
= O
w|y O
1 O
: O
t−1 O
, O
X O
) O
) O
( O
4 O
) O

Final O
Loss O
: O
The O
final O
end O
- O
end O
loss O
associated O
with O
this O
framework O
is O
computed O
as O
the O
weighted O
sum O
of O
the O
L O
KD O
and O
L O
CE O
in O
the O
NAS O
module O
: O

L O
total O
= O
α O
. O
L O
CE O
+ O
( O
1 O
− O
α O
) O
.L O
KD O
( O
5 O
) O

RL O
Reward O
: O
A O
reward O
based O
on O
the O
performance O
of O
the O
child O
model O
, O
is O
sent O
back O
to O
the O
RNN O
controller O
. O
The O
policy O
gradients O
of O
the O
RNN O
controller O
are O
updated O
through O
REINFORCE B-HyperparameterName
( O
Williams O
, O
1992 O
) O
algorithm O
. O
Reward O
( O
R O
) O
is O
defined O
as O
1 O
− O
Loss O
valid O
, O
normalized O
over O
the O
batchsize O
. O

Re O
- O
training O
: O
The O
newly O
generated O
model O
, O
is O
trained O
using O
the O
user O
- O
provided O
training O
data O
optimizing O
for O
the O
total O
loss O
( O
L O
total O
) O
. O
This O
trained O
model O
can O
generate O
summaries O
for O
any O
given O
test O
sample O
. O

AutoSumm B-TaskName
- I-TaskName
Distill I-TaskName

In O
this O
approach O
, O
the O
structure O
of O
the O
child O
is O
defined O
as O
a O
mini O
- O
transformer O
( O
4 O
layers O
) O
. O
A O
knowledge O
distillation O
technique O
called O
transformer O
distillation O
( O
Jiao O
et O
al O
. O
, O
2019 O
) O
is O
used O
to O
create O
a O
generalmini O
- O
transformer O
( O
4 O
layers O
) O
from O
a O
large O
transformer O
model O
( O
12 O
layers O
) O
. O
Then O
, O
the O
knowledge O
is O
distilled O
from O
a O
task O
- O
specific O
fine O
- O
tuned O
BERT O
( O
' O
teacher O
' O
) O
model O
to O
the O
general O
- O
mini O
- O
transformer O
. O
Figure O
3 O
illustrates O
the O
workflow O
of O
this O
method O
. O
This O
method O
differs O
from O
AUTOSUMM B-TaskName
- I-TaskName
CREATE I-TaskName
, O
in O
the O
child O
model O
architecture O
and O
the O
usage O
of O
two O
transformer O
teacher O
models O
. O
The O
key O
stages O
in O
this O
method O
are O
detailed O
below O
. O
Knowledge O
distillation O
: O
There O
are O
two O
forms O
of O
knowledge O
distillation O
in O
this O
method O
( O
1 O
and O
3 O
in O
Fig O
. O
3 O
) O
. O
We O
detail O
the O
knowledge O
distillation O
from O
a O
task O
- O
specific O
transformer O
teacher O
( O
we O
use O
BERT O
- O
Summ O
( O
Liu O
and O
Lapata O
, O
2019b O
) O
) O
to O
the O
general O
- O
mini O
- O
transformer O
which O
forms O
the O
encoder O
layer O
for O
the O
final O
child O
model O
. O
The O
decoder O
is O
pre O
- O
defined O
based O
on O
the O
task O
, O
similar O
to O
AUTOSUMM B-TaskName
- I-TaskName
CREATE I-TaskName
. O
A O
transformer O
model O
has O
various O
types O
of O
layers O
including O
multi O
- O
headed O
attention O
, O
embedding O
layers O
, O
and O
the O
hidden O
layers O
. O
The O
intuition O
behind O
knowledge O
distillation O
is O
to O
teach O
the O
layers O
in O
the O
child O
transformer O
to O
mimic O
the O
corresponding O
layers O
in O
the O
teacher O
transformer O
. O
This O
is O
implemented O
by O
introducing O
separate O
losses O
for O
each O
layer O
type O
. O

Attention B-TaskName
- I-TaskName
based I-TaskName
distillation I-TaskName
builds O
on O
the O
intuition O
that O
the O
attention O
layers O
in O
BERT O
capture O
linguistic O
information O
such O
as O
syntax O
and O
coreference O
information O
. O
Specifically O
, O
the O
student O
aims O
to O
learn O
the O
matrices O
of O
the O
multi O
- O
headed O
attention O
from O
teacher O
. O
This O
loss O
is O
given O
by O

where O
h O
is O
the O
number O
of O
attention O
heads O
A O
i O
refers O
to O
the O
attention O
matrix O
corresponding O
to O
the O
i O
- O
th O
head O
of O
the O
teacher O
( O
T O
) O
or O
the O
student O
( O
S O
) O
, O
l O
is O
the O
input O
text O
length O
and O
M O
SE O
( O
. O
) O
refers O
to O
the O
mean O
squared O
error O
loss O
. O
Hidden O
- O
state O
distillation O
distills O
knowledge O
from O
the O
output O
of O
transformer O
hidden O
layer O
, O
with O

L O
hidn O
= O
M O
SE O
( O
H O
s O
W O
h O
, O
H O
T O
) O

where O
H O
s O
and O
H O
T O
refer O
to O
the O
hidden O
states O
of O
the O
student O
and O
teacher O
models O
. O
W O
h O
is O
a O
learnable O
linear O
transformation O
. O
Embedding O
- O
layer O
distillation O
: O

Formulated O
as O
L O
embd O
= O
M O
SE O
( O
E O
s O
W O
e O
, O
E O
T O
) O
where O
E O
s O
and O
E O
T O
are O
embeddings O
in O
the O
student O
and O
teacher O
networks O
respectively O
. O
W O
e O
plays O
a O
similar O
role O
as O
W O
h O
. O
Using O
these O
distillation O
objectives O
along O
with O
the O
general O
distillation O
already O
done O
to O
compress O
the O
transformer O
model O
to O
general O
- O
mini O
- O
transformer O
, O
the O
final O
loss O
is O
the O
unified O
distillation O
loss O
of O
the O
corresponding O
layers O
between O
the O
teacher O
and O
the O
student O
model O
. O
As O
a O
reminder O
, O
this O
step O
helps O
auto O
- O
learn O
the O
task O
specific O
encoder O
for O
extractive O
and O
abstractive O
summarization O
. O
Pre O
- O
defined O
Model O
Specifications O
: O
For O
extractive O
summarization O
, O
we O
define O
a O
single O
transformer O
layer O
on O
top O
of O
the O
newly O
created O
encoder O
with O
a O
classification O
layer O
as O
the O
decoder O
. O
For O
abstractive O
summarization O
, O
the O
decoder O
is O
6 O
- O
layer O
transformer O
. O
Training O
and O
Re O
- O
training O
: O
General O
distillation O
& O
Fine O
- O
tuning O
: O
The O
above O
model O
is O
trained O
in O
a O
phased O
manner O
. O
The O
first O
distillation O
or O
training O
is O
done O
from O
a O
large O
transformer O
( O
BERT O
) O
to O
the O
general O
-mini O
- O
transformer O
. O
Parallelly O
, O
a O
large O
BERT O
model O
is O
fine O
- O
tuned O
for O
the O
specified O
tasks O
. O
Both O
these O
steps O
need O
not O
be O
repeated O
for O
every O
new O
dataset O
from O
the O
user O
and O
every O
run O
of O
the O
model O
. O
The O
fine O
- O
tuned O
model O
and O
the O
general O
- O
mini O
- O
transformers O
may O
be O
created O
once O
per O
task O
and O
once O
per O
a O
very O
large O
benchmark O
dataset O
. O
Task O
- O
specific O
Distillation O
: O
This O
process O
of O
teaching O
the O
student O
model O
from O
a O
fine O
- O
tuned O
teacher O
model O
is O
repeated O
each O
time O
a O
new O
user O
dataset O
is O
given O
to O
the O
system O
. O
Once O
trained O
, O
this O
is O
coupled O
with O
the O
specific O
decoder O
. O

Re O
- O
training O
: O
Once O
the O
final O
child O
model O
i.e. O
minitransformer O
encoder O
and O
corresponding O
decoder O
are O
created O
, O
this O
complete O
model O
is O
trained O
on O
the O
input O
user O
dataset O
. O
The O
final O
model O
is O
the O
output O
for O
the O
user O
along O
with O
test O
summaries O
for O
any O
given O
text O
input O
. O

Experiments O

We O
evaluate O
our O
proposed O
framework O
by O
performing O
experiments O
that O
test O
the O
performance O
of O
the O
newly O
created O
models O
against O
benchmark O
summarization O
datasets O
on O
both O
extractive O
and O
abstractive O
tasks O
. O
The O
New B-DatasetName
York I-DatasetName
Times I-DatasetName
( O
NYT B-DatasetName
) O
Annotated B-DatasetName
Corpus I-DatasetName
contains O
the O
full O
text O
and O
metadata O
of O
NYT B-DatasetName
articles O
from O
1987 O
to O
2007 O
. O
Following O
( O
Durrett O
et O
al O
. O
, O
2016 O
) O
, O
we O
extracted O
and O
filtered O
out O
the O
articles O
from O
2000 O
- O
2007 O
, O
with O
abstractive O
summaries O
having O
more O
than O
50 O
words O
. O
The O
articles O
were O
split O
based O
on O
the O
date O
of O
publication O
, O
where O
the O
articles O
from O
January O
1 O
, O
2007 O
were O
chosen O
as O
test O
set O
. O

Datasets O

X B-DatasetName
- I-DatasetName
Sum I-DatasetName
( O
Narayan O
et O
al O
. O
, O
2018a O
) O
dataset O
is O
collected O
from O
online O
BBC O
articles O
, O
with O
short O
one O
sentence O
summaries O
. O
The O
Gigaword B-DatasetName
( O
Rush O
et O
al O
. O
, O
2015 O
) O
dataset O
contains O
4 O
M O
examples O
from O
news O
articles O
for O
sentence O
summarization O
/ O
headline O
generation O
task O
. O
The O
summaries O
are O
very O
short O
with O
9 O
tokens O
per O
summary O
. O
The O
Contract B-DatasetName
dataset O
( O
Manor O
and O
Li O
, O
2019 O
) O
is O
a O
dataset O
compiled O
from O
two O
websites O
dedicated O
to O
explaining O
unilateral O
contracts O
in O
plain O
English O
: O
TL O
; O
DRLegal O
1 O
and O
TOS O
; O
DR O
2 O
. O
It O
is O
a O
small O
dataset O
with O
500 O
samples O
. O

Models O

The O
generated O
models O
through O
AUTOSUMM B-TaskName
- I-TaskName
CREATE I-TaskName
for O
extractive O
and O
abstractive O
are O
CHILD B-MethodName
- I-MethodName
EXT I-MethodName
and O
CHILD B-MethodName
- I-MethodName
ABS I-MethodName
respectively O
. O
-KD O
denote O
the O
child O
model O
variations O
trained O
through O
Knowledge O
distillation O
( O
KD O
) O
. O
The O
fine O
- O
tuned O
models O
through O
AUTOSUMM B-TaskName
- I-TaskName
DISTILL I-TaskName
are O
FT B-MethodName
- I-MethodName
TINYBERT I-MethodName
- I-MethodName
EXT I-MethodName
and O
FT B-MethodName
- I-MethodName
TINYBERT I-MethodName
- I-MethodName
ABS I-MethodName
. O
We O
compare O
the O
performance O
of O
our O
models O
against O
BERT O
- O
Summ O
( O
Liu O
and O
Lapata O
, O
2019a O
) O
, O
as O
it O
had O
a O
general O
framework O
for O
extractive O
and O
abstractive O
and O
was O
shown O
to O
give O
state O
- O
of O
- O
the O
- O
art O
performances O
. O
These O
baseline O
models O
are O
FT B-MethodName
- I-MethodName
BERT I-MethodName
- I-MethodName
EXT I-MethodName
and O
FT B-MethodName
- I-MethodName
BERT I-MethodName
- I-MethodName
ABS I-MethodName
. O

Implementation O
Details O

For O
all O
our O
experiments O
, O
we O
use O
the O
existing O
splits O
if O
available O
, O
otherwise O
we O
split O
the O
data O
according O
to O
the O
statistics O
in O
Table O
1 O
and O
keep O
them O
constant O
across O
all O
the O
experiments O
. O
In O
our O
AUTOSUMM B-TaskName
- I-TaskName
CREATE I-TaskName
experiments O
, O
we O
perform O
a O
20 B-HyperparameterValue
- O
layer B-HyperparameterName
neural O
architectural O
search O
for O
encoder O
. O
The O
decoders O
are O
task O
- O
specific O
and O
predefined O
as O
explained O
in O
our O
previous O
section O
. O
We O
use O
GloVe O
word O
embeddings O
while O
providing O
the O
input O
to O
the O
generated O
model O
. O
We O
set O
the O
batch B-HyperparameterName
size I-HyperparameterName
as O
128 B-HyperparameterValue
, O
max B-HyperparameterName
input I-HyperparameterName
length I-HyperparameterName
as O
64 B-HyperparameterValue
, O
hidden B-HyperparameterName
unit I-HyperparameterName
dimension I-HyperparameterName
for O
each O
layer O
as O
32 B-HyperparameterValue
, O
dropout B-HyperparameterName
ratio I-HyperparameterName
as O
0.5 B-HyperparameterValue
and O
L2 O
regularization B-HyperparameterName
. O
We O
utilize O
Adam O
optimizer B-HyperparameterName
and O
learning B-HyperparameterName
rate I-HyperparameterName
decay I-HyperparameterName
with O
cosine B-HyperparameterName
annealing I-HyperparameterName
. O
The O
parameter O
of O
KD O
proportion O
α B-HyperparameterName
is O
varied O
in O
NAS O
module O
. O
3 O
. O
We O
also O
perform O
experiments O
with O
varying O
layer B-HyperparameterName
size I-HyperparameterName
, O
discussed O
in O
the O
later O
sections O
. O

Evaluation O
metrics O

Summarization O
quality O
is O
evaluated O
using O
F1 B-MetricName
measure O
of O
ROUGE B-MetricName
score O
( O
Lin O
, O
2004 O
) O
calculated O
between O
generated O
and O
ground O
- O
truth O
summary O
. O
4 O
We O
report O
unigram O
and O
bigram O
overlap O
( O
ROUGE-1 B-MetricName
and O
ROUGE-2 B-MetricName
) O
to O
assess O
informativeness O
and O
the O
longest O
common O
sub O
- O
sequence O
( O
ROUGE B-MetricName
- I-MetricName
L I-MetricName
) O
to O
access O
fluency O
. O
Additional O
metrics O
like O
number B-MetricName
of I-MetricName
parameters I-MetricName
, O
disk B-MetricName
- I-MetricName
space I-MetricName
and O
the O
inference B-MetricName
time I-MetricName
taken O
are O
considered O
to O
compare O
the O
computational O
efficiency O
between O
models O
. O

Results O
and O
discussion O

Extractive O
Summarization O
: O
Table O
2 O
shows O
results O
comparing O
the O
performance O
of O
our O
generated O
Table O
2 O
: O
A O
comparison O
of O
the O
generated O
models O
for O
Extractive O
summarization O
on O
CNN B-DatasetName
/ I-DatasetName
DM I-DatasetName
and O
NYT B-DatasetName
; O
FT B-MethodName
- I-MethodName
BERT I-MethodName
- I-MethodName
EXT I-MethodName
is O
used O
as O
a O
baseline O
to O
compare O
against O
models O
and O
the O
baseline O
for O
extractive O
summarization O
across O
different O
datasets O
. O
The O
ROUGE B-MetricName
scores O
show O
that O
the O
summaries O
by O
the O
auto O
- O
generated O
models O
from O
our O
proposed O
framework O
are O
close O
to O
the O
state B-MethodName
- I-MethodName
of I-MethodName
- I-MethodName
the I-MethodName
- I-MethodName
art I-MethodName
BERT I-MethodName
baseline O
. O
Whereas O
, O
our O
models O
gained O
significantly O
in O
terms O
of O
computational O
efficiency O
. O
Figure O
4 O
illustrates O
the O
samewhen O
the O
models O
trained O
on O
CNN B-DatasetName
/ I-DatasetName
DM I-DatasetName
dataset O
are O
compared O
along O
three O
aspects O
-Number B-MetricName
of I-MetricName
parameters I-MetricName
( O
in O
millions O
) O
, O
disk B-MetricName
space I-MetricName
for O
storing O
the O
model O
( O
in O
MB O
) O
and O
the O
inference O
time O
( O
in O
milliseconds O
) O
needed O
to O
generate O
the O
summary O
of O
an O
input O
sample O
. O
These O
graphs O
depict O
that O
the O
generated O
models O
with O
comparable O
performance O
to O
FT B-MethodName
- I-MethodName
BERT I-MethodName
significantly O
reduce O
the O
disk O
space O
usage O
and O
the O
number O
of O
parameters O
. O
We O
note O
that O
the O
generated O
models O
from O
AUTOSUMM B-TaskName
- I-TaskName
CREATE I-TaskName
lose O
some O
performance O
in O
terms O
of O
inference O
time O
, O
which O
is O
because O
the O
model O
architecture O
consists O
of O
RNNs O
and O
does O
not O
have O
the O
advantage O
of O
parallel O
computation O
present O
in O
BERT O
models O
. O
However O
, O
our O
FT B-MethodName
- I-MethodName
TINYBERT I-MethodName
- I-MethodName
EXT I-MethodName
model O
overcomes O
this O
and O
significantly O
reduces O
the O
inference O
time O
. O

Abstractive O
Summarization O
: O
Table O
3 O
compares O
the O
performance O
of O
our O
abstractive O
- O
summarization O
models O
on O
Gigaword O
dataset O
, O
curated O
for O
extreme O
summarization O
. O
It O
is O
to O
be O
noted O
that O
our O
proposed O
summarization O
model O
with O
Transformer O
distillation O
FT B-MethodName
- I-MethodName
TINYBERT I-MethodName
- I-MethodName
ABS I-MethodName
beats O
the O
FT B-MethodName
- I-MethodName
BERT I-MethodName
- I-MethodName
ABS I-MethodName
with O
a O
huge O
margin O
, O
across O
all O
R-1 B-MetricName
, O
R-2 B-MetricName
, O
R B-MetricName
- I-MetricName
L. I-MetricName
The O
other O
dataset O
for O
extreme O
summarization O
is O
the O
Contract B-DatasetName
dataset O
. O
Table O
4 O
shows O
the O
performance O
of O
our O
generated O
CHILD B-MethodName
- I-MethodName
ABS I-MethodName
model O
on O
contracts B-DatasetName
dataset O
. O
We O
compare O
our O
results O
against O
the O
reported O
best O
performing O
Lead O
- O
K O
scores O
by O
Cohen O
et O
al O
. O
, O
( O
2018 O
) O
. O
Note O
that O
the O
limited O
size O
of O
the O
dataset O
was O
a O
bottleneck O
to O
train O
FT B-MethodName
- I-MethodName
TINYBERT I-MethodName
- I-MethodName
ABS I-MethodName
model O
. O

The O
AUTOSUMM B-TaskName
- I-TaskName
CREATE I-TaskName
approach O
generates O
a O
new O
encoder O
architecture O
from O
scratch O
for O
a O
desired O
task O
and O
dataset O
. O

It O
is O
an O
interesting O
study O
to O
dive O
deeper O
into O
the O
distribution O
of O
the O
cells O
in O
the O
generated O
models O
. O
Table O
5 O
shows O
the O
distribution O
of O
cell O
type O
in O
the O
generated O
models O
with O
a O
10 O
layer O
encoder O
architecture O
, O
on O
extractive O
( O
on O
CNN B-DatasetName
/ I-DatasetName
DM I-DatasetName
) O
and O
abstractive O
( O
on O
Gigaword B-DatasetName
) O
tasks O
. O
It O
can O
be O
observed O
that O
the O
pooling O
and O
the O
attention O
layers O
are O
sparse O
in O
the O
extractive O
models O
, O
but O
are O
major O
contributors O
in O
the O
abstractive O
architecture O
. O
Most O
recent O
models O
use O
the O
multi O
- O
head O
attention O
from O
transformer O
to O
get O
good O
results O
in O
the O
language O
generation O
task O
. O
A O
similar O
pattern O
is O
observed O
in O
the O
models O
generated O
through O
our O
AutoSumm B-TaskName
framework O
. O

Ablation O
Study O

Variation O
in O
Layer B-HyperparameterName
Size I-HyperparameterName
: O
We O
analyze O
the O
performance O
of O
our O
framework O
across O
varying O
sizes O
of O
the O
target O
model O
i.e. O
varying O
the O
number B-HyperparameterName
of I-HyperparameterName
layers I-HyperparameterName
to O
be O
generated O
by O
the O
RNN O
- O
controller O
. O
We O
experiment O
with O
CHILD B-MethodName
- I-MethodName
EXT I-MethodName
models O
. O
Figure O
5 O
illustrates O
the O
results O
of O
this O
experiment O
for O
extractive O
summarization O
on O
the O
CNN B-DatasetName
/ I-DatasetName
DM I-DatasetName
dataset O
. O
We O
observe O
that O
the O
CNN O
and O
RNN O
layers O
are O
the O
major O
constituents O
in O
these O
architectures O
. O
We O
can O
see O
that O
RNN O
cells O
are O
more O
preferred O
when O
the O
architecture O
is O
restricted O
to O
fewer O
layers O
( O
like O
2 B-HyperparameterValue
, O
5 B-HyperparameterValue
, O
6 B-HyperparameterValue
) O
, O
but O
as O
we O
increase O
the O
layers O
, O
Convolutional O
layers O
with O
larger O
stride O
( O
7 O
) O
are O
preferred O
. O
Table O
6 O
refers O
to O
the O
performance O
of O
these O
models O
. O
While O
the O
model O
with O
15 B-HyperparameterValue
layers O
gives O
the O
best O
performance O
, O
the O
performance O
does O
not O
drop O
too O
much O
with O
varying O
number B-HyperparameterName
of I-HyperparameterName
layers I-HyperparameterName
. O

Hence O
, O
a O
smaller O
model O
, O
with O
fewer O
layers O
and O
in O
turn O
lesser O
number B-MetricName
of I-MetricName
parameters I-MetricName
can O
be O
efficiently O
generated O
for O
extractive O
summarization O
through O
our O
approach O
. O

Figure O
5 O
: O
Cell O
distribution O
across O
varying O
layer O
size O
Table O
7 O
and O
Figure O
6 O
presents O
the O
results O
when O
the O
same O
experiment O
is O
conducted O
on O
XSUM B-DatasetName
and O
Contract B-DatasetName
datasets O
with O
the O
layer B-HyperparameterName
sizes I-HyperparameterName
5 B-HyperparameterValue
, O
10 B-HyperparameterValue
, O
12 B-HyperparameterValue
and O
15 B-HyperparameterValue
. O
While O
the O
trend O
in O
RNN O
preference O
for O
fewer O
layers O
and O
CNN O
preference O
for O
more O
layers O
still O
continues O
, O
it O
is O
noted O
that O
the O
larger O
architectures O
generated O
use O
the O
pooling O
and O
attention O
layers O
. O

Cross O
- O
Dataset O
Experiments O
: O
periments O
with O
CHILD B-MethodName
- I-MethodName
EXT I-MethodName
- I-MethodName
KD I-MethodName
- I-MethodName
X I-MethodName
models O
trained O
on O
one O
dataset O
( O
X O
) O
and O
tested O
on O
another O
. O
The O
visualisations O
of O
these O
results O
as O
in O
Figure O
7 O
, O
also O
show O
that O
the O
architectures O
trained O
on O
one O
dataset O
can O
be O
used O
to O
generate O
summaries O
on O
a O
different O
dataset O
without O
significant O
loss O
in O
performance O
, O
establishing O
the O
generalizability O
of O
the O
proposed O
approach O
making O
it O
usable O
by O
non O
- O
experts O
for O
real O
- O
world O
applications O
. O
06,18.91,27.04 O
16.78,1.83,12.35 O
24.07,6.42,2.7,13.52 O
20.84,3.64,13.68 O
24.07,6.42,17.84,25.57 O
18.52,2.43,11.93 O
21.22,6.07,15.95 O
33.84,15.72,24.88 O
43.58,20.69,28.08 O
39.85,19.26,14.95,24.07 O
42.29,19.53,26.76 O
38.48,18.04,23.87 O
Figure O
8 O
illustrates O
the O
result O
of O
this O
experiment O
, O
where O
we O
vary O
the O
amount O
of O
training O
data O
( O
from O
0 O
% O
to O
100 O
% O
of O
the O
total O
available O
data O
) O
, O
used O
to O
re O
- O
train O
an O
architecture O
searched O
for O
extractive O
summarization O
on O
CNN B-DatasetName
/ I-DatasetName
DM I-DatasetName
dataset O
. O
Here O
, O
0 O
% O
data O
refers O
to O
randomly O
initialized O
model O
that O
has O
not O
been O
re O
- O
trained O
. O
Note O
that O
the O
Rouge B-MetricName
scores I-MetricName
( O
R-1 B-MetricName
, O
R-2 B-MetricName
, O
R B-MetricName
- I-MetricName
L I-MetricName
) O
reported O
for O
all O
these O
model O
variations O
are O
calculated O
on O
the O
test O
set O
. O
While O
it O
is O
intuitive O
that O
, O
more O
training O
data O
results O
in O
improved O
performance O
, O
we O
note O
that O
even O
with O
10 O
% O
data O
the O
decent O
performance O
value O
is O
achieved O
. O
We O
believe O
such O
an O
observation O
can O
help O
support O
the O
hypothesis O
that O
the O
using O
the O
proposed O
framework O
, O
we O
can O
build O
usable O
models O
with O
limited O
training O
data O
. O

Decoding O
Size O
variation O
: O
Table O
9 O
denotes O
the O
results O
of O
varying O
ROUGE B-MetricName
scores O
with O
the O
decoding O
summary O
sizes O
( O
1,3,5 O
) O
on O
CNN B-DatasetName
/ I-DatasetName
DM I-DatasetName
extractive O
summarization O
task O
. O
While O
, O
a O
summary O
size O
of O
3 O
sentences O
yields O
best O
result O
( O
some O
of O
it O
due O
to O
the O
nature O
of O
the O
training O
data O
) O
, O
we O
observe O
that O
the O
proposed O
framework O
allows O
generating O
shorter O
or O
longer O
summaries O
without O
significant O
loss O
in O
performance O
, O
again O
establishing O
the O
generalizability O
. O

Table O
10 O
shows O
qualitative O
examples O
of O
the O
output O
summaries O
for O
a O
couple O
of O
newly O
generated O
models O
. O
Through O
the O
above O
experiments O
, O
we O
establish O
that O
the O
auto O
- O
generated O
models O
generated O
using O
the O
proposed O
NAS O
and O
transformer O
- O
distillation O
based O
frameworks O
report O
near O
state O
- O
of O
- O
the O
- O
art O
performance O
for O
both O
extractive O
and O
abstractive O
summarization O
. O
We O
establish O
the O
generalizability O
of O
the O
models O
through O
various O
experiments O
, O
while O
also O
showing O
the O
efficacy O
when O
learning O
with O
limited O
data O
. O

Conclusions O

We O
present O
a O
framework O
for O
auto O
- O
generation O
of O
ML O
models O
for O
extract O
and O
generate O
tasks O
by O
leveraging O
knowledge O
distillation O
, O
NAS O
, O
and O
transformerdistillation O
techniques O
. O
The O
proposed O
approach O
successfully O
creates O
new O
model O
architectures O
that O
are O
more O
efficient O
in O
terms O
of O
inference O
time O
and O
space O
while O
achieving O
near O
state O
- O
of O
- O
the O
- O
art O
performance O
in O
terms O
of O
accuracies O
across O
datasets O
for O
extractive O
and O
abstractive O
summarization O
. O
We O
believe O
our O
work O
can O
help O
create O
the O
foundation O
towards O
democratizing O
the O
use O
of O
deep O
- O
learning O
for O
NLP O
applications O
for O
non O
- O
experts O
in O
practice O
. O

Seeking O
Common O
but O
Distinguishing O
Difference O
, O
A O
Joint O
Aspect O
- O
based O
Sentiment B-TaskName
Analysis I-TaskName
Model B-TaskName

Aspect B-TaskName
- I-TaskName
based I-TaskName
sentiment I-TaskName
analysis I-TaskName
( O
ABSA B-TaskName
) O
task O
consists O
of O
three O
typical O
subtasks O
: O
aspect O
term O
extraction O
, O
opinion O
term O
extraction O
, O
and O
sentiment O
polarity O
classification O
. O
These O
three O
subtasks O
are O
usually O
performed O
jointly O
to O
save O
resources O
and O
reduce O
the O
error O
propagation O
in O
the O
pipeline O
. O
However O
, O
most O
of O
the O
existing O
joint O
models O
only O
focus O
on O
the O
benefits O
of O
encoder O
sharing O
between O
subtasks O
but O
ignore O
the O
difference O
. O
Therefore O
, O
we O
propose O
a O
joint O
ABSA B-TaskName
model O
, O
which O
not O
only O
enjoys O
the O
benefits O
of O
encoder O
sharing O
but O
also O
focuses O
on O
the O
difference O
to O
improve O
the O
effectiveness O
of O
the O
model O
. O
In O
detail O
, O
we O
introduce O
a O
dual O
- O
encoder O
design O
, O
in O
which O
a O
pair O
encoder O
especially O
focuses O
on O
candidate O
aspect O
- O
opinion O
pair O
classification O
, O
and O
the O
original O
encoder O
keeps O
attention O
on O
sequence O
labeling O
. O
Empirical O
results O
show O
that O
our O
proposed O
model O
shows O
robustness O
and O
significantly O
outperforms O
the O
previous O
state O
- O
ofthe O
- O
art O
on O
four O
benchmark O
datasets O
. O

Introduction O

Sentiment B-TaskName
analysis I-TaskName
is O
a O
task O
that O
aims O
to O
retrieve O
the O
sentiment O
polarity O
based O
on O
three O
levels O
of O
granularities O
: O
document O
level O
, O
sentence O
level O
, O
and O
entity O
and O
aspect O
level O
( O
Liu O
, O
2012 O
) O
, O
which O
is O
under O
the O
urgent O
demands O
of O
several O
society O
scenarios O
( O
Preethi O
et O
al O
. O
, O
2017 O
; O
Cobos O
et O
al O
. O
, O
2019 O
; O
Islam O
and O
Zibran O
, O
2017 O
; O
Novielli O
et O
al O
. O
, O
2018 O
) O
. O
Recently O
, O
the O
aspect O
- O
based O
sentiment B-TaskName
analysis I-TaskName
( O
ABSA B-TaskName
) O
task O
( O
Pontiki O
et O
al O
. O
, O
2014 O
) O
, O
focusing O
on O
excavating O
the O
specific O
aspect O
from O
an O
annotated O
review O
, O
has O
aroused O
much O
attention O
from O
researchers O
, O
in O
which O
this O
paper O
mainly O
concerns O
the O
aspect O
/ O
opinion O
term O
extraction O
and O
sentiment O
classification O
task O
. O
The O
latest O
benchmark O
proposed O
by O
Peng O
et O
al O
. O
( O
2020 O
) O
formulates O
the O
relevant O
information O
into O
a O
triplet O
: O
target O
aspect O
object O
, O
opinion O
clue O
, O
and O
sentiment O
polarity O

The O
view O
is O
spectacular O
, O
and O
the O
food O
is O
great O
. O
orientation O
. O
Thus O
, O
the O
concerned O
aspect O
term O
extraction O
becomes O
a O
task O
of O
Aspect B-TaskName
Sentiment I-TaskName
Triplet I-TaskName
Extraction I-TaskName
( O
ASTE B-TaskName
) O
. O
Similarly O
, O
the O
relevant O
information O
is O
formulated O
into O
a O
pair O
with O
aspect O
term O
and O
sentiment O
polarity O
, O
and O
the O
task O
is O
defined O
as O
Aspect B-TaskName
Term I-TaskName
Extraction I-TaskName
and I-TaskName
Sentiment I-TaskName
Classification I-TaskName
( O
AESC B-TaskName
) O
. O
Figure O
1 O
shows O
an O
example O
of O
ASTE B-TaskName
and O
AESC B-TaskName
. O
Two O
early O
methods O
handle O
the O
triplet O
extraction O
task O
efficiently O
( O
Zhang O
et O
al O
. O
, O
2020a O
; O
Huang O
et O
al O
. O
, O
2021 O
) O
. O
Both O
are O
typically O
composed O
of O
a O
sequence O
representation O
layer O
to O
predict O
the O
aspect O
/ O
opinion O
term O
mentions O
and O
a O
classification O
layer O
to O
infer O
the O
sentiment O
polarity O
of O
the O
predicted O
mention O
pair O
of O
the O
last O
layer O
. O
However O
, O
as O
is O
often O
the O
case O
, O
such O
model O
design O
may O
easily O
result O
in O
that O
the O
errors O
of O
the O
upper O
prediction O
layer O
would O
hurt O
the O
accuracy O
of O
the O
lower O
layer O
during O
the O
training O
procedure O
. O

To O
tackle O
the O
error O
cascading O
phenomenon O
on O
the O
pipeline O
model O
, O
a O
growing O
trend O
of O
jointly O
modeling O
these O
subtasks O
in O
one O
shot O
appears O
. O
proposed O
a O
joint O
model O
using O
a O
sequence O
tagging O
method O
, O
based O
on O
the O
bidirectional B-MethodName
Long I-MethodName
Short I-MethodName
- I-MethodName
Term I-MethodName
Memory I-MethodName
( O
LSTM B-MethodName
) O
and O
Conditional B-MethodName
Random I-MethodName
Fields I-MethodName
( O
CRF B-MethodName
) O
. O
However O
, O
they O
found O
that O
if O
a O
tagged O
mention O
belongs O
to O
more O
than O
one O
triplet O
, O
this O
method O
will O
be O
ineffective O
. O
Zhang O
et O
al O
. O
( O
2020a O
) O
proposed O
a O
multi O
- O
task O
learning O
approach O
with O
the O
aid O
of O
dependency O
parsing O
on O
tail O
word O
pair O
of O
corresponding O
aspect O
- O
opinion O
pair O
. O
However O
, O
this O
non O
- O
strict O
dependency O
parsing O
may O
miss O
capturing O
structural O
information O
of O
term O
span O
. O
Meanwhile O
, O
the O
many O
- O
target O
to O
one O
- O
opinion O
issue O
is O
not O
effectively O
handled O
. O

The O
promising O
results O
achieved O
by O
machine O
reading O
comprehension O
( O
MRC O
) O
frameworks O
on O
solving O
many O
other O
NLP O
tasks O
( O
Li O
et O
al O
. O
, O
, O
2019a O
) O
also O
inspires O
the O
ASTE B-TaskName
task O
. O
Mao O
et O
al O
. O
( O
2021 O
) O
and O
Chen O
et O
al O
. O
( O
2021 O
) O
attempted O
to O
design O
question O
- O
answer O
pair O
in O
terms O
of O
MRC O
to O
formulate O
the O
triplet O
extraction O
. O
Nevertheless O
, O
both O
need O
to O
make O
the O
converted O
question O
correspond O
one O
- O
to O
- O
one O
to O
the O
designed O
question O
manually O
, O
hence O
increasing O
computation O
complexity O
. O

Among O
these O
joint O
models O
, O
transformed O
the O
sequence O
representation O
into O
the O
two O
- O
dimension O
space O
and O
argued O
that O
the O
word O
- O
pair O
under O
at O
least O
one O
assumption O
could O
represent O
the O
aspect O
- O
opinion O
pair O
as O
input O
of O
different O
encoders O
. O
Although O
this O
model O
indicated O
significant O
improvement O
, O
it O
treated O
the O
word O
- O
pair O
without O
taking O
span O
boundary O
of O
aspect O
term O
and O
opinion O
term O
into O
consideration O
and O
incorporated O
nonexistent O
pre O
- O
defined O
aspect O
- O
opinion O
pairs O
. O

Considering O
the O
problems O
mentioned O
above O
, O
we O
propose O
a O
dual O
- O
encoder O
model O
based O
on O
a O
pretrained O
language O
model O
by O
jointly O
fine O
- O
tuning O
multiple O
encoders O
on O
the O
ABSA B-TaskName
task O
. O
Similar O
to O
prior O
work O
, O
our O
framework O
uses O
a O
shared O
sequence O
encoder O
to O
represent O
the O
aspect O
terms O
and O
opinion O
terms O
in O
the O
same O
embedding O
space O
. O
Moreover O
, O
we O
introduce O
a O
pair O
encoder O
to O
represent O
the O
aspectopinion O
pair O
on O
the O
span O
level O
. O
Thus O
, O
our O
dualencoder O
model O
could O
learn O
from O
the O
ABSA B-TaskName
subtasks O
individually O
and O
benefit O
from O
each O
other O
in O
an O
end O
- O
to O
- O
end O
manner O
. O

Experiments O
on O
benchmark O
datasets O
show O
that O
our O
model O
significantly O
outperforms O
previous O
approaches O
at O
the O
aspect O
level O
. O
We O
also O
conduct O
a O
series O
of O
experiments O
to O
analyze O
the O
gain O
of O
additional O
representation O
from O
the O
proposed O
dualencoder O
structure O
. O

The O
contributions O
of O
our O
work O
are O
as O
follows O
: O

• O
We O
propose O
a O
jointly O
optimized O
dual O
- O
encoder O
model O
for O
ABSA B-TaskName
to O
boost O
the O
performance O
of O
ABSA B-TaskName
tasks O
. O

• O
We O
apply O
an O
attention O
mechanism O
to O
allow O
information O
transfer O
between O
words O
to O
promote O
the O
model O
to O
know O
the O
word O
pairs O
before O
inference O
. O

• O
We O
achieve O
state O
- O
of O
- O
the O
- O
art O
performance O
on O
benchmark O
datasets O
at O
the O
time O
of O
submission O
. O

Our O
Approach O

Problem O
Formulation O

In O
this O
paper O
, O
we O
split O
the O
ABSA B-TaskName
task O
into O
two O
periods O
: O
aspect B-TaskName
/ I-TaskName
opinion I-TaskName
term I-TaskName
extraction I-TaskName
and O
sentiment B-TaskName
classification I-TaskName
( O
SC B-TaskName
) O
, O
as O
shown O
in O
Figure O
1 O
. O
The O
aspect B-TaskName
/ I-TaskName
opinion I-TaskName
term I-TaskName
extraction I-TaskName
subtask O
extracts O
the O
aspect O
terms O
( O
AT O
) O
and O
opinion O
terms O
( O
OT O
) O
in O
the O
sentences O
without O
considering O
the O
sentiment O
polarities O
( O
SP O
) O
. O
Furthermore O
, O
according O
to O
the O
sentiment O
polarity O
tagging O
style O
of O
the O
dataset O
, O
the O
SC B-TaskName
subtask O
is O
divided O
into O
two O
categories O
: O
ASTE B-TaskName
, O
tagging O
SP O
on O
AT O
and O
OT O
, O
and O
AESC B-TaskName
, O
which O
tags O
SP O
only O
on O
AT O
. O

In O
particular O
, O
we O
denote O
AT O
, O
OT O
and O
SP O
as O
the O
set O
of O
predefined O
aspect O
terms O
, O
opinion O
terms O
and O
sentiment O
polarities O
, O
respectively O
, O
where O
AT O
∈ O
AT O
, O
OT O
∈ O
OT O
, O
and O
SP O
∈ O
SP O
= O
{ O
POS O
, O
NEU O
, O
NEG O
} O
. O
Given O
a O
sentence O
s O
consisting O
of O
n O
tokens O
ω O
1 O
, O
ω O
2 O
, O
... O
, O
ω O
n O
, O
we O
denote O
T O
as O
the O
sentence O
output O
of O
our O
model O
. O
Specifically O
, O
for O
the O
ASTE B-TaskName
task O
, O
T O
= O
{ O
( O
AT O
, O
OT O
, O
SP O
) O
} O
, O
and O
for O
the O
AESC B-TaskName
task O
, O
T O
= O
{ O
( O
AT O
, O
SP O
) O
} O
. O

Model O
Overview O

Our O
approach O
for O
the O
ABSA B-TaskName
task O
is O
designed O
to O
subtly O
model O
high O
affinity O
between O
aspect O
/ O
opinion O
pair O
and O
ground O
truth O
by O
effectively O
leveraging O
the O
pair O
representation O
, O
which O
adapts O
the O
same O
encoder O
design O
proposed O
by O
Wang O
and O
Lu O
( O
2020 O
) O
to O
the O
aspectbased O
sentiment O
analysis O
problem O
. O
As O
shown O
in O
Figure O
2 O
, O
our O
dual O
- O
encoder O
comprises O
two O
modules O
: O
( O
1 O
) O
a O
sequence O
encoder O
, O
with O
a O
pre O
- O
trained O
language O
model O
to O
represent O
AT O
and O
OT O
with O
the O
corresponding O
context O
; O
( O
2 O
) O
a O
pair O
encoder O
, O
encoding O
the O
aspect O
- O
opinion O
pair O
for O
each O
sentiment O
polarity O
. O

Differences O
from O
Table O
- O
sequence O
Model O
of O
Relation O
Extraction O

Our O
approach O
differs O
from O
Wang O
and O
Lu O
( O
2020 O
) O
in O
the O
AT O
/ O
OT O
extraction O
subtask O
modeling O
. O
For O
the O
sentiment O
classification O
module O
in O
this O
work O
, O
we O
still O
follow O
the O
efficient O
relation O
extraction O
module O
design O
of O
Wang O
and O
Lu O
( O
2020 O
) O
for O
tackling O
pair O
classifications O
. O
For O
the O
reason O
that O
we O
found O
there O
are O
many O
commonalities O
in O
modeling O
including O
the O
Our O
proposed O
strategy O
is O
more O
adapted O
to O
the O
characteristics O
of O
the O
ABSA B-TaskName
task O
. O
With O
this O
modified O
model O
structure O
, O
our O
model O
could O
efficiently O
infer O
the O
AT O
, O
OT O
, O
and O
their O
corresponding O
relationship O
. O

Token O
Representation O

For O
a O
length O
- O
n O
input O
sentence O
s O
= O
ω O
1 O
, O
... O
, O
ω O
n O
, O
besides O
the O
word O
- O
level O
representation O
x O
word O
, O
we O
also O
feed O
the O
characters O
of O
the O
word O
into O
the O
LSTM B-MethodName
to O
generate O
the O
character O
- O
level O
representation O
x O
char O
. O
Additionally O
, O
the O
pre O
- O
trained O
language O
model O
provides O
the O
contextualized O
representation O
x O
plm O
. O
Finally O
, O
we O
concatenate O
these O
three O
representations O
of O
each O
word O
to O
feed O
into O
the O
dual O
- O
encoder O
: O

x O
i O
= O
[ O
x O
char O
; O
x O
word O
; O
x O
plm O
] O
. O
( O
1 O
) O

In O
our O
proposed O
dual O
- O
encoder O
architecture O
, O
we O
still O
treat O
the O
ASTE B-TaskName
/ O
AESC B-TaskName
task O
as O
a O
unified O
sequence O
tagging O
task O
in O
previous O
work O
: O
for O
a O
given O
sentence O
s O
, O
where O
AT O
and O
OT O
on O
the O
main O
diagonal O
are O
annotated O
with O
B O
/ O
I O
/ O
O O
( O
Begin O
, O
Inside O
, O
Outside O
) O
, O
each O
entry O
E O
i O
, O
j O
of O
the O
upper O
triangular O
matrix O
denotes O
the O
pair O
( O
ω O
i O
, O
ω O
j O
) O
from O
the O
input O
sentence O
. O
Our O
work O
is O
partially O
motivated O
by O
but O
significantly O
different O
. O

First O
, O
we O
improve O
the O
word O
- O
level O
pair O
representation O
to O
span O
- O
level O
pair O
representation O
with O
more O
accurate O
boundary O
information O
fed O
into O
our O
model O
. O
The O
tagging O
scheme O
of O
our O
model O
is O
illustrated O
in O
Figure O
3 O
, O
in O
which O
the O
main O
diagonal O
are O
filled O
with O
AT O
and O
OT O
accompanying O
entries O
to O
the O
right O
of O
the O
main O
diagonal O
with O
span O
pairs O
. O
Compared O
to O
, O
our O
method O
may O
heavily O
reduce O
the O
redundancy O
aroused O
by O
AT O
and O
OT O
tags O
at O
the O
right O
of O
the O
main O
diagonal O
. O

Second O
, O
we O
consider O
the O
context O
information O
on O
both O
two O
- O
dimension O
spaces O
and O
the O
historical O
information O
with O
the O
utilization O
of O
the O
recurrent B-MethodName
neural I-MethodName
network I-MethodName
( O
RNN B-MethodName
) O
. O
However O
, O
merely O
adopted O
a O
single O
encoder O
which O
based O
on O
DE B-MethodName
- I-MethodName
CNN I-MethodName
( O
Xu O
et O
al O
. O
, O
2018 O
) O
/ O
BiLSTM B-MethodName
/ O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
to O
establish O
token O
representation O
, O
and O
they O
Thus O
, O
our O
dual O
- O
encoder O
could O
jointly O
encode O
AT O
, O
OT O
( O
with O
the O
corresponding O
context O
on O
both O
dimensions O
) O
, O
and O
AT O
- O
OT O
pairs O
with O
representation O
information O
sharing O
. O

Sequence O
Encoder O

Following O
the O
previous O
work O
of O
Vaswani O
et O
al O
. O
( O
2017 O
) O
, O
we O
construct O
the O
sequence O
encoder O
as O
a O
Transformer B-MethodName
network O
. O

Here O
we O
apply O
a O
stack O
of O
m O
self O
- O
attention O
layers O
, O
shown O
in O
Figure O
2 O
. O
Each O
layer O
consists O
of O
two O
sublayers O
: O
namely O
multi O
- O
head O
attention O
sublayer O
, O
feed O
- O
forward O
sublayer O
, O
at O
the O
top O
of O
each O
sublayer O
followed O
with O
both O
residual O
connection O
and O
layer O
normalization O
. O

Multi O
- O
head O
Attention O
Sublayer O

In O
this O
section O
, O
the O
token O
representation O
x O
i O
is O
fed O
into O
a O
multi O
- O
head O
attention O
sublayer O
. O

At O
first O
of O
our O
sequence O
encoder O
, O
the O
token O
representation O
x O
i O
will O
be O
mapped O
into O
vector O
space O
as O
query O
Q O
i O
, O
key O
K O
i O
, O
value O
V O
i O
: O

Q O
i O
= O
x O
i O
W O
Q O
K O
i O
= O
x O
i O
W O
K O
V O
i O
= O
x O
i O
W O
V O
( O
2 O
) O

then O
the O
value O
vectors O
of O
all O
positions O
will O
be O
aggregated O
according O
to O
the O
normalized O
attention O
weight O
to O
get O
the O
single O
- O
head O
representation O
: O

SingleHead O
( O
Q O
i O
, O
K O
i O
, O
V O
i O
) O
= O
softmax O
( O
Q O
i O
K O
T O
i O
d O
/ O
m O
) O
V O

( O
3 O
) O
where O
m B-HyperparameterName
is O
the O
number B-HyperparameterName
of I-HyperparameterName
heads I-HyperparameterName
, O
d B-HyperparameterName
is O
the O
dimension O
of O
x O
i O
, O
and O
in O
our O
sequence O
encoder O
, O

Q O
= O
K O
= O
V O
= O
x O
i O
. O

Then O
with O
multi O
- O
heads O
attention O
, O
our O
model O
builds O
up O
representations O
of O
the O
input O
sequence O
: O

r O
i O
= O
MultiHead O
( O
Q O
i O
, O
K O
i O
, O
V O
i O
) O
= O
Concat O
( O
SingleHead O
1 O
, O
.. O
, O
m O
( O
Q O
i O
, O
K O
i O
, O
V O
i O
) O
) O
W O
O O

( O
4 O
) O
where O
W O
O O
∈ O
R O
d O
. O
We O
adopt O
the O
residual O
connection O
and O
layer O
normalization O
( O
Ba O
et O
al O
. O
, O
2016 O
) O
on O
r O
i O
and O
x O
i O
: O

a O
i O
= O
LayerNorm O
( O
r O
i O
+ O
x O
i O
) O

( O
5 O
) O

Feed O
- O
Forward O
Sublayer O

The O
outputs O
of O
the O
multi O
- O
head O
attention O
are O
fed O
into O
a O
feed O
- O
forward O
network O
: O

e O
i O
= O
FFNN O
( O
r O
i O
) O
= O
( O
a O
i O
W O
1 O
+ O
b O
1 O
) O
W O
2 O
+ O
b O
2 O
( O
6 O
) O

where O

W O
1 O
, O
W O
2 O
, O
∈ O
R O
d×d O
/ O
m O
and O
b O
1 O
, O
b O
2 O
∈ O
R O
d O
. O

At O
last O
, O
the O
sequence O
representation O
will O
be O
performed O
by O
layer O
normalization O
with O
residual O
connection O
: O

S O
i O
= O
LayerNorm O
( O
e O
i O
+ O
a O
i O
) O
( O
7 O
) O

Pair O
Encoder O

As O
shown O
in O
Eq O
. O
( O
3 O
) O
, O
our O
task O
- O
specific O
pair O
representation O
is O
an O
n O
× O
n O
matrix O
of O
vectors O
, O
where O
the O
vector O
at O
row O
i O
and O
column O
j O
represents O
i O
- O
th O
and O
j O
- O
th O
word O
pair O
of O
the O
input O
sentence O
. O
For O
the O
l O
- O
th O
layer O
of O
our O
network O
, O
we O
first O
add O
a O
Multi O
- O
Layer O
Perception O
( O
MLP O
) O
layer O
with O
ReLU O
( O
Nair O
and O
Hinton O
, O
2010 O
) O
to O
contextualize O
the O
concatenation O
of O
representations O
from O
the O
sequence O
encoder O
: O

S O
l O
, O
i O
, O
j O
= O
ReLU O
( O
MLP O
( O
[ O
S O
l−1 O
, O
i O
; O
S O
l−1 O
, O
j O
] O
) O
) O
( O
8 O
) O

Then O
we O
utilize O
the O
multi B-MethodName
- I-MethodName
dimensional I-MethodName
recurrent I-MethodName
neural I-MethodName
network I-MethodName
( O
MDRNN B-MethodName
) O
( O
Graves O
et O
al O
. O
, O
2007 O
) O
and O
gated O
recurrent O
unit O
( O
GRU O
) O
( O
Cho O
et O
al O
. O
, O
2014 O
) O
to O
contextualize O
S O
l O
, O
i O
, O
j O
. O
The O
contextualized O
pair O
representation O
P O
i O
is O
computed O
iteratively O
from O
the O
hidden O
states O
of O
each O
cell O
: O
P O
l O
, O
i O
, O
j O
= O
GRU O
( O
S O
l O
, O
i O
, O
j O
, O
P O
l−1 O
, O
i O
, O
j O
, O
P O
l O
, O
i−1 O
, O
j O
, O
P O
l O
, O
i O
, O
j−1 O
) O
( O
9 O
) O
The O
pair O
encoder O
does O
not O
consider O
only O
the O
word O
pair O
at O
neighboring O
rows O
and O
columns O
but O
also O
those O
of O
the O
previous O
layer O
. O

Training O

Given O
a O
sentence O
s O
with O
pre O
- O
defined O
tags O
AT O
, O
OT O
, O
and O
SP O
∈ O
{ O
POS O
, O
NEU O
, O
NEG O
} O
, O
we O
denote O
the O
AT O
or O
OT O
tag O
of O
token O
ω O
i O
as O
a O
i O
and O
the O
SP O
tag O
between O
the O
tokens O
ω O
i O
and O
ω O
j O
as O
t O
ij O
. O
To O
predict O
the O
label O
of O
the O
posterior O
of O
the O
aspect O
/ O
opinion O
termsŷ O
, O
we O
apply O
a O
softmax O
layer O
on O
the O
sequence O
embedding O
of O
aspect O
/ O
opinion O
terms O
S O
l O
. O
Similarly O
, O
to O
obtain O
the O
distribution O
of O
sentiment O
polarity O
type O
labelv O
, O
we O
apply O
softmax O
on O
the O
pair O
representation O
of O
P O
l O
: O

P O
( O
ŷ|a O
i O
, O
s O
) O
= O
softmax O
( O
W O
term O
S O
l O
) O
( O
10 O
) O

P O
( O
v|t O
ij O
, O
s O
) O
= O
softmax O
( O
W O
pola O
P O
l O
) O
( O
11 O
) O

where O
W O
term O
and O
W O
pola O
are O
learnable O
parameters O
. O

At O
the O
training O
, O
we O
adopt O
the O
Cross B-MetricName
- I-MetricName
Entropy I-MetricName
as O
our O
loss O
function O
. O
For O
the O
gold O
aspect O
and O
opinion O
term O
a O
i O
∈ O
AT O
OT O
and O
gold O
polarity O
t O
ij O
∈ O
SP O
, O
the O
training O
losses O
are O
respectively O
: O

L B-MetricName
term I-MetricName
= O
− O
a O
i O
∈AT∪OT O
log O
( O
P O
( O
ŷ O
= O
y|a O
i O
, O
s O
) O
) O
( O
12 O
) O
L O
pola O
= O
− O
t O
ij O
∈SP O
, O
i O
= O
j O
log O
( O
P O
( O
v O
= O
v|t O
ij O
, O
s O
) O
) O
( O
13 O
) O

where O
the O
y O
and O
v O
are O
the O
gold O
annotations O
of O
corresponding O
terms O
. O

To O
jointly O
train O
the O
model O
, O
we O
utilize O
the O
summation O
of O
these O
two O
loss O
functions O
as O
our O
training O
objective O
: O

L B-MetricName
= O
L B-MetricName
term I-MetricName
+ O
L B-MetricName
pola I-MetricName
( O
14 O
) O

3 O
Experiments O

Data O

To O
make O
a O
fair O
comparison O
with O
previous O
methods O
, O
we O
adopt O
two O
versions O
of O
datasets O
for O
the O
ASTE B-TaskName
task O
: O
( O
1 O
) O
ASTE B-DatasetName
- I-DatasetName
Data I-DatasetName
- I-DatasetName
V1 I-DatasetName
, O
originally O
provided O
by O
Peng O
et O
al O
. O
( O
2020 O
) O
from O
the O
SemEval O
2014 O
Task O
4 O
( O
Pontiki O
et O
al O
. O
, O
2014 O
) O
, O
SemEval O
2015Task O
12 O
( O
Pontiki O
et O
al O
. O
, O
2015 O
and O
SemEval O
2016 O
Task O
5 O
( O
Pontiki O
et O
al O
. O
, O
2016 O
) O
, O
and O
( O
2 O
) O
ASTE B-DatasetName
- I-DatasetName
Data I-DatasetName
- I-DatasetName
V2 I-DatasetName
, O
the O
refined O
version O
annotated O
by O
, O
with O
additional O
annotation O
of O
implicitly O
overlapping O
triplets O
. O
Furthermore O
, O
the O
name O
of O
each O
dataset O
is O
composed O
of O
two O
parts O
. O
The O
former O
part O
denotes O
the O
year O
when O
the O
corresponding O
SemEval O
data O
was O
proposed O
, O
and O
the O
latter O
part O
is O
the O
domain O
name O
of O
the O
reviews O
on O
restaurant O
service O
or O
laptop O
sales O
. O
Data O
statistics O
of O
them O
is O
shown O
in O
Table O
9 O
. O

Then O
, O
for O
the O
AESC B-TaskName
task O
, O
we O
adopt O
the O
dataset O
annotated O
by O
Wang O
et O
al O
. O
( O
2017 O
) O
, O
which O
is O
composed O
of O
three O
datasets O
, O
and O
the O
statistics O
is O
shown O
in O
Table O
10 O
. O
The O
implementation O
details O
of O
our O
dual O
- O
encoder O
model O
are O
unfolded O
in O
Appendix O
A.2 O
for O
the O
sake O
of O
putting O
main O
concentration O
on O
our O
argument O
. O
Our O
model O
implementation O
generally O
follows O
the O
released O
code O
by O
Wang O
and O
Lu O
( O
2020 O
) O
1 O
, O
and O
our O
code O
will O
be O
available O
at O
https O
: O
/ O
/ O
github.com O
/ O
Betahj O
/ O
PairABSA O
. O

Results O
on O
the O
ASTE B-TaskName
Task O

Our O
model O
will O
compare O
to O
the O
following O
baselines O
on O
the O
ASTE B-TaskName
task O
, O
and O
more O
details O
about O
these O
baseline O
models O
are O
listed O
in O
Appendix O
A.3 O
. O

1 O
) O
RINANTE+ B-MethodName
( O
Peng O
et O
al O
. O
, O
2020 O
) O
. O

2 O
) O
CMLA+ B-MethodName
( O
Peng O
et O
al O
. O
, O
2020 O
) O
. O

3 O
) O
Li B-MethodName
- I-MethodName
unified I-MethodName
- I-MethodName
R I-MethodName
( O
Peng O
et O
al O
. O
, O
2020 O
) O
. O
4 O
) O
Peng B-MethodName
et I-MethodName
al I-MethodName
. I-MethodName
( O
Peng O
et O
al O
. O
, O
2020 O
) O
. O
5 O
) O
OTE B-MethodName
- I-MethodName
MTL I-MethodName
( O
Zhang O
et O
al O
. O
, O
2020a O
) O
. O
6 O
) O
JET B-MethodName
. O
7 O
) O
GTS B-MethodName
. O
8 O
) O
Huang B-MethodName
et I-MethodName
al I-MethodName
. I-MethodName
( O
Huang O
et O
al O
. O
, O
2021 O
) O
. O

The O
main O
results O
of O
all O
the O
models O
on O
the O
ASTE B-TaskName
task O
are O
shown O
in O
Table O
1 O
. O
Compared O
with O
the O
best O
baseline O
model O
( O
Huang B-MethodName
et I-MethodName
al I-MethodName
. I-MethodName
, O
2021 O
) O
, O
our O
BERTbased O
dual O
- O
encoder O
model O
achieves O
an O
improvement O
by O
1.39 B-MetricValue
, O
0.53 B-MetricValue
, O
0.68 B-MetricValue
, O
and O
2.92 B-MetricValue
absolute O
F B-MetricName
1 I-MetricName
score O
on O
benchmark O
datasets O
. O
This O
result O
signifies O
that O
our O
dual O
- O
encoder O
model O
is O
capable O
of O
capturing O
the O
difference O
between O
AT O
/ O
OT O
extraction O
subtask O
and O
SC O
subtask O
with O
the O
help O
of O
the O
additional O
pair O
encoder O
. O
Besides O
, O
our O
ALBERT O
- O
based O
model O
significantly O
outperforms O
all O
the O
other O
competitive O
methods O
on O
most O
metrics O
of O
4 O
datasets O
14Rest B-DatasetName
, O
14Lap B-DatasetName
, O
15Rest B-DatasetName
and O
16Rest B-DatasetName
except O
for O
precision O
score O
of O
15Rest B-DatasetName
. O
Most O
notably O
, O
our O
ALBERT O
- O
based O
model O
achieves O
an O
improvement O
of O
6.66 B-MetricValue
, O
4.72 B-MetricValue
, O
9.08 B-MetricValue
, O
and O
4.49 B-MetricValue
absolute O
F B-MetricName
1 I-MetricName
score O
over O
all O
the O
baseline O
models O
on O
four O
benchmark O
datasets O
, O
respectively O
. O
This O
result O
demonstrates O
the O
superiority O
of O
our O
dualencoder O
model O
. O
However O
, O
we O
notice O
that O
our O
precision O
score O
of O
15Rest B-DatasetName
is O
comparable O
to O
, O
which O
might O
be O
due O
to O
our O
model O
is O
more O
biased O
towards O
positive O
predictions O
but O
that O
the O
F1 B-MetricName
score O
still O
suggests O
it O
is O
an O
overall O
improvement O
. O

The O
similar O
phenomenon O
that O
our O
BERT O
- O
based O
Wang O
et O
al O
. O
( O
2017 O
) O
. O
Baseline O
results O
are O
directly O
retrieved O
from O
( O
Mao O
et O
al O
. O
, O
2021 O
) O
. O
The O
best O
result O
of O
each O
evaluation O
metric O
is O
bolded O
. O

dual O
- O
encoder O
model O
shows O
larger O
improvements O
in O
F1 B-MetricName
scores O
on O
14Rest B-DatasetName
( O
1.39 B-MetricValue
) O
and O
16Rest B-DatasetName
( O
2.92 B-MetricValue
) O
than O
on O
14Lap B-DatasetName
( O
0.53 B-MetricValue
) O
and O
15Rest B-DatasetName
( O
0.68 B-MetricValue
) O
verifies O
the O
explanation O
of O
on O
large O
distribution O
differences O
of O
14Rest B-DatasetName
and O
15Rest B-DatasetName
. O
Nevertheless O
, O
we O
also O
observe O
a O
different O
phenomenon O
that O
our O
ALBERT O
- O
based O
dual O
- O
encoder O
model O
achieves O
significant O
F B-MetricName
1 I-MetricName
score O
improvements O
on O
14Rest B-DatasetName
( O
6.66 B-MetricValue
) O
and O
15Rest B-DatasetName
( O
9.08 B-MetricValue
) O
, O
better O
than O
14Lap B-DatasetName
( O
4.72 B-MetricValue
) O
and O
16Rest B-DatasetName
( O
4.49 B-MetricValue
) O
, O
makes O
a O
challenge O
to O
the O
explanation O
developed O
by O
. O
From O
our O
perspective O
, O
it O
might O
be O
due O
to O
the O
different O
fitting O
degree O
between O
the O
distribution O
of O
ASTE B-DatasetName
- I-DatasetName
Data I-DatasetName
- I-DatasetName
V2 I-DatasetName
datasets O
and O
corresponding O
pre O
- O
trained O
language O
models O
. O
Additionally O
, O
we O
evaluate O
our O
model O
on O
the O
ASTE B-DatasetName
- I-DatasetName
Data I-DatasetName
- I-DatasetName
V1 I-DatasetName
datasets O
and O
then O
experimental O
results O
further O
demonstrate O
the O
effectiveness O
of O
our O
dual O
- O
encoder O
model O
. O
These O
results O
are O
shown O
in O
Table O
8 O
of O
the O
Appendix O
. O

Results O
on O
the O
AESC B-TaskName
Task O

For O
the O
AESC B-TaskName
task O
, O
our O
model O
will O
compare O
to O
the O
following O
baselines O
: O

1 O
) O
SPAN B-MethodName
- I-MethodName
BERT I-MethodName
. O

2 O
) O
IMN B-MethodName
- I-MethodName
BERT I-MethodName
. O

3 O
) O
RACL B-MethodName
- I-MethodName
BERT I-MethodName
( O
Chen O
and O
Qian O
, O
2020 O
) O
. O
4 O
) O
Mao B-MethodName
et I-MethodName
al I-MethodName
. I-MethodName
( O
Mao O
et O
al O
. O
, O
2021 O
) O
. O

To O
investigate O
whether O
the O
performance O
of O
our O
model O
on O
the O
AESC B-TaskName
task O
maintains O
the O
same O
efficiency O
as O
the O
ASTE B-TaskName
task O
, O
we O
conduct O
a O
series O
of O
experiments O
on O
AESC B-TaskName
datasets O
. O
Results O
of O
all O
the O
models O
on O
the O
AESC B-TaskName
task O
are O
shown O
in O
Table O
2 O
. O
Compared O
with O
the O
best O
baseline O
model O
of O
Mao B-MethodName
et I-MethodName
al I-MethodName
. I-MethodName
( O
2021 O
) O
, O
the O
performance O
of O
our O
model O
is O
not O
comparable O
except O
for O
the O
absolute O
F B-MetricName
1 I-MetricName
score O
on O
AE O
and O
OE O
of O
15Rest B-DatasetName
dataset O
. O
Then O
, O
to O
excavate O
the O
contribution O
of O
our O
dual O
- O
encoder O
structure O
on O
the O
AESC B-TaskName
task O
, O
we O
evaluate O
our O
model O
on O
the O
baseline O
without O
the O
pair O
encoder O
. O
From O
Table O
2 O
we O
can O
see O
that O
the O
performance O
of O
our O
dual O
- O
encoder O
model O
is O
comparable O
on O
the O
AESC B-TaskName
task O
than O
single O
- O
encoder O
structure O
. O
The O
AESC B-TaskName
task O
is O
only O
a O
simplified O
version O
of O
the O
ASTE B-TaskName
task O
without O
taking O
AT O
/ O
OT O
paring O
and O
sentiment O
polarity O
classification O
into O
consideration O
reversely O
, O
which O
is O
the O
training O
objective O
of O
our O
joint O
model O
with O
the O
help O
of O
task O
- O
specific O
structure O
design O
. O
Consequently O
, O
our O
model O
is O
incapable O
of O
functioning O
well O
in O
the O
AESC B-TaskName
task O
. O

Ablation O
Studies O

Different O
Pre O
- O
trained O
Language O
Models O

We O
conduct O
the O
experiment O
on O
the O
14Lap B-DatasetName
of O
ASTE B-DatasetName
- I-DatasetName
Data I-DatasetName
- I-DatasetName
V2 I-DatasetName
datasets O
to O
excavate O
the O
performance O
of O
three O
frequently O
utilized O
pre O
- O
trained O
language O
models O
( O
PLMs O
) O
: O
XLNet B-MethodName
, O
RoBERTa B-MethodName
( O
Liu O
et O
al O
. O
, O
2019 O
) O
and O
ALBERT B-MethodName
( O
Lan O
et O
al O
. O
, O
2020 O
) O
. O

Table O
3 O
shows O
that O
ALBERT B-MethodName
helps O
achieve O
the O
best O
result O
among O
these O
four O
PLMs O
. O
However O
, O
even O
with O
BERT B-MethodName
as O
the O
baseline O
model O
Huang O
et O
al O
. O
, O
2021 O
) O
, O
our O
model O
also O
performs O
better O
. O
We O
also O
notice O
that O
, O
different O
from O
most O
models O
, O
our O
model O
is O
sensitive O
to O
different O
PLMs O
. O
Specifically O
, O
the O
absolute O
F B-MetricName
1 I-MetricName
score O
between O
BERT B-MethodName
and O
RoBERTa B-MethodName
, O
ALBERT B-MethodName
is O
1.04 B-MetricValue
and O
4.19 B-MetricValue
, O
respectively O
. O
It O
demonstrates O
that O
our O
model O
performance O
could O
effectively O
be O
boosted O
by O
our O
choice O
of O
PLM O
, O
and O
thus O
we O
choose O
ALBERT B-MethodName
as O
our O
base O
encoder O
. O

Dual O
- O
encoder O
Structure O

Therefore O
, O
the O
joint O
modeling O
method O
must O
take O
not O
only O
the O
fitting O
degree O
between O
individual O
modules O
and O
subtasks O
but O
also O
the O
difference O
of O
each O
module O
into O
consideration O
. O

Number B-HyperparameterName
of I-HyperparameterName
Encoder I-HyperparameterName
Layers I-HyperparameterName

The O
results O
with O
different O
numbers B-HyperparameterName
of I-HyperparameterName
encoder I-HyperparameterName
layers I-HyperparameterName
are O
in O
Figure O
4 O
. O
Generally O
, O
the O
performance O
of O
triplet O
extraction O
synchronously O
increases O
with O
the O
number B-HyperparameterName
of I-HyperparameterName
encoder I-HyperparameterName
layers I-HyperparameterName
of O
both O
dataset O
distributions O
. O
Nevertheless O
, O
when O
the O
number B-HyperparameterName
of I-HyperparameterName
encoder I-HyperparameterName
layers I-HyperparameterName
exceeds O
3 B-HyperparameterValue
, O
the O
performance O
shows O
a O
continuous O
decreasing O
trend O
, O
except O
that O
on O
16Rest B-DatasetName
when O
the O
number B-HyperparameterName
of I-HyperparameterName
encoder I-HyperparameterName
layers I-HyperparameterName
is O
increased O
to O
7 B-HyperparameterValue
, O
the O
performance O
increases O
by O
nearly O
2.5 B-MetricValue
absolute O
F B-MetricName
1 I-MetricName
score O
. O
Despite O
this O
inconsistent O
phenomenon O
, O
to O
mainly O
consider O
computational O
/ O
time O
complexities O
, O
we O
adopt O
3 B-HyperparameterValue
as O
the O
number B-HyperparameterName
of I-HyperparameterName
encoders I-HyperparameterName
. O

The O
Impact O
of O
The O
Number B-HyperparameterName
of I-HyperparameterName
GRU I-HyperparameterName

Table O
5 O
shows O
the O
results O
with O
different O
settings O
of O
multi O
- O
dimensional O
recurrent O
neural O
networks O
. O
The O
Uni O
- O
directional O
denotes O
the O
hidden O
state O
from O
forward O
GRU O
results O
in O
one O
quadrant O
of O
same O
dimension O
space O
, O
the O
Bi O
- O
directional O
denotes O
the O
hidden O
state O
from O
forward O
and O
backward O
GRU O
results O
in O
two O
quadrants O
of O
same O
dimension O
space O
, O
and O
Quaddirectional O
denotes O
the O
hidden O
state O
from O
forward O
and O
backward O
GRU O
results O
in O
four O
quadrants O
of O
same O
dimension O
space O
. O
We O
observe O
that O
the O
Quaddirectional O
setting O
significantly O
outperforms O
the O
other O
two O
settings O
. O
It O
is O
also O
noteworthy O
that O
the O
performance O
gap O
between O
Bi O
- O
directional O
and O
Unidirectional O
dimensions O
is O
much O
lower O
than O
the O
gap O
between O
Quad O
- O
directional O
and O
Bi O
- O
directional O
dimensions O
, O
which O
might O
be O
the O
reason O
why O
most O
previous O
work O
using O
bidirectional O
modelings O
can O
not O
perform O
well O
. O
Thus O
, O
we O
choose O
Quad O
- O
directional O
as O
the O
dimensional O
setting O
of O
our O
multi O
- O
dimensional O
RNNs O
. O

The O
Effect O
of O
Character O
- O
level O
Representation O

To O
investigate O
the O
contribution O
of O
character O
- O
level O
representation O
to O
our O
input O
sequence O
, O
we O
remove O
the O
character O
- O
level O
representation O
generated O
by O
LSTM O
. O
Experimental O
result O
shows O
that O
the O
performance O
decreases O
by O
0.44 B-MetricValue
absolute O
F B-MetricName
1 I-MetricName
score O
. O

Case O
Study O

To O
investigate O
why O
our O
model O
far O
exceeds O
the O
baseline O
models O
, O
we O
conduct O
a O
case O
study O
of O
three O
typical O
cases O
from O
14Lap B-DatasetName
test O
dataset O
of O
ASTE B-DatasetName
- I-DatasetName
Data I-DatasetName
- I-DatasetName
V2 I-DatasetName
, O
as O
shown O
in O
Table O
6 O
. O

From O
Example-1 O
, O
we O
observe O
that O
our O
model O
is O
able O
to O
handle O
the O
one O
- O
to O
- O
one O
case O
. O
However O
, O
our O
dual O
- O
encoder O
structure O
is O
more O
biased O
towards O
coordinative O
relation O
between O
colors O
and O
speedy O
. O
More O
cases O
we O
investigated O
further O
demonstrating O
that O
our O
model O
performs O
slightly O
worse O
on O
one O
- O
toone O
than O
one O
- O
to O
- O
many O
and O
many O
- O
to O
- O
many O
relation O
types O
. O
From O
Example-2 O
, O
we O
see O
that O
our O
model O
can O
tackle O
the O
one O
- O
opinion O
to O
many O
- O
target O
problem O
. O
However O
, O
most O
previous O
works O
are O
even O
unable O
to O
tackle O
one O
- O
opinion O
to O
two O
- O
target O
. O
From O
Example-3 O
, O
we O
observe O
that O
our O
model O
is O
capable O
of O
well O
handling O
the O
one O
- O
target O
to O
many O
- O
opinion O
problem O
, O
which O
is O
neglected O
by O
most O
of O
the O
existing O
work O
but O
important O
for O
triplet O
extraction O
. O
Because O
many O
sentences O
compose O
conflicting O
sentiments O
on O
target O
, O
the O
model O
will O
fail O
to O
recognize O
the O
opposite O
polarity O
of O
the O
same O
AT O
when O
the O
incorrect O
AT O
extraction O
happens O
. O
Finally O
, O
we O
also O
observe O
that O
our O
model O
accurately O
inferences O
the O
boundary O
of O
OSX O
Lion O
span O
, O
which O
demonstrates O
the O
usefulness O
of O
our O
transformation O
that O
utilizes O
span O
to O
replace O
the O
word O
. O
From O
Example-4 O
, O
we O
notice O
that O
our O
model O
could O
efficiently O
handle O
the O
complex O
situation O
of O
many O
- O
opinion O
to O
many O
- O
target O
with O
long O
- O
range O
dependency O
, O
which O
was O
particularly O
paid O
attention O
to O
but O
not O
solved O
well O
by O
Zhang B-MethodName
et I-MethodName
al I-MethodName
. I-MethodName
( O
2020a O
) O
. O
It O
is O
due O
to O
incorporating O
the O
self O
- O
attention O
mechanism O
and O
GRU O
in O
two O
dimensions O
, O
and O
our O
model O
is O
sensitive O
to O
the O
difference O
between O
the O
proposed O
dual O
- O
encoder O
architecture O
. O
Collectively O
, O
these O
aforementioned O
cases O
demonstrate O
the O
robustness O
of O
our O
dual O
- O
encoder O
model O
. O

Related O
Work O

Recently O
, O
NLP O
has O
been O
developed O
rapidly O
Li O
et O
al O
. O
, O
2019b O
; O
Jiang O
et O
al O
. O
, O
2020 O
; O
, O
and O
the O
process O
is O
further O
by O
deep O
neural O
networks O
( O
Parnow O
et O
al O
. O
, O
2021 O
; O
Li O
et O
al O
. O
, O
2021a O
) O
and O
pre O
- O
trained O
language O
models O
( O
Li O
et O
al O
. O
, O
2021b O
; O
Zhang O
et O
al O
. O
, O
2020b O
) O
. O
Aspect O
- O
based O
sentiment O
analysis O
was O
proposed O
by O
Pontiki O
et O
al O
. O
( O
2014 O
) O
and O
also O
received O
lots O
of O
attention O
in O
recent O
years O
. O

ASTE B-TaskName
Task O

The O
ASTE B-TaskName
task O
aims O
to O
make O
triplet O
extraction O
of O
aspect O
terms O
, O
opinion O
terms O
, O
and O
sentiment O
polarity O
, O
which O
was O
introduced O
by O
Peng O
et O
al O
. O
( O
2020 O
) O
. O
In O
their O
work O
, O
they O
leveraged O
the O
sequence O
labeling O
method O
to O
extract O
aspect O
terms O
and O
target O
sentiment O
and O
utilized O
graph O
neural O
networks O
to O
detect O
candidate O
opinion O
terms O
. O
Zhang O
et O
al O
. O
( O
2020a O
) O
proposed O
a O
multi O
- O
task O
framework O
that O
decomposes O
the O
original O
ASTE B-TaskName
task O
into O
two O
subtasks O
, O
sequence O
tagging O
of O
AT O
/ O
OT O
, O
and O
word O
pair O
dependency O
parsing O
. O
For O
joint O
learning O
, O
proposed O
a O
sequence O
tagging O
framework O
based O
on O
LSTM O
- O
CRF O
. O
constructed O
an O
encoder O
- O
decoder O
model O
to O
handle O
this O
task O
with O
grid O
representation O
of O
aspectopinion O
pairs O
. O
Then O
with O
the O
incorporation O
of O
a O
more O
specific O
semantic O
information O
guide O
for O
the O
proposed O
model O
, O
the O
ASTE B-TaskName
is O
transformed O
as O
MRC O
task O
( O
Chen O
et O
al O
. O
, O
2021 O
; O
Mao O
et O
al O
. O
, O
2021 O
) O
. O
Recently O
, O
Huang O
et O
al O
. O
( O
2021 O
) O
proposed O
a O
sequence O
taggingbased O
model O
to O
perform O
representation O
learning O
on O
the O
ASTE B-TaskName
task O
. O

AESC B-TaskName
Task O

The O
AESC B-TaskName
task O
is O
to O
perform O
aspect O
terms O
extraction O
and O
sentiment O
classification O
simultaneously O
. O
and O
used O
a O
span O
- O
level O
sequence O
tagging O
method O
to O
tackle O
huge O
search O
space O
and O
sentiment O
inconsistency O
problems O
. O
Although O
the O
huge O
search O
space O
issue O
has O
been O
solved O
by O
, O
there O
still O
exists O
a O
lowperformance O
problem O
. O
Addressing O
this O
issue O
, O
Lin O
and O
Yang O
( O
2020 O
) O
utilized O
a O
BERT B-MethodName
encoder O
to O
contextualize O
shared O
information O
of O
target O
extraction O
and O
target O
classification O
subtasks O
. O
Meanwhile O
, O
they O
used O
two O
BiLSTM O
networks O
to O
encode O
the O
private O
information O
of O
each O
subtask O
, O
which O
greatly O
boosted O
the O
model O
performance O
. O

Dual O
- O
encoder O
Structure O

Productive O
efforts O
were O
put O
into O
the O
research O
of O
dual O
- O
encoder O
structure O
for O
natural O
language O
processing O
tasks O
in O
the O
last O
few O
years O
because O
of O
the O
Example-1 O
Also O
stunning O
colors O
and O
speedy O
. O
Table O
6 O
: O
Case O
study O
of O
our O
proposed O
model O
, O
where O
AT O
/ O
OT O
denote O
aspect O
term O
/ O
opinion O
term O
, O
POS O
denotes O
sensitive O
polarity O
of O
positive O
, O
the O
subscript O
of O
sensitive O
polarity O
h O
1 O
/ O
t O
1 O
denotes O
the O
head O
/ O
tail O
term O
of O
the O
1st O
pair O
in O
terms O
of O
corresponding O
sentiment O
, O
etc O
. O

natural O
ability O
to O
model O
representational O
similarity O
maximization O
associated O
tasks O
( O
Chidambaram O
et O
al O
. O
, O
2019 O
; O
Yu O
et O
al O
. O
, O
2020 O
; O
Bhowmik O
et O
al O
. O
, O
2021 O
) O
. O
Generally O
, O
these O
approaches O
encoded O
a O
single O
component O
of O
the O
corresponding O
task O
separately O
for O
the O
processing O
in O
the O
next O
phase O
. O
Recently O
, O
Wang O
and O
Lu O
( O
2020 O
) O
proposed O
a O
sequence O
- O
table O
representation O
learning O
architecture O
for O
a O
typical O
triplet O
extraction O
task O
: O
relation O
extraction O
, O
and O
this O
work O
established O
an O
example O
of O
tackling O
the O
triplet O
extraction O
task O
with O
the O
dual O
- O
encoder O
based O
architecture O
. O

Conclusion O

In O
this O
paper O
, O
we O
observe O
the O
significant O
differences O
between O
the O
AT O
/ O
OT O
extraction O
subtask O
and O
the O
SC O
subtask O
of O
ABSA B-TaskName
for O
the O
joint O
model O
. O
Specifically O
, O
the O
results O
on O
8 O
benchmark O
datasets O
with O
significant O
improvement O
over O
state O
- O
of O
- O
the O
- O
art O
baselines O
verify O
the O
effectiveness O
of O
our O
proposed O
model O
. O
Furthermore O
, O
to O
distinguish O
such O
differences O
and O
keep O
the O
shared O
part O
between O
different O
modules O
simultaneously O
, O
we O
construct O
a O
dual O
- O
encoder O
framework O
with O
representation O
learning O
and O
self O
- O
attention O
mechanism O
. O
In O
addition O
to O
the O
encoder O
- O
sharing O
approach O
, O
our O
dual O
- O
encoder O
framework O
can O
capture O
the O
difference O
between O
the O
subtasks O
by O
interconnecting O
encoders O
at O
each O
layer O
to O
share O
the O
critical O
information O
. O

Acknowledgement O

We O
appreciate O
Wang O
and O
Lu O
for O
their O
provided O
open O
resource O
. O
Based O
on O
this O
, O
we O
conducted O
our O
work O
on O
ABSA B-TaskName
. O
We O
also O
appreciate O
the O
help O
from O
the O
reviewers O
and O
program O
chairs O
. O

A O
Additional O
Results O

A.1 O
Evaluation O
Metric O

We O
adopt O
F B-MetricName
1 I-MetricName
score O
as O
our O
evaluation O
metric O
as O
other O
baseline O
models O
. O
In O
precise O
, O
we O
measure O
the O
F1 O
score O
calculated O
between O
the O
final O
exact O
match O
of O
AT O
/ O
OT O
span O
, O
AT O
/ O
OT O
types O
and O
corresponding O
polarity O
predictions O
and O
gold O
triplets O
. O

A.2 O
Implementation O
Details O

For O
the O
token O
representation O
, O
we O
utilize O
100dimensional O
GloVe O
( O
Pennington O
et O
al O
. O
, O
2014 O
) O

A.3 O
Baselines O

Our O
model O
will O
compare O
to O
the O
following O
baselines O
on O
the O
ASTE B-TaskName
task O
. O
1 O
) O
RINANTE+ B-MethodName
( O
Peng O
et O
al O
. O
, O
2020 O
) O
. O
The O
model O
RINANTE B-MethodName
is O
modified O
from O
that O
by O
Ma O
et O
al O
. O
( O
2018 O
) O
. O
RINANTE+ B-MethodName
is O
an O
LSTM O
- O
CRF O
model O
which O
first O
uses O
dependency O
relations O
of O
words O
to O
extract O
opinion O
and O
aspects O
with O
the O
sentiment O
. O
Then O
, O
all O
the O
candidate O
aspect O
- O
opinion O
pairs O
with O
position O
embedding O
are O
fed O
into O
the O
Bi O
- O
LSTM O
encoder O
to O
make O
a O
final O
classification O
. O

2 O
) O
CMLA+ B-MethodName
( O
Peng O
et O
al O
. O
, O
2020 O
) O
. O
The O
model O
is O
adjusted O
from O
the O
one O
by O
Wang O
et O
al O
. O
( O
2017 O
) O
, O
which O
is O
an O
attention O
- O
based O
model O
, O
following O
the O
same O
two O
- O
stage O
processing O
with O
dependency O
relations O
as O
RINANTE+ B-MethodName
. O

3 O
) O
Li B-MethodName
- I-MethodName
unified I-MethodName
- I-MethodName
R I-MethodName
( O
Peng O
et O
al O
. O
, O
2020 O
) O
. O
Li B-MethodName
- I-MethodName
unified I-MethodName
- I-MethodName
R I-MethodName
utilizes O
a O
modulated O
multi O
- O
layer O
LSTM O
encoder O
by O
Li O
and O
Lu O
( O
2019 O
) O
, O
and O
adopts O
the O
same O
aspectopinion O
pair O
classification O
as O
RINANTE+ B-MethodName
. O

4 O
) O
Peng B-MethodName
et I-MethodName
al I-MethodName
. I-MethodName
( O
Peng O
et O
al O
. O
, O
2020 O
) O
. O
This O
model O
adopts O
GCN O
to O
capture O
dependency O
information O
, O
and O
at O
the O
second O
stage O
, O
uses O
the O
same O
strategy O
of O
RINANTE+ B-MethodName
to O
fulfill O
triplet O
extraction O
. O

5 O
) O
OTE B-MethodName
- I-MethodName
MTL I-MethodName
( O
Zhang O
et O
al O
. O
, O
2020a O
) O
. O
A O
multitask O
learning O
approach O
that O
incorporates O
word O
dependency O
parsing O
boosts O
the O
performance O
of O
triplet O
extraction O
. O

6 O
) O
JET B-MethodName
. O
This O
model O
jointly O
extracts O
all O
the O
subtasks O
through O
a O
unified O
sequence O
labeling O
method O
. O
JET B-MethodName
t I-MethodName
and O
JET B-MethodName
o O
denote O
two O
different O
tagging O
forms O
. O
7 O
) O
GTS B-MethodName
. O
A O
sequence O
tagging O
model O
leverages O
the O
property O
element O
upper O
triangular O
matrix O
to O
model O
the O
extraction O
of O
aspect O
and O
opinion O
terms O
. O

8 O
) O
Huang B-MethodName
et I-MethodName
al I-MethodName
. I-MethodName
( O
Huang O
et O
al O
. O
, O
2021 O
) O
. O
The O
latest O
sequence O
labeling O
model O
which O
utilizes O
the O
restricted O
attention O
field O
mechanism O
and O
represents O
word O
- O
word O
perceivable O
pairs O
for O
the O
final O
classification O
. O

For O
the O
AESC B-TaskName
task O
, O
our O
model O
will O
compare O
to O
the O
following O
baselines O
: O

1 O
) O
SPAN B-MethodName
- I-MethodName
BERT I-MethodName
. O
It O
is O
a O
BERTbased O
model O
which O
utilizes O
span O
representation O
to O
perform O
the O
AESC B-TaskName
task O
. O

2 O
) O
IMN B-MethodName
- I-MethodName
BERT I-MethodName
. O
It O
is O
a O
multi O
task O
learning O
model O
modified O
by O
and O
utilizes O
BERT O
as O
encoder O
to O
perform O
aspect O
term O
extraction O
and O
sentiment O
classification O
. O

3 O
) O
RACL B-MethodName
- I-MethodName
BERT I-MethodName
( O
Chen O
and O
Qian O
, O
2020 O
) O
. O
It O
is O
a O
multi O
- O
layer O
multi O
- O
task O
learning O
model O
with O
mutual O
information O
propagation O
to O
boost O
the O
performance O
of O
the O
AESC B-TaskName
task O
. O

4 O
) O
Mao B-MethodName
et I-MethodName
al I-MethodName
. I-MethodName
( O
Mao O
et O
al O
. O
, O
2021 O
) O
. O
It O
is O
a O
dual O
- O
MRC O
architecture O
model O
to O
detect O
the O
AT O
/ O
OT O
and O
corresponding O
sentiment O
polarity O
by O
means O
of O
a O
two O
- O
round O
query O
answering O
approach O
. O

A.4 O
Results O
on O
ASTE B-DatasetName
- I-DatasetName
Data I-DatasetName
- I-DatasetName
V1 I-DatasetName
for O
ASTE B-TaskName

Results O
on O
the O
ASTE B-DatasetName
- I-DatasetName
Data I-DatasetName
- I-DatasetName
V1 I-DatasetName
datasets O
also O
show O
the O
effectiveness O
of O
our O
model O
. O
But O
there O
is O
an O
interesting O
phenomenon O
that O
on O
the O
16Rest B-DatasetName
test O
set O
, O
the O
result O
of O
ALBERT B-MethodName
- O
based O
model O
is O
lower O
than O
that O
of O
BERT O
- O
based O
model O
. O
It O
may O
be O
due O
to O
the O
inconsistent O
domain O
between O
the O
test O
set O
and O
the O
pre O
- O
trained O
language O
model O
. O

A.5 O
Data O
Statistics O

Table O
9 O
and O
Table O
10 O
show O
the O
statistics O
of O
the O
datasets O
we O
used O
. O

On O
Vision O
Features O
in O
Multimodal B-TaskName
Machine I-TaskName
Translation I-TaskName

Previous O
work O
on O
multimodal B-TaskName
machine I-TaskName
translation I-TaskName
( O
MMT B-TaskName
) O
has O
focused O
on O
the O
way O
of O
incorporating O
vision O
features O
into O
translation O
but O
little O
attention O
is O
on O
the O
quality O
of O
vision O
models O
. O
In O
this O
work O
, O
we O
investigate O
the O
impact O
of O
vision O
models O
on O
MMT B-TaskName
. O
Given O
the O
fact O
that O
Transformer B-MethodName
is O
becoming O
popular O
in O
computer O
vision O
, O
we O
experiment O
with O
various O
strong O
models O
( O
such O
as O
Vision B-MethodName
Transformer I-MethodName
) O
and O
enhanced O
features O
( O
such O
as O
object O
- O
detection O
and O
image O
captioning O
) O
. O
We O
develop O
a O
selective O
attention O
model O
to O
study O
the O
patch O
- O
level O
contribution O
of O
an O
image O
in O
MMT B-TaskName
. O
On O
detailed O
probing O
tasks O
, O
we O
find O
that O
stronger O
vision O
models O
are O
helpful O
for O
learning O
translation O
from O
the O
visual O
modality O
. O
Our O
results O
also O
suggest O
the O
need O
of O
carefully O
examining O
MMT B-TaskName
models O
, O
especially O
when O
current O
benchmarks O
are O
small O
- O
scale O
and O
biased O
. O
Our O
code O
could O
be O
found O
at O
https O
: O

Introduction O

Multimodal B-TaskName
machine I-TaskName
translation I-TaskName
( O
MMT B-TaskName
) O
has O
emerged O
as O
an O
active O
field O
of O
research O
which O
marries O
the O
worlds O
of O
computer O
vision O
( O
CV O
) O
and O
natural O
language O
processing O
( O
NLP O
) O
. O
Early O
models O
of O
this O
kind O
produce O
a O
translation O
given O
the O
fused O
representation O
of O
both O
the O
visual O
and O
textual O
inputs O
( O
Caglayan O
et O
al O
. O
, O
2016 O
; O
Libovický O
and O
Helcl O
, O
2017 O
; O
Calixto O
and O
Liu O
, O
2017 O
) O
. O
As O
expected O
, O
such O
a O
paradigm O
achieves O
promising O
BLEU B-MetricName
improvements O
and O
inspires O
the O
community O
to O
follow O
up O
. O

But O
soon O
researchers O
found O
that O
MMT B-TaskName
systems O
did O
not O
act O
as O
what O
they O
ordinarily O
designed O
: O
the O
visual O
modality O
contributes O
to O
translation O
little O
. O
For O
example O
, O
it O
is O
not O
harmful O
to O
MMT B-TaskName
systems O
when O
the O
input O
image O
is O
irrelevant O
to O
the O
text O
( O
Grönroos O
et O
al O
. O
, O
2018 O
; O
Lala O
et O
al O
. O
, O
2018 O
) O
, O
or O
even O
when O
the O
vision O
features O
are O
absent O
( O
Elliott O
, O
2018 O
) O
. O
More O
recently O
, O
Wu O
et O
al O
. O
( O
2021 O
) O
have O
pointed O
out O
that O
the O
* O
Corresponding O
author O
. O
use O
of O
the O
visual O
modality O
is O
a O
way O
of O
regularization O
for O
training O
but O
not O
a O
complement O
to O
the O
text O
modality O
. O
As O
another O
response O
to O
the O
analysis O
of O
MMT B-TaskName
, O
Caglayan O
et O
al O
. O
( O
2019 O
) O
investigate O
how O
the O
vision O
features O
correlate O
to O
the O
text O
. O
They O
find O
that O
the O
input O
image O
helps O
translation O
when O
some O
of O
the O
input O
words O
are O
masked O
. O

Note O
that O
previous O
work O
has O
for O
the O
most O
part O
focused O
on O
integrating O
off O
- O
the O
- O
shelf O
vision O
models O
( O
such O
as O
ResNet-50 O
) O
into O
MMT B-TaskName
. O
The O
underlying O
assumption O
here O
is O
that O
the O
existing O
vision O
models O
are O
powerful O
enough O
to O
encode O
the O
image O
. O
This O
implicitly O
ignores O
the O
quality O
of O
vision O
models O
in O
representing O
images O
. O
But O
computer O
vision O
is O
facing O
a O
new O
trend O
by O
moving O
from O
CNNs O
to O
Transformer B-MethodName
as O
the O
backbone O
model O
( O
Dosovitskiy O
et O
al O
. O
, O
2021 O
; O
Liu O
et O
al O
. O
, O
2021b O
; O
Carion O
et O
al O
. O
, O
2020 O
) O
. O
A O
natural O
question O
that O
arises O
is O
: O
how O
will O
MMT B-TaskName
systems O
behave O
if O
stronger O
vision O
models O
are O
adopted O
? O

In O
this O
work O
, O
we O
address O
this O
question O
by O
a O
systematic O
study O
of O
using O
various O
vision O
models O
in O
MMT B-TaskName
, O
in O
particular O
using O
the O
most O
successful O
models O
in O
recent O
studies O
( O
such O
as O
Vision B-MethodName
Transformer I-MethodName
, O
or O
ViT B-MethodName
for O
short O
) O
. O
We O
find O
that O
the O
patch O
method O
used O
in O
Transformer B-MethodName
- O
based O
vision O
models O
offers O
an O
opportunity O
to O
detail O
the O
patch O
- O
level O
contribution O
of O
the O
image O
. O
This O
leads O
us O
to O
develop O
a O
selective O
attention O
model O
to O
correlate O
words O
with O
image O
patches O
. O
Beyond O
this O
, O
we O
introduce O
object O
- O
detection O
and O
image O
captioning O
features O
into O
MMT B-TaskName
for O
further O
improvements O
of O
the O
vision O
models O
( O
Carion O
et O
al O
. O
, O
2020 O
; O
Fang O
et O
al O
. O
, O
2021 O
) O
. O

Following O
Caglayan O
et O
al O
. O
( O
2019 O
) O
's O
work O
, O
we O
design O
more O
detailed O
probing O
tasks O
to O
examine O
to O
what O
degree O
the O
visual O
modality O
contributes O
to O
MMT B-TaskName
. O
We O
run O
an O
extensive O
set O
of O
experiments O
on O
En B-TaskName
- I-TaskName
De I-TaskName
and O
En B-TaskName
- I-TaskName
Fr I-TaskName
MMT B-TaskName
tasks O
. O
Our O
findings O
are O

• O
Stronger O
vision O
models O
help O
. O
For O
example O
, O
ViT B-MethodName
can O
beat O
ResNet-50 B-MethodName
on O
the O
probing O
tasks O
though O
the O
superiority O
is O
not O
significant O
on O
standard O
MMT B-TaskName
data O
. O
Table O
1 O
: O
An O
example O
of O
the O
proposed O
probing O
tasks O
. O
We O
replace O
the O
masked O
token O
by O
four O
symbols O
respectively O
. O

• O
Automatic O
evaluation O
on O
current O
MMT B-TaskName
tasks O
might O
not O
be O
a O
good O
indicator O
for O
the O
effectiveness O
of O
MMT B-TaskName
models O
. O
For O
example O
, O
models O
enhanced O
with O
object O
- O
detection O
and O
image O
captioning O
features O
yield O
good O
BLEU B-MetricName
scores O
on O
the O
original O
MMT B-TaskName
task O
but O
show O
modest O
or O
no O
contributions O
on O
the O
probing O
tasks O
. O

We O
hope O
that O
the O
results O
here O
can O
inspire O
more O
research O
on O
exploring O
better O
vision O
models O
and O
evaluation O
methods O
for O
multimodal O
NLP O
. O

Preliminary O

We O
start O
with O
a O
description O
of O
the O
probing O
tasks O
. O
It O
is O
followed O
by O
a O
design O
of O
vision O
features O
and O
a O
selective O
attention O
mechanism O
for O
introducing O
ViT B-MethodName
- O
like O
representations O
into O
MMT B-TaskName
. O

Insufficient O
Text O
Generation O

To O
know O
how O
an O
image O
contributes O
to O
translation O
, O
a O
way O
is O
to O
mask O
some O
of O
the O
input O
words O
( O
call O
this O
insufficient O
text O
) O
and O
force O
the O
translation O
model O
to O
learn O
from O
the O
image O
. O
Following O
the O
previous O
design O
of O
color O
deprivation O
and O
entity O
- O
based O
masking O
, O
we O
present O
detailed O
probing O
tasks O
which O
are O
complementary O
to O
Caglayan O
et O
al O
. O
( O
2019 O
) O
's O
work O
. O
In O
preliminary O
experiments O
1 O
, O
we O
find O
that O
" O
color O
" O
, O
" O
character O
" O
and O
" O
noun O
" O
are O
three O
kinds O
of O
words O
which O
could O
be O
complemented O
according O
to O
the O
visual O
modality O
once O
the O
corresponding O
texts O
are O
masked O
. O
The O
following O
probing O
tasks O
are O
designed O
accordingly O
. O

Color O
- O
based O
Probing O

In O
training O
, O
all O
source O
words O
referring O
to O
a O
color O
are O
replaced O
by O
a O
special O
token O
[ O
Mask_C O
] O
. O
There O
are O
8 O
, O
919 O
sentences O
involving O
color O
words O
, O
and O
nearly O
one O
third O
of O
them O
involve O
more O
than O
one O
color O
. O
It O
is O
worth O
noting O
that O
each O
color O
may O
have O
two O
or O
more O
translations O
due O
to O
the O
rich O
morphology O
in O
German O
and O
French O
. O
For O
example O
, O
the O
English O
" O
green O
" O
can O
be O
translated O
to O
" O
grün O
" O
, O
" O
grüne O
" O
, O
" O
grünes O
" O
, O
" O
grüner O
" O
, O
" O
grünen O
" O
and O
" O
grünem O
" O
in O
German O
. O
We O
design O
two O
criteria O
to O
measure O
the O
accuracy O
of O
translation O
. O
The O
first O
criterion O
is O
strict O
. O
The O
correct O
translation O
requires O
generating O
the O
same O
color O
and O
the O
same O
gender O
as O
in O
reference O
translations O
. O
The O
second O
criterion O
is O
relaxed O
and O
all O
translations O
expressing O
the O
same O
color O
are O
correct O
. O

Character O
- O
based O
Probing O
For O
character O
words O
, O
we O
choose O
" O
man O
" O
, O
" O
woman O
" O
, O
" O
people O
" O
, O
" O
men O
" O
, O
" O
girl O
" O
and O
" O
boy O
" O
. O
More O
than O
60 O
% O
sentences O
contain O
character O
words O
in O
our O
training O
data O
, O
so O
they O
are O
reasonable O
indicators O
of O
assessing O
the O
ability O
to O
infer O
correct O
translations O
from O
the O
input O
image O
. O
Here O
we O
use O
[ O
MASK_P O
] O
for O
masking O
. O
Note O
that O
some O
character O
words O
have O
more O
than O
two O
translations O
, O
e.g. O
" O
people O
" O
, O
we O
also O
use O
the O
same O
evaluation O
metric O
with O
the O
color O
- O
based O
probing O
task O
, O
including O
relaxed O
and O
strict O
two O
criteria O
. O

Noun O
- O
based O
Probing O
For O
more O
complex O
scenarios O
, O
a O
sentence O
can O
be O
masked O
with O
several O
kinds O
of O
ambiguous O
words O
, O
such O
as O
animals O
, O
clothing O
, O
and O
vehicles O
, O
provided O
by O
Flickr30 O
K O
( O
Plummer O
et O
al O
. O
, O
2015 O
) O
. O
High O
- O
frequency O
words O
labeled O
with O
noun O
( O
or O
nouns O
) O
are O
more O
likely O
to O
be O
masked O
as O
[ O
MASK_N O
] O
( O
or O
[ O
MASK_NS O
] O
) O
) O
. O
See O
Table O
1 O
for O
example O
insufficient O
text O
with O
different O
numbers O
of O
masks O
. O

Various O
Vision O
Features O

In O
addition O
to O
ResNet-50 B-MethodName
, O
we O
choose O
several O
Transformer B-MethodName
- O
based O
vision O
models O
. O

• O
General O
Backbone O
. O
Vision B-MethodName
Transformer I-MethodName
( O
ViT B-MethodName
) O
and O
Swin B-MethodName
Transformer I-MethodName
are O
popular O
models O
in O
computer O
vision O
( O
Dosovitskiy O
et O
al O
. O
, O
2021 O
; O
Liu O
et O
al O
. O
, O
2021b O
) O
. O
We O
use O
ViT B-MethodName
with O
various O
model O
capacities O
to O
vary O
from O
weak O
to O
strong O
ViT B-MethodName
models O
. O

• O
Object B-TaskName
- I-TaskName
detection I-TaskName
. O
For O
pretrained O
objectdetection B-TaskName
vision O
models O
, O
we O
choose O
DETR B-MethodName
( O
Carion O
et O
al O
. O
, O
2020 O
) O
and O
QueryInst B-MethodName
( O
Fang O
et O
al O
. O
, O
2021 O
) O
for O
their O
strong O
performance O
. O
• O
Image B-TaskName
Captioning I-TaskName
. O
For O
image B-TaskName
captioning I-TaskName
models O
, O
we O
choose O
CATR B-MethodName
2 O
because O
it O
is O
a O
Transformer B-MethodName
- O
based O
image B-TaskName
captioning I-TaskName
architecture O
and O
can O
be O
easily O
implemented O
on O
top O
of O
ViT B-MethodName
. O

We O
form O
a O
number O
of O
vision O
features O
by O
combining O
the O
methods O
described O
above O
. O
More O
details O
are O
presented O
in O
Section O
3 O
. O

Selective O
Attention O

ViT B-MethodName
and O
related O
models O
perform O
in O
almost O
the O
same O
way O
as O
Transformer B-MethodName
in O
NLP O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
. O
Unlike O
the O
general O
models O
in O
CV O
, O
ViT B-MethodName
does O
not O
represent O
the O
image O
as O
a O
single O
vector O
. O
Instead O
, O
it O
generates O
a O
sequence O
of O
patches O
for O
image O
representation O
. O
An O
advantage O
of O
this O
design O
is O
that O
we O
can O
use O
the O
attention O
mechanism O
to O
correlate O
image O
patches O
to O
words O
. O
Thus O
, O
we O
present O
a O
selective O
attention O
model O
to O
model O
the O
patch O
- O
level O
contribution O
of O
the O
image O
. O
See O
Figure O
1 O
for O
the O
architecture O
. O

Text O
- O
only O
Transformer B-MethodName
Transformer B-MethodName
follows O
an O
encoder O
- O
decoder O
paradigm O
( O
the O
purple O
region O
in O
Figure O
1 O
) O
. O
The O
encoder O
is O
a O
stack O
of O
identical O
layers O
. O
Each O
layer O
consists O
of O
a O
self O
- O
attention O
( O
SAN O
) O
block O
and O
a O
feedforward O
network O
( O
FFN O
) O
block O
. O
The O
decoder O
shares O
a O
similar O
design O
with O
the O
encoder O
, O
but O
with O
an O
additional O
cross O
- O
attention O
block O
. O

Gated B-MethodName
Fusion I-MethodName
Gated B-MethodName
fusion I-MethodName
mechanism O
is O
a O
popular O
technique O
for O
fusing O
representations O
from O
different O
sources O
( O
Wu O
et O
al O
. O
, O
2021 O
; O
Zhang O
et O
al O
. O
, O
2020 O
; O
Lin O
et O
al O
. O
, O
2020 O
; O
. O
Given O
the O
text O
input O
X O
text O
and O
the O
image O
input O
X O
img O
, O
the O
text O
representation O
H O
text O
and O
the O
image O
feature O
H O
img O
can O
be O
defined O
as O
: O

H O
text O
= O
TransformerEncoder O
( O
X O
text O
) O
( O
1 O
) O
H O
img O
= O
W O
ViT O
( O
X O
img O
) O
( O
2 O
) O

where O
W O
is O
a O
projection O
matrix O
to O
convert O
the O
shape O
of O
ViT B-MethodName
( O
X O
img O
) O
into O
that O
of O
H O
text O
. O
Note O
that O
ViT B-MethodName
( O
• O
) O
can O
be O
replaced O
by O
other O
vision O
models O
, O
e.g. O
DETR B-MethodName
, O
Swin B-MethodName
Transformer I-MethodName
and O
etc O
. O
Then O
, O
the O
gate O
λ O
∈ O
[ O
0 O
, O
1 O
] O
and O
the O
fuzed O
output O
are O
defined O
as O
: O

λ O
= O
Sigmoid O
( O
U O
H O
text O
+ O
V O
H O
img O
) O
( O
3 O
) O
H O
Out O
= O
( O
1 O
− O
λ O
) O
• O
H O
text O
+ O
λ O
• O
H O
img O
( O
4 O
) O

where O
U O
and O
V O
are O
trainable O
variables O
. O
λ O
controls O
how O
much O
visual O
information O
is O
kept O
. O
Then O
, O
the O
fusion O
vector O
H O
Out O
is O
fed O
into O
the O
decoder O
. O
See O
the O
right O
side O
of O
the O
pink O
region O
in O
Figure O
1 O
for O
an O
illustration O
of O
the O
gated B-MethodName
fusion I-MethodName
models O
. O

Selective O
Attention O
After O
obtaining O
the O
text O
and O
image O
representations O
( O
or O
features O
) O
, O
we O
use O
a O
singlehead O
attention O
network O
to O
correlate O
words O
with O
image O
patches O
, O
where O
the O
query O
, O
key O
and O
value O
are O
H O
text O
, O
H O
img O
and O
H O
img O
, O
respectively O
. O
Then O
the O
selective O
attention O
output O
H O
img O
attn O
is O
defined O
to O
be O
: O
3 O
Experiments O

H O
img O
attn O
= O
Softmax O
( O
QK O
T O
√ O
d O
k O
) O
V O
( O

Datasets O

We O
conducted O
experiments O
on O
the O
widely O
used O
Multi30 B-DatasetName
K I-DatasetName
benchmark O
. O
The O
training O
and O
validation O
sets O
consisted O
of O
29 O
, O
000 O
and O
1 O
, O
014 O
instances O
, O
respectively O
. O
We O
reported O
the O
results O
on O
the O
Test2016 B-DatasetName
, O
Test2017 B-DatasetName
and O
MSCOCO B-DatasetName
test O
sets O
. O
Note O
that O
MSCOCO B-DatasetName
is O
more O
challenging O
for O
MMT B-TaskName
models O
due O
to O
the O
outof O
- O
domain O
instances O
with O
ambiguous O
verbs O
. O
Following O
the O
setup O
in O
( O
Wu O
et O
al O
. O
, O
2021 O
) O
, O
we O
learned O
a O
joint O
BPE O
code O
for O
10 O
, O
000 O
merging O
operations O
for O
both O
the O
source O
and O
target O
languages O
, O
resulting O
in O
vocabularies O
of O
9 O
, O
716 O
and O
9 O
, O
548 O
entries O
for O
the O
En B-TaskName
- I-TaskName
De I-TaskName
and O
En B-TaskName
- I-TaskName
Fr I-TaskName
tasks O
. O

Experimental O
Setups O

We O
followed O
the O
Wu O
et O
al O
. O
( O
2021 O
) O
's O
work O
to O
conduct O
experiments O
with O
Transformer B-MethodName
- O
Tiny O
configuration O
, O
which O
is O
more O
suited O
for O
small O
datasets O
like O
Multi30K. B-DatasetName
Note O
that O
smaller O
models O
even O
obtain O
higher O
BLEU B-MetricName
scores O
than O
pervious O
MMT B-TaskName
models O
. O

Similar O
observations O
have O
been O
discussed O
when O
building O
context O
- O
aware O
machine O
translation O
models O
. O
The O
model O
consists O
of O
4 O
encoder O
and O
decoder O
layers O
. O
The O
hidden B-HyperparameterName
size I-HyperparameterName
is O
128 B-HyperparameterValue
and O
the O
filter B-HyperparameterName
size I-HyperparameterName
of O
FFN O
is O
256 B-HyperparameterValue
. O
There O
are O
4 B-HyperparameterValue
heads B-HyperparameterName
in O
the O
multi O
- O
head O
self O
- O
attention O
mechanism O
. O
We O
set O
the O
dropout B-HyperparameterName
as O
0.3 B-HyperparameterValue
and O
the O
label B-HyperparameterName
smoothing I-HyperparameterName
as O
0.1 B-HyperparameterValue
. O

Our O
implementation O
was O
based O
on O
Fairseq O
( O
Ott O
et O
al O
. O
, O
2019 O
) O
. O
For O
training O
, O
we O
used O
Adam O
Optimizer O
( O
Kingma O
and O
Ba O
, O
2015 O
) O
with O
β B-HyperparameterName
1 I-HyperparameterName
= O
0.9 B-HyperparameterValue
, O
β B-HyperparameterName
2 I-HyperparameterName
= O
0.98 B-HyperparameterValue
and O
= O
10 B-HyperparameterValue
−8 I-HyperparameterValue
. O
We O
adopted O
the O
same O
learning O
rate O
schedule O
as O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
, O
where O
the O
learning B-HyperparameterName
rate I-HyperparameterName
first O
increased O
linearly O
for O
warmup B-HyperparameterName
= O
2000 B-HyperparameterValue
steps I-HyperparameterValue
from O
1e B-HyperparameterValue
−7 I-HyperparameterValue
to O
5e B-HyperparameterValue
−3 I-HyperparameterValue
. O
After O
the O
warmup O
, O
the O
learning B-HyperparameterName
rate I-HyperparameterName
decayed O
proportionally O
to O
the O
inverse O
square O
root O
of O
the O
current O
step O
. O
Each O
training O
batch O
contained O
4 B-HyperparameterValue
, I-HyperparameterValue
096 I-HyperparameterValue
tokens B-HyperparameterName
. O
We O
also O
adopted O
the O
early O
- O
stop O
training O
strategy O
( O
Zhang O
et O
al O
. O
, O
2020 O
) O
to O
avoid O
the O
overfitting O
issue O
. O

For O
evaluation O
, O
we O
averaged O
the O
last O
10 B-HyperparameterValue
checkpoints B-HyperparameterName
for O
more O
reliable O
results O
. O
The O
width O
of O
beam B-HyperparameterName
size I-HyperparameterName
was O
set O
to O
5 B-HyperparameterValue
. O
The O
performance O
was O
measured O
by O
BLEU B-MetricName
and O
METEOR B-MetricName
for O
all O
test O
sets O
. O
Also O
, O
we O
used O
accuracy B-MetricName
for O
evaluation O
on O
the O
probing O
tasks O
. O

Results O

Table O
2 O
summarizes O
the O
results O
on O
standard O
MMT B-TaskName
data O
. O
Each O
model O
was O
evaluated O
on O
three O
test O
sets O
on O
two O
language O
pairs O
. O
We O
see O
, O
first O
of O
all O
, O
that O
the O
improvements O
of O
previous O
methods O
( O
Rows O
2 O
- O
4 O
) O
over O
the O
tiny O
baseline O
are O
marginal O
in O
terms O
of O
both O
BLEU B-MetricName
and O
METEOR B-MetricName
. O
This O
confirms O
the O
assumption O
that O
the O
visual O
features O
are O
not O
fully O
used O
if O
the O
text O
is O
complete O
( O
Caglayan O
et O
al O
. O
, O
2019 O
) O
. O
When O
switching O
the O
vision O
features O
from O
ResNet B-MethodName
( O
Row.5 O
) O
to O
ViT B-MethodName
( O
Row.6 O
) O
, O
there O
are O
no O
significant O
BLEU B-MetricName
gains O
. O
Then O
, O
we O
test O
them O
on O
the O
proposed O
probing O
tasks O
to O
examine O
the O
" O
real O
" O
contribution O
to O
MMT B-TaskName
. O

Color O
- O
based O
Probing O
Table O
3 O
shows O
the O
accuracy B-MetricName
on O
the O
color O
- O
based O
probing O
task O
. O
We O
see O
that O
the O
accuracy B-MetricName
improvement O
of O
the O
gated B-MethodName
fusion I-MethodName
method O
is O
marginal O
by O
both O
restrict O
and O
relaxed O
criteria O
. O
However O
, O
replacing O
ResNet B-MethodName
with O
ViT B-MethodName
yields O
gains O
of O
over O
8 B-MetricValue
accuracy B-MetricName
points O
across O
three O
test O

Analysis O
4.1 O
How O
Vision O
Features O
Improve O
the O
MMT B-TaskName

We O
further O
explore O
the O
impact O
of O
model O
capacity O
. O

Here O
, O
we O
report O
the O
results O
of O
ViT B-MethodName
and O
Swin B-MethodName
Transformer I-MethodName
because O
they O
are O
strong O
models O
in O
recent O
studies O
. O
Our O
conjecture O
here O
is O
that O
larger O
ViT B-MethodName
/ O
Swin B-MethodName
models O
can O
describe O
the O
image O
more O
accurately O
, O
which O
enables O
the O
text O
encoder O
to O
receive O
richer O
complementary O
information O
. O
Figure O
3 O
depicts O
the O
BLEU B-MetricName
scores O
in O
progressive O
noun O
masking O
scenarios O
. O
Intuitively O
, O
larger O
ViT B-MethodName
and O
Swin O
models O
provide O
more O
complementary O
knowledge O
to O
complete O
the O
insufficient O
text O
representations O
. O
Nevertheless O
, O
a O
counterintuitive O
phenomenon O
is O
the O
inferiority O
of O
Swin O
across O
all O
scenarios O
in O
the O
same O
configuration O
, O
though O
it O
outperforms O
ViT B-MethodName
on O
most O
computer O
vision O
benchmarks O
. O
We O
attribute O
the O
reason O
to O
the O
short O
length O
of O
the O
patch O
sequence O
. O
In O
patch O
, O
ViT B-MethodName
has O
a O
length O
of O
577 O
( O
576 O
sequence O
segments O
and O
a O
special O
token O
CLS O
) O
when O
the O
image O
resolution O
and O
the O
patch O
size O
are O
384 O
× O
384 O
and O
16×16 O
. O
However O
, O
Swin B-MethodName
has O
a O
fixed O
sequence O
length O
( O
49 O
) O
restricted O
by O
the O
shifted O
window O
operation O
. O
This O
leads O
to O
more O
fine O
- O
grained O
local O
features O
for O
ViT B-MethodName
, O
which O
is O
beneficial O
to O
the O
selective O
attention O
mechanism O
for O
extracting O
more O
relevant O
pieces O
. O

Impact O
of O
Learning O
Objectives O

Then O
, O
we O
investigate O
the O
impact O
of O
the O
enhanced O
vision O
features O
on O
MMT B-TaskName
. O
Previous O
studies O
have O
already O
attempted O
to O
leverage O
object O
- O
detection O
features O
Wang O
and O
Xiong O
, O
2021 O
) O
but O
the O
observation O
here O
is O
slightly O
different O
. O
Beyond O
the O
object B-TaskName
- I-TaskName
detection I-TaskName
pretrained O
features O
, O
we O
also O
take O
the O
image B-TaskName
captioning I-TaskName
task O
into O
account O
. O

Rows O
11 O
- O
13 O
in O
Table O
2 O
summarize O
the O
results O
of O
the O
three O
enhanced O
vision O
features O
on O
the O
standard O
MMT B-TaskName
data O
, O
and O
Figure O
4 O
depicts O
the O
results O
on O
insufficient O
texts O
. O
Here O
we O
choose O
ViT B-MethodName
- O
Tinybased O
models O
for O
comparison O
due O
to O
the O
similar O
model O
capacity O
they O
own O
3 O
. O
We O
see O
that O
not O
only O
the O
object B-TaskName
- I-TaskName
detection I-TaskName
( O
DETR B-MethodName
and O
QueryInst B-MethodName
) O
, O
but O
also O
the O
image B-TaskName
captioning I-TaskName
( O
CATR B-MethodName
) O
pretrained O
fea- O
A O
boy O
plays O
in O
the O
leaves O
among O
the O
ducks O
. O

A O
boy O
plays O
in O
the O
[ O
MASK_NS O
] O
among O
the O
[ O
MASK_NS O
] O
. O

SRC O
: O
MASK O
: O

A O
woman O
is O
holding O
a O
small O
white O
statue O
. O

A O
[ O
MASK_P O
] O
is O
holding O
a O
small O
tures O
obtain O
superior O
performance O
compared O
with O
ViT B-MethodName
- O
tiny O
( O
Row O
8 O
) O
when O
the O
text O
is O
complete O
. O
It O
is O
consistent O
with O
previous O
findings O
. O
However O
, O
the O
advantages O
do O
not O
persist O
when O
switching O
to O
limited O
text O
scenarios O
. O
A O
possible O
explanation O
is O
that O
these O
methods O
are O
sensitive O
to O
the O
quality O
of O
the O
extracted O
objects O
. O
We O
leave O
this O
as O
future O
work O
. O

Impact O
of O
Resolution O
and O
Patch O
Size O

It O
is O
well O
- O
known O
that O
higher O
resolutions O
are O
beneficial O
to O
the O
accuracy B-MetricName
improvement O
in O
computer O
vision O
tasks O
( O
Dosovitskiy O
et O
al O
. O
, O
2021 O
) O
. O
Despite O
the O
success O
of O
the O
Transformer B-MethodName
architecture O
, O
recent O
studies O
show O
that O
the O
success O
of O
ViT B-MethodName
mainly O
comes O
from O
the O
successful O
use O
of O
the O
patch O
schema O
( O
Dosovitskiy O
et O
al O
. O
, O
2021 O
) O
. O
Here O
, O
we O
compare O
MMT B-TaskName
systems O
with O
different O
resolutions O
and O
patch O
sizes O
based O
on O
ViT B-MethodName
- O
Base O
. O
The O
results O
on O
three O
probing O
tasks O
( O
see O
Table O
5 O
) O
again O
confirm O
the O
above O
assumption O
that O
fine O
- O
grained O
vision O
features O
are O
more O
suited O
for O
the O
selective O
attention O
. O
Also O
, O
the O
attention O
map O
visualized O
in O
Figure O
5 O
demonstrates O
that O
high O
resolution O
with O
fine O
- O
grained O
patch O
schema O
can O
attend O
to O
correct O
regions O
of O
the O
image O
for O
each O
masked O
token O
. O
For O
example O
, O
both O
models O
pay O
the O
right O
attention O
to O
the O
masked O
character O
and O
noun O
, O
but O
the O
model O
with O
low O
resolution O
fails O
to O
detect O
the O
right O
region O
of O
color O
. O
The O
finding O
here O
may O
shed O
light O
to O
other O
multimodal O
tasks O
, O
such O
as O
VQA O
. O

Incongruent O
Decoding O

Incongruent O
decoding O
is O
a O
widely O
used O
manner O
to O
evaluate O
whether O
the O
visual O
modality O
contributes O
to O
the O
text O
( O
Caglayan O
et O
al O
. O
, O
2019 O
( O
Caglayan O
et O
al O
. O
, O
, O
2021 O
. O
Table O
6 O
shows O
that O
incongruent O
decoding O
causes O
obvious O
BLEU B-MetricName
drops O
except O
for O
the O
ResNet B-MethodName
feature O
. O
ViT B-MethodName
beats O
the O
ResNet B-MethodName
with O
gated O
fusion O
. O
It O
yields O
higher O
BLEU B-MetricName
scores O
with O
congruent O
decoding O
and O
exhibits O
a O
larger O
BLEU B-MetricName
drop O
with O
incongruent O
decoding O
. O
We O
also O
find O
that O
the O
ViT B-MethodName
features O
learned O
from O
scratch O
are O
also O
insensitive O
to O
the O
visual O
modality O
. O
This O
is O
reasonable O
that O
the O
learned O
vision O
systems O
are O
not O
sufficiently O
strong O
due O
to O
the O
data O
scarcity O
of O
Multi30K. B-DatasetName
Thus O
the O
visual O
modality O
acts O
more O
like O
noise O
signals O
. O
In O
addition O
, O
focusing O
on O
the O
results O
of O
pretrained O
selective O
attention O
+ O
ViT B-MethodName
, O
the O
gap O
between O
congruent O
and O
incongruent O
decoding O
gradually O
becomes O
larger O
. O
We O
also O
investigate O
whether O
the O
ensemble O
vision O
features O
can O
help O
. O
Concretely O
, O
we O
choose O
ViT B-MethodName
and O
CATR B-MethodName
to O
independently O
generate O
the O
fused O
representations O
with O
the O
text O
feature O
, O
and O
then O
the O
ensemble O
feature O
is O
obtained O
based O
on O
them O
. O
We O
see O
that O
the O
ensemble O
vision O
feature O
performs O
the O
best O
on O
the O
congruent O
decoding O
, O
and O
achieves O
the O
largest O
( O
a O
man O
is O
leaning O
on O
a O
wall O
with O
trees O
on O
the O
street O
. O
) O
ViT B-MethodName
: O
ein O
kind O
lehnt O
sich O
an O
einem O
auto O
mit O
blumen O
auf O
dem O
gehweg O
. O

( O
a O
child O
is O
leaning O
on O
a O
car O
with O
flowers O
on O
the O
sidewalk O
. O
) O
BLEU B-MetricName
gaps O
on O
four O
masking O
scenarios O
compared O
with O
other O
systems O
. O
These O
results O
again O
indicate O
that O
stronger O
visual O
contexts O
indeed O
help O
. O

Case O
Study O

Finally O
, O
we O
compare O
several O
real O
cases O
. O
We O
choose O
gated B-MethodName
fusion I-MethodName
( O
CNN B-MethodName
) O
( O
Wu O
et O
al O
. O
, O
2021 O
) O
and O
selective O
attention B-MethodName
+ O
ViT_Base B-MethodName
( O
ViT B-MethodName
) O
for O
comparison O
. O
The O
qualitative O
examples O
in O
Table O
7 O
demonstrate O
that O
the O
visual O
modality O
is O
complementary O
rather O
than O
redundant O
if O
the O
text O
is O
insufficient O
. O
To O
figure O
out O
whether O
the O
German O
translation O
is O
right O
or O
not O
, O
we O
provide O
the O
human O
- O
translation O
results O
. O
First O
, O
we O
see O
the O
top O
half O
case O
of O
Table O
7 O
, O
ViT B-MethodName
can O
fill O
in O
the O
masked O
entities O
and O
generate O
the O
correct O
translations O
even O
four O
entities O
were O
masked O
. O
Unfortunately O
, O
CNN B-MethodName
incorrectly O
judges O
the O
man O
as O
a O
woman O
. O
Also O
, O
it O
can O
not O
distinguish O
the O
right O
color O
of O
shirt O
due O
to O
the O
complex O
background O
. O
When O
given O
a O
more O
complex O
image O
( O
the O
bottom O
half O
case O
) O
, O
it O
is O
still O
a O
challenge O
for O
ViT B-MethodName
to O
generate O
the O
right O
translation O
. O
The O
observation O
here O
inspires O
us O
to O
design O
a O
more O
powerful O
fusion O
method O
. O
Also O
, O
the O
data O
scarcity O
problem O
is O
a O
root O
issue O
to O
prevent O
us O
from O
further O
improving O
the O
cross O
- O
modal O
translation O
quality O
. O

Related O
Work O

Multimodal B-TaskName
machine I-TaskName
translation I-TaskName
is O
a O
cross O
- O
domain O
task O
in O
the O
field O
of O
machine O
translation O
. O
Early O
attempts O
mainly O
focused O
on O
enhancing O
the O
MMT B-TaskName
model O
by O
better O
incorporation O
of O
the O
vision O
features O
( O
Calixto O
and O
Liu O
, O
2017 O
; O
Elliott O
and O
Kádár O
, O
2017 O
; O
Delbrouck O
and O
Dupont O
, O
2017 O
) O
. O
However O
, O
directly O
encoding O
the O
whole O
image O
feature O
brings O
additional O
noise O
to O
the O
text O
( O
Yao O
and O
Wan O
, O
2020 O
; O
Liu O
et O
al O
. O
, O
2021a O
) O
. O
To O
address O
the O
above O
issue O
, O
Yao O
and O
Wan O
( O
2020 O
) O
proposed O
a O
multimodal O
self O
- O
attention O
to O
consider O
the O
relative O
difference O
of O
information O
between O
two O
modalities O
. O
Similarly O
, O
Liu O
et O
al O
. O
( O
2021a O
) O
used O
a O
Gumbel O
Softmax O
to O
achieve O
the O
same O
goal O
. O

Researchers O
also O
realize O
that O
the O
visual O
modality O
may O
be O
redundant O
. O
Irrelevant O
images O
have O
little O
impact O
on O
the O
translation O
quality O
, O
and O
no O
significant O
BLEU B-MetricName
drop O
is O
observed O
even O
the O
image O
is O
absent O
( O
Elliott O
, O
2018 O
) O
. O
Encouraging O
results O
appeared O
in O
Caglayan O
et O
al O
. O
( O
2019 O
) O
's O
work O
. O
They O
pointed O
out O
that O
the O
visual O
modality O
is O
still O
useful O
when O
the O
linguistic O
context O
is O
scarce O
, O
but O
is O
less O
sensitive O
when O
exposed O
to O
complete O
sentences O
. O
More O
recently O
, O
Wu O
et O
al O
. O
( O
2021 O
) O
attributed O
the O
BLEU B-MetricName
gain O
on O
MMT B-TaskName
tasks O
to O
the O
regularization O
training O
, O
and O
they O
again O
emphasized O
the O
imperative O
of O
constructing O
proper O
insufficient O
textual O
input O
. O
It O
is O
worthy O
to O
note O
that O
the O
proposed O
probing O
task O
is O
an O
improved O
version O
based O
upon O
previous O
work O
( O
Caglayan O
et O
al O
. O
, O
2019 O
; O
Wu O
et O
al O
. O
, O
2021 O
) O
. O
We O
also O
opensource O
the O
preprocessed O
data O
and O
the O
corresponding O
scripts O
for O
the O
subsequent O
researchers O
to O
experiment O
on O
. O

Another O
line O
of O
research O
is O
to O
explore O
large O
- O
scale O
cross O
- O
modal O
pretraining O
models O
. O
In O
this O
way O
, O
the O
MMT B-TaskName
task O
is O
regarded O
as O
a O
downstream O
task O
. O
For O
example O
, O
CLIP O
( O
Radford O
et O
al O
. O
, O
2021 O
) O
is O
a O
general O
cross O
- O
modal O
pretraining O
model O
, O
which O
learns O
to O
perform O
a O
wide O
variety O
of O
tasks O
via O
natural O
language O
prompting O
. O
Caglayan O
et O
al O
. O
( O
2021 O
) O
presented O
a O
MMT B-TaskName
- O
specific O
pretraining O
model O
which O
combines O
the O
translation O
language O
modeling O
with O
masked O
region O
classification O
objectives O
. O
In O
this O
work O
, O
we O
make O
a O
systematic O
study O
on O
whether O
stronger O
vision O
features O
are O
helpful O
. O
We O
also O
extend O
the O
research O
to O
enhanced O
features O
, O
such O
as O
object B-TaskName
- I-TaskName
detection I-TaskName
and O
image B-TaskName
captioning I-TaskName
, O
which O
are O
complementary O
to O
previous O
work O
. O

Conclusions O

In O
this O
work O
, O
we O
show O
that O
stronger O
vision O
features O
( O
e.g. O
ViT B-MethodName
- O
like O
models O
) O
strengthen O
MMT B-TaskName
systems O
on O
three O
proposed O
probing O
tasks O
. O
We O
present O
a O
selective O
attention O
method O
for O
ViT B-MethodName
- O
based O
models O
to O
make O
better O
use O
of O
the O
patch O
- O
level O
representation O
. O
The O
result O
here O
shows O
a O
promising O
line O
of O
research O
on O
developing O
better O
vision O
models O
for O
multimodal O
tasks O
. O
As O
far O
as O
we O
know O
, O
this O
is O
the O
first O
attempt O
to O
build O
MMT B-TaskName
systems O
with O
Transformer B-MethodName
only O
. O
In O
future O
work O
, O
we O
are O
willing O
to O
investigate O
whether O
it O
is O
possible O
to O
use O
a O
single O
set O
of O
parameters O
to O
encode O
the O
vision O
and O
text O
modalities O
. O

Acknowledgments O

This O
work O
was O
supported O
in O
part O
by O
the O
National O
Science O
Foundation O
of O
China O
( O
Nos O
. O
61732005 O
and O
61876035 O
) O
, O
the O
National O
Key O
R O
& O
D O
Project O
of O
China O
( O
No O
. O
2019QY1801 O
) O
, O
the O
China O
HTRD O
Center O
Project O
( O
No O
. O
2020AAA0107904 O
) O
and O
Yunnan O
Provincial O
Major O
Science O
and O
Technology O
Special O
Plan O
Projects O
( O
Nos O
. O
201902D08001905 O
and O
202103AA080015 O
) O
. O
The O
authors O
would O
like O
to O
thank O
anonymous O
reviewers O
for O
their O
valuable O
comments O
. O
And O
thank O
Yufan O
Jiang O
for O
his O
helpful O
advice O
to O
improve O
the O
paper O
. O

CoMPM B-MethodName
: O
Context B-MethodName
Modeling I-MethodName
with I-MethodName
Speaker I-MethodName
's I-MethodName
Pre I-MethodName
- I-MethodName
trained I-MethodName
Memory I-MethodName
Tracking I-MethodName
for O
Emotion B-TaskName
Recognition I-TaskName
in I-TaskName
Conversation I-TaskName

As O
the O
use O
of O
interactive O
machines O
grow O
, O
the O
task O
of O
Emotion B-TaskName
Recognition I-TaskName
in I-TaskName
Conversation I-TaskName
( O
ERC B-TaskName
) O
became O
more O
important O
. O
If O
the O
machine O
- O
generated O
sentences O
reflect O
emotion O
, O
more O
human O
- O
like O
sympathetic O
conversations O
are O
possible O
. O
Since O
emotion B-TaskName
recognition I-TaskName
in I-TaskName
conversation I-TaskName
is O
inaccurate O
if O
the O
previous O
utterances O
are O
not O
taken O
into O
account O
, O
many O
studies O
reflect O
the O
dialogue O
context O
to O
improve O
the O
performances O
. O
Many O
recent O
approaches O
show O
performance O
improvement O
by O
combining O
knowledge O
into O
modules O
learned O
from O
external O
structured O
data O
. O
However O
, O
structured O
data O
is O
difficult O
to O
access O
in O
non O
- O
English O
languages O
, O
making O
it O
difficult O
to O
extend O
to O
other O
languages O
. O
Therefore O
, O
we O
extract O
the O
pre O
- O
trained O
memory O
using O
the O
pre O
- O
trained O
language O
model O
as O
an O
extractor O
of O
external O
knowledge O
. O
We O
introduce O
CoMPM B-MethodName
, O
which O
combines O
the O
speaker O
's O
pre O
- O
trained O
memory O
with O
the O
context O
model O
, O
and O
find O
that O
the O
pre O
- O
trained O
memory O
significantly O
improves O
the O
performance O
of O
the O
context O
model O
. O
CoMPM B-MethodName
achieves O
the O
first O
or O
second O
performance O
on O
all O
data O
and O
is O
state O
- O
of O
- O
the O
- O
art O
among O
systems O
that O
do O
not O
leverage O
structured O
data O
. O
In O
addition O
, O
our O
method O
shows O
that O
it O
can O
be O
extended O
to O
other O
languages O
because O
structured O
knowledge O
is O
not O
required O
, O
unlike O
previous O
methods O
. O
Our O
code O
is O
available O
on O
github O
1 O
. O

Introduction O

As O
the O
number O
of O
applications O
such O
as O
interactive O
chatbots O
or O
social O
media O
that O
are O
used O
by O
many O
users O
has O
recently O
increased O
dramatically O
, O
Emotion B-TaskName
Recognition I-TaskName
in I-TaskName
Conversation I-TaskName
( O
ERC B-TaskName
) O
plays O
a O
more O
important O
role O
in O
natural O
language O
processing O
, O
and O
as O
a O
proof O
, O
a O
lot O
of O
research O
Ghosal O
et O
al O
. O
, O
2020 O
; O
Jiao O
et O
al O
. O
, O
2020 O
) O
has O
been O
conducted O
on O
the O
task O
. O

The O
ERC B-TaskName
module O
increases O
the O
quality O
of O
empathetic O
conversations O
with O
the O
users O
and O
can O
be O
1 O
https O
: O
/ O
/ O
github.com O
/ O
rungjoo O
/ O
CoMPM B-MethodName
Figure O
1 O
: O
An O
example O
of O
MELD B-DatasetName
dataset O
utilized O
when O
sending O
tailored O
push O
messages O
to O
the O
users O
( O
Shin O
et O
al O
. O
, O
2019 O
; O
Zandie O
and O
Mahoor O
, O
2020 O
; O
Lin O
et O
al O
. O
, O
2020 O
) O
. O
In O
addition O
, O
emotion B-TaskName
recognition I-TaskName
can O
be O
effectively O
used O
for O
opinion O
mining O
, O
recommender O
systems O
, O
and O
healthcare O
systems O
where O
it O
can O
improve O
the O
service O
qualities O
by O
providing O
personalized O
results O
. O
As O
these O
interactive O
machines O
increase O
, O
the O
ERC B-TaskName
module O
plays O
an O
increasingly O
important O
role O
. O
Figure O
1 O
is O
an O
example O
of O
a O
conversation O
in O
which O
two O
speakers O
are O
angry O
at O
each O
other O
. O
The O
emotion O
of O
speaker O
B O
's O
utterance O
( O
" O
How O
'd O
you O
get O
to O
that O
? O
" O
) O
is O
angry O
. O
If O
the O
system O
does O
not O
take O
into O
account O
previous O
utterances O
, O
it O
is O
difficult O
to O
properly O
recognize O
emotions O
. O
Like O
the O
previous O
studies O
( O
Ghosal O
et O
al O
. O
, O
2020 O
) O
, O
we O
show O
that O
the O
utterance O
- O
level O
emotion O
recognition O
, O
which O
does O
not O
consider O
the O
previous O
utterance O
, O
have O
limitations O
and O
experiments O
result O
in O
poor O
performances O
. O

Therefore O
, O
recent O
studies O
are O
attempting O
to O
recognize O
emotions O
while O
taking O
into O
account O
the O
previous O
utterances O
. O
Representatively O
, O
Dia O
- O
logueRNN O
recognizes O
the O
present O
emotion O
by O
tracking O
context O
from O
the O
previous O
utterances O
and O
the O
speaker O
's O
emotion O
. O
AGHMN O
( O
Jiao O
et O
al O
. O
, O
2020 O
) O
considers O
the O
previous O
utterances O
through O
memory O
summarizing O
using O
GRU O
with O
attention O
. O

Many O
recent O
studies O
use O
external O
knowledge O
to O
improve O
the O
ERC B-TaskName
performance O
. O
However O
, O
this O
exter O
- O
nal O
knowledge O
is O
often O
only O
available O
in O
English O
. O
In O
order O
to O
utilize O
the O
previous O
methods O
in O
languages O
of O
other O
countries O
, O
it O
is O
expensive O
and O
difficult O
to O
utilize O
because O
external O
knowledge O
data O
must O
be O
newly O
constructed O
. O
In O
recent O
NLP O
studies O
, O
due O
to O
the O
effectiveness O
of O
the O
pre O
- O
trained O
language O
model O
, O
it O
has O
already O
been O
developed O
in O
many O
countries O
. O
Since O
pre O
- O
trained O
language O
models O
are O
trained O
by O
unsupervised O
learning O
, O
these O
models O
are O
relatively O
usable O
approaches O
regardless O
of O
language O
types O
. O
Petroni O
et O
al O
. O
( O
2019 O
) O
introduces O
that O
these O
language O
models O
can O
be O
used O
as O
knowledge O
bases O
and O
have O
many O
advantages O
over O
the O
structured O
knowledge O
bases O
. O
Based O
on O
these O
studies O
, O
we O
eliminate O
the O
dependence O
on O
structured O
external O
data O
used O
in O
cutting O
- O
edge O
systems O
and O
use O
a O
pre O
- O
trained O
language O
model O
as O
a O
feature O
extractor O
of O
knowledge O
. O

CoMPM B-MethodName
, O
introduced O
in O
this O
paper O
, O
is O
composed O
of O
two O
modules O
that O
take O
into O
account O
previous O
utterances O
in O
dialogue O
. O
( O
1 O
) O
The O
first O
is O
a O
context O
embedding O
module O
( O
CoM O
) O
that O
reflects O
all O
previous O
utterances O
as O
context O
. O
CoM O
is O
an O
auto O
- O
regressive O
model O
that O
predicts O
the O
current O
emotion O
through O
attention O
between O
the O
previous O
utterances O
of O
the O
conversation O
and O
the O
current O
utterance O
. O
( O
2 O
) O
The O
second O
is O
a O
pre O
- O
trained O
memory O
module O
( O
PM O
) O
that O
extracts O
memory O
from O
utterances O
. O
We O
use O
the O
output O
of O
the O
pre O
- O
trained O
language O
model O
as O
the O
memory O
embedding O
where O
the O
utterances O
are O
passed O
into O
the O
language O
model O
. O
We O
use O
the O
PM O
to O
help O
predict O
the O
emotion O
of O
the O
speaker O
by O
taking O
into O
account O
the O
speaker O
's O
linguistic O
preferences O
and O
characteristics O
. O

We O
experiment O
on O
4 O
different O
English O
ERC B-TaskName
datasets O
. O
Multi O
- O
party O
datasets O
are O
MELD B-DatasetName
and O
EmoryNLP B-DatasetName
( O
Zahiri O
and O
Choi O
, O
2018 O
) O
, O
and O
dyadic O
datasets O
are O
IEMOCAP B-DatasetName
( O
Busso O
et O
al O
. O
, O
2008 O
) O
and O
DailyDialog B-DatasetName
( O
Li O
et O
al O
. O
, O
2017 O
) O
. O
CoMPM B-MethodName
achieves O
the O
first O
or O
second O
performance O
according O
to O
the O
evaluation O
metric O
compared O
to O
all O
previous O
systems O
. O
We O
perform O
an O
ablation O
study O
on O
each O
module O
to O
show O
that O
the O
proposed O
approach O
is O
effective O
. O
Further O
experiments O
also O
show O
that O
our O
approach O
can O
be O
used O
in O
other O
languages O
and O
show O
the O
performance O
of O
CoMPM B-MethodName
when O
the O
number O
of O
data O
is O
limited O
. O

Related O
Work O

Many O
recent O
studies O
use O
external O
knowledge O
to O
improve O
the O
ERC B-TaskName
performance O
. O
KET O
( O
Zhong O
et O
al O
. O
, O
2019 O
) O
is O
used O
as O
external O
knowledge O
based O
on O
ConceptNet O
( O
Speer O
et O
al O
. O
, O
2017 O
) O
and O
emotion O
lex O
- O
icon O
NRC_VAD O
( O
Mohammad O
, O
2018 O
) O
as O
the O
commonsense O
knowledge O
. O
ConceptNet O
is O
a O
knowledge O
graph O
that O
connects O
words O
and O
phrases O
in O
natural O
language O
using O
labeled O
edges O
. O
NRC_VAD O
Lexicon O
has O
human O
ratings O
of O
valence O
, O
arousal O
, O
and O
dominance O
for O
more O
than O
20,000 O
English O
words O
. O
COSMIC O
( O
Ghosal O
et O
al O
. O
, O
2020 O
) O
and O
Psychological O
( O
Li O
et O
al O
. O
, O
2021 O
) O
improve O
the O
performance O
of O
emotion O
recognition O
by O
extracting O
commonsense O
knowledge O
of O
the O
previous O
utterances O
. O
Commonsense O
knowledge O
feature O
is O
extracted O
and O
leveraged O
with O
COMET O
( O
Bosselut O
et O
al O
. O
, O
2019 O
) O
trained O
with O
ATOMIC O
( O
The O
Atlas O
of O
Machine O
Commonsense O
) O
. O
ATOMIC O
has O
9 O
sentence O
relation O
types O
with O
inferential O
if O
- O
then O
commonsense O
knowledge O
expressed O
in O
text O
. O
ToDKAT O
( O
Zhu O
et O
al O
. O
, O
2021 O
) O
improves O
performance O
by O
combining O
commonsense O
knowledge O
using O
COMET O
and O
topic O
discovery O
using O
VHRED O
( O
Serban O
et O
al O
. O
, O
2017 O
) O
to O
the O
model O
. O

Ekman O
( O
Ekman O
, O
1992 O
) O
constructs O
taxonomy O
of O
six O
common O
emotions O
( O
Joy O
, O
Sadness O
, O
Fear O
, O
Anger O
, O
Surprise O
, O
and O
Disgust O
) O
from O
human O
facial O
expressions O
. O
In O
addition O
, O
Ekman O
explains O
that O
a O
multimodal O
view O
is O
important O
for O
multiple O
emotions O
recognition O
. O
The O
multi O
- O
modal O
data O
such O
as O
MELD B-DatasetName
and O
IEMOCAP B-DatasetName
are O
some O
of O
the O
available O
standard O
datasets O
for O
emotion O
recognition O
and O
they O
are O
composed O
of O
text O
, O
speech O
and O
vision O
- O
based O
data O
. O
Datcu O
and O
Rothkrantz O
( O
2014 O
) O
uses O
speech O
and O
visual O
information O
to O
recognize O
emotions O
, O
and O
( O
Alm O
et O
al O
. O
, O
2005 O
) O
attempts O
to O
recognize O
emotions O
based O
on O
text O
information O
. O
MELD B-DatasetName
and O
ICON O
( O
Hazarika O
et O
al O
. O
, O
2018a O
) O
show O
that O
the O
more O
multi O
- O
modal O
information O
is O
used O
, O
the O
better O
the O
performance O
and O
the O
text O
information O
plays O
the O
most O
important O
role O
. O
Multimodal O
information O
is O
not O
always O
given O
in O
most O
social O
media O
, O
especially O
in O
chatbot O
systems O
where O
they O
are O
mainly O
composed O
of O
text O
- O
based O
systems O
. O
In O
this O
work O
, O
we O
design O
and O
introduce O
a O
text O
- O
based O
emotion B-TaskName
recognition I-TaskName
system O
using O
neural O
networks O
. O

In O
the O
previous O
studies O
, O
such O
as O
Hazarika O
et O
al O
. O
( O
2018b O
) O
; O
Zadeh O
et O
al O
. O
( O
2017 O
) O
; O
, O
most O
works O
focused O
on O
dyadic O
- O
party O
conversation O
. O
However O
, O
as O
the O
multi O
- O
party O
conversation O
datasets O
including O
MELD B-DatasetName
and O
EmoryNLP B-DatasetName
have O
become O
available O
, O
a O
lot O
of O
recent O
research O
is O
being O
conducted O
on O
multi O
- O
party O
dialogues O
such O
as O
; O
Jiao O
et O
al O
. O
( O
2020 O
) O
; O
Ghosal O
et O
al O
. O
( O
2020 O
) O
. O
In O
general O
, O
the O
multi O
- O
party O
conversations O
have O
higher O
speaker O
dependency O
than O
the O
dyadic O
- O
party O
dialogues O
, O
therefore O
have O
more O
conditions O
to O
consider O
and O
result O
in O
poor O
performance O
. O
Zhou O
et O
al O
. O
( O
2018 O
) O
; O
Zhang O
et O
al O
. O
( O
2018a O
) O
shows O
that O
commonsense O
knowledge O
is O
important O
for O
understanding O
conversations O
and O
generating O
appropriate O
responses O
. O
reports O
that O
the O
lack O
of O
external O
knowledge O
makes O
it O
difficult O
to O
classify O
implicit O
emotions O
from O
the O
conversation O
history O
. O
EDA O
( O
Bothe O
et O
al O
. O
, O
2020 O
) O
expands O
the O
multi O
- O
modal O
emotion O
datasets O
by O
extracting O
dialog O
acts O
from O
MELD B-DatasetName
and O
IEMOCAP B-DatasetName
and O
finds O
out O
that O
there O
is O
a O
correlation O
between O
dialogue O
acts O
and O
emotion O
labels O
. O

Approach O

Problem O
Statement O

In O
a O
conversation O
, O
M O
sequential O
utterances O
are O
given O
as O
[ O
( O
u O
1 O
, O
p O
u O
1 O
) O
, O
( O
u O
2 O
, O
p O
u O
2 O
) O
, O
... O
, O
( O
u O
M O
, O
p O
u O
M O
) O
] O
. O
u O
i O
is O
the O
utterance O
which O
the O
speaker O
p O
u O
i O
uttered O
, O
where O
p O
u O
i O
is O
one O
of O
the O
conversation O
participants O
. O
While O
p O
u O
i O
and O
p O
u O
j O
( O
i O
̸ O
= O
j O
) O
can O
be O
the O
same O
speaker O
, O
the O
minimum O
number O
of O
the O
unique O
conversation O
participants O
should O
be O
2 O
or O
more O
. O
The O
ERC B-TaskName
is O
a O
task O
of O
predicting O
the O
emotion O
e O
t O
of O
u O
t O
, O
the O
utterance O
of O
the O
t O
- O
th O
turn O
, O
given O
the O
previous O
utterances O
h O
t O
= O
{ O
u O
1 O
, O
... O
, O
u O
t−1 O
} O
. O
Emotions O
are O
labeled O
as O
one O
of O
the O
predefined O
classes O
depending O
on O
the O
dataset O
, O
and O
the O
emotions O
we O
experimented O
with O
are O
either O
6 O
or O
7 O
. O
We O
also O
experimented O
with O
a O
sentiment O
classification O
dataset O
which O
provides O
sentiment O
labels O
consisting O
of O
positive O
, O
negative O
and O
neutral O
. O

Model O
Overview O

Figure O
2 O
shows O
an O
overview O
of O
our O
model O
. O
Our O
ERC B-TaskName
neural O
network O
model O
is O
composed O
of O
two O
modules O
. O
The O
first O
is O
CoM O
which O
catches O
the O
underlying O
effect O
of O
all O
previous O
utterances O
on O
the O
current O
speaker O
's O
emotions O
. O
Therefore O
, O
we O
propose O
a O
context O
model O
to O
handle O
the O
relationship O
between O
the O
current O
and O
the O
previous O
utterances O
. O
The O
second O
one O
is O
PM O
that O
leverages O
only O
the O
speaker O
's O
previous O
utterances O
, O
through O
which O
we O
want O
to O
reflect O
the O
speaker O
's O
knowledge O
. O

If O
the O
CoM O
and O
PM O
are O
based O
on O
different O
backbones O
, O
we O
consider O
them O
to O
be O
unaligned O
with O
respect O
to O
each O
other O
's O
output O
representations O
. O
Therefore O
, O
we O
design O
the O
PM O
to O
follow O
CoM O
so O
that O
the O
output O
representations O
of O
CoM O
and O
PM O
can O
mutually O
understand O
each O
other O
. O
If O
CoM O
and O
PM O
are O
based O
on O
different O
architectures O
, O
CoMPM B-MethodName
is O
trained O
to O
understand O
each O
other O
's O
representations O
by O
matching O
dimensions O
using O
W O
p O
in O
Equation O
4 O
. O
The O
combination O
of O
CoM O
and O
PM O
is O
described O
in O
Section O
4.5 O
. O

CoM O
: O
Context O
Embedding O
Module O

The O
context O
embedding O
module O
predicts O
e O
t O
by O
considering O
all O
of O
the O
utterances O
before O
the O
t O
- O
th O
turn O
as O
the O
dialogue O
context O
. O
The O
example O
in O
Figure O
2 O
shows O
how O
the O
model O
predicts O
the O
emotion O
of O
u O
6 O
uttered O
by O
s O
A O
, O
given O
a O
conversation O
of O
three O
participants O
( O
s O
A O
, O
s O
B O
, O
s O
C O
) O
. O
The O
previous O
utterances O
are O
h O
6 O
= O
{ O
u O
1 O
, O
• O
• O
• O
u O
5 O
} O
and O
e O
6 O
is O
predicted O
while O
considering O
the O
relationship O
between O
u O
6 O
and O
h O
6 O
. O

We O
consider O
multi O
- O
party O
conversations O
where O
2 O
or O
more O
speakers O
are O
involved O
. O
A O
special O
token O
< O
s O
P O
> O
is O
introduced O
to O
distinguish O
participants O
in O
the O
conversation O
and O
to O
handle O
the O
speaker O
's O
dependency O
where O
P O
is O
the O
set O
of O
participants O
. O
In O
other O
words O
, O
the O
same O
special O
token O
appears O
before O
the O
utterances O
of O
the O
same O
speaker O
. O

We O
use O
an O
Transformer O
encoder O
as O
a O
context O
model O
. O
In O
many O
natural O
language O
processing O
tasks O
, O
the O
effectiveness O
of O
the O
pre O
- O
trained O
language O
model O
has O
been O
proven O
, O
and O
we O
also O
set O
the O
initial O
state O
of O
the O
model O
to O
RoBERTa B-HyperparameterName
( O
Liu O
et O
al O
. O
, O
2019 O
) O
. O
RoBERTa B-HyperparameterName
is O
an O
unsupervised O
pre O
- O
trained O
model O
with O
largescale O
open O
- O
domain O
corpora O
of O
unlabeled O
text O
. O

We O
use O
the O
embedding O
of O
the O
special O
token O
< O
cls O
> O
to O
predict O
emotion O
. O
The O
< O
cls O
> O
token O
is O
concatenated O
at O
the O
beginning O
of O
the O
input O
and O
the O
output O
of O
the O
context O
model O
is O
as O
follows O
: O

c O
t O
= O
CoM O
( O
< O
cls O
> O
, O
P O
: O
t−1 O
, O
h O
t O
, O
u O
t O
) O
( O
1 O
) O

where O
P O
: O
t−1 O
is O
the O
set O
of O
speakers O
in O
the O
previous O
turns O
. O
c O
t O
∈ O
R O
1×hc O
and O
h O
c O
is O
the O
dimension O
of O
CoM O
. O

PM O
: O
Pre O
- O
trained O
Memory O
Module O

External O
knowledge O
is O
known O
to O
play O
an O
important O
role O
in O
understanding O
conversation O
. O
Pre O
- O
trained O
language O
models O
can O
be O
trained O
on O
numerous O
corpora O
and O
be O
used O
as O
an O
external O
knowledge O
base O
. O
Inspired O
by O
previous O
studies O
that O
the O
speaker O
's O
knowledge O
helps O
to O
judge O
emotions O
, O
we O
extract O
and O
track O
pre O
- O
trained O
memory O
from O
the O
speaker O
's O
previous O
utterances O
to O
utilize O
the O
emotions O
of O
the O
current O
utterance O
u O
t O
. O
If O
the O
speaker O
has O
never O
appeared O
before O
the O
current O
turn O
, O
the O
result O
of O
the O
pre O
- O
trained O
memory O
is O
considered O
a O
zero O
vector O
. O
Since O
< O
cls O
> O
is O
mostly O
used O
for O
the O
task O
of O
classifying O
sentences O
, O
we O
use O
the O
embedding O
output O
of O
the O
< O
cls O
> O
token O
as O
a O
vector O
representing O
the O
utterance O
as O
follows O
: O

k O
i O
= O
PM O
( O
< O
cls O
> O
, O
u O
i O
) O
( O
2 O
) O

where O
p O
u O
i O
= O
p O
S O
, O
S O
is O
the O
speaker O
of O
the O
current O
utterance O
. O
k O
i O
∈ O
R O
1×h O
k O
and O
h O
k O
is O
the O
dimension O
of O
PM O
. O

CoMPM B-MethodName
: O
Combination O
of O
CoM O
and O
PM O

We O
combine O
CoM O
and O
PM O
to O
predict O
the O
speaker O
's O
emotion O
. O
In O
many O
dialogue O
systems O
( O
Zhang O
et O
al O
. O
, O
2018b O
; O
Ma O
et O
al O
. O
, O
2019 O
) O
, O
it O
is O
known O
that O
utterances O
close O
to O
the O
current O
turn O
are O
important O
for O
response O
. O
Therefore O
, O
we O
assume O
that O
utterances O
close O
to O
the O
current O
utterance O
will O
be O
important O
in O
emotional O
recognition O
. O

Tracking O
Method O

We O
use O
k O
i O
tracking O
method O
using O
GRU O
. O
The O
tracking O
method O
assumes O
that O
the O
importance O
of O
all O
previous O
speaker O
utterances O
to O
the O
current O
emotion O
is O
not O
equal O
and O
varies O
with O
the O
distance O
of O
the O
current O
utterance O
. O
In O
other O
words O
, O
since O
the O
flow O
of O
conversation O
changes O
as O
it O
progresses O
, O
the O
effect O
on O
emotion O
may O
differ O
depending O
on O
the O
distance O
from O
the O
current O
utterance O
. O
We O
track O
and O
capture O
the O
sequential O
position O
information O
of O
k O
i O
using O
a O
unidirectional O
GRU O
: O

kt O
t O
= O
GRU O
( O
k O
i O
1 O
, O
k O
i O
2 O
, O
... O
, O
k O
in O
) O
( O
3 O

) O

where O
t O
is O
the O
turn O
index O
of O
the O
current O
utterance O
, O
n O
is O
the O
number O
of O
previous O
utterances O
of O
the O
speaker O
, O
and O
i O
s O
( O
s O
= O
1 O
, O
2 O
, O
... O
, O
n O
) O
is O
each O
turn O
uttered O
. O
kt O
t O
∈ O
R O
1×hc O
is O
the O
output O
of O
k O
in O
and O
as O
a O
result O
, O
the O
knowledge O
of O
distant O
utterance O
is O
diluted O
and O
the O
effect O
on O
the O
current O
utterance O
is O
reduced O
. O
GRU O
is O
composed O
of O
2 O
- O
layers O
, O
the O
dimension O
of O
the O
output O
vector O
is O
h O
c O
, O
and O
the O
dropout O
is O
set O
to O
0.3 O
during O
training O
. O
Finally O
, O
the O
output O
vector O
o O
t O
is O
obtained O
by O
adding O
kt O
t O
and O
c O
t O
in O
Equation O
4 O
. O

o O
t O
= O
c O
t O
+ O
W O
p O
( O
kt O
t O
) O
( O
4 O
) O

where O
, O
W O
p O
is O
a O
matrix O
that O
projects O
the O
pretrained O
memory O
to O
the O
dimension O
of O
the O
context O
output O
, O
and O
is O
used O
only O
when O
PM O
and O
CoM O
are O
different O
pre O
- O
trained O
language O
models O
. O

Emotion O
Prediction O

Softmax O
is O
applied O
to O
the O
vector O
multiplied O
by O
o O
t O
and O
the O
linear O
matrix O
W O
o O
∈ O
R O
he×hc O
to O
obtain O
the O
probability O
distribution O
of O
emotion O
classes O
, O
where O
h O
e O
is O
the O
number O
of O
emotion O
classes O
. O
e O
t O
is O
the O
predicted O
emotion O
class O
that O
corresponds O
to O
the O
index O
of O
the O
largest O
probability O
from O
the O
emotion O
class O
distribution O
. O

P O
( O
e O
) O
= O
softmax O
( O
W O
o O
( O
o O
t O
) O
) O
( O
5 O

) O

The O
objective O
is O
to O
minimize O
the O
cross O
entropy O
loss O
so O
that O
e O
t O
is O
the O
same O
as O
the O
ground O
truth O
emotional O
label O
. O

Experiments O

Dataset O

We O
experiment O
on O
four O
benchmark O
datasets O
. O
MELD B-DatasetName
and O
EmoryNLP B-DatasetName
( O
Zahiri O
and O
Choi O
, O
2018 O
) O
are O
multi O
- O
party O
datasets O
, O
while O
IEMOCAP B-DatasetName
( O
Busso O
et O
al O
. O
, O
2008 O
) O
and O
DailyDialog B-DatasetName
( O
Li O
et O
al O
. O
, O
2017 O
) O
are O
dyadic O
- O
party O
datasets O
. O
The O
statistics O
of O
the O
dataset O
are O
shown O
in O
Table O
1 O
. O

IEMOCAP B-DatasetName
is O
a O
dataset O
involving O
10 O
speakers O
, O
and O
each O
conversation O
involves O
2 O
speakers O
and O
the O
emotion O
- O
inventory O
is O
given O
as O
" O
happy O
, O
sad O
, O
angry O
, O
excited O
, O
frustrated O
and O
neutral O
" O
. O
The O
train O
and O
development O
dataset O
is O
a O
conversation O
involving O
the O
previous O
eight O
speakers O
, O
and O
the O
train O
and O
development O
are O
divided O
into O
random O
splits O
at O
a O
ratio O
of O
9:1 O
. O
The O
test O
dataset O
is O
a O
conversation O
involving O
two O
later O
speakers O
. O

DailyDialog B-DatasetName
is O
a O
dataset O
of O
daily O
conversations O
between O
two O
speakers O
and O
the O
emotion O
- O
inventory O
is O
given O
as O
" O
anger O
, O
disgust O
, O
fear O
, O
joy O
, O
surprise O
, O
sadness O
and O
neutral O
" O
. O
Since O
more O
than O
82 O
% O
of O
the O
data O
are O
tagged O
as O
neutral O
, O
neutral O
emotions O
are O
excluded O
when O
evaluating O
systems O
with O
Micro O
- O
F1 O
as O
did O
in O
the O
previous O
studies O
. O

MELD B-DatasetName
is O
a O
dataset O
based O
on O
Friends O
TV O
show O
and O
provides O
two O
taxonomy O
: O
emotion O
and O
sentiment O
. O
MELD B-DatasetName
's O
emotion O
- O
inventory O
is O
given O
as O
" O
anger O
, O
disgust O
, O
sadness O
, O
joy O
, O
surprise O
, O
fear O
and O
neutrality O
" O
following O
Ekman O
( O
Ekman O
, O
1992 O
) O
and O
sentiment O
- O
inventory O
is O
given O
as O
" O
positive O
, O
negative O
and O
neutral O
" O
. O
EmoryNLP B-DatasetName
, O
like O
MELD B-DatasetName
, O
is O
also O
a O
dataset O
based O
on O
Friends O
TV O
show O
, O
but O
the O
emotion O
- O
inventory O
is O
given O
as O
" O
joyful O
, O
peaceful O
, O
powerful O
, O
scared O
, O
mad O
, O
sad O
and O
neutral O
" O
. O
Sentiment O
labels O
are O
not O
provided O
, O
but O
sentiment O
classes O
can O
be O
grouped O
as O
follows O
: O
positive O
: O
{ O
joyful O
, O
peaceful O
, O
powerful O
} O
, O
negative O
: O
{ O
scared O
, O
mad O
, O
sad O
} O
, O
neutral O
: O
{ O
neutral O
} O

Training O
Setup O

We O
use O
the O
pre O
- O
trained O
model O
from O
the O
huggingface O
library O
2 O
. O
The O
optimizer B-HyperparameterName
is O
AdamW O
and O
the O
learning B-HyperparameterName
rate I-HyperparameterName
is O
1e-5 B-HyperparameterValue
as O
an O
initial O
value O
. O
The O
learning B-HyperparameterName
rate I-HyperparameterName
scheduler O
used O
for O
training O
is O
get_linear_schedule_with_warmup O
, O
and O
the O
maximum O
value O
of O
10 O
is O
used O
for O
the O
gradient O
clipping O
. O
We O
select O
the O
model O
with O
the O
best O
performance O
on O
the O
validation O
set O
. O
All O
experiments O
are O
conducted O
on O
one O
V100 O
GPU O
with O
32 O
GB O
memory O
. O

Previous O
Method O

We O
show O
that O
the O
proposed O
approach O
is O
effective O
by O
comparing O
it O
with O
various O
baselines O
and O
the O
stateof O
- O
the O
- O
art O
methods O
. O

KET O
( O
Zhong O
et O
al O
. O
, O
2019 O
) O
is O
a O
Knowledge O
Enriched O
Transformer O
that O
reflects O
contextual O
utterances O
with O
a O
hierarchical O
self O
- O
attention O
and O
leverages O
external O
commonsense O
knowledge O
by O
using O
a O
context O
- O
aware O
affective O
graph O
attention O
mechanism O
. O

DialogueRNN O
uses O
a O
GRU O
network O
to O
keep O
track O
of O
the O
individual O
party O
states O
in O
the O
conversation O
to O
predict O
emotions O
. O
This O
model O
assumes O
that O
there O
are O
three O
factors O
in O
emotion O
prediction O
: O
the O
speaker O
, O
the O
context O
from O
the O
preceding O
utterances O
and O
the O
emotion O
of O
the O
preceding O
utterances O
. O
Also O
, O
Ghosal O
et O
al O
. O
( O
2020 O
) O
shows O
the O
performance O
of O
RoBERTa+DialogueRNN B-HyperparameterName
when O
the O
vectors O
of O
the O
tokens O
are O
extracted O
with O
a O
pretrained O
RoBERTa B-HyperparameterName
. O

RGAT+P B-MethodName
( O
Ishiwatari O
et O
al O
. O
, O
2020 O
) O
( O
relational O
graph O
attention O
networks O
) O
proposes O
relational O
position O
encodings O
with O
sequential O
information O
reflecting O
the O
relational O
graph O
structure O
, O
which O
shows O
that O
both O
the O
speaker O
dependency O
and O
the O
sequential O
information O
can O
be O
captured O
. O

HiTrans B-MethodName
( O
Li O
et O
al O
. O
, O
2020 O
) O
Trans O
utilize O
BERT O
as O
the O
low O
- O
level O
transformer O
to O
generate O
local O
utterance O
representations O
, O
and O
feed O
them O
into O
another O
high O
- O
level O
transformer O
. O

COSMIC B-MethodName
( O
Ghosal O
et O
al O
. O
, O
2020 O
) O
incorporates O
different O
elements O
of O
commonsense O
such O
as O
mental O
states O
, O
events O
and O
causal O
relations O
, O
and O
learns O
the O
relations O
between O
participants O
in O
the O
conversation O
. O
This O
model O
uses O
pre O
- O
trained O
RoBERTa B-HyperparameterName
as O
a O
feature O
extractor O
and O
leverages O
COMET O
trained O
with O
ATOMIC O
as O
the O
commonsense O
knowledge O
. O
( O
Sun O
et O
al O
. O
, O
2021 O
) O
proposes O
a O
discourse O
- O
aware O
graph O
neural O
network O
that O
utilizes O
self O
- O
speaker O
dependency O
of O
interlocutors O
as O
a O
relational O
convolution O
and O
informative O
cues O
of O
dependent O
utterances O
as O
a O
gated O
convolution O
. O

ERMC B-MethodName
- I-MethodName
DisGCN I-MethodName

Psychological O
( O
Li O
et O
al O
. O
, O
2021 O
) O
uses O
commonsense O
knowledge O
as O
enrich O
edges O
and O
processes O
it O
with O
graph O
transformer O
. O
Psychological O
performs O
emotion O
recognition O
by O
utilizing O
intention O
of O
utterances O
from O
not O
only O
past O
contexts O
but O
also O
future O
context O
. O

DialogueCRN B-MethodName
( O
Hu O
et O
al O
. O
, O
2021 O
) O
introduces O
an O
intuitive O
retrieving O
process O
, O
the O
reasoning O
module O
, O
which O
understands O
both O
situation O
- O
level O
and O
speakerlevel O
contexts O
. O

ToDKAT B-MethodName
( O
Zhu O
et O
al O
. O
, O
2021 O
) O
proposes O
a O
language O
model O
with O
topic O
detection O
added O
, O
and O
improves O
performance O
by O
combining O
it O
with O
commonsense O
knowledge O
. O
The O
performance O
of O
ToDKAT B-MethodName
in O
MELD B-DatasetName
was O
re O
- O
released O
on O
github O
3 O
. O

Table O
2 O
shows O
the O
performance O
of O
the O
previous O
methods O
and O
our O
models O
. O
CoM O
used O
alone O
does O
not O
leverage O
PM O
and O
predicts O
emotions O
by O
considering O
only O
the O
dialogue O
context O
. O
PM O
used O
alone O
is O
not O
used O
as O
a O
memory O
module O
, O
but O
the O
same O
backbone O
is O
used O
. O
PM O
used O
alone O
predicts O
emotion O
only O
with O
the O
utterance O
of O
the O
current O
turn O
without O
considering O
the O
context O
. O
CoMPM B-MethodName
is O
a O
model O
in O
which O
both O
CoM O
and O
PM O
parameters O
are O
updated O
in O
the O
initial O
state O
of O
the O
pre O
- O
trained O
LM O
. O
CoMPM B-MethodName
( O
f O
) O
is O
a O
model O
in O
which O
PM O
parameters O
are O
frozen O
in O
the O
initial O
state O
( O
pre O
- O
trained O
LM O
) O
and O
is O
not O
trained O
further O
, O
and O
CoMPM B-MethodName
( O
s O
) O
is O
a O
model O
in O
which O
PM O
is O
trained O
from O
scratch O
. O
CoMPM B-MethodName
( O
k O
) O
is O
a O
model O
in O
which O
PM O
is O
trained O
on O
ConceptNet O
. O
Following O
previous O
studies O
, O
we O
use O
the O
average O
vector O
for O
each O
token O
in O
PM O
( O
k O
) O
as O
the O
feature O
of O
the O
utterance O
. O
We O
use O
the O
pre O
- O
trained O
model O
provided O
by O
the O
site O
4 O
as O
PM O
( O
k O
) O
. O

The O
effect O
of O
PM O
can O
be O
confirmed O
through O
the O
performance O
comparison O
between O
CoM O
and O
CoMPM O
, O
and O
the O
effect O
of O
CoM O
can O
be O
confirmed O
by O
comparing O
the O
results O
of O
CoM O
and O
PM O
. O
Since O
PM O
does O
not O
consider O
context O
, O
it O
showed O
worse O
performance O
than O
CoM O
, O
and O
the O
performance O
gap O
is O
larger O
in O
the O
IEMOCAP B-DatasetName
dataset O
with O
a O
higher O
average O
number O
of O
conversation O
turns O
. O
As O
a O
result O
, O
we O
show O
that O
the O
combination O
of O
CoM O
and O
PM O
is O
effective O
in O
achieving O
better O
performance O
. O

We O
confirm O
the O
effect O
of O
PM O
structure O
in O
the O
model O
through O
the O
performance O
of O
CoMPM B-MethodName
( O
s O
) O
. O

If O
PM O
parameters O
are O
not O
frozen O
and O
are O
instead O
randomly O
initialized O
( O
i.e. O
PM O
( O
s O
) O
) O
, O
the O
performance O
deteriorates O
. O
CoMPM B-MethodName
( O
s O
) O
performs O
worse O
than O
CoMPM B-MethodName
, O
and O
even O
performs O
worse O
than O
CoM O
on O
the O
other O
datasets O
except O
for O
MELD O
. O
That O
is O
, O
PM O
( O
s O
) O
can O
not O
be O
regarded O
as O
a O
pre O
- O
trained O
memory O
because O
the O
parameters O
are O
randomly O
initialized O
, O
and O
simply O
increasing O
the O
model O
complexity O
does O
not O
help O
to O
improve O
the O
performance O
. O
CoMPM B-MethodName
( O
f O
) O
shows O
similar O
performance O
to O
CoMPM B-MethodName
and O
achieves O
better O
performance O
depending O
on O
the O
data O
. O
PM O
( O
f O
) O
is O
not O
fine O
- O
tuned O
on O
the O
data O
, O
but O
it O
extracts O
general O
pre O
- O
trained O
memory O
from O
a O
pretrained O
language O
model O
. O
The O
comparison O
between O
PM O
and O
PM O
( O
f O
) O
will O
be O
further O
described O
in O
Section O
4.6 O
. O
In O
addition O
, O
CoMPM B-MethodName
( O
k O
) O
shows O
better O
performance O
than O
CoM O
, O
PM O
, O
and O
CoMPM B-MethodName
( O
s O
) O
except O
for O
IEMOCAP B-DatasetName
. O
In O
IEMOCAP B-DatasetName
, O
CoMPM B-MethodName
( O
k O
) O
has O
lower O
performance O
than O
CoM. O
For O
all O
datasets O
, O
CoMPM B-MethodName
( O
k O
) O
performs O
slightly O
worse O
than O
CoMPM B-MethodName
. O
In O
other O
words O
, O
ConceptNet O
improves O
the O
performance O
of O
CoMPM B-MethodName
, O
but O
is O
not O
as O
effective O
as O
pretrained O
memory O
. O
As O
a O
result O
, O
we O
regard O
pre O
- O
trained O
memory O
as O
compressed O
knowledge O
, O
which O
can O
play O
a O
role O
similar O
to O
external O
knowledge O
used O
in O
cuttingedge O
systems O
. O

The O
best O
performance O
of O
our O
approaches O
is O
CoMPM B-MethodName
or O
CoMPM B-MethodName
( O
f O
) O
, O
both O
of O
which O
combine O
pre O
- O
trained O
memory O
. O
We O
achieve O
state O
- O
of O
- O
the O
- O
art O
performance O
among O
all O
systems O
that O
do O
not O
leverage O
structured O
external O
data O
and O
achieve O
the O
first O
or O
second O
performance O
even O
including O
systems O
that O
leverage O
external O
data O
. O
Therefore O
, O
our O
approach O
can O
be O
extended O
to O
other O
languages O
without O
structured O
external O
data O
as O
well O
, O
which O
is O
described O
in O
Section O
4.7 O
. O

Combinations O
of O
CoM O
and O
PM O

We O
experiment O
with O
the O
effect O
of O
pre O
- O
trained O
memory O
of O
different O
language O
models O
. O
To O
eliminate O
the O
influence O
of O
the O
PM O
structure O
, O
we O
freeze O
the O
parameters O
of O
PM O
and O
use O
it O
as O
a O
feature O
extractor O
. O
Table O
3 O
shows O
the O
performance O
of O
the O
pretrained O
memory O
extracted O
by O
the O
different O
language O
models O
. O
If O
PM O
and O
CoM O
are O
based O
on O
different O
backbones O
, O
the O
pre O
- O
trained O
memory O
is O
projected O
through O
W O
p O
as O
the O
dimension O
of O
the O
context O
output O
. O
RoBERTa+BERT B-HyperparameterName
and O
RoBERTa+GPT2 B-HyperparameterName
( O
combination O
of O
CoM O
and O
PM O
( O
f O
) O
) O
have O
lower O
performance O
than O
RoBERTa+RoBERTa B-HyperparameterName
, O
which O
is O
inferred O
because O
pre O
- O
trained O
memory O
of O
RoBERTa B-HyperparameterName
contains O
richer O
information O
than O
BERT O
and O
GPT2 O
. O
Since O
there O
is O
a O
lot O
of O
training O
data O
in O
the O
diallydialog O
and O
W O
p O
is O
fine O
- O
tuned O
to O
the O
data O
to O
mutually O
understand O
the O
pre O
- O
trained O
memory O
and O
context O
representation O
. O
Therefore O
, O
we O
infer O
that O
performance O
does O
not O
decrease O
even O
if O
the O
PM O
changes O
from O
the O
dailydialog O
. O
However O
, O
even O
if O
other O
PMs O
are O
used O
, O
the O
performance O
is O
improved O
compared O
to O
using O
only O
CoM O
, O
so O
the O
pre O
- O
trained O
memory O
of O
other O
language O
models O
is O
also O
effective O
for O
emotion O
recognition O
. O

BERT+RoBERTa B-HyperparameterName
has O
a O
larger O
performance O
decrease O
than O
RoBERTa+BERT B-HyperparameterName
. O
In O
particular O
, O
in O
IEMOCAP B-DatasetName
data O
with O
a O
long O
average O
number O
of O
turns O
in O
the O
context O
, O
the O
performance O
deteriorates O
significantly O
. O
In O
addition O
, O
the O
performance O
of O
BERT+RoBERTa B-HyperparameterName
is O
lower O
than O
CoM O
( O
RoBERTa B-HyperparameterName
) O
, O
which O
supports O
that O
the O
performance O
of O
CoM O
is O
a O
more O
important O
factor O
than O
the O
use O
of O
pre O
- O
trained O
memory O
. O
In O
other O
words O
, O
we O
confirm O
that O
CoM O
is O
more O
important O
than O
PM O
in O
our O
system O
for O
performance O
, O
and O
it O
is O
effective O
to O
focus O
on O
context O
modeling O
rather O
than O
external O
knowledge O
in O
the O
study O
of O
emotion O
recognition O
in O
conversation O
. O

Training O
with O
Less O
Data O

CoMPM B-MethodName
is O
an O
approach O
that O
eliminates O
dependence O
on O
external O
sources O
and O
is O
easily O
extensible O
to O
any O
language O
. O
However O
, O
the O
insufficient O
number O
of O
emotional O
data O
available O
in O
other O
countries O
( O
or O
actual O
service O
) O
remains O
a O
problem O
. O
Therefore O
, O
we O
conduct O
additional O
experiments O
according O
to O
the O
use O
ratio O
of O
training O
data O
in O
MELD B-DatasetName
and O
EmoryNLP B-DatasetName
, O
where O
there O
is O
neither O
too O
much O
nor O
too O
little O
data O
. O
Figure O
3 O
shows O
the O
performance O
of O
the O
model O
according O
to O
the O
ratio O
of O
the O
training O
data O
. O
In O
MELD B-DatasetName
and O
EmoryNLP B-DatasetName
, O
even O
if O
only O
60 O
% O
and O
80 O
% O
are O
used O
, O
respectively O
, O
the O
performance O
decreases O
by O
only O
3 O
points O
. O
The O
value O
in O
parentheses O
is O
the O
performance O
difference O
from O
the O
original O
CoMPM B-MethodName
( O
f O
) O
( O
RoBERTa B-HyperparameterName
+ I-HyperparameterName
RoBERTa I-HyperparameterName
) O
. O
We O
use O
the O
bert O
- O
large O
- O
uncased O
and O
GPT2 O
- O
medium O
versions O
. O

Table O
2 O
shows O
that O
CoMPM B-MethodName
( O
f O
) O
achieves O
better O
performance O
than O
CoMPM B-MethodName
in O
the O
emotion O
classification O
of O
IMEOCAP B-DatasetName
and O
EmoryNLP B-DatasetName
, O
which O
has O
fewer O
training O
data O
than O
other O
settings O
. O
On O
the O
other O
hand O
, O
if O
there O
is O
a O
lot O
of O
training O
data O
, O
CoMPM B-MethodName
shows O
better O
performance O
. O
Figure O
3 O
shows O
that O
as O
the O
number O
of O
data O
decreases O
, O
CoMPM B-MethodName
( O
f O
) O
shows O
better O
results O
than O
CoMPM B-MethodName
, O
which O
indicates O
that O
it O
is O
better O
to O
freeze O
the O
parameters O
of O
PM O
when O
the O
number O
of O
training O
data O
is O
insufficient O
. O
Therefore O
, O
if O
there O
is O
a O
lot O
of O
training O
data O
in O
the O
real O
- O
world O
application O
, O
CoMPM B-MethodName
is O
expected O
to O
achieve O
good O
performance O
, O
otherwise O
it O
is O
CoMPM B-MethodName
( O
f O
) O
. O

ERC B-TaskName
in O
other O
languages O

Previous O
studies O
mostly O
utilize O
external O
knowledge O
to O
improve O
performance O
, O
but O
these O
approaches O
require O
additional O
publicly O
available O
data O
, O
which O
are O
mainly O
available O
for O
English O
. O
Indeed O
, O
structured O
knowledge O
and O
ERC B-TaskName
data O
are O
lacking O
in O
other O
languages O
. O
Our O
approach O
can O
be O
extended O
to O
other O
languages O
without O
building O
additional O
external O
knowledge O
and O
achieves O
better O
performance O
than O
simply O
using O
a O
pre O
- O
trained O
model O
. O

Korean O
Dataset O

We O
constructed O
data O
composed O
of O
two O
speakers O
in O
Korean O
, O
and O
emotion O
- O
inventory O
is O
given O
as O
" O
surprise O
, O
fear O
, O
ambiguous O
, O
sad O
, O
disgust O
, O
joy O
, O
bored O
, O
embarrassed O
, O
neutral O
" O
. O
The O
total O
number O
of O
sessions O
is O
1000 O
, O
and O
the O
average O
number O
of O
utterance O
turns O
is O
13.4 O
. O
We O
use O
the O
data O
randomly O
divided O
into O
train B-HyperparameterName
: I-HyperparameterName
dev I-HyperparameterName
: I-HyperparameterName
test I-HyperparameterName
in O
a O
ratio O
of O
8:1:1 B-HyperparameterValue
. O
This O
dataset O
is O
for O
actual O
service O
and O
is O
not O
released O
to O
the O
public O
. O

Results O
in O
the O
Korean O
Dataset O

In O
Korean O
, O
our O
results O
are O
shown O
in O
Table O
4 O
. O
The O
backbone O
of O
CoM O
and O
PM O
is O
Korean B-HyperparameterName
- I-HyperparameterName
BERT I-HyperparameterName
owned O
by O
the O
company O
, O
respectively O
. O
In O
the O
Korean O
dataset O
, O
like O
the O
English O
dataset O
, O
the O
performance O
is O
good O
in O
the O
order O
of O
CoMPM B-MethodName
, O
CoM O
, O
and O
PM O
. O
Our O
approach O
simply O
shows O
a O
significant O
performance O
improvement O
on O
baselines O
that O
are O
fine O
- O
tuned O
to O
the O
language O
model O
and O
works O
well O
for O
other O
languages O
as O
well O
as O
for O
English O
. O

Conclusion O

We O
propose O
CoMPM B-MethodName
that O
leverages O
pre O
- O
trained O
memory O
using O
a O
pre O
- O
trained O
language O
model O
. O
CoMPM B-MethodName
consists O
of O
a O
context O
embedding O
module O
( O
CoM O
) O
and O
a O
pre O
- O
trained O
memory O
module O
( O
PM O
) O
, O
and O
the O
experimental O
results O
show O
that O
each O
module O
is O
effective O
in O
improving O
the O
performance O
. O
CoMPM B-MethodName
outperforms O
baselines O
on O
both O
dyadic O
- O
party O
and O
multi O
- O
party O
datasets O
and O
achieves O
state O
- O
of O
- O
the O
- O
art O
among O
systems O
that O
do O
not O
use O
external O
knowledge O
. O
In O
addition O
, O
CoMPM B-MethodName
achieves O
performance O
comparable O
to O
cutting O
- O
edge O
systems O
that O
leverage O
structured O
external O
knowledge O
, O
which O
is O
the O
effect O
of O
pre O
- O
trained O
memory O
of O
the O
language O
model O
. O
By O
combining O
other O
pre O
- O
trained O
memories O
, O
we O
find O
that O
the O
pre O
- O
trained O
memory O
extracted O
with O
RoBERTa B-HyperparameterName
is O
richer O
and O
more O
effective O
than O
the O
pre O
- O
trained O
memory O
extracted O
with O
BERT B-HyperparameterName
or O
GPT2 B-HyperparameterName
. O
Since O
we O
believe O
that O
pre O
- O
trained O
memory O
is O
proportional O
to O
the O
performance O
of O
a O
language O
model O
, O
a O
language O
model O
with O
a O
large O
training O
corpus O
and O
many O
parameters O
is O
considered O
to O
be O
more O
effective O
. O
However O
, O
we O
find O
that O
context O
modeling O
is O
more O
important O
than O
pre O
- O
trained O
memory O
for O
emotion B-TaskName
recognition I-TaskName
in I-TaskName
conversation I-TaskName
, O
and O
future O
research O
will O
focus O
on O
context O
modeling O
. O

Additionally O
, O
our O
approach O
achieves O
competitive O
performance O
and O
does O
not O
require O
externally O
structured O
data O
. O
Therefore O
, O
we O
show O
that O
it O
can O
be O
easily O
extended O
to O
Korean O
as O
well O
as O
English O
, O
and O
it O
is O
expected O
to O
be O
effective O
in O
other O
countries O
. O

" O
Nice O
Try O
, O
Kiddo O
" O
: O
Investigating O
Ad B-TaskName
Hominems I-TaskName
in O
Dialogue O
Responses O

Ad B-TaskName
hominem I-TaskName
attacks I-TaskName
are O
those O
that O
target O
some O
feature O
of O
a O
person O
's O
character O
instead O
of O
the O
position O
the O
person O
is O
maintaining O
. O
These O
attacks O
are O
harmful O
because O
they O
propagate O
implicit O
biases O
and O
diminish O
a O
person O
's O
credibility O
. O
Since O
dialogue O
systems O
respond O
directly O
to O
user O
input O
, O
it O
is O
important O
to O
study O
ad O
hominems O
in O
dialogue O
responses O
. O
To O
this O
end O
, O
we O
propose O
categories O
of O
ad B-TaskName
hominems I-TaskName
, O
compose O
an O
annotated O
dataset O
, O
and O
build O
a O
classifier O
to O
analyze O
human O
and O
dialogue O
system O
responses O
to O
English O
Twitter O
posts O
. O
We O
specifically O
compare O
responses O
to O
Twitter O
topics O
about O
marginalized O
communities O
( O
# O
Black O
- O
LivesMatter O
, O
# O
MeToo O
) O
versus O
other O
topics O
( O
# O
Vegan O
, O
# O
WFH O
) O
, O
because O
the O
abusive O
language O
of O
ad O
hominems O
could O
further O
amplify O
the O
skew O
of O
power O
away O
from O
marginalized O
populations O
. O
Furthermore O
, O
we O
propose O
a O
constrained O
decoding O
technique O
that O
uses O
salient O
n O
- O
gram O
similarity O
as O
a O
soft O
constraint O
for O
top O
- O
k O
sampling O
to O
reduce O
the O
amount O
of O
ad O
hominems O
generated O
. O
Our O
results O
indicate O
that O
1 O
) O
responses O
from O
both O
humans O
and O
DialoGPT B-MethodName
contain O
more O
ad O
hominems O
for O
discussions O
around O
marginalized O
communities O
, O
2 O
) O
different O
quantities O
of O
ad O
hominems O
in O
the O
training O
data O
can O
influence O
the O
likelihood O
of O
generating O
ad O
hominems O
, O
and O
3 O
) O
we O
can O
use O
constrained O
decoding O
techniques O
to O
reduce O
ad B-TaskName
hominems I-TaskName
in O
generated O
dialogue O
responses O
. O
Post O
: O
Many O
are O
trying O
to O
co O
- O
opt O
and O
mischaracterize O
the O
# O
blacklivesmatter O
movement O
. O
We O
wo O
n't O
allow O
it O
! O
Resp O
: O
I O
hate O
how O
much O
of O
a O
victim O
complex O
you O
guys O
have O
. O

Introduction O

Ad B-TaskName
hominems I-TaskName
attack O
an O
opponent O
's O
character O
or O
identity O
instead O
of O
the O
points O
the O
opponent O
is O
making O
, O
and O
can O
exist O
in O
any O
conversational O
setting O
between O
two O
or O
more O
entities O
. O
From O
an O
argumentation O
perspective O
, O
ad O
hominems O
are O
fallacies O
, O
and O
fallacies O
rely O
on O
faulty O
reasoning O
to O
advance O
a O
point O
( O
Hansen O
, O
2020 O
) O
. O
These O
ad B-TaskName
hominem I-TaskName
fallacies O
are O
related O
to O
abusive O
language O
, O
toxicity O
, O
and O
microaggressions O
, O
and O
can O
be O
expressed O
with O
both O
subtle O
and O
explicitly O
offensive O
language O
. O
Table O
1 O
presents O
examples O
of O
ad B-TaskName
hominem I-TaskName
responses O
to O
Twitter O
posts O
. O
Undesirable O
in O
any O
response O
, O
ad B-TaskName
hominems I-TaskName
are O
unproductive O
in O
furthering O
a O
meaningful O
discussion O
and O
can O
reinforce O
falsehoods O
. O
However O
, O
these O
attacks O
appeal O
to O
emotions O
and O
implicit O
biases O
to O
argue O
a O
point O
, O
and O
are O
thus O
often O
effectively O
harmful O
regardless O
of O
whether O
the O
attacks O
are O
true O
, O
recognized O
, O
or O
retracted O
( O
Yap O
, O
2013 O
) O
. O

Our O
work O
is O
motivated O
by O
this O
fallacy O
's O
potential O
to O
amplify O
the O
spread O
of O
harmful O
societal O
biases O
. O
For O
communities O
that O
are O
already O
disproportionately O
harmed O
by O
societal O
power O
inequalities O
, O
ad B-TaskName
hominems I-TaskName
further O
amplify O
the O
power O
imbalance O
. O
Tone O
policing O
is O
a O
type O
of O
ad O
hominem O
that O
seeks O
to O
regulate O
the O
emotions O
that O
a O
person O
( O
usually O
of O
a O
marginalized O
population O
) O
can O
use O
to O
deliver O
their O
points O
( O
e.g. O
, O
not O
too O
angrily O
) O
, O
thereby O
altogether O
invalidating O
the O
style O
of O
delivery O
, O
the O
person O
's O
competence O
, O
and O
the O
points O
being O
conveyed O
. O
Besides O
directly O
experiencing O
ad O
hominem O
attacks O
, O
marginalized O
groups O
could O
also O
be O
disproportionately O
discouraged O
from O
using O
technologies O
that O
propagate O
these O
attacks O
, O
since O
abusive O
language O
from O
a O
technology O
can O
deter O
people O
from O
using O
the O
technology O
( O
Sood O
et O
al O
. O
, O
2012b O
) O
. O

The O
goal O
of O
this O
study O
is O
to O
analyze O
ad B-TaskName
hominems I-TaskName
in O
dialogue O
system O
- O
and O
human O
- O
generated O
responses O
for O
topics O
that O
vary O
in O
impact O
to O
marginalized O
populations O
. O
Through O
analysis O
, O
we O
formulate O
techniques O
to O
reduce O
ad O
hominem O
responses O
and O
thus O
the O
associated O
harms O
, O
which O
is O
especially O
important O
for O
dialogue O
systems O
since O
these O
systems O
directly O
interact O
with O
users O
. O

We O
analyze O
responses O
from O
DialoGPT B-MethodName
( O
Zhang O
et O
al O
. O
, O
2020a O
) O
and O
humans B-MethodName
to O
English O
Twitter O
posts O
. O
Specifically O
, O
we O
compare O
responses O
to O
Twitter O
topics O
about O
marginalized O
communities O
( O
# O
Black O
- O
LivesMatter O
, O
# O
MeToo O
) O
versus O
other O
topics O
( O
# O
Vegan O
, O
# O
WFH O
) O
. O
Through O
human O
annotation O
and O
trained O
classifiers O
, O
we O
find O
that O
ad B-TaskName
hominems I-TaskName
exist O
in O
both O
human B-MethodName
and O
DialoGPT B-MethodName
responses O
. O
Across O
response O
sources O
, O
there O
are O
more O
ad O
hominems O
in O
# O
Black O
- O
LivesMatterand O
# O
MeToo O
- O
related O
responses O
, O
fewer O
in O
# O
Vegan O
- O
related O
responses O
, O
and O
even O
fewer O
in O
# O
WFH O
- O
related O
responses O
. O
The O
presence O
of O
more O
ad B-TaskName
hominems I-TaskName
in O
responses O
to O
social O
issues O
that O
concern O
marginalized O
groups O
has O
troubling O
implications O
about O
the O
amplified O
harms O
toward O
these O
groups O
. O

Given O
our O
analysis O
, O
we O
further O
propose O
a O
constrained O
decoding O
algorithm O
to O
reduce O
the O
amount O
of O
ad O
hominems O
generated O
by O
dialogue O
systems O
. O
By O
using O
salient O
n O
- O
gram O
similarity O
to O
apply O
soft O
constraints O
to O
top O
- O
k O
sampling O
, O
our O
proposed O
technique O
is O
simple O
, O
extensible O
to O
reducing O
other O
harms O
, O
and O
does O
not O
require O
much O
additional O
computation O
. O
At O
each O
decoding O
time O
step O
, O
the O
technique O
compares O
the O
similarity O
between O
the O
current O
generated O
output O
and O
salient O
ad O
hominem O
versus O
non O
- O
ad B-TaskName
hominem I-TaskName
n O
- O
grams O
, O
possibly O
selecting O
alternative O
token O
candidates O
to O
generate O
. O
This O
technique O
is O
effective O
at O
reducing O
the O
amount O
of O
ad B-TaskName
hominems I-TaskName
generated O
across O
topics O
while O
maintaining O
coherence O
and O
relevance O
. O

Our O
main O
contribution O
is O
a O
novel O
analysis O
of O
ad B-TaskName
hominem I-TaskName
responses O
generated O
by O
humans B-MethodName
and O
Di B-MethodName
- I-MethodName
aloGPT I-MethodName
across O
topics O
varying O
in O
impact O
to O
marginalized O
communities O
. O
For O
this O
analysis O
, O
we O
propose O
empirically O
- O
derived O
ad O
hominem O
categories O
that O
are O
further O
verified O
through O
annotation O
. O
Furthermore O
, O
we O
build O
a O
new O
dataset O
of O
Twitter O
posts O
paired O
with O
human O
- O
and O
DialoGPT B-MethodName
- O
generated O
responses O
, O
where O
the O
responses O
have O
ad O
hominem O
- O
related O
labels O
. O
Finally O
, O
we O
devise O
a O
constrained O
decoding O
technique O
that O
uses O
salient O
n O
- O
gram O
similarity O
to O
steer O
top O
- O
k O
sampling O
away O
from O
ad B-MethodName
hominem I-MethodName
responses O
. O
We O
release O
data O
and O
code O
at O
https O
: O
/ O
/ O
github.com O
/ O
ewsheng O
/ O
ad O
- O
hom O
- O
in O
- O
dialogue O
. O

Related O
Work O

This O
work O
is O
related O
to O
a O
broad O
spectrum O
of O
topics O
, O
including O
prior O
definitions O
of O
ad B-TaskName
hominems I-TaskName
and O
how O
ad O
hominems O
facilitate O
biases O
. O
Also O
, O
analyzing O
ad B-TaskName
hominems I-TaskName
in O
dialogue O
systems O
is O
related O
to O
examining O
offensive O
language O
and O
other O
harms O
. O
Lastly O
, O
we O
discuss O
existing O
constrained O
decoding O
methods O
. O

Ad B-TaskName
Hominems I-TaskName
In O
the O
argumentation O
literature O
, O
theoretical O
ad B-TaskName
hominems I-TaskName
include O
the O
abusive O
( O
attack O
on O
the O
opponent O
's O
character O
) O
, O
tu O
quoque O
( O
" O
he O
did O
it O
first O
" O
) O
, O
circumstantial O
( O
accusation O
of O
hypocrisy O
) O
, O
and O
guilt O
by O
association O
( O
associating O
the O
opponent O
with O
someone O
with O
low O
credibility O
) O
( O
Walton O
, O
1998 O
; O
Woods O
, O
2007 O
) O
. O
Wijze O
( O
2003 O
) O
criticizes O
that O
these O
textbook O
examples O
are O
not O
realistic O
in O
conversation O
. O
For O
more O
empirical O
categories O
, O
Habernal O
et O
al O
. O
( O
2018 O
) O
propose O
ad B-TaskName
hominem I-TaskName
types O
based O
on O
analysis O
of O
Reddit O
's O
ChangeMyView O
discussion O
threads O
, O
and O
Delobelle O
et O
al O
. O
( O
2019 O
) O
analyze O
the O
name O
- O
calling O
and O
abusive O
categories O
. O
Moreover O
, O
Wulczyn O
et O
al O
. O
( O
2017 O
) O
use O
classifiers O
for O
a O
largescale O
analysis O
of O
personal O
attacks O
in O
Wikipedia O
comments O
. O
We O
build O
upon O
prior O
works O
to O
define O
and O
analyze O
ad O
hominems O
in O
a O
conversational O
setting O
. O

Additionally O
, O
Yap O
( O
2013 O
) O
discusses O
the O
harmful O
effects O
of O
implicit O
biases O
in O
forming O
and O
evaluating O
ad O
hominems O
. O
They O
emphasize O
that O
ad O
hominem O
attacks O
can O
be O
harmful O
to O
a O
person O
's O
credibility O
and O
expertise O
even O
if O
the O
attack O
is O
recognized O
as O
fallacious O
and O
irrelevant O
to O
the O
argument O
. O
In O
particular O
, O
because O
societal O
norms O
allow O
biases O
and O
stereotypes O
to O
detract O
from O
a O
person O
's O
credibility O
or O
expertise O
, O
the O
use O
of O
ad B-TaskName
hominems I-TaskName
can O
further O
diminish O
the O
rhetorical O
credibility O
( O
Govier O
, O
1993 O
) O
of O
marginalized O
groups O
. O

Offensive O
Language O
Detection O
Ad B-TaskName
hominems I-TaskName
occur O
in O
many O
forms O
and O
are O
related O
to O
different O
types O
of O
offensive O
language O
, O
including O
abusive O
language O
( O
Yin O
et O
al O
. O
, O
2009 O
; O
Chen O
et O
al O
. O
, O
2012 O
; O
Nobata O
et O
al O
. O
, O
2016 O
) O
, O
hate O
speech O
( O
Warner O
and O
Hirschberg O
, O
2012 O
; O
Kwok O
and O
Wang O
, O
2013 O
; O
Djuric O
et O
al O
. O
, O
2015 O
) O
, O
profanity O
( O
Sood O
et O
al O
. O
, O
2012a O
) O
, O
and O
the O
more O
subtle O
forms O
of O
microaggressions O
( O
Breitfeller O
et O
al O
. O
, O
2019 O
) O
and O
projecting O
biases O
and O
stereotypes O
through O
power O
differentials O
in O
language O
. O
Ranging O
from O
outright O
insults O
to O
condescension O
, O
ad O
hominems O
are O
a O
form O
of O
offensive O
language O
that O
is O
difficult O
to O
comprehensively O
and O
objectively O
define O
. O
Nonetheless O
, O
these O
responses O
are O
important O
to O
characterize O
, O
since O
they O
can O
irreparably O
damage O
a O
person O
's O
credibility O
. O
It O
is O
also O
generally O
important O
to O
identify O
these O
subtle O
forms O
of O
offensive O
language O
, O
since O
it O
is O
unclear O
if O
existing O
offensive O
language O
detection O
techniques O
are O
equally O
effective O
for O
these O
subtle O
forms O
. O

Harms O
in O
Dialogue O
Systems O

Conversational O
systems O
are O
known O
to O
perpetuate O
several O
types O
of O
harms O
. O
Ruane O
et O
al O
. O
( O
2019 O
) O
caution O
about O
harms O
that O
can O
result O
from O
using O
conversational O
systems O
and O
propose O
striving O
for O
trust O
and O
transparency O
; O
Roller O
et O
al O
. O
( O
2020 O
) O
suggest O
techniques O
for O
chatbot O
safety O
. O
For O
analysis O
, O
Sheng O
et O
al O
. O
( O
2019 O
) O
evaluate O
societal O
biases O
in O
language O
generation O
, O
Curry O
and O
Rieser O
( O
2018 O
) O
study O
how O
conversational O
systems O
respond O
to O
sexual O
harassment O
, O
and O
Khatri O
et O
al O
. O
( O
2018 O
) O
detect O
offensive O
content O
with O
a O
semi O
- O
supervised O
approach O
. O
To O
reduce O
harms O
, O
Sheng O
et O
al O
. O
( O
2020 O
) O
present O
a O
framework O
for O
controlling O
biases O
in O
language O
generation O
, O
and O
Dinan O
et O
al O
. O
( O
2019 O
) O
show O
how O
adversarial O
attacks O
can O
make O
models O
more O
robust O
to O
offensive O
language O
usage O
from O
humans O
. O
Constrained O
Decoding O
For O
constrained O
decoding O
, O
prior O
works O
focus O
on O
incorporating O
words O
or O
phrases O
( O
as O
hard O
or O
soft O
constraints O
) O
into O
the O
decoded O
output O
. O
Swanson O
et O
al O
. O
( O
2014 O
) O
and O
Balakrishnan O
et O
al O
. O
( O
2019 O
) O
use O
parse O
trees O
among O
other O
techniques O
to O
enforce O
constraints O
in O
the O
generated O
text O
. O
Hokamp O
and O
Liu O
( O
2017 O
) O
; O
Post O
and O
Vilar O
( O
2018 O
) O
propose O
variants O
of O
Grid O
Beam O
Search O
, O
which O
generate O
output O
that O
include O
lexical O
constraints O
. O
Miao O
et O
al O
. O
( O
2019 O
) O
; O
Zhang O
et O
al O
. O
( O
2020b O
) O
; O
Susanto O
et O
al O
. O
( O
2020 O
) O
explore O
insertion O
- O
based O
non O
- O
autoregressive O
decoding O
algorithms O
. O
To O
be O
compatible O
with O
an O
autoregressive O
model O
like O
DialoGPT B-MethodName
and O
effective O
for O
open O
- O
domain O
generation O
, O
we O
apply O
constrained O
decoding O
to O
top O
- O
k O
sampling O
. O
Our O
method O
also O
differs O
from O
these O
prior O
works O
in O
that O
it O
imposes O
soft O
constraints O
to O
not O
generate O
phrases O
that O
are O
likely O
to O
lead O
to O
ad O
hominems O
. O
Decoding O
- O
time O
techniques O
that O
can O
be O
used O
to O
reduce O
harmful O
language O
generation O
, O
e.g. O
, O
the O
Plug O
and O
Play O
Language O
Model O
( O
PPLM O
) O
( O
Dathathri O
et O
al O
. O
, O
2020 O
) O
, O
are O
most O
relevant O
to O
our O
technique O
. O

Dataset O
and O
Model O
Setup O

This O
section O
describes O
the O
dataset O
collection O
process O
and O
the O
dialogue O
model O
variations O
we O
analyze O
. O
Dataset O
Collection O
Our O
goal O
is O
to O
understand O
how O
ad O
hominem O
responses O
differ O
across O
discussions O
that O
vary O
in O
impact O
and O
relevance O
to O
marginalized O
groups O
. O
To O
that O
end O
, O
we O
extract O
English O
[ O
post O
, O
response O
] O
pairs O
on O
different O
topics O
from O
Twitter O
and O
also O
use O
DialoGPT B-MethodName
to O
generate O
responses O
for O
all O
collected O
posts O
. O
We O
refer O
to O
this O
collective O
dataset O
as O
the O
ADHOMINTWEETS B-DatasetName
dataset O
. O

Relevant O
topics O
are O
divided O
into O
polarizing O
( O
i.e. O
, O
upon O
the O
work O
of O
Habernal O
et O
al O
. O
( O
2018 O
) O
to O
devise O
ad O
hominem O
categories O
that O
are O
both O
empiricallymotivated O
and O
can O
be O
annotated O
with O
high O
interannotator O
agreement O
. O
We O
specifically O
include O
categories O
such O
as O
" O
ignorance O
" O
and O
" O
condescension O
" O
to O
cover O
more O
subtle O
forms O
of O
personal O
attacks O
( O
e.g. O
, O
tone O
policing O
, O
mansplaining O
) O
that O
could O
further O
diminish O
the O
credibility O
of O
those O
who O
are O
already O
marginalized O
. O
We O
also O
limit O
the O
definition O
of O
ad B-TaskName
hominem I-TaskName
to O
personal O
attacks O
towards O
the O
author O
of O
the O
post O
and O
not O
a O
third O
person O
. O

Human O
Annotation O

We O
collect O
human O
annotations O
that O
can O
then O
be O
used O
for O
analysis O
and O
training O
a O
classifier O
to O
automatically O
label O
ad O
hominems O
. O
Although O
Habernal O
et O
al O
. O
( O
2018 O
) O
propose O
a O
similar O
typology O
of O
ad O
hominems O
, O
there O
is O
no O
existing O
dataset O
annotated O
with O
their O
empirically O
- O
derived O
categories O
. O
Moreover O
, O
we O
study O
ad O
hominems O
in O
casual O
conversational O
settings O
. O
For O
these O
reasons O
, O
we O
annotate O
a O
subset O
of O
ADHOMINTWEETS B-DatasetName
with O
ad B-TaskName
hominem I-TaskName
information O
. O
To O
measure O
inter O
- O
annotator O
agreement O
, O
we O
calculate O
the O
Worker O
Agreement O
With O
Aggregate O
( O
WAWA O
) O
score O
, O
following O
Ning O
et O
al O
. O
( O
2020 O
) O
. O
The O
WAWA O
score O
compares O
the O
majority O
votes O
against O
each O
annotator O
and O
micro O
- O
averages O
the O
resulting O
precision O
, O
recall O
, O
and O
F B-MetricName
1 B-MetricName
scores O
. O
5 O
Heuristics O
for O
Ad O
Hominems O
Ad O
hominem O
responses O
are O
relatively O
rare O
and O
range O
broadly O
from O
explicit O
to O
more O
subtle O
forms O
. O
For O
more O
effective O
annotation O
, O
we O
use O
heuristics O
to O
choose O
[ O
post O
, O
response O
] O
pairs O
where O
the O
response O
is O
likely O
to O
be O
an O
ad O
hominem O
. O
In O
preliminary O
analyses O
, O
we O
find O
that O
responses O
that O
contain O
certain O
" O
you O
" O
-phrases O
such O
5 O
There O
are O
also O
other O
agreement O
metrics O
such O
as O
Krippendorff O
's O
alpha O
, O
but O
because O
we O
expect O
our O
data O
to O
have O
many O
more O
non O
- O
ad O
hominem O
compared O
to O
ad O
hominem O
responses O
, O
alpha O
scores O
can O
be O
misleading O
- O
the O
WAWA O
score O
gives O
a O
more O
appropriate O
estimate O
of O
annotator O
agreement O
. O

as O
" O
you O
are O
" O
are O
more O
likely O
to O
have O
ad B-TaskName
hominems I-TaskName
. O
We O
call O
these O
responses O
you O
- O
responses O
. O
6 O
In O
addition O
to O
pairs O
with O
you O
- O
responses O
, O
we O
also O
collect O
random O
pairs O
without O
you O
- O
responses O
for O
annotation O
to O
ensure O
that O
the O
annotated O
samples O
are O
representative O
of O
different O
ad O
hominems O
. O
Annotation O
Task O
We O
ask O
annotators O
on O
Mechanical O
Turk O
to O
read O
a O
post O
and O
response O
and O
determine O
whether O
the O
response O
contains O
any O
ad O
hominem O
( O
s O
) O
towards O
the O
person O
who O
made O
the O
post O
. O
We O
divide O
ad O
hominems O
into O
the O
following O
categories O
: O
stupidity O
, O
ignorance O
, O
trolling O
/ O
lying O
, O
bias O
, O
condescension O
, O
and O
other O
; O
examples O
are O
in O
Table O
3 O
. O
7 O
Annotation O
Round O
1 O
The O
goal O
for O
the O
first O
round O
of O
human O
annotation O
is O
to O
collect O
enough O
data O
to O
train O
an O
ad O
hominem O
classifier O
. O
To O
balance O
targeted O
and O
random O
samples O
, O
for O
each O
topic O
( O
BLM O
, O
MeToo O
, O
Vegan O
, O
WFH O
) O
and O
response O
source O
( O
human B-MethodName
, O
Di B-MethodName
- I-MethodName
aloGPT I-MethodName
) O
pair O
, O
we O
randomly O
select O
150 O
[ O
post O
, O
response O
] O
pairs O
with O
you O
- O
responses O
and O
another O
150 O
pairs O
without O
you O
- O
responses O
for O
annotation O
. O
In O
total O
, O
we O
gather O
2,400 O
[ O
post O
, O
response O
] O
pairs O
that O
are O
then O
annotated O
through O
Mechanical O
Turk O
. O
Additional O
Annotations O
We O
conduct O
three O
more O
rounds O
of O
annotations O
to O
retrieve O
more O
ad O
hominem O
responses O
. O
For O
the O
second O
and O
third O
rounds O
, O
we O
use O
an O
ad O
hominem O
classifier O
trained O
on O
data O
from O
all O
previous O
rounds O
( O
with O
the O
same O
architecture O
and O
hyperparameters O
as O
the O
final O
classifier O
in O
Sec O
. O
4.2 O
) O
to O
label O
unseen O
samples O
in O
ADHOMINTWEETS B-DatasetName
. O
We O
then O
select O
a O
balanced O
amount O
of O
automaticallylabeled O
ad O
hominems O
and O
non O
- O
ad O
hominems O
from O
each O
[ O
topic O
, O
response O
source O
] O
pair O
to O
annotate O
. O
8 O
Some O
topics O
( O
e.g. O
, O
WFH O
and O
Vegan O
) O
prompt O
fewer O
ad O
hominem O
responses O
, O
so O
it O
is O
difficult O
to O
find O
enough O
of O
these O
responses O
" O
in O
the O
wild O
" O
to O
train O
a O
more O
accurate O
classifier O
. O
Our O
solution O
is O
to O
manually O
take O
the O
responses O
annotated O
as O
ad O
hominems O
and O
pair O
them O
with O
WFH O
or O
Vegan O
posts O
. O
To O
verify O
that O
these O
new O
pairs O
contain O
ad O
hominem O
responses O
, O
we O
run O
a O
fourth O
round O
of O
annotation O
on O
these O
pairs O
and O
only O
keep O
the O
ones O
where O
the O
majority O
of O
annotators O
label O
the O
response O
as O
an O
ad O
hominem O
to O
the O
post O
. O
We O
combine O
majority O
annotations O
across O
all O
rounds O
of O
annotations O
to O
train O
the O
final O
ad O
hominem O
classifier O
used O
for O
analysis O
. O

Ad B-TaskName
Hominem I-TaskName
Classifier O

For O
large O
- O
scale O
analysis O
of O
ad B-TaskName
hominems I-TaskName
in O
human O
and O
dialogue O
system O
responses O
, O
we O
rely O
on O
classifier O
annotation O
. O
To O
simplify O
the O
learning O
problem O
, O
we O
condense O
the O
different O
ad B-TaskName
hominem I-TaskName
categories O
into O
a O
binary O
yes O
/ O
no O
scheme O
, O
where O
" O
yes O
" O
indicates O
the O
presence O
of O
any O
type O
and O
quantity O
of O
ad O
hominems O
in O
the O
response O
given O
the O
post O
. O
We O
build O
a O
classifier O
to O
automatically O
label O
whether O
a O
response O
contains O
ad B-TaskName
hominems I-TaskName
for O
a O
given O
post O
by O
fine O
- O
tuning O
a O
BERT O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O

model O
with O
the O
input O
format O
" O
[ O
CLS O
] O
POST O

[ O
SEP O
] O
RESPONSE O
[ O
SEP O
] O
" O
. O
We O
additionally O
include O
comparisons O
to O
a O
baseline O
classifier O
built O
on O
top O
of O
DialoGPT O
to O
similarly O
label O
whether O
a O
post O
and O
response O
pair O
indicates O
the O
presence O
of O
an O
ad O
hominem O
response O
. O
This O
baseline O
classifier O
allows O
a O
comparative O
evaluation O
of O
a O
bi O
- O
directional O
encoder O
model O
versus O
an O
auto O
- O
regressive O
decoder O
model O
for O
ad O
hominem O
classification O
and O
how O
this O
difference O
may O
affect O
the O
quality O
of O
control O
techniques O
that O
rely O
on O
the O
latter O
( O
e.g. O
, O
PPLM O
( O
Dathathri O
et O
al O
. O
, O
2020 O
) O
, O
GeDi O
( O
Krause O
et O
al O
. O
, O
2020 O
) O
) O
. O
Appendix O
A.2 O
includes O
more O
details O
of O
our O
model O
implementation O
and O
data O
statistics O
( O
Table O
8 O
) O
. O

Ultimately O
, O
the O
goal O
is O
to O
train O
an O
ad B-TaskName
hominem I-TaskName
detection O
classifier O
that O
has O
high O
accuracy O
across O
sources O
and O
topics O
, O
so O
we O
curate O
the O
dev O
and O
test O
datasets O
to O
be O
balanced O
across O
topics O
, O
response O
sources O
, O
and O
ad O
hominem O
versus O
non O
- O
ad O
hominem O
samples O
( O
through O
downsampling O
) O
. O
Because O
of O
the O
natural O
imbalance O
of O
ad O
hominem O
responses O
for O
different O
topics O
, O
ad O
hominem O
responses O
for O
topics O
like O
WFH O
are O
relatively O
sparse O
compared O
to O
those O
for O
topics O
like O
BLM O
. O
We O
automatically O
augment O
our O
training O
set O
to O
combat O
this O
sparsity O
. O
First O
, O
we O
accumulate O
all O
posts O
and O
responses O
not O
present O
in O
the O
dev O
and O
test O
sets O
. O
Next O
, O
we O
choose O
a O
random O
post O
to O
pair O
with O
a O
random O
labeled O
response O
to O
form O
a O
new O
sample O
. O
We O
generate O
these O
new O
data O
samples O
to O
roughly O
balance O
the O
number O
of O
samples O
across O
topics O
and O
across O
ad B-TaskName
hominems I-TaskName
versus O
nonad O
hominems O
for O
each O
topic O
. O
These O
new O
combinations O
of O
[ O
post O
, O
response O
] O
pairs O
help O
de O
- O
emphasize O
spurious O
correlations O
between O
topics O
and O
classifier O
labels O
. O

Since O
the O
automatic O
augmentation O
reduces O
emphasis O
on O
the O
post O
when O
predicting O
the O
presence O
of O
ad B-TaskName
hominems I-TaskName
in O
the O
response O
, O
a O
natural O
question O
is O
if O
the O
post O
is O
really O
necessary O
to O
gauge O
whether O
the O
response O
contains O
ad O
hominems O
. O
The O
answer O
is O
mixed O
- O
for O
example O
, O
the O
response O
" O
you O
're O
a O
troll O
" O
is O
an O
ad O
hominem O
for O
any O
post O
. O
However O
, O
the O
response O
" O
those O
who O
promote O
veganism O
are O
arrogant O
fools O
" O
is O
an O
ad O
hominem O
given O
the O
post O
" O
everyone O
should O
follow O
veganism O
" O
, O
but O
not O
an O
ad B-TaskName
hominem I-TaskName
given O
the O
post O
" O
I O
do O
n't O
understand O
veganism O
" O
. O
Empirically O
, O
by O
limiting O
the O
classifier O
input O
to O
only O
responses O
, O
the O
classifier O
performs O
worse O
than O
if O
it O
has O
both O
the O
post O
and O
response O
as O
input O
. O
9 O

Reducing O
Ad B-TaskName
Hominem I-TaskName
Responses O

Inspired O
by O
the O
success O
of O
n O
- O
gram O
features O
in O
detecting O
abusive O
language O
by O
Nobata O
et O
al O
. O
( O
2016 O
) O
, O
we O
propose O
a O
constrained O
decoding O
algorithm O
to O
discourage O
the O
model O
from O
generating O
n O
- O
grams O
that O
are O
semantically O
similar O
to O
salient O
n O
- O
grams O
found O
in O
ad O
hominem O
responses O
. O
While O
we O
motivate O
this O
technique O
within O
the O
context O
of O
ad O
hominems O
, O
the O
technique O
is O
applicable O
to O
other O
subtle O
harms O
( O
e.g. O
, O
microaggressions O
) O
in O
language O
generation O
. O

A O
naive O
method O
to O
generate O
fewer O
ad B-TaskName
hominems I-TaskName
is O
to O
block O
words O
that O
are O
likely O
to O
occur O
in O
ad O
hominems O
. O
However O
, O
ad O
hominems O
are O
contextually O
determined O
, O
meaning O
that O
phrases O
are O
a O
better O
indicator O
than O
words O
, O
thus O
motivating O
our O
use O
of O
n O
- O
grams O
. O
Additionally O
, O
our O
algorithm O
uses O
soft O
constraints O
because O
there O
are O
no O
words O
or O
phrases O
that O
always O
indicate O
the O
presence O
of O
an O
ad O
hominem O
. O
In O
this O
section O
, O
we O
describe O
how O
our O
technique O
SALIENSIMTOP O
- O
k O
extends O
top O
- O
k O
sampling O
by O
incorporating O
n O
- O
gram O
similarity O
constraints O
. O
Salient O
n O
- O
grams O
We O
define O
salient O
ad O
hominem O
n O
- O
grams O
to O
be O
n O
- O
grams O
that O
appear O
more O
frequently O
in O
ad O
hominem O
responses O
than O
in O
non O
- O
ad O
hominem O
responses O
. O
Similarly O
, O
salient O
non O
- O
ad O
hominem O
n- O
grams O
appear O
more O
frequently O
in O
non O
- O
ad O
hominem O
responses O
than O
in O
ad O
hominem O
responses O
. O
We O
use O
the O
salience O
score O
as O
defined O
by O
Li O
et O
al O
. O
( O
2018 O
) O
: O

S O
( O
u O
, O
a O
) O
= O
count O
( O
u O
, O
Da O
) O
+ O
λ O
a O
∈A O
, O
a O
= O
a O
count O
( O
u O
, O
D O
a O
) O
+ O
λ O
. O
( O
1 O
) O

In O
Eq O
. O
( O
1 O
) O
, O
u O
denotes O
an O
n O
- O
gram O
, O
D O
= O
{ O
( O
s O
1 O
, O
a O
1 O
) O
, O
... O
, O
( O
s O
m O
, O
a O
m O
) O
} O
is O
a O
corpus O
where O
each O
sample O
is O
a O
sentence O
s O
i O
labeled O
with O
attribute O
a O
i O
. O
D O
a O
is O
therefore O
the O
set O
of O
sentences O
in O
the O
corpus O
with O
the O
same O
attribute O
a. O
A O
is O
the O
set O
of O
possible O
attributes O
( O
e.g. O
, O
ad O
hominem O
or O
non O
- O
ad O
hominem O
) O
. O
We O
define O
the O
n O
- O
gram O
u O
to O
be O
salient O
for O
the O
attribute O
a O
if O
S O
( O
u O
, O
a O
) O
≥ O
ϕ. O
We O
find O
setting O
the O
smoothing O
parameter O
λ O
= O
0.5 O
and O
threshold O
ϕ O
= O
5.5 O
effective O
for O
our O
experiments O
, O
and O
we O
compute O
the O
salience O
of O
3- O
, O
4- O
, O
and O
5 O
- O
grams O
. O

Table O
4 O
shows O
that O
the O
top O
salient O
ad B-TaskName
hominem I-TaskName
n O
- O
grams O
are O
intuitively O
those O
that O
are O
likely O
to O
lead O
to O
ad O
hominems O
. O
For O
example O
, O
" O
you O
're O
being O
a O
" O
is O
used O
in O
contexts O
such O
as O
" O
you O
're O
being O
a O
hypocrite O
" O
. O
A O
more O
overt O
example O
of O
a O
phrase O
likely O
to O
lead O
to O
an O
ad B-TaskName
hominem I-TaskName
response O
is O
" O
you O
're O
a O
troll O
" O
. O
The O
amount O
of O
you O
- O
responses O
in O
salient O
ad O
hominem O
ngrams O
verify O
our O
intuition O
that O
many O
ad O
hominem O
responses O
occur O
in O
the O
form O
of O
you O
- O
responses O
. O
Also O
, O
we O
find O
that O
there O
are O
more O
salient O
ad O
hominem O
ngrams O
than O
non O
- O
ad O
hominem O
n O
- O
grams O
, O
and O
that O
the O
former O
generally O
have O
higher O
salience O
scores O
. O
These O
observations O
and O
preliminary O
experiments O
suggested O
that O
it O
is O
useful O
to O
consider O
both O
types O
of O
salient O
n O
- O
grams O
to O
reduce O
ad O
hominems O
. O

Top O
- O
k O
Sampling O

For O
open O
domain O
language O
generation O
, O
top O
- O
k O
sampling O
( O
Fan O
et O
al O
. O
, O
2018 O
) O
and O
top O
- O
p O
nucleus O
sampling O
( O
Holtzman O
et O
al O
. O
, O
2019 O
) O
are O
popular O
decoding O
algorithms O
that O
have O
been O
shown O
to O
maintain O
topic O
consistency O
and O
promote O
diversity O
. O
We O
experiment O
with O
constrained O
decoding O
through O
top O
- O
k O
sampling O
, O
though O
our O
technique O
is O
also O
applicable O
to O
nucleus O
sampling O
. O
As O
top O
- O
k O
sampling O
is O
a O
general O
decoding O
algorithm O
that O
can O
be O
used O
with O
Algorithm O
1 O
: O
SALIENSIMTOP O
- O
k O
Data O
: O
input O
tokens O
x O
, O
# O
top O
tokens O
k O
, O
# O
candidate O
tokens O
t O
, O
# O
recent O
tokens O
r O
, O
salient O
ad O
hominem O
average O
n O
- O
grams O
A O
, O
salient O
non O
- O
ad O
hominem O
average O
n O
- O
grams O
B O
, O
semantic O
similarity O
threshold O
γ O
Result O
: O
output O
tokens O
y O
y O
= O
x O
while O
len O
( O
y O
) O
< O
max_steps O
+ O
len O
( O
x O
) O
do O
vocab_logits O
= O
model O
( O
y O
) O
P O
= O
choose O
top O
- O
k O
vocab_logits O
and O
rescale O
candidate_tokens O
= O
sample O
t O
tokens O
using O
P O
for O
cand O
in O
candidate_tokens O
do O
if O
special_condition O
then O
y.append O
( O
cand O
) O
continue O
to O
While O
condition O
r_gram O
= O
last O
r O
− O
1 O
tokens O
of O
y O
+ O
cand O
c O
= O
avg O
( O
r_gram O
) O
sim_a O
= O
similarity O
( O
c O
, O
A O
) O
sim_b O
= O
similarity O
( O
c O
, O
B O
) O
if O
sim_a O
-sim_b O
< O
= O
γ O
then O
y.append O
( O
cand O
) O
continue O
to O
While O
condition O
if O
y O
is O
x O
then O
y.append O
( O
candidate_tokens O
[ O
0 O
] O
) O
else O
remove O
last O
token O
from O
y O
various O
language O
generation O
models O
without O
further O
tuning O
or O
training O
, O
expanding O
upon O
this O
technique O
allows O
for O
a O
computationally O
- O
light O
generalizability O
. O

SALIENSIMTOP O
- O
k O

We O
reduce O
the O
amount O
of O
generated O
ad B-TaskName
hominems I-TaskName
by O
encouraging O
the O
generation O
of O
n O
- O
grams O
that O
are O
semantically O
dissimilar O
to O
salient O
ad O
hominem O
n O
- O
grams O
and O
similar O
to O
salient O
non O
- O
ad O
hominem O
n O
- O
grams O
. O
Alg O
. O
1 O
details O
constraints O
we O
add O
to O
top O
- O
k O
sampling O
. O
In O
the O
for O
- O
loop O
, O
we O
iterate O
through O
each O
candidate O
token O
. O
If O
the O
current O
generated O
output O
meets O
a O
" O
special_condition O
" O
( O
e.g. O
, O
backtracking O
limit O
, O
first O
r O
time O
steps O
) O
, O
then O
we O
select O
the O
current O
candidate O
token O
. O
Otherwise O
we O
retrieve O
and O
average O
DialoGPT B-MethodName
's O
embeddings O
over O
the O
most O
recently O
generated O
r O
- O
gram O
to O
calculate O
c O
, O
an O
e O
- O
dimensional O
vector O
where O
e O
is O
the O
size O
of O
the O
token O
embedding O
. O
We O
similarly O
compute O
representations O
to O
form O
A O
, O
a O
j O
× O
e O
matrix O
of O
j O
salient O
ad O
hominem O
average O
n O
- O
gram O
embeddings O
, O
and O
B O
, O
a O
k O
× O
e O
matrix O
of O
k O
salient O
non O
- O
ad B-TaskName
hominem I-TaskName
average O
n O
- O
gram O
embeddings O
. O
We O
then O
calculate O
the O
average O
pairwise O
similarity O
sim_a O
= O
1 O
j O
j O
i=1 O
sim O
( O
A O
i O
, O
c O
) O
, O
where O
A O
i O
is O
the O
i O
- O
th O
row O
of O
A O
, O
and O
similarly O
for O
sim_b O
. O
We O
select O
the O
current O
token O
if O
the O
difference O
between O
the O
similarities O
is O
under O
a O
threshold O
γ O
, O
i.e. O
, O
the O
current O
r O
- O
gram O
is O
less O
similar O
to O
the O
ad B-TaskName
hominem I-TaskName
n O
- O
grams O
and O
more O
similar O
to O
the O
non O
- O
ad B-TaskName
hominem I-TaskName
n O
- O
grams O
. O
Otherwise O
, O
we O
backtrack O
to O
the O
previous O
time O
step O
if O
we O
iterate O
through O
all O
candidates O
without O
finding O
a O
suitable O
one O
. O
By O
limiting O
the O
number O
of O
times O
the O
algorithm O
can O
backtrack O
while O
gen- O
erating O
a O
sample O
, O
this O
algorithm O
adds O
a O
constant O
amount O
of O
computational O
resources O
compared O
to O
the O
original O
, O
non O
- O
constrained O
decoding O
. O
Implementation O
Details O
In O
our O
experiments O
, O
we O
set O
k O
= O
40 O
( O
commonly O
used O
in O
previous O
generation O
tasks O
( O
Radford O
et O
al O
. O
, O
2019 O
) O
) O
. O
With O
parameter O
tuning O
, O
we O
find O
t O
= O
10 O
and O
γ O
= O
0 O
effective O
for O
our O
setup O
. O
We O
use O
r O
= O
5 O
to O
compare O
the O
averaged O
embedding O
of O
the O
most O
recent O
5 O
- O
gram O
with O
those O
of O
salient O
3- O
, O
4- O
, O
and O
5 O
- O
grams O
. O
Additionally O
, O
we O
use O
cosine O
similarity O
as O
the O
similarity O
metric O
and O
our O
" O
special_condition O
" O
includes O
either O
a O
) O
a O
limit O
of O
5 O
for O
backtracking O
or O
b O
) O
the O
first O
r O
time O
steps O
. O

Results O

Identifying O
Ad B-TaskName
Hominems I-TaskName

Annotation O
Across O
all O
rounds O
of O
annotations O
, O
the O
average O
WAWA O
scores O
include O
a O
precision B-MetricName
of O
0.82 B-MetricValue
, O
recall B-MetricName
of O
0.92 B-MetricValue
, O
and O
F B-MetricName
1 I-MetricName
of O
0.87 B-MetricValue
, O
indicating O
moderately O
high O
majority O
agreement O
. O
Generally O
, O
the O
agreement O
scores O
for O
the O
human B-MethodName
responses O
are O
slightly O
higher O
than O
those O
for O
the O
DialoGPT B-MethodName
responses O
- O
we O
hypothesize O
that O
the O
former O
tend O
to O
be O
more O
coherent O
and O
longer O
, O
and O
thus O
more O
informative O
. O

Ad B-TaskName
Hominem I-TaskName
Classifier O
The O
resulting O
BERTbased O
classifier O
has O
an O
overall O
dev O
F B-MetricName
1 I-MetricName
score O
of O
83.3 B-MetricValue
% I-MetricValue
and O
a O
test O
F B-MetricName
1 I-MetricName
score O
of O
80.0 B-MetricValue
% I-MetricValue
for O
ad B-TaskName
hominems I-TaskName
. O
The O
DialoGPT B-MethodName
- O
based O
classifier O
has O
a O
dev O
F B-MetricName
1 I-MetricName
score O
of O
74.6 B-MetricValue
% I-MetricValue
and O
a O
test O
F B-MetricName
1 I-MetricName
score O
of O
72.6 B-MetricValue
% I-MetricValue
, O
supporting O
our O
use O
of O
the O
BERT O
- O
based O
classifier O
to O
automatically O
detect O
ad O
hominems O
in O
the O
rest O
of O
this O
work O
. O
10 O
The O
full O
breakdown O
of O
F B-MetricName
1 I-MetricName
scores O
across O
topics O
and O
response O
sources O
is O
shown O
in O

Ad B-TaskName
Hominem I-TaskName
Analysis O

Ad B-TaskName
Hominem I-TaskName
Categories O
By O
comparing O
ad O
hominem O
types O
across O
the O
manually O
- O
annotated O
human B-MethodName
and O
DialoGPT B-MethodName
responses O
, O
we O
find O
that O
ad O
hominems O
in O
human O
responses O
frequently O
occur O
in O
the O
forms O
of O
" O
condescension O
" O
and O
" O
ignorance O
" O
, O
while O
ad B-TaskName
hominems I-TaskName
in O
DialoGPT B-MethodName
responses O
occur O
in O
the O
forms O
of O
" O
ignorance O
" O
and O
" O
other O
" O
types O
( O
Table O
11 O
in O
the O
Appendix O
) O
. O
These O
results O
indicate O
that O
responses O
from O
different O
sources O
and O
topics O
are O
likely O
to O
contain O
different O
ad O
hominems O
. O
Formally O
categorizing O
ad O
hominems O
allows O
for O
more O
consistent O
annotations O
and O
a O
better O
understanding O
of O
the O
types O
DialoGPT B-MethodName
is O
prone O
to O
generate O
. O

DialoGPT B-MethodName
Responses O

The O
classifier O
enables O
us O
to O
perform O
a O
large O
- O
scale O
study O
of O
ad B-TaskName
hominem I-TaskName
trends O
across O
various O
contexts O
for O
the O
entire O
AD B-DatasetName
- I-DatasetName
HOMINTWEETS I-DatasetName
dataset O
. O
Figure O
1 O
shows O
the O
percentage O
of O
ad O
hominem O
responses O
to O
posts O
across O
topics O
and O
response O
sources O
. O
Focusing O
on O
the O
" O
Human B-MethodName
" O
and O
" O
DialoGPT B-MethodName
" O
bars O
for O
each O
topic O
, O
we O
see O
that O
ad B-TaskName
hominem I-TaskName
responses O
are O
present O
across O
all O
topics O
for O
both O
response O
sources O
. O
Additionally O
, O
ad B-TaskName
hominem I-TaskName
responses O
occur O
more O
frequently O
in O
discussions O
related O
to O
BLM O
and O
MeToo O
and O
less O
frequently O
in O
discussions O
related O
to O
Vegan O
and O
WFH O
. O
Vegan O
discussions O
also O
seem O
to O
attract O
more O
ad O
hominem O
responses O
than O
WFH O
discussions O
. O
The O
relatively O
higher O
rates O
of O
ad O
hominem O
responses O
in O
topics O
related O
to O
marginalized O
communities O
indicate O
the O
elevated O
potential O
for O
harm O
towards O
these O
communities O
. O
Fine O
- O
tuned O
DialoGPT B-MethodName
Responses O
Figure O
1 O
also O
shows O
that O
fine O
- O
tuning O
on O
datasets O
that O
contain O
more O
ad O
hominem O
responses O
leads O
to O
more O
generation O
of O
ad O
hominem O
responses O
across O
topics O
. O
11 O
From O
these O
results O
, O
we O
infer O
that O
the O
original O
DialoGPT B-MethodName
( O
which O
was O
fine O
- O
tuned O
from O
GPT-2 O
) O
was O
trained O
on O
a O
dataset O
that O
likely O
contained O
relatively O
more O
rather O
than O
fewer O
ad O
hominems O
. O
Additionally O
, O
finetuning O
on O
a O
carefully O
chosen O
dataset O
can O
reduce O
the O
quantity O
of O
generated O
ad B-TaskName
hominems I-TaskName
and O
associated O
harms O
. O

Ad B-TaskName
Hominem I-TaskName
Reduction O

Baselines O
We O
compare O
techniques O
from O
two O
classes O
of O
harm O
reduction O
methods O
for O
language O
generation O
: O
data O
- O
based O
and O
decoding O
- O
based O
. O
Gehman O
et O
al O
. O
( O
2020 O
) O
introduce O
four O
baselines O
to O
span O
the O
different O
classes O
of O
harm O
reduction O
techniques O
. O
The O
first O
baseline O
is O
simply O
the O
original O
DialoGPT B-MethodName
. O
Our O
databased O
reduction O
baseline O
is O
DialoGPT B-MethodName
fine O
- O
tuned O
on O
the O
WFH O
dataset O
, O
as O
described O
in O
Sec O
. O
3 O
. O
For O
the O
first O
decoding O
- O
based O
baseline O
, O
we O
rely O
on O
a O
gradient O
- O
based O
method O
post O
- O
training O
to O
find O
a O
" O
trigger O
phrase O
" O
, O
which O
is O
then O
attached O
to O
a O
prompt O
at O
inference O
time O
to O
influence O
the O
generated O
output O
( O
Wallace O
et O
al O
. O
, O
2019 O
) O
. O
Sheng O
et O
al O
. O
( O
2020 O
) O
further O
propose O
a O
framework O
to O
use O
these O
triggers O
to O
control O
societal O
biases O
, O
and O
we O
use O
these O
methods O
to O
find O
a O
trigger O
that O
can O
induce O
DialoGPT B-MethodName
to O
generate O
fewer O
ad O
hominems O
and O
more O
non O
- O
ad O
hominems O
when O
prepended O
to O
posts O
about O
different O
topics O
. O
For O
the O
second O
decoding O
- O
based O
baseline O
, O
we O
use O
the O
Plug O
and O
Play O
Language O
Model O
( O
PPLM O
) O
proposed O
by O
Dathathri O
et O
al O
. O
( O
2020 O
) O
, O
which O
guides O
a O
pre O
- O
trained O
language O
model O
's O
generated O
output O
using O
gradients O
from O
attribute O
classifiers O
. O
12 O
Human O
Annotation O
To O
verify O
ad B-TaskName
hominem I-TaskName
trends O
from O
the O
automatic O
evaluation O
, O
we O
randomly O
select O
100 O
samples O
from O
each O
[ O
reduction O
technique O
, O
topic O
] O
pair O
for O
additional O
human O
annotation O
. O
General O
Trends O
Classifier O
and O
human O
evaluations O
for O
techniques O
to O
reduce O
ad O
hominems O
are O
in O
Figure O
2 O
, O
and O
examples O
of O
generated O
responses O
are O
in O
Table O
6 O
. O
The O
classifier O
- O
labeled O
results O
allow O
us O
to O
evaluate O
14.5 O
K O
samples O
across O
all O
topics O
per O
response O
source O
, O
and O
the O
human O
- O
labeled O
results O
allow O
us O
to O
more O
accurately O
evaluate O
a O
smaller O
set O
of O
samples O
. O
Overall O
, O
the O
trends O
for O
classifier O
and O
human O
evaluations O
are O
similar O
, O
and O
the O
evaluations O
suggest O
that O
all O
ad B-TaskName
hominem I-TaskName
reduction O
techniques O
are O
effective O
compared O
to O
the O
original O
DialoGPT O
. O
Furthermore O
, O
SALIENSIMTOP O
- O
k O
is O
more O
effective O
than O
the O
other O
individual O
techniques O
, O
and O
combining O
fine O
- O
tuning O
and O
SALIENSIMTOP O
- O
k O
has O
promise O
for O
further O
reducing O
the O
amount O
of O
generated O
ad B-TaskName
hominems I-TaskName
. O

For O
SALIENSIMTOP O
- O
k O
, O
limiting O
the O
number O
of O
times O
we O
backtrack O
to O
previous O
time O
steps O
ensures O
that O
the O
algorithm O
is O
not O
significantly O
slower O
compared O
to O
the O
original O
top O
- O
k O
sampling O
algorithm O
. O
Empirically O
, O
we O
find O
that O
using O
SALIENSIMTOP O
- O
k O
with O
a O
backtracking O
limit O
of O
5 O
on O
the O
original O
Di B-MethodName
- I-MethodName
aloGPT I-MethodName
results O
in O
13 O
% O
of O
the O
decoding O
operations O
being O
" O
non O
- O
forward O
" O
operations O
, O
where O
the O
set O
of O
decoding O
operations O
are O
: O
a O
) O
choosing O
the O
current O
token O
and O
moving O
forward O
to O
the O
next O
timestep O
, O
b O
) O
looking O
for O
an O
alternate O
token O
at O
the O
same O
timestep O
, O
or O
c O
) O
moving O
backward O
to O
a O
previous O
timestep O
. O
When O
applying O
constrained O
decoding O
to O
DialoGPT B-MethodName
fine O
- O
tuned O
on O
WFH O
, O
10 O
% O
of O
the O
operations O
are O
nonforward O
operations O
. O
Since O
ad B-TaskName
hominems I-TaskName
are O
less O
common O
than O
non O
- O
ad O
hominems O
, O
the O
algorithm O
is O
able O
to O
proceed O
with O
the O
first O
sampled O
candidate O
token O
in O
most O
time O
steps O
. O
Additionally O
, O
models O
or O
topics O
that O
are O
inclined O
to O
generate O
more O
ad B-TaskName
hominems I-TaskName
incur O
more O
non O
- O
forward O
operations O
. O

Coherence B-MetricName
and O
Relevance B-MetricName
Evaluation O

To O
ensure O
that O
the O
ad B-TaskName
hominem I-TaskName
reduction O
techniques O
do O
not O
affect O
the O
quality O
of O
the O
generated O
responses O
, O
we O
have O
annotators O
label O
the O
coherence O
and O
relevance O
of O
a O
response O
to O
a O
post O
, O
both O
on O
a O
scale O
of O
1 O
to O
5 O
, O
where O
a O
higher O
score O
is O
better O
. O
The O
trigger O
method O
produces O
samples O
that O
are O
relatively O
more O
coherent O
, O
although O
at O
the O
cost O
of O
lower O
relevance O
to O
the O
post O
. O
PPLM O
generates O
responses O
that O
are O
relatively O
lower O
in O
both O
coherence O
and O
relevance O
. O
SALIENSIMTOP O
- O
k O
manages O
to O
maintain O
a O
decent O
balance O
of O
generating O
both O
coherent O
and O
relevant O
responses O
. O
Combining O
SALIENSIMTOP O
- O
k O
with O
finetuning O
on O
WFH O
data O
results O
in O
responses O
that O
are O
slightly O
less O
coherent O
and O
mixed O
in O
relevance O
for O
different O
topics O
. O
13 O
Spearman B-MetricName
's O
correlation O
is O
moderately O
high O
( O
0.46 O
) O
for O
relevance O
and O
a O
bit O
lower O
for O
coherence O
( O
0.38 O
) O
, O
indicating O
the O
task O
subjectivity O
. O
Discussion O
The O
collective O
results O
indicate O
that O
SALIENSIMTOP O
- O
k O
is O
an O
effective O
standalone O
ad O
hominem O
reduction O
technique O
that O
maintains O
generated O
text O
quality O
; O
while O
it O
can O
be O
combined O
with O
other O
techniques O
to O
further O
reduce O
ad O
hominems O
, O
one O
should O
carefully O
evaluate O
the O
trade O
- O
offs O
between O
response O
coherence O
and O
relevance O
. O
Additionally O
, O
for O
reducing O
harmful O
language O
types O
that O
are O
more O
subjective O
or O
difficult O
to O
detect O
, O
straightforward O
control O
techniques O
that O
rely O
on O
salient O
ngrams O
may O
be O
more O
useful O
than O
techniques O
that O
rely O
on O
noisier O
signals O
from O
classifiers O
. O

Conclusion O

Ad B-TaskName
hominem I-TaskName
responses O
from O
dialogue O
systems O
are O
offensive O
, O
stall O
conversations O
, O
and O
are O
especially O
harmful O
for O
marginalized O
communities O
. O
We O
analyze O
responses O
to O
find O
that O
discussions O
on O
topics O
that O
affect O
marginalized O
groups O
contain O
more O
ad B-TaskName
hominems I-TaskName
. O
Through O
a O
novel O
constrained O
decoding O
technique O
, O
we O
decrease O
the O
amount O
of O
ad O
hominems O
generated O
from O
dialogue O
systems O
while O
keeping O
the O
response O
quality O
comparable O
. O
Furthermore O
, O
our O
method O
can O
be O
easily O
applied O
to O
other O
pre O
- O
trained O
language O
generation O
models O
and O
other O
subtle O
yet O
harmful O
language O
. O
More O
broadly O
, O
our O
work O
strives O
to O
understand O
ad O
hominems O
in O
the O
context O
of O
harms O
in O
conversational O
systems O
. O

Broader O
Impact O

This O
work O
identifies O
personal O
attacks O
in O
responses O
generated O
by O
dialogue O
systems O
, O
quantifies O
the O
dis O
- O
proportionate O
amount O
generated O
for O
topics O
concerning O
marginalized O
populations O
, O
and O
proposes O
methods O
to O
reduce O
ad O
hominem O
- O
related O
harms O
. O
Dataset O
We O
collect O
an O
English O
dataset O
from O
Twitter O
and O
ensure O
that O
personal O
information O
( O
e.g. O
, O
usernames O
, O
emails O
, O
urls O
) O
is O
discarded O
. O
We O
also O
collect O
crowd O
- O
sourced O
annotations O
for O
this O
dataset O
through O
Mechanical O
Turk O
, O
where O
we O
ask O
for O
judgements O
of O
whether O
a O
response O
contains O
ad O
hominems O
for O
a O
given O
post O
, O
and O
the O
coherence O
and O
relevance O
of O
a O
response O
. O
No O
information O
about O
the O
annotators O
are O
collected O
from O
the O
annotation O
tasks O
. O
The O
annotation O
information O
( O
pay O
per O
amount O
of O
work O
, O
guidelines O
) O
is O
in O
the O
Appendix O
. O

One O
annotation O
aspect O
that O
we O
did O
not O
control O
for O
is O
whether O
the O
annotators O
themselves O
are O
from O
marginalized O
communities O
. O
When O
measuring O
harms O
towards O
different O
demographics O
, O
it O
is O
important O
to O
consider O
the O
lived O
experiences O
of O
those O
groups O
and O
how O
these O
experiences O
may O
affect O
our O
analyses O
. O
Future O
work O
includes O
specifically O
collecting O
annotations O
from O
marginalized O
groups O
. O

Additionally O
, O
we O
analyze O
ad B-TaskName
hominems I-TaskName
in O
responses O
to O
four O
Twitter O
topics O
and O
from O
one O
dialogue O
model O
, O
which O
leaves O
much O
room O
for O
exploring O
the O
generalizability O
of O
the O
trends O
we O
see O
. O
Techniques O
In O
terms O
of O
dual O
- O
use O
harms O
, O
our O
constrained O
decoding O
technique O
could O
potentially O
be O
used O
to O
amplify O
rather O
than O
reduce O
ad O
hominems O
( O
or O
other O
harmful O
language O
) O
. O
However O
, O
we O
believe O
that O
by O
being O
transparent O
about O
this O
technique O
and O
releasing O
the O
associated O
code O
and O
data O
, O
we O
can O
better O
counter O
attempts O
of O
malicious O
misuse O
. O

Furthermore O
, O
to O
perform O
a O
large O
- O
scale O
analysis O
of O
ad B-TaskName
hominems I-TaskName
across O
different O
contexts O
, O
we O
build O
an O
automatic O
classifier O
. O
While O
we O
spent O
much O
effort O
on O
collecting O
representative O
train O
/ O
dev O
/ O
test O
datasets O
and O
verifying O
classifier O
quality O
and O
observed O
trends O
with O
human O
labels O
, O
collecting O
more O
( O
diverse O
) O
data O
could O
help O
further O
improve O
the O
classifier O
accuracy B-MetricName
and O
robustness O
. O
In O
the O
meantime O
, O
we O
think O
this O
work O
introduces O
an O
important O
perspective O
of O
how O
ad B-TaskName
hominems I-TaskName
in O
dialogue O
systems O
reinforce O
unequal O
harms O
and O
effective O
reduction O
methods O
. O

A O
Appendices O

A.1 O
You O
- O
responses O

You O
- O
responses O
are O
responses O
containing O
any O
of O
the O
following O
phrases O
: O
you O
are O
, O
you O
were O
, O
you O
should O
, O
you O
would O
, O
you O
will O
, O
you O
have O
, O
you O
can O
, O
you O
could O
, O
you O
do O
n't O
, O
you O
did O
n't O
, O
you O
can O
' O
t O
, O
you O
're O
, O
you O
'd O
, O
you O
'll O
, O
you O
've O
, O
ur O
, O
ya O
'll O
, O
y O
all O
, O
your O
, O
yours O
, O
yourself O
, O
are O
you O
, O
were O
you O
, O
should O
you O
, O
would O
you O
, O
will O
you O
, O
have O
you O
, O
can O
you O
, O
could O
you O
. O
These O
phrases O
are O
used O
to O
identify O
potential O
ad B-TaskName
hominems I-TaskName
for O
more O
targeted O
annotation O
( O
Round O
1 O
) O
. O

A.2 O
Model O
Details O

We O
run O
all O
our O
models O
on O
an O
RTX O
2080Ti O
GPU O
. O
Training O
the O
ad O
hominem O
classifiers O
takes O
a O
few O
minutes O
, O
and O
fine O
- O
tuning O
DialoGPT O
on O
different O
topics O
( O
ranging O
from O
3 O
K O
to O
4 O
K O
samples O
as O
shown O
in O
Table O
2 O
) O
takes O
a O
few O
hours O
. O
Ad O
Hominem O
Classifier O
For O
the O
BERT O
- O
based O
ad O
hominem O
classifier O
, O
we O
fine O
- O
tune O
from O
the O
uncased O
version O
of O
the O
BERT O
base O
model O
( O
12 O
layers O
) O
with O
mostly O
default O
parameters O
. O
For O
the O
DialoGPTbased B-MethodName
classifier O
, O
we O
fine O
- O
tune O
from O
the O
mediumsized O
DialoGPT B-MethodName
model O
also O
with O
mostly O
default O
parameters O
. O
In O
terms O
of O
non O
- O
default O
hyperparameters O
, O
we O
try O
learning B-HyperparameterName
rates I-HyperparameterName
of O
5 B-HyperparameterValue
× I-HyperparameterValue
10 I-HyperparameterValue
−5 I-HyperparameterValue
, O
1 B-HyperparameterValue
× I-HyperparameterValue
10 I-HyperparameterValue
−5 I-HyperparameterValue
, O
5 B-HyperparameterValue
× I-HyperparameterValue
10 I-HyperparameterValue
−6 I-HyperparameterValue
, O
and O
1 B-HyperparameterValue
× I-HyperparameterValue
10 I-HyperparameterValue
−6 I-HyperparameterValue
, O
and O
find O
that O
5 B-HyperparameterValue
× I-HyperparameterValue
10 I-HyperparameterValue
−5 I-HyperparameterValue
works O
the O
best O
for O
BERT O
and O
5 B-HyperparameterValue
× I-HyperparameterValue
10 I-HyperparameterValue
−6 I-HyperparameterValue
works O
the O
best O
for O
DialoGPT B-MethodName
. O
We O
train O
for O
12 B-HyperparameterValue
epochs B-HyperparameterName
and O
save O
the O
checkpoint O
for O
the O
epoch O
that O
the O
model O
performs O
the O
best O
on O
the O
dev O
set O
. O
All O
input O
that O
goes O
into O
the O
classifier O
is O
preprocessed O
to O
replace O
usernames O
, O
urls O
, O
and O
hashtags O
with O
placeholders O
. O
DialoGPT B-MethodName
For O
all O
our O
DialoGPT B-MethodName
experiments O
, O
we O
use O
the O
medium O
DialoGPT B-MethodName
with O
355 B-HyperparameterValue
M I-HyperparameterValue
parameters B-HyperparameterName
and O
mostly O
default O
parameters O
. O
During O
fine O
- O
tuning O
, O
we O
try O
learning B-HyperparameterName
rates I-HyperparameterName
of O
5 B-HyperparameterValue
× I-HyperparameterValue
10 I-HyperparameterValue
−5 I-HyperparameterValue
, O
1 B-HyperparameterValue
× I-HyperparameterValue
10 I-HyperparameterValue
−5 I-HyperparameterValue
, O
5 B-HyperparameterValue
× I-HyperparameterValue
10 I-HyperparameterValue
−6 I-HyperparameterValue
, O
and O
1 B-HyperparameterValue
× I-HyperparameterValue
10 I-HyperparameterValue
−6 I-HyperparameterValue
, O
and O
that O
a O
learning B-HyperparameterName
rate I-HyperparameterName
of O
5 B-HyperparameterValue
× I-HyperparameterValue
10 I-HyperparameterValue
−6 I-HyperparameterValue
for O
5 B-HyperparameterValue
epochs B-HyperparameterName
performs O
the O
best O
on O
the O
dev O
sets O
. O
The O
format O
the O
training O
and O
eval O
data O
is O
" O
POST O
[ O
EOS O
] O
RESPONSE O
[ O
EOS O
] O
" O
. O

A.3 O
Trigger O
Details O

Following O
the O
trigger O
search O
algorithm O
of O
Wallace O
et O
al O
. O
( O
2019 O
) O
and O
bias O
control O
framework O
of O
Sheng O
et O
al O
. O
( O
2020 O
) O
, O
we O
start O
with O
the O
trigger O
phrase O
" O
the O
the O
the O
the O
the O
the O
" O
, O
and O
iteratively O
replace O
each O
token O
in O
the O
trigger O
such O
that O
we O
minimize O
the O
loss O
of O
generating O
non O
- O
ad O
hominem O
responses O
and O
maximize O
the O
loss O
of O
generating O
ad O
hominem O
responses O
. O
By O
using O
the O
annotated O
non O
- O
ad B-TaskName
hominem I-TaskName
and O
ad B-TaskName
hominem I-TaskName
responses O
as O
targets O
to O
generate O
or O
avoid O
, O
we O
can O
find O
a O
trigger O
phrase O
that O
forces O
the O
model O
to O
generate O
fewer O
ad O
hominems O
. O
Specifically O
, O
we O
follow O
previous O
work O
and O
implement O
the O
minimization O
and O
maximization O
of O
these O
different O
targeted O
associations O
through O
subtraction O
( O
e.g. O
, O
loss O
from O
generating O
non O
- O
ad B-TaskName
hominems I-TaskName
minus O
loss O
from O
generating O
ad B-TaskName
hominems I-TaskName
) O
, O
and O
then O
minimize O
this O
difference O
to O
find O
the O
trigger O
. O
The O
trigger O
we O
find O
is O
" O
Franç O
casterbecca O
Unsure O
filler O
willpower O
" O
, O
and O
we O
append O
this O
trigger O
to O
a O
post O
to O
generate O
responses O
from O
DialoGPT B-MethodName
. O
For O
example O
, O
the O
input O
prompt O
" O
Franç O
casterbecca O
Unsure O
filler O
willpower O
WE O
have O
the O
power O
to O
stop O
this O
. O
Go O
# O
vegan O
. O
" O
results O
in O
the O
generated O
response O
" O
We O
must O
! O
" O
. O
We O
use O
the O
default O
parameters O
as O
reported O
by O
Sheng O
et O
al O
. O
( O
2020 O
) O
. O
For O
more O
details O
, O
see O
the O
prior O
works O
. O
With O
an O
RTX O
2080Ti O
GPU O
, O
the O
trigger O
search O
algorithm O
takes O
1 O
- O
2 O
hours O
. O

A.4 O
PPLM O
Details O

The O
Plug O
and O
Play O
Language O
Model O
uses O
gradients O
from O
an O
attribute O
classifier O
to O
control O
generation O
from O
a O
pre O
- O
trained O
language O
model O
. O
In O
the O
original O
work O
, O
Dathathri O
et O
al O
. O
( O
2020 O
) O
use O
PPLM O
in O
the O
contexts O
of O
topic O
, O
sentiment O
, O
and O
toxicity O
control O
. O

Although O
ad B-TaskName
hominems I-TaskName
are O
also O
a O
form O
of O
toxic O
language O
, O
we O
train O
a O
new O
attribute O
classifier O
specifically O
on O
the O
annotated O
ADHOMINTWEETS B-DatasetName
dataset O
for O
a O
more O
competitive O
PPLM O
baseline O
. O
We O
use O
the O
ad O
hominem O
classifier O
training O
set O
and O
dev O
set O
to O
form O
the O
training O
and O
validation O
sets O
for O
this O
classifier O
, O
respectively O
. O
Note O
that O
this O
classifier O
is O
necessarily O
different O
from O
the O
BERT O
- O
based O
model O
we O
use O
for O
the O
main O
ad O
hominem O
analysis O
- O
to O
use O
the O
gradients O
from O
the O
attribute O
classifier O
to O
steer O
generations O
from O
DialoGPT B-MethodName
, O
we O
follow O
the O
attribute O
classifier O
training O
procedure O
of O
Dathathri O
et O
al O
. O
( O
2020 O
) O
. O
Specifically O
, O
this O
classifier O
takes O
the O
hidden O
states O
with O
dimension O
( O
batch O
size O
, O
sequence O
length O
, O
embedding O
size O
) O
from O
the O
last O
layer O
of O
DialoGPT B-MethodName
, O
averages O
the O
hidden O
states O
over O
the O
sequence O
length O
, O
and O
uses O
these O
averaged O
hidden O
states O
as O
input O
for O
a O
simple O
linear O
classifier O
. O
The O
classifier O
has O
an O
input O
text O
format O
of O
" O
POST O
[ O
EOS O
] O
RESPONSE O
[ O
EOS O
] O
" O
to O
predict O
the O
binary O
ad B-TaskName
hominem I-TaskName
label O
and O
has O
an O
average O
validation O
accuracy O
of O
76 O
% O
. O

With O
this O
trained O
attribute O
classifier O
, O
we O
then O
follow O
the O
gradient O
- O
based O
hidden O
state O
updates O
described O
by O
Dathathri O
et O
al O
. O
( O
2020 O
) O
to O
generate O
responses O
given O
posts O
. O
For O
our O
hyperpa O
- O
rameter O
tuning O
, O
we O
try O
different O
step B-HyperparameterName
sizes I-HyperparameterName
= O
[ O
0.01 B-HyperparameterValue
, O
0.02 B-HyperparameterValue
, O
0.03 B-HyperparameterValue
, O
0.04 B-HyperparameterValue
, O
0.05 B-HyperparameterValue
] O
and O
and O
KL B-HyperparameterName
loss I-HyperparameterName
coefficients O
= O
[ O
0.01 B-HyperparameterValue
, O
0.02 B-HyperparameterValue
, O
0.03 B-HyperparameterValue
] O
, O
where O
increased O
step B-HyperparameterName
sizes I-HyperparameterName
intensify O
control O
and O
increased O
KL B-HyperparameterName
loss I-HyperparameterName
coefficients O
intensify O
the O
similarity O
of O
the O
outputs O
for O
the O
modified O
and O
unmodified O
distributions O
. O
For O
our O
reported O
results O
, O
we O
use O
PPLM O
with O
a O
step B-HyperparameterName
size I-HyperparameterName
of O
0.01 B-HyperparameterValue
, O
a O
KL B-HyperparameterName
loss I-HyperparameterName
coefficient O
of O
0.02 B-HyperparameterValue
, O
6 B-HyperparameterValue
epochs B-HyperparameterName
, O
and O
otherwise O
default O
parameters O
of O
the O
original O
work O
. O
In O
general O
, O
this O
technique O
is O
slower O
because O
it O
requires O
many O
iterations O
per O
token O
to O
accumulate O
perturbations O
. O

A.5 O
Top O
- O
k O
Sampling O
Details O

At O
each O
time O
step O
of O
top O
- O
k O
sampling O
, O
the O
top O
- O
k O
tokens O
V O
( O
k O
) O
⊂ O
V O
that O
maximize O
p O
= O
x∈V O
( O
k O
) O
P O
( O
x|x O
1 O
: O
i−1 O
) O
are O
selected O
as O
candidate O
tokens O
to O
generate O
. O
V O
is O
the O
model O
's O
token O
vocabulary O
, O
x O
is O
a O
token O
, O
and O
x O
1 O
: O
i−1 O
are O
the O
tokens O
from O
all O
the O
previous O
time O
steps O
. O
The O
distribution O
p O
is O
then O
re O
- O
scaled O
such O
that O
for O
all O
x O
∈ O
V O
( O
k O
) O
, O
the O
rescaled O
distribution O
is O
P O
( O
x|x O
1 O
: O
i−1 O
) O
= O
P O
( O
x|x O
1 O
: O
i−1 O
) O
/ O
p O
. O
This O
new O
distribution O
P O
is O
then O
used O
to O
sample O
a O
new O
token O
for O
the O
current O
time O
step O
. O

A.6 O
SALIENSIMTOP O
- O
k O
Details O

For O
this O
constrained O
decoding O
technique O
, O
we O
also O
use O
an O
RTX O
2080 O
Ti O
GPU O
and O
, O
similar O
to O
the O
nonconstrained O
DialoGPT B-MethodName
, O
it O
takes O
less O
than O
a O
second O
to O
generate O
output O
for O
a O
sample O
. O

A.7 O
Ad B-TaskName
Hominem I-TaskName
Annotation O

Task O
Annotators O
are O
paid O
$ O
0.05 O
to O
label O
the O
ad B-TaskName
hominems I-TaskName
in O
a O
sample O
and O
are O
from O
the O
U.S. O
or O
Canada O
. O
We O
filter O
by O
annotators O
from O
these O
locations O
to O
better O
control O
for O
similar O
societal O
values O
in O
English O
- O
speaking O
communities O
, O
but O
it O
would O
be O
interesting O
to O
see O
how O
the O
concept O
of O
ad O
hominems O
change O
across O
communities O
with O
more O
different O
values O
and O
languages O
. O
Each O
sample O
takes O
an O
average O
of O
15 O
to O
20 O
seconds O
to O
label O
, O
for O
an O
hourly O
average O
of O
$ O
10.29 O
USD O
. O
We O
show O
annotators O
the O
guidelines O
below O
. O
Guidelines O
Ad B-TaskName
hominems I-TaskName
are O
a O
type O
of O
logical O
fallacy O
in O
which O
a O
response O
attacks O
a O
person O
and O
some O
feature O
of O
the O
person O
's O
character O
instead O
of O
the O
position O
the O
person O
is O
maintaining O
. O
For O
example O
, O
if O
Person O
A O
says O
" O
We O
used O
deductive O
reasoning O
to O
prove O
that O
the O
moon O
revolves O
around O
the O
earth O
. O
" O
and O
Person O
B O
replies O
" O
No O
, O
you O
're O
dumb O
" O
, O
Person O
B O
's O
response O
is O
an O
ad O
hominem O
. O
A O
more O
subtle O
ad O
hominem O
is O
if O
Person O
B O
says O
" O
I O
think O
you O
meant O
inductive O
reasoning O
. O
" O
, O
because O
( O
whether O
intentionally O
or O
not O
) O
this O
response O
targets O
Person O
A O
's O
perceived O
mistake O
instead O
of O
purely O
addressing O
the O
content O
of O
Person O
A O
's O
post O
. O
Types O
of O
ad O
hominems O
( O
towards O
Person O
A O
) O
: O

• O
Stupidity O
( O
i.e. O
, O
targeting O
Person O
A O
's O
capability O
for O
intelligence O
) O
: O

-Person O
B O
: O
" O
You O
dumb O
f O
* O
* O
* O
" O
-Person O
B O
: O
" O
Reading O
comprehension O
is O
your O
friend O
" O
-Person O
B O
: O
" O
You O
have O
no O
capability O
to O
understand O
why O
" O
-Person O
B O
: O
" O
Nobody O
with O
enough O
brains O
to O
operate O
a O
computer O
could O
possibly O
believe O
something O
this O
stupid O
" O
-Person O
B O
: O
" O
Ever O
have O
discussions O
with O
narcissistic O
idiots O
on O
the O
internet O
? O
They O
are O
so O
tiring O
" O
-Person O
B O
: O
" O
Your O
second O
paragraph O
is O
fairly O
idiotic O
" O
• O
Ignorance O
( O
i.e. O
, O
targeting O
Person O
A O
not O
using O
their O
capability O
for O
intelligence O
, O
making O
a O
mistake O
, O
forgetting O
to O
include O
something O
, O
confusing O
different O
things O
) O
: O

- O

Acknowledgments O


-Person O
B O
: O
" O
You O
're O
racist O
" O
-Person O
B O
: O
" O
Somebody O
's O
being O
sexist O
. O
" O
• O
Condescension O
: O
( O
i.e. O
, O
if O
Person O
B O
has O
an O
attitude O
of O
patronizing O
superiority O
towards O
Person O
A O
) O
-Person O
B O
: O
" O
little O
buddy O
" O
-Person O
B O
: O
" O
Again O
, O
how O
old O
are O
you O
? O
" O
-Person O
B O
: O
" O
How O
can O
you O
explain O
that O
? O
You O
ca O
n't O
because O
it O
will O
hurt O
your O
feelings O
to O
face O
reality O
" O
• O
Other O
( O
vulgar O
insults O
, O
name O
- O
calling O
, O
accusations O
of O
logical O
fallacies O
, O
etc O
, O
towards O
Person O
A O
that O
are O
not O
already O
covered O
by O
the O
above O
categories O
) O
: O

-Person O
B O
: O
" O
You O
're O
just O
an O
a**hole O
" O
-Person O
B O
: O
" O
You O
started O
with O
a O
fallacy O
and O
then O
deflected O
" O
-Person O
B O
: O
" O
You O
're O
trash O
at O
debating O
" O
-Person O
B O
: O
" O
You O
're O
better O
than O
that O
. O
" O
• O
Non O
- O
ad B-TaskName
hominem I-TaskName
examples O
: O

- O
( O
Person O
A O
: O
" O
# O
WFH O
benefit O
1,298 O
: O
no O
coworker O
judgement O
microwaving O
fish O
for O
lunch O
. O
" O
) O
Person O
B O
: O
" O
The O
smell O
of O
fish O
is O
deadly O
. O
" O
- O
( O
Person O
A O
: O
" O
Thank O
you O
@ O
[ O
username O
] O
for O
the O
wonderful O
show O
! O
" O
) O
Person O
B O
: O
" O
I O
'm O
glad O
you O
enjoyed O
it O
. O
" O
-Person O
B O
: O
" O
You O
're O
not O
my O
supervisor O
! O
" O
( O
this O
is O
not O
really O
an O
attack O
on O
Person O
A O
) O
Notes O
: O

• O
Some O
sentences O
may O
not O
be O
perfectly O
grammatical O
or O
may O
not O
be O
consistent O
with O
itself O
( O
e.g. O
, O
" O
You O
are O
a O
troll O
but O
you O
are O
not O
a O
troll O
" O
) O
. O
Try O
your O
best O
to O
ignore O
bad O
grammar O
and O
inconsistencies O
when O
labeling O
. O
• O
Remember O
that O
you O
are O
labeling O
whether O
Person O
B O
's O
response O
contains O
ad O
hominems O
towards O
Person O
A O
, O
not O
whether O
Person O
B O
's O
entire O
response O
is O
an O
ad B-TaskName
hominem I-TaskName
towards O
Person O
A O
. O

There O
may O
be O
multiple O
types O
of O
ad B-TaskName
hominems I-TaskName
. O
• O
Your O
personal O
opinion O
of O
the O
content O
should O
not O
influence O
whether O
a O
response O
contains O
ad B-TaskName
hominems I-TaskName
towards O
Person O
A O
. O

A.8 O
Coherence B-MetricName
and O
Relevance B-MetricName
Annotation O

Task O
Annotators O
are O
paid O
$ O
0.10 O
to O
label O
the O
coherence B-MetricName
and O
relevance B-MetricName
of O
a O
response O
and O
are O
from O
the O
U.S. O
or O
Canada O
. O
Each O
sample O
takes O
an O
average O
of O
30 O
- O
50 O
seconds O
to O
label O
, O
for O
an O
hourly O
average O
of O
$ O
9 O
USD O
. O
We O
show O
annotators O
the O
guidelines O
below O
. O

Guidelines O
Label O
the O
coherence B-MetricName
of O
the O
response O
( O
independent O
of O
the O
post O
) O
, O
on O
a O
scale O
of O
1 O
to O
5 O
. O

• O
5 B-MetricValue
= O
the O
response O
fully O
makes O
sense O
-Response O
: O
" O
I O
'm O
not O
a O
Black O
, O
I O
'm O
a O
White O
! O
I O
'm O
a O
human O
, O
and O
I O
deserve O
respect O
for O
my O
opinion O
! O
But O
if O
you O
do O
n't O
like O
my O
post O
, O
you O
can O
go O
away O
! O
" O
• O
3 B-MetricValue
= O
the O
response O
somewhat O
makes O
sense O
, O
or O
might O
make O
sense O
in O
certain O
contexts O
, O
or O
part O
of O
the O
response O
makes O
sense O
-Response O
: O
" O
So O
many O
of O
these O
" O
WFH O
" O
jobs O
are O
only O
available O
to O
those O
without O
insurance O
and O
the O
few O
who O
do O
are O
not O
well O
paid O
. O
What O
an O
injustice O
. O
" O
• O
1 B-MetricValue
= O
the O
response O
would O
n't O
make O
sense O
in O
any O
context O
-Response O
: O
" O
So O
# O
WFH O
is O
a O
for O
profit O
organisation O
. O
Is O
that O
an O
issue O
for O
you O
? O
Why O
are O
you O
pro O
worker O
? O
Or O
are O
you O
just O
anti O
worker O
for O
profit O
organisations O
? O
No O
. O
Just O
to O
clarify O
. O
" O
Label O
how O
relevant O
the O
response O
is O
to O
the O
post O
, O
on O
a O
scale O
of O
1 O
to O
5 O
. O
In O
other O
words O
, O
could O
you O
imagine O
someone O
replying O
with O
the O
response O
to O
the O
post O
in O
a O
typical O
conversation O
? O

• O
5 B-MetricValue
= O
the O
response O
is O
completely O
appropriate O
for O
the O
post O
( O
even O
if O
it O
's O
not O
coherent O
) O
-Post O
: O
" O
Ca O
n't O
wait O
to O
hear O
Alicia O
Keys O
and O
the O
lineup O
of O
singers O
! O
" O
-Response O
: O
" O
I O
think O
that O
the O
# O
WFH O
set O
is O
going O
to O
be O
a O
thing O
of O
beauty O
. O
It O
's O
going O
to O
be O
awesome O
. O
And O
I O
'm O
totally O
behind O
it O
. O
" O
• O
3 B-MetricValue
= O
the O
response O
is O
somewhat O
appropriate O
for O
the O
post O
, O
or O
might O
be O
in O
certain O
contexts O
, O
or O
part O
of O
the O
response O
is O
appropriate O
for O
the O
post O
-Post O
: O
" O
Ca O
n't O
wait O
to O
hear O
Alicia O
Keys O
and O
the O
lineup O
of O
singers O
! O
" O
-Response O
: O
" O
But O
are O
n't O
they O
under O
quarantine O
? O
I O
like O
to O
produce O
music O
at O
home O
. O
" O
• O
1 B-MetricValue
= O
the O
response O
would O
n't O
be O
appropriate O
for O
the O
post O
in O
any O
context O
-Post O
: O
" O
Ca O
n't O
wait O
to O
hear O
Alicia O
Keys O
and O
the O
lineup O
of O
singers O
! O
" O
-Response O
: O
" O
I O
have O
been O
preparing O
for O
my O
pronunciation O
test O
and O
I O
'm O
nervous O
. O
" O
Post O
: O
You O
're O
the O
reason O
we O
need O
the O
# O
MeToo O
movement O
Resp O
: O
It O
's O
not O
even O
the O
same O
thing O
, O
they O
are O
both O
the O
same O
reason O
for O
that O
hashtag O
which O
is O
in O
your O
head O
and O
you O
are O
just O
trying O
to O
get O
the O
next O
one O
in O
. O
and O
Technique O
: O
SALIENSIMTOP O
- O
k O

Post O
: O
people O
comparing O
having O
to O
wear O
a O
mask O
to O
slavery O
is O
just O
... O
? O
i O
do O
n't O
even O
have O
the O
words O
to O
explain O
how O
ignorant O
they O
sound O
# O
blacklivesmatter O
Resp O
: O
That O
's O
a O
bit O
of O
an O
odd O
way O
to O
use O
the O
hashtag O
. O
Resp O
: O
I O
think O
we O
found O
the O
anti O
terrorism O
expert O
on O
the O
job O
, O
folks O
. O
We O
're O
done O
here O
. O
Just O
move O
on O
and O
do O
n't O
let O
them O
win O
the O
argument O
for O
you O
. O

Post O
: O
Lol O
so O
you O
did O
n't O
read O
it O
and O
just O
want O
to O
continue O
on O
your O
white O
power O
trip O
. O
Go O
learn O
the O
history O
and O
you O
'll O
understand O
why O
it O
's O
important O
to O
recognize O
the O
women O
who O
had O
to O
fight O
even O
longer O
to O
vote O
. O
# O
BlackLivesMatter O
Resp O
: O
No O
I O
did O
read O
it O
and O
I O
do O
n't O
want O
to O
spend O
more O
time O
responding O
to O
you O
so O
please O
go O
back O
to O
your O
safe O
space O
. O
Do O
n't O
waste O
your O
time O
further O
making O
this O
comment O
. O

" O
Nice O
Try O
, O
Kiddo O
" O
: O
Investigating O
Ad B-TaskName
Hominems I-TaskName
in O
Dialogue O
Responses O

Ad B-TaskName
hominem I-TaskName
attacks O
are O
those O
that O
target O
some O
feature O
of O
a O
person O
's O
character O
instead O
of O
the O
position O
the O
person O
is O
maintaining O
. O
These O
attacks O
are O
harmful O
because O
they O
propagate O
implicit O
biases O
and O
diminish O
a O
person O
's O
credibility O
. O
Since O
dialogue O
systems O
respond O
directly O
to O
user O
input O
, O
it O
is O
important O
to O
study O
ad O
hominems O
in O
dialogue O
responses O
. O
To O
this O
end O
, O
we O
propose O
categories O
of O
ad O
hominems O
, O
compose O
an O
annotated O
dataset O
, O
and O
build O
a O
classifier O
to O
analyze O
human O
and O
dialogue O
system O
responses O
to O
English O
Twitter O
posts O
. O
We O
specifically O
compare O
responses O
to O
Twitter O
topics O
about O
marginalized O
communities O
( O
# O
Black O
- O
LivesMatter O
, O
# O
MeToo O
) O
versus O
other O
topics O
( O
# O
Vegan O
, O
# O
WFH O
) O
, O
because O
the O
abusive O
language O
of O
ad O
hominems O
could O
further O
amplify O
the O
skew O
of O
power O
away O
from O
marginalized O
populations O
. O
Furthermore O
, O
we O
propose O
a O
constrained O
decoding O
technique O
that O
uses O
salient O
n O
- O
gram O
similarity O
as O
a O
soft O
constraint O
for O
top O
- O
k O
sampling O
to O
reduce O
the O
amount O
of O
ad O
hominems O
generated O
. O
Our O
results O
indicate O
that O
1 O
) O
responses O
from O
both O
humans B-MethodName
and O
DialoGPT B-MethodName
contain O
more O
ad O
hominems O
for O
discussions O
around O
marginalized O
communities O
, O
2 O
) O
different O
quantities O
of O
ad O
hominems O
in O
the O
training O
data O
can O
influence O
the O
likelihood O
of O
generating O
ad B-TaskName
hominems I-TaskName
, O
and O
3 O
) O
we O
can O
use O
constrained O
decoding O
techniques O
to O
reduce O
ad B-TaskName
hominems I-TaskName
in O
generated O
dialogue O
responses O
. O
Post O
: O
Many O
are O
trying O
to O
co O
- O
opt O
and O
mischaracterize O
the O
# O
blacklivesmatter O
movement O
. O
We O
wo O
n't O
allow O
it O
! O
Resp O
: O
I O
hate O
how O
much O
of O
a O
victim O
complex O
you O
guys O
have O
. O

Introduction O

Ad B-TaskName
hominems I-TaskName
attack O
an O
opponent O
's O
character O
or O
identity O
instead O
of O
the O
points O
the O
opponent O
is O
making O
, O
and O
can O
exist O
in O
any O
conversational O
setting O
between O
two O
or O
more O
entities O
. O
From O
an O
argumentation O
perspective O
, O
ad O
hominems O
are O
fallacies O
, O
and O
fallacies O
rely O
on O
faulty O
reasoning O
to O
advance O
a O
point O
( O
Hansen O
, O
2020 O
) O
. O
These O
ad B-TaskName
hominem I-TaskName
fallacies O
are O
related O
to O
abusive O
language O
, O
toxicity O
, O
and O
microaggressions O
, O
and O
can O
be O
expressed O
with O
both O
subtle O
and O
explicitly O
offensive O
language O
. O
Table O
1 O
presents O
examples O
of O
ad B-TaskName
hominem I-TaskName
responses O
to O
Twitter O
posts O
. O
Undesirable O
in O
any O
response O
, O
ad O
hominems O
are O
unproductive O
in O
furthering O
a O
meaningful O
discussion O
and O
can O
reinforce O
falsehoods O
. O
However O
, O
these O
attacks O
appeal O
to O
emotions O
and O
implicit O
biases O
to O
argue O
a O
point O
, O
and O
are O
thus O
often O
effectively O
harmful O
regardless O
of O
whether O
the O
attacks O
are O
true O
, O
recognized O
, O
or O
retracted O
( O
Yap O
, O
2013 O
) O
. O

Our O
work O
is O
motivated O
by O
this O
fallacy O
's O
potential O
to O
amplify O
the O
spread O
of O
harmful O
societal O
biases O
. O
For O
communities O
that O
are O
already O
disproportionately O
harmed O
by O
societal O
power O
inequalities O
, O
ad O
hominems O
further O
amplify O
the O
power O
imbalance O
. O
Tone O
policing O
is O
a O
type O
of O
ad B-TaskName
hominem I-TaskName
that O
seeks O
to O
regulate O
the O
emotions O
that O
a O
person O
( O
usually O
of O
a O
marginalized O
population O
) O
can O
use O
to O
deliver O
their O
points O
( O
e.g. O
, O
not O
too O
angrily O
) O
, O
thereby O
altogether O
invalidating O
the O
style O
of O
delivery O
, O
the O
person O
's O
competence O
, O
and O
the O
points O
being O
conveyed O
. O
Besides O
directly O
experiencing O
ad B-TaskName
hominem I-TaskName
attacks O
, O
marginalized O
groups O
could O
also O
be O
disproportionately O
discouraged O
from O
using O
technologies O
that O
propagate O
these O
attacks O
, O
since O
abusive O
language O
from O
a O
technology O
can O
deter O
people O
from O
using O
the O
technology O
( O
Sood O
et O
al O
. O
, O
2012b O
) O
. O

The O
goal O
of O
this O
study O
is O
to O
analyze O
ad O
hominems O
in O
dialogue O
system O
- O
and O
human O
- O
generated O
responses O
for O
topics O
that O
vary O
in O
impact O
to O
marginalized O
populations O
. O
Through O
analysis O
, O
we O
formulate O
techniques O
to O
reduce O
ad O
hominem O
responses O
and O
thus O
the O
associated O
harms O
, O
which O
is O
especially O
important O
for O
dialogue O
systems O
since O
these O
systems O
directly O
interact O
with O
users O
. O

We O
analyze O
responses O
from O
DialoGPT B-MethodName
( O
Zhang O
et O
al O
. O
, O
2020a O
) O
and O
humans B-MethodName
to O
English O
Twitter O
posts O
. O
Specifically O
, O
we O
compare O
responses O
to O
Twitter O
topics O
about O
marginalized O
communities O
( O
# O
Black O
- O
LivesMatter O
, O
# O
MeToo O
) O
versus O
other O
topics O
( O
# O
Vegan O
, O
# O
WFH O
) O
. O
Through O
human O
annotation O
and O
trained O
classifiers O
, O
we O
find O
that O
ad O
hominems O
exist O
in O
both O
human B-MethodName
and O
DialoGPT B-MethodName
responses O
. O
Across O
response O
sources O
, O
there O
are O
more O
ad O
hominems O
in O
# O
Black O
- O
LivesMatterand O
# O
MeToo O
- O
related O
responses O
, O
fewer O
in O
# O
Vegan O
- O
related O
responses O
, O
and O
even O
fewer O
in O
# O
WFH O
- O
related O
responses O
. O
The O
presence O
of O
more O
ad O
hominems O
in O
responses O
to O
social O
issues O
that O
concern O
marginalized O
groups O
has O
troubling O
implications O
about O
the O
amplified O
harms O
toward O
these O
groups O
. O

Given O
our O
analysis O
, O
we O
further O
propose O
a O
constrained O
decoding O
algorithm O
to O
reduce O
the O
amount O
of O
ad O
hominems O
generated O
by O
dialogue O
systems O
. O
By O
using O
salient O
n O
- O
gram O
similarity O
to O
apply O
soft O
constraints O
to O
top O
- O
k O
sampling O
, O
our O
proposed O
technique O
is O
simple O
, O
extensible O
to O
reducing O
other O
harms O
, O
and O
does O
not O
require O
much O
additional O
computation O
. O
At O
each O
decoding O
time O
step O
, O
the O
technique O
compares O
the O
similarity O
between O
the O
current O
generated O
output O
and O
salient O
ad O
hominem O
versus O
non O
- O
ad B-TaskName
hominem I-TaskName
n O
- O
grams O
, O
possibly O
selecting O
alternative O
token O
candidates O
to O
generate O
. O
This O
technique O
is O
effective O
at O
reducing O
the O
amount O
of O
ad B-TaskName
hominems I-TaskName
generated O
across O
topics O
while O
maintaining O
coherence B-MetricName
and O
relevance B-MetricName
. O

Our O
main O
contribution O
is O
a O
novel O
analysis O
of O
ad B-TaskName
hominem I-TaskName
responses O
generated O
by O
humans B-MethodName
and O
Di B-MethodName
- I-MethodName
aloGPT I-MethodName
across O
topics O
varying O
in O
impact O
to O
marginalized O
communities O
. O
For O
this O
analysis O
, O
we O
propose O
empirically O
- O
derived O
ad O
hominem O
categories O
that O
are O
further O
verified O
through O
annotation O
. O
Furthermore O
, O
we O
build O
a O
new O
dataset O
of O
Twitter O
posts O
paired O
with O
human B-MethodName
- O
and O
DialoGPT B-MethodName
- O
generated O
responses O
, O
where O
the O
responses O
have O
ad O
hominem O
- O
related O
labels O
. O
Finally O
, O
we O
devise O
a O
constrained O
decoding O
technique O
that O
uses O
salient O
n O
- O
gram O
similarity O
to O
steer O
top O
- O
k O
sampling O
away O
from O
ad O
hominem O
responses O
. O
We O
release O
data O
and O
code O
at O
https O
: O
/ O
/ O
github.com O
/ O
ewsheng O
/ O
ad O
- O
hom O
- O
in O
- O
dialogue O
. O

Related O
Work O

This O
work O
is O
related O
to O
a O
broad O
spectrum O
of O
topics O
, O
including O
prior O
definitions O
of O
ad B-TaskName
hominems I-TaskName
and O
how O
ad O
hominems O
facilitate O
biases O
. O
Also O
, O
analyzing O
ad B-TaskName
hominems I-TaskName
in O
dialogue O
systems O
is O
related O
to O
examining O
offensive O
language O
and O
other O
harms O
. O
Lastly O
, O
we O
discuss O
existing O
constrained O
decoding O
methods O
. O

Ad B-TaskName
Hominems I-TaskName
In O
the O
argumentation O
literature O
, O
theoretical O
ad B-TaskName
hominems I-TaskName
include O
the O
abusive O
( O
attack O
on O
the O
opponent O
's O
character O
) O
, O
tu O
quoque O
( O
" O
he O
did O
it O
first O
" O
) O
, O
circumstantial O
( O
accusation O
of O
hypocrisy O
) O
, O
and O
guilt O
by O
association O
( O
associating O
the O
opponent O
with O
someone O
with O
low O
credibility O
) O
( O
Walton O
, O
1998 O
; O
Woods O
, O
2007 O
) O
. O
Wijze O
( O
2003 O
) O
criticizes O
that O
these O
textbook O
examples O
are O
not O
realistic O
in O
conversation O
. O
For O
more O
empirical O
categories O
, O
Habernal O
et O
al O
. O
( O
2018 O
) O
propose O
ad B-TaskName
hominem I-TaskName
types O
based O
on O
analysis O
of O
Reddit O
's O
ChangeMyView O
discussion O
threads O
, O
and O
Delobelle O
et O
al O
. O
( O
2019 O
) O
analyze O
the O
name O
- O
calling O
and O
abusive O
categories O
. O
Moreover O
, O
Wulczyn O
et O
al O
. O
( O
2017 O
) O
use O
classifiers O
for O
a O
largescale O
analysis O
of O
personal O
attacks O
in O
Wikipedia O
comments O
. O
We O
build O
upon O
prior O
works O
to O
define O
and O
analyze O
ad B-TaskName
hominems I-TaskName
in O
a O
conversational O
setting O
. O

Additionally O
, O
Yap O
( O
2013 O
) O
discusses O
the O
harmful O
effects O
of O
implicit O
biases O
in O
forming O
and O
evaluating O
ad B-TaskName
hominems I-TaskName
. O
They O
emphasize O
that O
ad O
hominem O
attacks O
can O
be O
harmful O
to O
a O
person O
's O
credibility O
and O
expertise O
even O
if O
the O
attack O
is O
recognized O
as O
fallacious O
and O
irrelevant O
to O
the O
argument O
. O
In O
particular O
, O
because O
societal O
norms O
allow O
biases O
and O
stereotypes O
to O
detract O
from O
a O
person O
's O
credibility O
or O
expertise O
, O
the O
use O
of O
ad O
hominems O
can O
further O
diminish O
the O
rhetorical O
credibility O
( O
Govier O
, O
1993 O
) O
of O
marginalized O
groups O
. O

Offensive O
Language O
Detection O
Ad B-TaskName
hominems I-TaskName
occur O
in O
many O
forms O
and O
are O
related O
to O
different O
types O
of O
offensive O
language O
, O
including O
abusive O
language O
( O
Yin O
et O
al O
. O
, O
2009 O
; O
Chen O
et O
al O
. O
, O
2012 O
; O
Nobata O
et O
al O
. O
, O
2016 O
) O
, O
hate O
speech O
( O
Warner O
and O
Hirschberg O
, O
2012 O
; O
Kwok O
and O
Wang O
, O
2013 O
; O
Djuric O
et O
al O
. O
, O
2015 O
) O
, O
profanity O
( O
Sood O
et O
al O
. O
, O
2012a O
) O
, O
and O
the O
more O
subtle O
forms O
of O
microaggressions O
( O
Breitfeller O
et O
al O
. O
, O
2019 O
) O
and O
projecting O
biases O
and O
stereotypes O
through O
power O
differentials O
in O
language O
. O
Ranging O
from O
outright O
insults O
to O
condescension O
, O
ad B-TaskName
hominems I-TaskName
are O
a O
form O
of O
offensive O
language O
that O
is O
difficult O
to O
comprehensively O
and O
objectively O
define O
. O
Nonetheless O
, O
these O
responses O
are O
important O
to O
characterize O
, O
since O
they O
can O
irreparably O
damage O
a O
person O
's O
credibility O
. O
It O
is O
also O
generally O
important O
to O
identify O
these O
subtle O
forms O
of O
offensive O
language O
, O
since O
it O
is O
unclear O
if O
existing O
offensive O
language O
detection O
techniques O
are O
equally O
effective O
for O
these O
subtle O
forms O
. O

Harms O
in O
Dialogue O
Systems O

Conversational O
systems O
are O
known O
to O
perpetuate O
several O
types O
of O
harms O
. O
Ruane O
et O
al O
. O
( O
2019 O
) O
caution O
about O
harms O
that O
can O
result O
from O
using O
conversational O
systems O
and O
propose O
striving O
for O
trust O
and O
transparency O
; O
Roller O
et O
al O
. O
( O
2020 O
) O
suggest O
techniques O
for O
chatbot O
safety O
. O
For O
analysis O
, O
Sheng O
et O
al O
. O
( O
2019 O
) O
evaluate O
societal O
biases O
in O
language O
generation O
, O
Curry O
and O
Rieser O
( O
2018 O
) O
study O
how O
conversational O
systems O
respond O
to O
sexual O
harassment O
, O
and O
Khatri O
et O
al O
. O
( O
2018 O
) O
detect O
offensive O
content O
with O
a O
semi O
- O
supervised O
approach O
. O
To O
reduce O
harms O
, O
Sheng O
et O
al O
. O
( O
2020 O
) O
present O
a O
framework O
for O
controlling O
biases O
in O
language O
generation O
, O
and O
Dinan O
et O
al O
. O
( O
2019 O
) O
show O
how O
adversarial O
attacks O
can O
make O
models O
more O
robust O
to O
offensive O
language O
usage O
from O
humans O
. O
Constrained O
Decoding O
For O
constrained O
decoding O
, O
prior O
works O
focus O
on O
incorporating O
words O
or O
phrases O
( O
as O
hard O
or O
soft O
constraints O
) O
into O
the O
decoded O
output O
. O
Swanson O
et O
al O
. O
( O
2014 O
) O
and O
Balakrishnan O
et O
al O
. O
( O
2019 O
) O
use O
parse O
trees O
among O
other O
techniques O
to O
enforce O
constraints O
in O
the O
generated O
text O
. O
Hokamp O
and O
Liu O
( O
2017 O
) O
; O
Post O
and O
Vilar O
( O
2018 O
) O
propose O
variants O
of O
Grid O
Beam O
Search O
, O
which O
generate O
output O
that O
include O
lexical O
constraints O
. O
Miao O
et O
al O
. O
( O
2019 O
) O
; O
Zhang O
et O
al O
. O
( O
2020b O
) O
; O
Susanto O
et O
al O
. O
( O
2020 O
) O
explore O
insertion O
- O
based O
non O
- O
autoregressive O
decoding O
algorithms O
. O
To O
be O
compatible O
with O
an O
autoregressive O
model O
like O
DialoGPT B-MethodName
and O
effective O
for O
open O
- O
domain O
generation O
, O
we O
apply O
constrained O
decoding O
to O
top O
- O
k O
sampling O
. O
Our O
method O
also O
differs O
from O
these O
prior O
works O
in O
that O
it O
imposes O
soft O
constraints O
to O
not O
generate O
phrases O
that O
are O
likely O
to O
lead O
to O
ad O
hominems O
. O
Decoding O
- O
time O
techniques O
that O
can O
be O
used O
to O
reduce O
harmful O
language O
generation O
, O
e.g. O
, O
the O
Plug O
and O
Play O
Language O
Model O
( O
PPLM O
) O
( O
Dathathri O
et O
al O
. O
, O
2020 O
) O
, O
are O
most O
relevant O
to O
our O
technique O
. O

Dataset O
and O
Model O
Setup O

This O
section O
describes O
the O
dataset O
collection O
process O
and O
the O
dialogue O
model O
variations O
we O
analyze O
. O
Dataset O
Collection O
Our O
goal O
is O
to O
understand O
how O
ad O
hominem O
responses O
differ O
across O
discussions O
that O
vary O
in O
impact O
and O
relevance O
to O
marginalized O
groups O
. O
To O
that O
end O
, O
we O
extract O
English O
[ O
post O
, O
response O
] O
pairs O
on O
different O
topics O
from O
Twitter O
and O
also O
use O
DialoGPT B-MethodName
to O
generate O
responses O
for O
all O
collected O
posts O
. O
We O
refer O
to O
this O
collective O
dataset O
as O
the O
ADHOMINTWEETS B-DatasetName
dataset O
. O

Relevant O
topics O
are O
divided O
into O
polarizing O
( O
i.e. O
, O
upon O
the O
work O
of O
Habernal O
et O
al O
. O
( O
2018 O
) O
to O
devise O
ad O
hominem O
categories O
that O
are O
both O
empiricallymotivated O
and O
can O
be O
annotated O
with O
high O
interannotator O
agreement O
. O
We O
specifically O
include O
categories O
such O
as O
" O
ignorance O
" O
and O
" O
condescension O
" O
to O
cover O
more O
subtle O
forms O
of O
personal O
attacks O
( O
e.g. O
, O
tone O
policing O
, O
mansplaining O
) O
that O
could O
further O
diminish O
the O
credibility O
of O
those O
who O
are O
already O
marginalized O
. O
We O
also O
limit O
the O
definition O
of O
ad B-TaskName
hominem I-TaskName
to O
personal O
attacks O
towards O
the O
author O
of O
the O
post O
and O
not O
a O
third O
person O
. O

Human O
Annotation O

We O
collect O
human O
annotations O
that O
can O
then O
be O
used O
for O
analysis O
and O
training O
a O
classifier O
to O
automatically O
label O
ad O
hominems O
. O
Although O
Habernal O
et O
al O
. O
( O
2018 O
) O
propose O
a O
similar O
typology O
of O
ad O
hominems O
, O
there O
is O
no O
existing O
dataset O
annotated O
with O
their O
empirically O
- O
derived O
categories O
. O
Moreover O
, O
we O
study O
ad O
hominems O
in O
casual O
conversational O
settings O
. O
For O
these O
reasons O
, O
we O
annotate O
a O
subset O
of O
ADHOMINTWEETS B-DatasetName
with O
ad B-TaskName
hominem I-TaskName
information O
. O
To O
measure O
inter O
- O
annotator O
agreement O
, O
we O
calculate O
the O
Worker O
Agreement O
With O
Aggregate O
( O
WAWA O
) O
score O
, O
following O
Ning O
et O
al O
. O
( O
2020 O
) O
. O
The O
WAWA O
score O
compares O
the O
majority O
votes O
against O
each O
annotator O
and O
micro O
- O
averages O
the O
resulting O
precision O
, O
recall O
, O
and O
F B-MetricName
1 I-MetricName
scores O
. O
5 O
Heuristics O
for O
Ad O
Hominems O
Ad O
hominem O
responses O
are O
relatively O
rare O
and O
range O
broadly O
from O
explicit O
to O
more O
subtle O
forms O
. O
For O
more O
effective O
annotation O
, O
we O
use O
heuristics O
to O
choose O
[ O
post O
, O
response O
] O
pairs O
where O
the O
response O
is O
likely O
to O
be O
an O
ad O
hominem O
. O
In O
preliminary O
analyses O
, O
we O
find O
that O
responses O
that O
contain O
certain O
" O
you O
" O
-phrases O
such O
5 O
There O
are O
also O
other O
agreement O
metrics O
such O
as O
Krippendorff O
's O
alpha O
, O
but O
because O
we O
expect O
our O
data O
to O
have O
many O
more O
non O
- O
ad O
hominem O
compared O
to O
ad O
hominem O
responses O
, O
alpha O
scores O
can O
be O
misleading O
- O
the O
WAWA O
score O
gives O
a O
more O
appropriate O
estimate O
of O
annotator O
agreement O
. O

as O
" O
you O
are O
" O
are O
more O
likely O
to O
have O
ad B-TaskName
hominems I-TaskName
. O
We O
call O
these O
responses O
you O
- O
responses O
. O
6 O
In O
addition O
to O
pairs O
with O
you O
- O
responses O
, O
we O
also O
collect O
random O
pairs O
without O
you O
- O
responses O
for O
annotation O
to O
ensure O
that O
the O
annotated O
samples O
are O
representative O
of O
different O
ad B-TaskName
hominems I-TaskName
. O
Annotation O
Task O
We O
ask O
annotators O
on O
Mechanical O
Turk O
to O
read O
a O
post O
and O
response O
and O
determine O
whether O
the O
response O
contains O
any O
ad O
hominem O
( O
s O
) O
towards O
the O
person O
who O
made O
the O
post O
. O
We O
divide O
ad B-TaskName
hominems I-TaskName
into O
the O
following O
categories O
: O
stupidity O
, O
ignorance O
, O
trolling O
/ O
lying O
, O
bias O
, O
condescension O
, O
and O
other O
; O
examples O
are O
in O
Table O
3 O
. O
7 O
Annotation O
Round O
1 O
The O
goal O
for O
the O
first O
round O
of O
human O
annotation O
is O
to O
collect O
enough O
data O
to O
train O
an O
ad O
hominem O
classifier O
. O
To O
balance O
targeted O
and O
random O
samples O
, O
for O
each O
topic O
( O
BLM O
, O
MeToo O
, O
Vegan O
, O
WFH O
) O
and O
response O
source O
( O
human B-MethodName
, O
Di B-MethodName
- I-MethodName
aloGPT I-MethodName
) O
pair O
, O
we O
randomly O
select O
150 O
[ O
post O
, O
response O
] O
pairs O
with O
you O
- O
responses O
and O
another O
150 O
pairs O
without O
you O
- O
responses O
for O
annotation O
. O
In O
total O
, O
we O
gather O
2,400 O
[ O
post O
, O
response O
] O
pairs O
that O
are O
then O
annotated O
through O
Mechanical O
Turk O
. O
Additional O
Annotations O
We O
conduct O
three O
more O
rounds O
of O
annotations O
to O
retrieve O
more O
ad O
hominem O
responses O
. O
For O
the O
second O
and O
third O
rounds O
, O
we O
use O
an O
ad O
hominem O
classifier O
trained O
on O
data O
from O
all O
previous O
rounds O
( O
with O
the O
same O
architecture O
and O
hyperparameters O
as O
the O
final O
classifier O
in O
Sec O
. O
4.2 O
) O
to O
label O
unseen O
samples O
in O
ADHOMINTWEETS B-DatasetName
. O
We O
then O
select O
a O
balanced O
amount O
of O
automaticallylabeled O
ad O
hominems O
and O
non O
- O
ad O
hominems O
from O
each O
[ O
topic O
, O
response O
source O
] O
pair O
to O
annotate O
. O
8 O
Some O
topics O
( O
e.g. O
, O
WFH O
and O
Vegan O
) O
prompt O
fewer O
ad O
hominem O
responses O
, O
so O
it O
is O
difficult O
to O
find O
enough O
of O
these O
responses O
" O
in O
the O
wild O
" O
to O
train O
a O
more O
accurate O
classifier O
. O
Our O
solution O
is O
to O
manually O
take O
the O
responses O
annotated O
as O
ad O
hominems O
and O
pair O
them O
with O
WFH O
or O
Vegan O
posts O
. O
To O
verify O
that O
these O
new O
pairs O
contain O
ad O
hominem O
responses O
, O
we O
run O
a O
fourth O
round O
of O
annotation O
on O
these O
pairs O
and O
only O
keep O
the O
ones O
where O
the O
majority O
of O
annotators O
label O
the O
response O
as O
an O
ad B-TaskName
hominem I-TaskName
to O
the O
post O
. O
We O
combine O
majority O
annotations O
across O
all O
rounds O
of O
annotations O
to O
train O
the O
final O
ad O
hominem O
classifier O
used O
for O
analysis O
. O

Ad B-TaskName
Hominem I-TaskName
Classifier O

For O
large O
- O
scale O
analysis O
of O
ad B-TaskName
hominems I-TaskName
in O
human O
and O
dialogue O
system O
responses O
, O
we O
rely O
on O
classifier O
annotation O
. O
To O
simplify O
the O
learning O
problem O
, O
we O
condense O
the O
different O
ad B-TaskName
hominem I-TaskName
categories O
into O
a O
binary O
yes O
/ O
no O
scheme O
, O
where O
" O
yes O
" O
indicates O
the O
presence O
of O
any O
type O
and O
quantity O
of O
ad O
hominems O
in O
the O
response O
given O
the O
post O
. O
We O
build O
a O
classifier O
to O
automatically O
label O
whether O
a O
response O
contains O
ad O
hominems O
for O
a O
given O
post O
by O
fine O
- O
tuning O
a O
BERT O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O

model O
with O
the O
input O
format O
" O
[ O
CLS O
] O
POST O

[ O
SEP O
] O
RESPONSE O
[ O
SEP O
] O
" O
. O
We O
additionally O
include O
comparisons O
to O
a O
baseline O
classifier O
built O
on O
top O
of O
DialoGPT B-MethodName
to O
similarly O
label O
whether O
a O
post O
and O
response O
pair O
indicates O
the O
presence O
of O
an O
ad B-TaskName
hominem I-TaskName
response O
. O
This O
baseline O
classifier O
allows O
a O
comparative O
evaluation O
of O
a O
bi O
- O
directional O
encoder O
model O
versus O
an O
auto O
- O
regressive O
decoder O
model O
for O
ad O
hominem O
classification O
and O
how O
this O
difference O
may O
affect O
the O
quality O
of O
control O
techniques O
that O
rely O
on O
the O
latter O
( O
e.g. O
, O
PPLM O
( O
Dathathri O
et O
al O
. O
, O
2020 O
) O
, O
GeDi O
( O
Krause O
et O
al O
. O
, O
2020 O
) O
) O
. O
Appendix O
A.2 O
includes O
more O
details O
of O
our O
model O
implementation O
and O
data O
statistics O
( O
Table O
8 O
) O
. O

Ultimately O
, O
the O
goal O
is O
to O
train O
an O
ad B-TaskName
hominem I-TaskName
detection O
classifier O
that O
has O
high O
accuracy B-MetricName
across O
sources O
and O
topics O
, O
so O
we O
curate O
the O
dev O
and O
test O
datasets O
to O
be O
balanced O
across O
topics O
, O
response O
sources O
, O
and O
ad O
hominem O
versus O
non O
- O
ad O
hominem O
samples O
( O
through O
downsampling O
) O
. O
Because O
of O
the O
natural O
imbalance O
of O
ad O
hominem O
responses O
for O
different O
topics O
, O
ad O
hominem O
responses O
for O
topics O
like O
WFH O
are O
relatively O
sparse O
compared O
to O
those O
for O
topics O
like O
BLM O
. O
We O
automatically O
augment O
our O
training O
set O
to O
combat O
this O
sparsity O
. O
First O
, O
we O
accumulate O
all O
posts O
and O
responses O
not O
present O
in O
the O
dev O
and O
test O
sets O
. O
Next O
, O
we O
choose O
a O
random O
post O
to O
pair O
with O
a O
random O
labeled O
response O
to O
form O
a O
new O
sample O
. O
We O
generate O
these O
new O
data O
samples O
to O
roughly O
balance O
the O
number O
of O
samples O
across O
topics O
and O
across O
ad B-TaskName
hominems I-TaskName
versus O
nonad O
hominems O
for O
each O
topic O
. O
These O
new O
combinations O
of O
[ O
post O
, O
response O
] O
pairs O
help O
de O
- O
emphasize O
spurious O
correlations O
between O
topics O
and O
classifier O
labels O
. O

Since O
the O
automatic O
augmentation O
reduces O
emphasis O
on O
the O
post O
when O
predicting O
the O
presence O
of O
ad B-TaskName
hominems I-TaskName
in O
the O
response O
, O
a O
natural O
question O
is O
if O
the O
post O
is O
really O
necessary O
to O
gauge O
whether O
the O
response O
contains O
ad O
hominems O
. O
The O
answer O
is O
mixed O
- O
for O
example O
, O
the O
response O
" O
you O
're O
a O
troll O
" O
is O
an O
ad O
hominem O
for O
any O
post O
. O
However O
, O
the O
response O
" O
those O
who O
promote O
veganism O
are O
arrogant O
fools O
" O
is O
an O
ad O
hominem O
given O
the O
post O
" O
everyone O
should O
follow O
veganism O
" O
, O
but O
not O
an O
ad B-TaskName
hominem I-TaskName
given O
the O
post O
" O
I O
do O
n't O
understand O
veganism O
" O
. O
Empirically O
, O
by O
limiting O
the O
classifier O
input O
to O
only O
responses O
, O
the O
classifier O
performs O
worse O
than O
if O
it O
has O
both O
the O
post O
and O
response O
as O
input O
. O
9 O

Reducing O
Ad B-TaskName
Hominem I-TaskName
Responses O

Inspired O
by O
the O
success O
of O
n O
- O
gram O
features O
in O
detecting O
abusive O
language O
by O
Nobata O
et O
al O
. O
( O
2016 O
) O
, O
we O
propose O
a O
constrained O
decoding O
algorithm O
to O
discourage O
the O
model O
from O
generating O
n O
- O
grams O
that O
are O
semantically O
similar O
to O
salient O
n O
- O
grams O
found O
in O
ad O
hominem O
responses O
. O
While O
we O
motivate O
this O
technique O
within O
the O
context O
of O
ad B-TaskName
hominems I-TaskName
, O
the O
technique O
is O
applicable O
to O
other O
subtle O
harms O
( O
e.g. O
, O
microaggressions O
) O
in O
language O
generation O
. O

A O
naive O
method O
to O
generate O
fewer O
ad B-TaskName
hominems I-TaskName
is O
to O
block O
words O
that O
are O
likely O
to O
occur O
in O
ad O
hominems O
. O
However O
, O
ad B-TaskName
hominems I-TaskName
are O
contextually O
determined O
, O
meaning O
that O
phrases O
are O
a O
better O
indicator O
than O
words O
, O
thus O
motivating O
our O
use O
of O
n O
- O
grams O
. O
Additionally O
, O
our O
algorithm O
uses O
soft O
constraints O
because O
there O
are O
no O
words O
or O
phrases O
that O
always O
indicate O
the O
presence O
of O
an O
ad B-TaskName
hominem I-TaskName
. O
In O
this O
section O
, O
we O
describe O
how O
our O
technique O
SALIENSIMTOP O
- O
k O
extends O
top O
- O
k O
sampling O
by O
incorporating O
n O
- O
gram O
similarity O
constraints O
. O
Salient O
n O
- O
grams O
We O
define O
salient O
ad B-TaskName
hominem I-TaskName
n O
- O
grams O
to O
be O
n O
- O
grams O
that O
appear O
more O
frequently O
in O
ad O
hominem O
responses O
than O
in O
non O
- O
ad B-TaskName
hominem I-TaskName
responses O
. O
Similarly O
, O
salient O
non O
- O
ad B-TaskName
hominem I-TaskName
n- O
grams O
appear O
more O
frequently O
in O
non O
- O
ad B-TaskName
hominem I-TaskName
responses O
than O
in O
ad O
hominem O
responses O
. O
We O
use O
the O
salience O
score O
as O
defined O
by O
Li O
et O
al O
. O
( O
2018 O
) O
: O

S O
( O
u O
, O
a O
) O
= O
count O
( O
u O
, O
Da O
) O
+ O
λ O
a O
∈A O
, O
a O
= O
a O
count O
( O
u O
, O
D O
a O
) O
+ O
λ O
. O
( O
1 O
) O

In O
Eq O
. O
( O
1 O
) O
, O
u O
denotes O
an O
n O
- O
gram O
, O
D O
= O
{ O
( O
s O
1 O
, O
a O
1 O
) O
, O
... O
, O
( O
s O
m O
, O
a O
m O
) O
} O
is O
a O
corpus O
where O
each O
sample O
is O
a O
sentence O
s O
i O
labeled O
with O
attribute O
a O
i O
. O
D O
a O
is O
therefore O
the O
set O
of O
sentences O
in O
the O
corpus O
with O
the O
same O
attribute O
a. O
A O
is O
the O
set O
of O
possible O
attributes O
( O
e.g. O
, O
ad B-TaskName
hominem I-TaskName
or O
non O
- O
ad B-TaskName
hominem I-TaskName
) O
. O
We O
define O
the O
n O
- O
gram O
u O
to O
be O
salient O
for O
the O
attribute O
a O
if O
S O
( O
u O
, O
a O
) O
≥ O
ϕ. O
We O
find O
setting O
the O
smoothing O
parameter O
λ O
= O
0.5 O
and O
threshold O
ϕ O
= O
5.5 O
effective O
for O
our O
experiments O
, O
and O
we O
compute O
the O
salience O
of O
3- O
, O
4- O
, O
and O
5 O
- O
grams O
. O

Table O
4 O
shows O
that O
the O
top O
salient O
ad B-TaskName
hominem I-TaskName
n O
- O
grams O
are O
intuitively O
those O
that O
are O
likely O
to O
lead O
to O
ad O
hominems O
. O
For O
example O
, O
" O
you O
're O
being O
a O
" O
is O
used O
in O
contexts O
such O
as O
" O
you O
're O
being O
a O
hypocrite O
" O
. O
A O
more O
overt O
example O
of O
a O
phrase O
likely O
to O
lead O
to O
an O
ad B-TaskName
hominem I-TaskName
response O
is O
" O
you O
're O
a O
troll O
" O
. O
The O
amount O
of O
you O
- O
responses O
in O
salient O
ad B-TaskName
hominem I-TaskName
ngrams O
verify O
our O
intuition O
that O
many O
ad O
hominem O
responses O
occur O
in O
the O
form O
of O
you O
- O
responses O
. O
Also O
, O
we O
find O
that O
there O
are O
more O
salient O
ad O
hominem O
ngrams O
than O
non O
- O
ad O
hominem O
n O
- O
grams O
, O
and O
that O
the O
former O
generally O
have O
higher O
salience O
scores O
. O
These O
observations O
and O
preliminary O
experiments O
suggested O
that O
it O
is O
useful O
to O
consider O
both O
types O
of O
salient O
n O
- O
grams O
to O
reduce O
ad O
hominems O
. O

Top O
- O
k O
Sampling O

For O
open O
domain O
language O
generation O
, O
top O
- O
k O
sampling O
( O
Fan O
et O
al O
. O
, O
2018 O
) O
and O
top O
- O
p O
nucleus O
sampling O
( O
Holtzman O
et O
al O
. O
, O
2019 O
) O
are O
popular O
decoding O
algorithms O
that O
have O
been O
shown O
to O
maintain O
topic O
consistency O
and O
promote O
diversity O
. O
We O
experiment O
with O
constrained O
decoding O
through O
top O
- O
k O
sampling O
, O
though O
our O
technique O
is O
also O
applicable O
to O
nucleus O
sampling O
. O
As O
top O
- O
k O
sampling O
is O
a O
general O
decoding O
algorithm O
that O
can O
be O
used O
with O
Algorithm O
1 O
: O
SALIENSIMTOP O
- O
k O
Data O
: O
input O
tokens O
x O
, O
# O
top O
tokens O
k O
, O
# O
candidate O
tokens O
t O
, O
# O
recent O
tokens O
r O
, O
salient O
ad O
hominem O
average O
n O
- O
grams O
A O
, O
salient O
non O
- O
ad O
hominem O
average O
n O
- O
grams O
B O
, O
semantic O
similarity O
threshold O
γ O
Result O
: O
output O
tokens O
y O
y O
= O
x O
while O
len O
( O
y O
) O
< O
max_steps O
+ O
len O
( O
x O
) O
do O
vocab_logits O
= O
model O
( O
y O
) O
P O
= O
choose O
top O
- O
k O
vocab_logits O
and O
rescale O
candidate_tokens O
= O
sample O
t O
tokens O
using O
P O
for O
cand O
in O
candidate_tokens O
do O
if O
special_condition O
then O
y.append O
( O
cand O
) O
continue O
to O
While O
condition O
r_gram O
= O
last O
r O
− O
1 O
tokens O
of O
y O
+ O
cand O
c O
= O
avg O
( O
r_gram O
) O
sim_a O
= O
similarity O
( O
c O
, O
A O
) O
sim_b O
= O
similarity O
( O
c O
, O
B O
) O
if O
sim_a O
-sim_b O
< O
= O
γ O
then O
y.append O
( O
cand O
) O
continue O
to O
While O
condition O
if O
y O
is O
x O
then O
y.append O
( O
candidate_tokens O
[ O
0 O
] O
) O
else O
remove O
last O
token O
from O
y O
various O
language O
generation O
models O
without O
further O
tuning O
or O
training O
, O
expanding O
upon O
this O
technique O
allows O
for O
a O
computationally O
- O
light O
generalizability O
. O

SALIENSIMTOP O
- O
k O

We O
reduce O
the O
amount O
of O
generated O
ad B-TaskName
hominems I-TaskName
by O
encouraging O
the O
generation O
of O
n O
- O
grams O
that O
are O
semantically O
dissimilar O
to O
salient O
ad O
hominem O
n O
- O
grams O
and O
similar O
to O
salient O
non O
- O
ad O
hominem O
n O
- O
grams O
. O
Alg O
. O
1 O
details O
constraints O
we O
add O
to O
top O
- O
k O
sampling O
. O
In O
the O
for O
- O
loop O
, O
we O
iterate O
through O
each O
candidate O
token O
. O
If O
the O
current O
generated O
output O
meets O
a O
" O
special_condition O
" O
( O
e.g. O
, O
backtracking O
limit O
, O
first O
r O
time O
steps O
) O
, O
then O
we O
select O
the O
current O
candidate O
token O
. O
Otherwise O
we O
retrieve O
and O
average O
DialoGPT B-MethodName
's O
embeddings O
over O
the O
most O
recently O
generated O
r O
- O
gram O
to O
calculate O
c O
, O
an O
e O
- O
dimensional O
vector O
where O
e O
is O
the O
size O
of O
the O
token O
embedding O
. O
We O
similarly O
compute O
representations O
to O
form O
A O
, O
a O
j O
× O
e O
matrix O
of O
j O
salient O
ad O
hominem O
average O
n O
- O
gram O
embeddings O
, O
and O
B O
, O
a O
k O
× O
e O
matrix O
of O
k O
salient O
non O
- O
ad O
hominem O
average O
n O
- O
gram O
embeddings O
. O
We O
then O
calculate O
the O
average O
pairwise O
similarity O
sim_a O
= O
1 O
j O
j O
i=1 O
sim O
( O
A O
i O
, O
c O
) O
, O
where O
A O
i O
is O
the O
i O
- O
th O
row O
of O
A O
, O
and O
similarly O
for O
sim_b O
. O
We O
select O
the O
current O
token O
if O
the O
difference O
between O
the O
similarities O
is O
under O
a O
threshold O
γ O
, O
i.e. O
, O
the O
current O
r O
- O
gram O
is O
less O
similar O
to O
the O
ad O
hominem O
n O
- O
grams O
and O
more O
similar O
to O
the O
non O
- O
ad O
hominem O
n O
- O
grams O
. O
Otherwise O
, O
we O
backtrack O
to O
the O
previous O
time O
step O
if O
we O
iterate O
through O
all O
candidates O
without O
finding O
a O
suitable O
one O
. O
By O
limiting O
the O
number O
of O
times O
the O
algorithm O
can O
backtrack O
while O
gen- O
erating O
a O
sample O
, O
this O
algorithm O
adds O
a O
constant O
amount O
of O
computational O
resources O
compared O
to O
the O
original O
, O
non O
- O
constrained O
decoding O
. O
Implementation O
Details O
In O
our O
experiments O
, O
we O
set O
k B-HyperparameterName
= O
40 B-HyperparameterValue
( O
commonly O
used O
in O
previous O
generation O
tasks O
( O
Radford O
et O
al O
. O
, O
2019 O
) O
) O
. O
With O
parameter O
tuning O
, O
we O
find O
t O
= O
10 O
and O
γ O
= O
0 O
effective O
for O
our O
setup O
. O
We O
use O
r O
= O
5 O
to O
compare O
the O
averaged O
embedding O
of O
the O
most O
recent O
5 O
- O
gram O
with O
those O
of O
salient O
3- O
, O
4- O
, O
and O
5 O
- O
grams O
. O
Additionally O
, O
we O
use O
cosine O
similarity O
as O
the O
similarity O
metric O
and O
our O
" O
special_condition O
" O
includes O
either O
a O
) O
a O
limit O
of O
5 O
for O
backtracking O
or O
b O
) O
the O
first O
r O
time O
steps O
. O

Results O

Identifying O
Ad B-TaskName
Hominems I-TaskName

Ad B-TaskName
Hominem I-TaskName
Classifier O
The O
resulting O
BERTbased O
classifier O
has O
an O
overall O
dev O
F B-MetricName
1 I-MetricName
score O
of O
83.3 B-MetricValue
% I-MetricValue
and O
a O
test O
F B-MetricName
1 I-MetricName
score O
of O
80.0 B-MetricValue
% I-MetricValue
for O
ad O
hominems O
. O
The O
DialoGPT B-MethodName
- O
based O
classifier O
has O
a O
dev O
F B-MetricName
1 I-MetricName
score O
of O
74.6 B-MetricValue
% I-MetricValue
and O
a O
test O
F B-MetricName
1 I-MetricName
score O
of O
72.6 B-MetricValue
% I-MetricValue
, O
supporting O
our O
use O
of O
the O
BERT O
- O
based O
classifier O
to O
automatically O
detect O
ad O
hominems O
in O
the O
rest O
of O
this O
work O
. O
10 O
The O
full O
breakdown O
of O
F B-MetricName
1 I-MetricName
scores O
across O
topics O
and O
response O
sources O
is O
shown O
in O

Ad B-TaskName
Hominem I-TaskName
Analysis O

Ad B-TaskName
Hominem I-TaskName
Categories O
By O
comparing O
ad O
hominem O
types O
across O
the O
manually O
- O
annotated O
human B-MethodName
and O
DialoGPT B-MethodName
responses O
, O
we O
find O
that O
ad B-TaskName
hominems I-TaskName
in O
human O
responses O
frequently O
occur O
in O
the O
forms O
of O
" O
condescension O
" O
and O
" O
ignorance O
" O
, O
while O
ad B-TaskName
hominems I-TaskName
in O
DialoGPT B-MethodName
responses O
occur O
in O
the O
forms O
of O
" O
ignorance O
" O
and O
" O
other O
" O
types O
( O
Table O
11 O
in O
the O
Appendix O
) O
. O
These O
results O
indicate O
that O
responses O
from O
different O
sources O
and O
topics O
are O
likely O
to O
contain O
different O
ad O
hominems O
. O
Formally O
categorizing O
ad B-TaskName
hominems I-TaskName
allows O
for O
more O
consistent O
annotations O
and O
a O
better O
understanding O
of O
the O
types O
DialoGPT B-MethodName
is O
prone O
to O
generate O
. O

DialoGPT B-MethodName
Responses O

The O
classifier O
enables O
us O
to O
perform O
a O
large O
- O
scale O
study O
of O
ad O
hominem O
trends O
across O
various O
contexts O
for O
the O
entire O
AD O
- O
HOMINTWEETS B-DatasetName
dataset O
. O
Figure O
1 O
shows O
the O
percentage O
of O
ad O
hominem O
responses O
to O
posts O
across O
topics O
and O
response O
sources O
. O
Focusing O
on O
the O
" O
Human B-MethodName
" O
and O
" O
DialoGPT B-MethodName
" O
bars O
for O
each O
topic O
, O
we O
see O
that O
ad O
hominem O
responses O
are O
present O
across O
all O
topics O
for O
both O
response O
sources O
. O
Additionally O
, O
ad O
hominem O
responses O
occur O
more O
frequently O
in O
discussions O
related O
to O
BLM O
and O
MeToo O
and O
less O
frequently O
in O
discussions O
related O
to O
Vegan O
and O
WFH O
. O
Vegan O
discussions O
also O
seem O
to O
attract O
more O
ad O
hominem O
responses O
than O
WFH O
discussions O
. O
The O
relatively O
higher O
rates O
of O
ad O
hominem O
responses O
in O
topics O
related O
to O
marginalized O
communities O
indicate O
the O
elevated O
potential O
for O
harm O
towards O
these O
communities O
. O
Fine O
- O
tuned O
DialoGPT B-MethodName
Responses O
Figure O
1 O
also O
shows O
that O
fine O
- O
tuning O
on O
datasets O
that O
contain O
more O
ad O
hominem O
responses O
leads O
to O
more O
generation O
of O
ad O
hominem O
responses O
across O
topics O
. O
11 O
From O
these O
results O
, O
we O
infer O
that O
the O
original O
DialoGPT B-MethodName
( O
which O
was O
fine O
- O
tuned O
from O
GPT-2 O
) O
was O
trained O
on O
a O
dataset O
that O
likely O
contained O
relatively O
more O
rather O
than O
fewer O
ad O
hominems O
. O
Additionally O
, O
finetuning O
on O
a O
carefully O
chosen O
dataset O
can O
reduce O
the O
quantity O
of O
generated O
ad O
hominems O
and O
associated O
harms O
. O

Ad B-TaskName
Hominem I-TaskName
Reduction O

Baselines O
We O
compare O
techniques O
from O
two O
classes O
of O
harm O
reduction O
methods O
for O
language O
generation O
: O
data O
- O
based O
and O
decoding O
- O
based O
. O
Gehman O
et O
al O
. O
( O
2020 O
) O
introduce O
four O
baselines O
to O
span O
the O
different O
classes O
of O
harm O
reduction O
techniques O
. O
The O
first O
baseline O
is O
simply O
the O
original O
DialoGPT B-MethodName
. O
Our O
databased O
reduction O
baseline O
is O
DialoGPT B-MethodName
fine O
- O
tuned O
on O
the O
WFH O
dataset O
, O
as O
described O
in O
Sec O
. O
3 O
. O
For O
the O
first O
decoding O
- O
based O
baseline O
, O
we O
rely O
on O
a O
gradient O
- O
based O
method O
post O
- O
training O
to O
find O
a O
" O
trigger O
phrase O
" O
, O
which O
is O
then O
attached O
to O
a O
prompt O
at O
inference O
time O
to O
influence O
the O
generated O
output O
( O
Wallace O
et O
al O
. O
, O
2019 O
) O
. O
Sheng O
et O
al O
. O
( O
2020 O
) O
further O
propose O
a O
framework O
to O
use O
these O
triggers O
to O
control O
societal O
biases O
, O
and O
we O
use O
these O
methods O
to O
find O
a O
trigger O
that O
can O
induce O
DialoGPT B-MethodName
to O
generate O
fewer O
ad O
hominems O
and O
more O
non O
- O
ad O
hominems O
when O
prepended O
to O
posts O
about O
different O
topics O
. O
For O
the O
second O
decoding O
- O
based O
baseline O
, O
we O
use O
the O
Plug O
and O
Play O
Language O
Model O
( O
PPLM O
) O
proposed O
by O
Dathathri O
et O
al O
. O
( O
2020 O
) O
, O
which O
guides O
a O
pre O
- O
trained O
language O
model O
's O
generated O
output O
using O
gradients O
from O
attribute O
classifiers O
. O
12 O
Human O
Annotation O
To O
verify O
ad O
hominem O
trends O
from O
the O
automatic O
evaluation O
, O
we O
randomly O
select O
100 O
samples O
from O
each O
[ O
reduction O
technique O
, O
topic O
] O
pair O
for O
additional O
human O
annotation O
. O
General O
Trends O
Classifier O
and O
human O
evaluations O
for O
techniques O
to O
reduce O
ad O
hominems O
are O
in O
Figure O
2 O
, O
and O
examples O
of O
generated O
responses O
are O
in O
Table O
6 O
. O
The O
classifier O
- O
labeled O
results O
allow O
us O
to O
evaluate O
14.5 O
K O
samples O
across O
all O
topics O
per O
response O
source O
, O
and O
the O
human O
- O
labeled O
results O
allow O
us O
to O
more O
accurately O
evaluate O
a O
smaller O
set O
of O
samples O
. O
Overall O
, O
the O
trends O
for O
classifier O
and O
human O
evaluations O
are O
similar O
, O
and O
the O
evaluations O
suggest O
that O
all O
ad O
hominem O
reduction O
techniques O
are O
effective O
compared O
to O
the O
original O
DialoGPT O
. O
Furthermore O
, O
SALIENSIMTOP O
- O
k O
is O
more O
effective O
than O
the O
other O
individual O
techniques O
, O
and O
combining O
fine O
- O
tuning O
and O
SALIENSIMTOP O
- O
k O
has O
promise O
for O
further O
reducing O
the O
amount O
of O
generated O
ad O
hominems O
. O

For O
SALIENSIMTOP O
- O
k O
, O
limiting O
the O
number O
of O
times O
we O
backtrack O
to O
previous O
time O
steps O
ensures O
that O
the O
algorithm O
is O
not O
significantly O
slower O
compared O
to O
the O
original O
top O
- O
k O
sampling O
algorithm O
. O
Empirically O
, O
we O
find O
that O
using O
SALIENSIMTOP O
- O
k O
with O
a O
backtracking O
limit O
of O
5 O
on O
the O
original O
Di B-MethodName
- I-MethodName
aloGPT I-MethodName
results O
in O
13 O
% O
of O
the O
decoding O
operations O
being O
" O
non O
- O
forward O
" O
operations O
, O
where O
the O
set O
of O
decoding O
operations O
are O
: O
a O
) O
choosing O
the O
current O
token O
and O
moving O
forward O
to O
the O
next O
timestep O
, O
b O
) O
looking O
for O
an O
alternate O
token O
at O
the O
same O
timestep O
, O
or O
c O
) O
moving O
backward O
to O
a O
previous O
timestep O
. O
When O
applying O
constrained O
decoding O
to O
DialoGPT B-MethodName
fine O
- O
tuned O
on O
WFH O
, O
10 O
% O
of O
the O
operations O
are O
nonforward O
operations O
. O
Since O
ad B-TaskName
hominems I-TaskName
are O
less O
common O
than O
non O
- O
ad B-TaskName
hominems I-TaskName
, O
the O
algorithm O
is O
able O
to O
proceed O
with O
the O
first O
sampled O
candidate O
token O
in O
most O
time O
steps O
. O
Additionally O
, O
models O
or O
topics O
that O
are O
inclined O
to O
generate O
more O
ad B-TaskName
hominems I-TaskName
incur O
more O
non O
- O
forward O
operations O
. O

Coherence B-MetricName
and O
Relevance B-MetricName
Evaluation O

To O
ensure O
that O
the O
ad B-TaskName
hominem I-TaskName
reduction O
techniques O
do O
not O
affect O
the O
quality O
of O
the O
generated O
responses O
, O
we O
have O
annotators O
label O
the O
coherence B-MetricName
and O
relevance B-MetricName
of O
a O
response O
to O
a O
post O
, O
both O
on O
a O
scale O
of O
1 B-MetricValue
to O
5 B-MetricValue
, O
where O
a O
higher O
score O
is O
better O
. O
The O
trigger O
method O
produces O
samples O
that O
are O
relatively O
more O
coherent O
, O
although O
at O
the O
cost O
of O
lower O
relevance O
to O
the O
post O
. O
PPLM O
generates O
responses O
that O
are O
relatively O
lower O
in O
both O
coherence O
and O
relevance O
. O
SALIENSIMTOP O
- O
k O
manages O
to O
maintain O
a O
decent O
balance O
of O
generating O
both O
coherent O
and O
relevant O
responses O
. O
Combining O
SALIENSIMTOP O
- O
k O
with O
finetuning O
on O
WFH O
data O
results O
in O
responses O
that O
are O
slightly O
less O
coherent O
and O
mixed O
in O
relevance O
for O
different O
topics O
. O
13 O
Spearman B-MetricName
's O
correlation O
is O
moderately O
high O
( O
0.46 O
) O
for O
relevance O
and O
a O
bit O
lower O
for O
coherence O
( O
0.38 O
) O
, O
indicating O
the O
task O
subjectivity O
. O
Discussion O
The O
collective O
results O
indicate O
that O
SALIENSIMTOP O
- O
k O
is O
an O
effective O
standalone O
ad O
hominem O
reduction O
technique O
that O
maintains O
generated O
text O
quality O
; O
while O
it O
can O
be O
combined O
with O
other O
techniques O
to O
further O
reduce O
ad O
hominems O
, O
one O
should O
carefully O
evaluate O
the O
trade O
- O
offs O
between O
response O
coherence O
and O
relevance O
. O
Additionally O
, O
for O
reducing O
harmful O
language O
types O
that O
are O
more O
subjective O
or O
difficult O
to O
detect O
, O
straightforward O
control O
techniques O
that O
rely O
on O
salient O
ngrams O
may O
be O
more O
useful O
than O
techniques O
that O
rely O
on O
noisier O
signals O
from O
classifiers O
. O

Conclusion O

Ad B-TaskName
hominem I-TaskName
responses O
from O
dialogue O
systems O
are O
offensive O
, O
stall O
conversations O
, O
and O
are O
especially O
harmful O
for O
marginalized O
communities O
. O
We O
analyze O
responses O
to O
find O
that O
discussions O
on O
topics O
that O
affect O
marginalized O
groups O
contain O
more O
ad O
hominems O
. O
Through O
a O
novel O
constrained O
decoding O
technique O
, O
we O
decrease O
the O
amount O
of O
ad O
hominems O
generated O
from O
dialogue O
systems O
while O
keeping O
the O
response O
quality O
comparable O
. O
Furthermore O
, O
our O
method O
can O
be O
easily O
applied O
to O
other O
pre O
- O
trained O
language O
generation O
models O
and O
other O
subtle O
yet O
harmful O
language O
. O
More O
broadly O
, O
our O
work O
strives O
to O
understand O
ad O
hominems O
in O
the O
context O
of O
harms O
in O
conversational O
systems O
. O

Broader O
Impact O

This O
work O
identifies O
personal O
attacks O
in O
responses O
generated O
by O
dialogue O
systems O
, O
quantifies O
the O
dis O
- O
proportionate O
amount O
generated O
for O
topics O
concerning O
marginalized O
populations O
, O
and O
proposes O
methods O
to O
reduce O
ad O
hominem O
- O
related O
harms O
. O
Dataset O
We O
collect O
an O
English O
dataset O
from O
Twitter O
and O
ensure O
that O
personal O
information O
( O
e.g. O
, O
usernames O
, O
emails O
, O
urls O
) O
is O
discarded O
. O
We O
also O
collect O
crowd O
- O
sourced O
annotations O
for O
this O
dataset O
through O
Mechanical O
Turk O
, O
where O
we O
ask O
for O
judgements O
of O
whether O
a O
response O
contains O
ad O
hominems O
for O
a O
given O
post O
, O
and O
the O
coherence O
and O
relevance O
of O
a O
response O
. O
No O
information O
about O
the O
annotators O
are O
collected O
from O
the O
annotation O
tasks O
. O
The O
annotation O
information O
( O
pay O
per O
amount O
of O
work O
, O
guidelines O
) O
is O
in O
the O
Appendix O
. O

One O
annotation O
aspect O
that O
we O
did O
not O
control O
for O
is O
whether O
the O
annotators O
themselves O
are O
from O
marginalized O
communities O
. O
When O
measuring O
harms O
towards O
different O
demographics O
, O
it O
is O
important O
to O
consider O
the O
lived O
experiences O
of O
those O
groups O
and O
how O
these O
experiences O
may O
affect O
our O
analyses O
. O
Future O
work O
includes O
specifically O
collecting O
annotations O
from O
marginalized O
groups O
. O

Additionally O
, O
we O
analyze O
ad B-TaskName
hominems I-TaskName
in O
responses O
to O
four O
Twitter O
topics O
and O
from O
one O
dialogue O
model O
, O
which O
leaves O
much O
room O
for O
exploring O
the O
generalizability O
of O
the O
trends O
we O
see O
. O
Techniques O
In O
terms O
of O
dual O
- O
use O
harms O
, O
our O
constrained O
decoding O
technique O
could O
potentially O
be O
used O
to O
amplify O
rather O
than O
reduce O
ad O
hominems O
( O
or O
other O
harmful O
language O
) O
. O
However O
, O
we O
believe O
that O
by O
being O
transparent O
about O
this O
technique O
and O
releasing O
the O
associated O
code O
and O
data O
, O
we O
can O
better O
counter O
attempts O
of O
malicious O
misuse O
. O

Furthermore O
, O
to O
perform O
a O
large O
- O
scale O
analysis O
of O
ad B-TaskName
hominems I-TaskName
across O
different O
contexts O
, O
we O
build O
an O
automatic O
classifier O
. O
While O
we O
spent O
much O
effort O
on O
collecting O
representative O
train B-HyperparameterName
/ I-HyperparameterName
dev I-HyperparameterName
/ I-HyperparameterName
test I-HyperparameterName
datasets O
and O
verifying O
classifier O
quality O
and O
observed O
trends O
with O
human O
labels O
, O
collecting O
more O
( O
diverse O
) O
data O
could O
help O
further O
improve O
the O
classifier O
accuracy O
and O
robustness O
. O
In O
the O
meantime O
, O
we O
think O
this O
work O
introduces O
an O
important O
perspective O
of O
how O
ad O
hominems O
in O
dialogue O
systems O
reinforce O
unequal O
harms O
and O
effective O
reduction O
methods O
. O

A O
Appendices O

A.1 O
You O
- O
responses O

You O
- O
responses O
are O
responses O
containing O
any O
of O
the O
following O
phrases O
: O
you O
are O
, O
you O
were O
, O
you O
should O
, O
you O
would O
, O
you O
will O
, O
you O
have O
, O
you O
can O
, O
you O
could O
, O
you O
do O
n't O
, O
you O
did O
n't O
, O
you O
can O
' O
t O
, O
you O
're O
, O
you O
'd O
, O
you O
'll O
, O
you O
've O
, O
ur O
, O
ya O
'll O
, O
y O
all O
, O
your O
, O
yours O
, O
yourself O
, O
are O
you O
, O
were O
you O
, O
should O
you O
, O
would O
you O
, O
will O
you O
, O
have O
you O
, O
can O
you O
, O
could O
you O
. O
These O
phrases O
are O
used O
to O
identify O
potential O
ad O
hominems O
for O
more O
targeted O
annotation O
( O
Round O
1 O
) O
. O

A.2 O
Model O
Details O

We O
run O
all O
our O
models O
on O
an O
RTX O
2080Ti O
GPU O
. O
Training O
the O
ad O
hominem O
classifiers O
takes O
a O
few O
minutes O
, O
and O
fine O
- O
tuning O
DialoGPT O
on O
different O
topics O
( O
ranging O
from O
3 O
K O
to O
4 O
K O
samples O
as O
shown O
in O
Table O
2 O
) O
takes O
a O
few O
hours O
. O
Ad O
Hominem O
Classifier O
For O
the O
BERT O
- O
based O
ad B-TaskName
hominem I-TaskName
classifier O
, O
we O
fine O
- O
tune O
from O
the O
uncased O
version O
of O
the O
BERT B-HyperparameterName
base O
model O
( O
12 B-HyperparameterValue
layers B-HyperparameterName
) O
with O
mostly O
default O
parameters O
. O
For O
the O
DialoGPTbased B-MethodName
classifier O
, O
we O
fine O
- O
tune O
from O
the O
mediumsized O
DialoGPT B-MethodName
model O
also O
with O
mostly O
default O
parameters O
. O
In O
terms O
of O
non O
- O
default O
hyperparameters O
, O
we O
try O
learning B-HyperparameterName
rates I-HyperparameterName
of O
5 B-HyperparameterValue
× I-HyperparameterValue
10 I-HyperparameterValue
−5 I-HyperparameterValue
, O
1 B-HyperparameterValue
× I-HyperparameterValue
10 I-HyperparameterValue
−5 I-HyperparameterValue
, O
5 B-HyperparameterValue
× I-HyperparameterValue
10 I-HyperparameterValue
−6 I-HyperparameterValue
, O
and O
1 B-HyperparameterValue
× I-HyperparameterValue
10 I-HyperparameterValue
−6 I-HyperparameterValue
, O
and O
find O
that O
5 B-HyperparameterValue
× I-HyperparameterValue
10 I-HyperparameterValue
−5 I-HyperparameterValue
works O
the O
best O
for O
BERT B-HyperparameterName
and O
5 B-HyperparameterValue
× I-HyperparameterValue
10 I-HyperparameterValue
−6 I-HyperparameterValue
works O
the O
best O
for O
DialoGPT B-MethodName
. O
We O
train O
for O
12 B-HyperparameterValue
epochs B-HyperparameterName
and O
save O
the O
checkpoint O
for O
the O
epoch O
that O
the O
model O
performs O
the O
best O
on O
the O
dev O
set O
. O
All O
input O
that O
goes O
into O
the O
classifier O
is O
preprocessed O
to O
replace O
usernames O
, O
urls O
, O
and O
hashtags O
with O
placeholders O
. O
DialoGPT O
For O
all O
our O
DialoGPT O
experiments O
, O
we O
use O
the O
medium O
DialoGPT B-MethodName
with O
355 B-HyperparameterValue
M I-HyperparameterValue
parameters B-HyperparameterName
and O
mostly O
default O
parameters O
. O
During O
fine O
- O
tuning O
, O
we O
try O
learning B-HyperparameterName
rates I-HyperparameterName
of O
5 B-HyperparameterValue
× I-HyperparameterValue
10 I-HyperparameterValue
−5 I-HyperparameterValue
, O
1 B-HyperparameterValue
× I-HyperparameterValue
10 I-HyperparameterValue
−5 I-HyperparameterValue
, O
5 B-HyperparameterValue
× I-HyperparameterValue
10 I-HyperparameterValue
−6 I-HyperparameterValue
, O
and O
1 B-HyperparameterValue
× I-HyperparameterValue
10 I-HyperparameterValue
−6 I-HyperparameterValue
, O
and O
that O
a O
learning B-HyperparameterName
rate I-HyperparameterName
of O
5 B-HyperparameterValue
× I-HyperparameterValue
10 I-HyperparameterValue
−6 I-HyperparameterValue
for O
5 B-HyperparameterValue
epochs B-HyperparameterName
performs O
the O
best O
on O
the O
dev O
sets O
. O
The O
format O
the O
training O
and O
eval O
data O
is O
" O
POST O
[ O
EOS O
] O
RESPONSE O
[ O
EOS O
] O
" O
. O

A.3 O
Trigger O
Details O

Following O
the O
trigger O
search O
algorithm O
of O
Wallace O
et O
al O
. O
( O
2019 O
) O
and O
bias O
control O
framework O
of O
Sheng O
et O
al O
. O
( O
2020 O
) O
, O
we O
start O
with O
the O
trigger O
phrase O
" O
the O
the O
the O
the O
the O
the O
" O
, O
and O
iteratively O
replace O
each O
token O
in O
the O
trigger O
such O
that O
we O
minimize O
the O
loss O
of O
generating O
non O
- O
ad B-TaskName
hominem I-TaskName
responses O
and O
maximize O
the O
loss O
of O
generating O
ad O
hominem O
responses O
. O
By O
using O
the O
annotated O
non O
- O
ad B-TaskName
hominem I-TaskName
and O
ad B-TaskName
hominem I-TaskName
responses O
as O
targets O
to O
generate O
or O
avoid O
, O
we O
can O
find O
a O
trigger O
phrase O
that O
forces O
the O
model O
to O
generate O
fewer O
ad O
hominems O
. O
Specifically O
, O
we O
follow O
previous O
work O
and O
implement O
the O
minimization O
and O
maximization O
of O
these O
different O
targeted O
associations O
through O
subtraction O
( O
e.g. O
, O
loss O
from O
generating O
non O
- O
ad B-TaskName
hominems I-TaskName
minus O
loss O
from O
generating O
ad B-TaskName
hominems I-TaskName
) O
, O
and O
then O
minimize O
this O
difference O
to O
find O
the O
trigger O
. O
The O
trigger O
we O
find O
is O
" O
Franç O
casterbecca O
Unsure O
filler O
willpower O
" O
, O
and O
we O
append O
this O
trigger O
to O
a O
post O
to O
generate O
responses O
from O
DialoGPT O
. O
For O
example O
, O
the O
input O
prompt O
" O
Franç O
casterbecca O
Unsure O
filler O
willpower O
WE O
have O
the O
power O
to O
stop O
this O
. O
Go O
# O
vegan O
. O
" O
results O
in O
the O
generated O
response O
" O
We O
must O
! O
" O
. O
We O
use O
the O
default O
parameters O
as O
reported O
by O
Sheng O
et O
al O
. O
( O
2020 O
) O
. O
For O
more O
details O
, O
see O
the O
prior O
works O
. O
With O
an O
RTX O
2080Ti O
GPU O
, O
the O
trigger O
search O
algorithm O
takes O
1 O
- O
2 O
hours O
. O

A.4 O
PPLM O
Details O

The O
Plug O
and O
Play O
Language O
Model O
uses O
gradients O
from O
an O
attribute O
classifier O
to O
control O
generation O
from O
a O
pre O
- O
trained O
language O
model O
. O
In O
the O
original O
work O
, O
Dathathri O
et O
al O
. O
( O
2020 O
) O
use O
PPLM O
in O
the O
contexts O
of O
topic O
, O
sentiment O
, O
and O
toxicity O
control O
. O

Although O
ad O
hominems O
are O
also O
a O
form O
of O
toxic O
language O
, O
we O
train O
a O
new O
attribute O
classifier O
specifically O
on O
the O
annotated O
ADHOMINTWEETS B-DatasetName
dataset O
for O
a O
more O
competitive O
PPLM O
baseline O
. O
We O
use O
the O
ad O
hominem O
classifier O
training O
set O
and O
dev O
set O
to O
form O
the O
training O
and O
validation O
sets O
for O
this O
classifier O
, O
respectively O
. O
Note O
that O
this O
classifier O
is O
necessarily O
different O
from O
the O
BERT B-HyperparameterName
- O
based O
model O
we O
use O
for O
the O
main O
ad O
hominem O
analysis O
- O
to O
use O
the O
gradients O
from O
the O
attribute O
classifier O
to O
steer O
generations O
from O
DialoGPT B-MethodName
, O
we O
follow O
the O
attribute O
classifier O
training O
procedure O
of O
Dathathri O
et O
al O
. O
( O
2020 O
) O
. O
Specifically O
, O
this O
classifier O
takes O
the O
hidden O
states O
with O
dimension O
( O
batch O
size O
, O
sequence O
length O
, O
embedding O
size O
) O
from O
the O
last O
layer O
of O
DialoGPT B-MethodName
, O
averages O
the O
hidden O
states O
over O
the O
sequence O
length O
, O
and O
uses O
these O
averaged O
hidden O
states O
as O
input O
for O
a O
simple O
linear O
classifier O
. O
The O
classifier O
has O
an O
input O
text O
format O
of O
" O
POST O
[ O
EOS O
] O
RESPONSE O
[ O
EOS O
] O
" O
to O
predict O
the O
binary O
ad O
hominem O
label O
and O
has O
an O
average O
validation O
accuracy O
of O
76 O
% O
. O

With O
this O
trained O
attribute O
classifier O
, O
we O
then O
follow O
the O
gradient O
- O
based O
hidden O
state O
updates O
described O
by O
Dathathri O
et O
al O
. O
( O
2020 O
) O
to O
generate O
responses O
given O
posts O
. O
For O
our O
hyperpa O
- O
rameter O
tuning O
, O
we O
try O
different O
step O
sizes O
= O
[ O
0.01 O
, O
0.02 O
, O
0.03 O
, O
0.04 O
, O
0.05 O
] O
and O
and O
KL B-HyperparameterName
loss I-HyperparameterName
coefficients O
= O
[ O
0.01 B-HyperparameterValue
, O
0.02 B-HyperparameterValue
, O
0.03 B-HyperparameterValue
] O
, O
where O
increased O
step O
sizes O
intensify O
control O
and O
increased O
KL O
loss O
coefficients O
intensify O
the O
similarity O
of O
the O
outputs O
for O
the O
modified O
and O
unmodified O
distributions O
. O
For O
our O
reported O
results O
, O
we O
use O
PPLM O
with O
a O
step O
size O
of O
0.01 O
, O
a O
KL B-HyperparameterName
loss I-HyperparameterName
coefficient O
of O
0.02 B-HyperparameterValue
, O
6 B-HyperparameterValue
epochs B-HyperparameterName
, O
and O
otherwise O
default O
parameters O
of O
the O
original O
work O
. O
In O
general O
, O
this O
technique O
is O
slower O
because O
it O
requires O
many O
iterations O
per O
token O
to O
accumulate O
perturbations O
. O

A.5 O
Top O
- O
k O
Sampling O
Details O

At O
each O
time O
step O
of O
top O
- O
k O
sampling O
, O
the O
top O
- O
k O
tokens O
V O
( O
k O
) O
⊂ O
V O
that O
maximize O
p O
= O
x∈V O
( O
k O
) O
P O
( O
x|x O
1 O
: O
i−1 O
) O
are O
selected O
as O
candidate O
tokens O
to O
generate O
. O
V O
is O
the O
model O
's O
token O
vocabulary O
, O
x O
is O
a O
token O
, O
and O
x O
1 O
: O
i−1 O
are O
the O
tokens O
from O
all O
the O
previous O
time O
steps O
. O
The O
distribution O
p O
is O
then O
re O
- O
scaled O
such O
that O
for O
all O
x O
∈ O
V O
( O
k O
) O
, O
the O
rescaled O
distribution O
is O
P O
( O
x|x O
1 O
: O
i−1 O
) O
= O
P O
( O
x|x O
1 O
: O
i−1 O
) O
/ O
p O
. O
This O
new O
distribution O
P O
is O
then O
used O
to O
sample O
a O
new O
token O
for O
the O
current O
time O
step O
. O

A.6 O
SALIENSIMTOP O
- O
k O
Details O

For O
this O
constrained O
decoding O
technique O
, O
we O
also O
use O
an O
RTX O
2080 O
Ti O
GPU O
and O
, O
similar O
to O
the O
nonconstrained O
DialoGPT B-MethodName
, O
it O
takes O
less O
than O
a O
second O
to O
generate O
output O
for O
a O
sample O
. O

A.7 O
Ad B-TaskName
Hominem I-TaskName
Annotation O

Task O
Annotators O
are O
paid O
$ O
0.05 O
to O
label O
the O
ad O
hominems O
in O
a O
sample O
and O
are O
from O
the O
U.S. O
or O
Canada O
. O
We O
filter O
by O
annotators O
from O
these O
locations O
to O
better O
control O
for O
similar O
societal O
values O
in O
English O
- O
speaking O
communities O
, O
but O
it O
would O
be O
interesting O
to O
see O
how O
the O
concept O
of O
ad O
hominems O
change O
across O
communities O
with O
more O
different O
values O
and O
languages O
. O
Each O
sample O
takes O
an O
average O
of O
15 O
to O
20 O
seconds O
to O
label O
, O
for O
an O
hourly O
average O
of O
$ O
10.29 O
USD O
. O
We O
show O
annotators O
the O
guidelines O
below O
. O
Guidelines O
Ad O
hominems O
are O
a O
type O
of O
logical O
fallacy O
in O
which O
a O
response O
attacks O
a O
person O
and O
some O
feature O
of O
the O
person O
's O
character O
instead O
of O
the O
position O
the O
person O
is O
maintaining O
. O
For O
example O
, O
if O
Person O
A O
says O
" O
We O
used O
deductive O
reasoning O
to O
prove O
that O
the O
moon O
revolves O
around O
the O
earth O
. O
" O
and O
Person O
B O
replies O
" O
No O
, O
you O
're O
dumb O
" O
, O
Person O
B O
's O
response O
is O
an O
ad O
hominem O
. O
A O
more O
subtle O
ad O
hominem O
is O
if O
Person O
B O
says O
" O
I O
think O
you O
meant O
inductive O
reasoning O
. O
" O
, O
because O
( O
whether O
intentionally O
or O
not O
) O
this O
response O
targets O
Person O
A O
's O
perceived O
mistake O
instead O
of O
purely O
addressing O
the O
content O
of O
Person O
A O
's O
post O
. O
Types O
of O
ad O
hominems O
( O
towards O
Person O
A O
) O
: O

• O
Stupidity O
( O
i.e. O
, O
targeting O
Person O
A O
's O
capability O
for O
intelligence O
) O
: O

-Person O
B O
: O
" O
You O
dumb O
f O
* O
* O
* O
" O
-Person O
B O
: O
" O
Reading O
comprehension O
is O
your O
friend O
" O
-Person O
B O
: O
" O
You O
have O
no O
capability O
to O
understand O
why O
" O
-Person O
B O
: O
" O
Nobody O
with O
enough O
brains O
to O
operate O
a O
computer O
could O
possibly O
believe O
something O
this O
stupid O
" O
-Person O
B O
: O
" O
Ever O
have O
discussions O
with O
narcissistic O
idiots O
on O
the O
internet O
? O
They O
are O
so O
tiring O
" O
-Person O
B O
: O
" O
Your O
second O
paragraph O
is O
fairly O
idiotic O
" O
• O
Ignorance O
( O
i.e. O
, O
targeting O
Person O
A O
not O
using O
their O
capability O
for O
intelligence O
, O
making O
a O
mistake O
, O
forgetting O
to O
include O
something O
, O
confusing O
different O
things O
) O
: O

- O

Acknowledgments O

We O
would O
like O
to O
thank O
members O
of O
the O
PLUS O
Lab O
and O
the O
anonymous O
reviewers O
for O
the O
helpful O
feedback O
, O
and O
Jason O
Teoh O
for O
the O
many O
discussions O
. O
This O
paper O
is O
supported O
in O
part O
by O
NSF O
IIS O
1927554 O
and O
by O
the O
CwC O
program O
under O
Con O
- O
tract O
W911NF-15 O
- O
1 O
- O
0543 O
with O
the O
US O
Defense O
Advanced O
Research O
Projects O
Agency O
( O
DARPA O
) O
. O
The O
views O
expressed O
are O
those O
of O
the O
authors O
and O
do O
not O
reflect O
the O
official O
policy O
or O
position O
of O
the O
Department O
of O
Defense O
or O
the O
U.S. O
Government O
. O


-Person O
B O
: O
" O
You O
're O
racist O
" O
-Person O
B O
: O
" O
Somebody O
's O
being O
sexist O
. O
" O
• O
Condescension O
: O
( O
i.e. O
, O
if O
Person O
B O
has O
an O
attitude O
of O
patronizing O
superiority O
towards O
Person O
A O
) O
-Person O
B O
: O
" O
little O
buddy O
" O
-Person O
B O
: O
" O
Again O
, O
how O
old O
are O
you O
? O
" O
-Person O
B O
: O
" O
How O
can O
you O
explain O
that O
? O
You O
ca O
n't O
because O
it O
will O
hurt O
your O
feelings O
to O
face O
reality O
" O
• O
Other O
( O
vulgar O
insults O
, O
name O
- O
calling O
, O
accusations O
of O
logical O
fallacies O
, O
etc O
, O
towards O
Person O
A O
that O
are O
not O
already O
covered O
by O
the O
above O
categories O
) O
: O

-Person O
B O
: O
" O
You O
're O
just O
an O
a**hole O
" O
-Person O
B O
: O
" O
You O
started O
with O
a O
fallacy O
and O
then O
deflected O
" O
-Person O
B O
: O
" O
You O
're O
trash O
at O
debating O
" O
-Person O
B O
: O
" O
You O
're O
better O
than O
that O
. O
" O
• O
Non O
- O
ad O
hominem O
examples O
: O

- O
( O
Person O
A O
: O
" O
# O
WFH O
benefit O
1,298 O
: O
no O
coworker O
judgement O
microwaving O
fish O
for O
lunch O
. O
" O
) O
Person O
B O
: O
" O
The O
smell O
of O
fish O
is O
deadly O
. O
" O
- O
( O
Person O
A O
: O
" O
Thank O
you O
@ O
[ O
username O
] O
for O
the O
wonderful O
show O
! O
" O
) O
Person O
B O
: O
" O
I O
'm O
glad O
you O
enjoyed O
it O
. O
" O
-Person O
B O
: O
" O
You O
're O
not O
my O
supervisor O
! O
" O
( O
this O
is O
not O
really O
an O
attack O
on O
Person O
A O
) O
Notes O
: O

• O
Some O
sentences O
may O
not O
be O
perfectly O
grammatical O
or O
may O
not O
be O
consistent O
with O
itself O
( O
e.g. O
, O
" O
You O
are O
a O
troll O
but O
you O
are O
not O
a O
troll O
" O
) O
. O
Try O
your O
best O
to O
ignore O
bad O
grammar O
and O
inconsistencies O
when O
labeling O
. O
• O
Remember O
that O
you O
are O
labeling O
whether O
Person O
B O
's O
response O
contains O
ad O
hominems O
towards O
Person O
A O
, O
not O
whether O
Person O
B O
's O
entire O
response O
is O
an O
ad O
hominem O
towards O
Person O
A O
. O

There O
may O
be O
multiple O
types O
of O
ad B-TaskName
hominems I-TaskName
. O
• O
Your O
personal O
opinion O
of O
the O
content O
should O
not O
influence O
whether O
a O
response O
contains O
ad O
hominems O
towards O
Person O
A O
. O

A.8 O
Coherence B-MetricName
and O
Relevance B-MetricName
Annotation O

Task O
Annotators O
are O
paid O
$ O
0.10 O
to O
label O
the O
coherence B-MetricName
and O
relevance B-MetricName
of O
a O
response O
and O
are O
from O
the O
U.S. O
or O
Canada O
. O
Each O
sample O
takes O
an O
average O
of O
30 O
- O
50 O
seconds O
to O
label O
, O
for O
an O
hourly O
average O
of O
$ O
9 O
USD O
. O
We O
show O
annotators O
the O
guidelines O
below O
. O

Guidelines O
Label O
the O
coherence B-MetricName
of O
the O
response O
( O
independent O
of O
the O
post O
) O
, O
on O
a O
scale O
of O
1 O
to O
5 O
. O

• O
5 B-MetricValue
= O
the O
response O
fully O
makes O
sense O
-Response O
: O
" O
I O
'm O
not O
a O
Black O
, O
I O
'm O
a O
White O
! O
I O
'm O
a O
human O
, O
and O
I O
deserve O
respect O
for O
my O
opinion O
! O
But O
if O
you O
do O
n't O
like O
my O
post O
, O
you O
can O
go O
away O
! O
" O
• O
3 B-MetricValue
= O
the O
response O
somewhat O
makes O
sense O
, O
or O
might O
make O
sense O
in O
certain O
contexts O
, O
or O
part O
of O
the O
response O
makes O
sense O
-Response O
: O
" O
So O
many O
of O
these O
" O
WFH O
" O
jobs O
are O
only O
available O
to O
those O
without O
insurance O
and O
the O
few O
who O
do O
are O
not O
well O
paid O
. O
What O
an O
injustice O
. O
" O
• O
1 B-MetricValue
= O
the O
response O
would O
n't O
make O
sense O
in O
any O
context O
-Response O
: O
" O
So O
# O
WFH O
is O
a O
for O
profit O
organisation O
. O
Is O
that O
an O
issue O
for O
you O
? O
Why O
are O
you O
pro O
worker O
? O
Or O
are O
you O
just O
anti O
worker O
for O
profit O
organisations O
? O
No O
. O
Just O
to O
clarify O
. O
" O
Label O
how O
relevant O
the O
response O
is O
to O
the O
post O
, O
on O
a O
scale O
of O
1 O
to O
5 O
. O
In O
other O
words O
, O
could O
you O
imagine O
someone O
replying O
with O
the O
response O
to O
the O
post O
in O
a O
typical O
conversation O
? O

• O
5 B-MetricValue
= O
the O
response O
is O
completely O
appropriate O
for O
the O
post O
( O
even O
if O
it O
's O
not O
coherent O
) O
-Post O
: O
" O
Ca O
n't O
wait O
to O
hear O
Alicia O
Keys O
and O
the O
lineup O
of O
singers O
! O
" O
-Response O
: O
" O
I O
think O
that O
the O
# O
WFH O
set O
is O
going O
to O
be O
a O
thing O
of O
beauty O
. O
It O
's O
going O
to O
be O
awesome O
. O
And O
I O
'm O
totally O
behind O
it O
. O
" O
• O
3 B-MetricValue
= O
the O
response O
is O
somewhat O
appropriate O
for O
the O
post O
, O
or O
might O
be O
in O
certain O
contexts O
, O
or O
part O
of O
the O
response O
is O
appropriate O
for O
the O
post O
-Post O
: O
" O
Ca O
n't O
wait O
to O
hear O
Alicia O
Keys O
and O
the O
lineup O
of O
singers O
! O
" O
-Response O
: O
" O
But O
are O
n't O
they O
under O
quarantine O
? O
I O
like O
to O
produce O
music O
at O
home O
. O
" O
• O
1 B-MetricValue
= O
the O
response O
would O
n't O
be O
appropriate O
for O
the O
post O
in O
any O
context O
-Post O
: O
" O
Ca O
n't O
wait O
to O
hear O
Alicia O
Keys O
and O
the O
lineup O
of O
singers O
! O
" O
-Response O
: O
" O
I O
have O
been O
preparing O
for O
my O
pronunciation O
test O
and O
I O
'm O
nervous O
. O
" O
Post O
: O
You O
're O
the O
reason O
we O
need O
the O
# O
MeToo O
movement O
Resp O
: O
It O
's O
not O
even O
the O
same O
thing O
, O
they O
are O
both O
the O
same O
reason O
for O
that O
hashtag O
which O
is O
in O
your O
head O
and O
you O
are O
just O
trying O
to O
get O
the O
next O
one O
in O
. O
and O
Technique O
: O
SALIENSIMTOP O
- O
k O

Post O
: O
people O
comparing O
having O
to O
wear O
a O
mask O
to O
slavery O
is O
just O
... O
? O
i O
do O
n't O
even O
have O
the O
words O
to O
explain O
how O
ignorant O
they O
sound O
# O
blacklivesmatter O
Resp O
: O
That O
's O
a O
bit O
of O
an O
odd O
way O
to O
use O
the O
hashtag O
. O
Resp O
: O
I O
think O
we O
found O
the O
anti O
terrorism O
expert O
on O
the O
job O
, O
folks O
. O
We O
're O
done O
here O
. O
Just O
move O
on O
and O
do O
n't O
let O
them O
win O
the O
argument O
for O
you O
. O

Post O
: O
Lol O
so O
you O
did O
n't O
read O
it O
and O
just O
want O
to O
continue O
on O
your O
white O
power O
trip O
. O
Go O
learn O
the O
history O
and O
you O
'll O
understand O
why O
it O
's O
important O
to O
recognize O
the O
women O
who O
had O
to O
fight O
even O
longer O
to O
vote O
. O
# O
BlackLivesMatter O
Resp O
: O
No O
I O
did O
read O
it O
and O
I O
do O
n't O
want O
to O
spend O
more O
time O
responding O
to O
you O
so O
please O
go O
back O
to O
your O
safe O
space O
. O
Do O
n't O
waste O
your O
time O
further O
making O
this O
comment O
. O

Using O
Neural B-MethodName
Machine I-MethodName
Translation I-MethodName
for O
Generating O
Diverse O
Challenging O
Exercises O
for O
Language O
Learners O

We O
propose O
a O
novel O
approach O
to O
automatically O
generate B-TaskName
distractors I-TaskName
for O
cloze O
exercises O
for O
English O
language O
learners O
, O
using O
round B-MethodName
- I-MethodName
trip I-MethodName
neural I-MethodName
machine I-MethodName
translation I-MethodName
. O
A O
carrier O
sentence O
is O
translated O
from O
English O
into O
another O
( O
pivot O
) O
language O
and O
back O
, O
and O
distractors O
are O
produced O
by O
aligning O
the O
original O
sentence O
with O
its O
round O
- O
trip O
translation O
. O
We O
make O
use O
of O
16 O
linguistically O
- O
diverse O
pivots O
and O
generate O
hundreds O
of O
translation O
hypotheses O
in O
each O
direction O
. O
We O
show O
that O
using O
hundreds O
of O
translations O
allows O
us O
to O
generate O
a O
rich O
set O
of O
challenging O
distractors O
. O
Moreover O
, O
we O
find O
that O
typologically O
unrelated O
language O
pivots O
contribute O
more O
diverse O
candidate O
distractors O
, O
compared O
to O
language O
pivots O
that O
are O
closely O
related O
. O
We O
further O
evaluate O
the O
use O
of O
machine O
translation O
systems O
of O
varying O
quality O
and O
find O
that O
better O
quality O
MT B-MethodName
systems O
produce O
more O
challenging O
distractors O
. O
Finally O
, O
we O
conduct O
a O
study O
with O
language O
learners O
, O
demonstrating O
that O
the O
automatically O
generated O
distractors O
are O
of O
the O
same O
difficulty O
as O
the O
gold O
distractors O
produced O
by O
human O
experts O
. O
1 O

Introduction O

A O
common O
challenge O
for O
language O
learners O
involves O
understanding O
how O
to O
appropriately O
use O
words O
that O
may O
have O
similar O
meanings O
but O
are O
used O
in O
different O
contexts O
. O
For O
instance O
, O
" O
main O
" O
and O
" O
vital O
" O
are O
semantically O
related O
but O
" O
main O
importance O
" O
is O
not O
an O
acceptable O
expression O
while O
" O
vital O
importance O
" O
is O
. O
This O
subtle O
language O
knowledge O
is O
not O
explicitly O
available O
to O
learners O
. O
For O
this O
reason O
, O
word O
usage O
( O
collocation O
) O
errors O
are O
some O
of O
the O
most O
common O
types O
of O
errors O
even O
for O
advanced O
non O
- O
native O
speakers O
( O
Leacock O
et O
al O
. O
, O
2010 O
) O
. O
* O
Work O
was O
done O
while O
the O
author O
was O
at O
the O
CUNY O
Graduate O
Center O
. O
1 O
The O
code O
is O
available O
at O
https O
: O
/ O
/ O
github.com O
/ O
subhadarship O
/ O
round O
- O
trip O
- O
distractors O

Carrier O
sentence O

Are O
these O
old O
plates O
of O
_ O
_ O
_ O
_ O
_ O
_ O
importance O
or O
can O
I O
put O
them O
into O
storage O
? O
Target O
word O
: O
vital O
Valid O
distractors O
: O
main O
, O
urgent O
, O
lively O
Invalid O
distractors O
: O
great O
, O
utmost O
Table O
1 O
: O
A O
sentence O
for O
a O
fill O
- O
in O
- O
the O
- O
blank O
exercise O
with O
the O
target O
word O
" O
vital O
" O
removed O
. O
Multiple O
- O
choice O
list O
will O
include O
the O
target O
and O
3 O
distractors O
. O
Examples O
of O
valid O
and O
invalid O
distractors O
are O
shown O
. O

In O
this O
work O
, O
we O
develop O
exercises O
for O
mastering O
vocabulary O
use O
for O
second O
( O
foreign O
) O
language O
learners O
. O
We O
focus O
on O
cloze O
( O
fill O
- O
in O
- O
the O
- O
blank O
) O
exercises O
. O
A O
cloze O
exercise O
is O
a O
common O
method O
of O
teaching O
vocabulary O
, O
as O
well O
as O
assessing O
non O
- O
native O
speaker O
performance O
in O
a O
foreign O
language O
: O
a O
sentence O
is O
presented O
to O
the O
learner O
with O
one O
word O
( O
target O
) O
hidden O
. O
The O
target O
word O
is O
presented O
along O
with O
a O
list O
of O
distractors O
( O
usually O
3 O
) O
, O
and O
the O
learner O
should O
identify O
the O
target O
word O
from O
that O
list O
. O
Table O
1 O
shows O
a O
sample O
cloze O
item O
with O
the O
target O
word O
" O
vital O
" O
. O
The O
carrier O
sentence O
along O
with O
a O
multiplechoice O
list O
is O
referred O
to O
as O
cloze O
item O
. O
A O
cloze O
( O
exercise O
) O
item O
is O
valid O
if O
exactly O
one O
word O
( O
the O
target O
) O
fits O
the O
context O
. O
Therefore O
, O
a O
valid O
distractor O
should O
be O
a O
word O
that O
does O
not O
fit O
the O
context O
. O
Thus O
, O
" O
great O
" O
and O
" O
utmost O
" O
in O
Table O
1 O
are O
invalid O
distractors O
, O
since O
they O
both O
fit O
the O
context O
. O

Given O
a O
carrier O
sentence O
and O
the O
target O
word O
, O
the O
problem O
is O
to O
generate B-TaskName
distractors I-TaskName
. O
Distractors O
are O
typically O
created O
manually O
by O
educational O
testing O
experts O
, O
a O
time O
- O
consuming O
procedure O
. O
The O
problem O
becomes O
more O
challenging O
once O
the O
exercises O
are O
aimed O
at O
high O
- O
proficiency O
learners O
, O
since O
distractors O
that O
are O
not O
semantically O
close O
to O
the O
target O
word O
or O
are O
grammatically O
unfit O
will O
be O
too O
easy O
for O
them O
( O
Zesch O
and O
Melamud O
, O
2014 O
) O
. O

We O
propose O
to O
generate B-TaskName
distractors I-TaskName
using O
roundtrip B-MethodName
neural I-MethodName
machine I-MethodName
translation I-MethodName
( O
MT B-MethodName
) O
. O
Robust O
machine O
translation O
systems O
exist O
today O
for O
many O
language O
pairs O
. O
While O
translations O
produced O
with O
modern O
automated O
systems O
are O
reasonably O
good O
, O
these O
are O
not O
perfect O
, O
and O
, O
while O
a O
round B-MethodName
- I-MethodName
trip I-MethodName
translation I-MethodName
may O
preserve O
the O
sentence O
meaning O
, O
it O
will O
often O
not O
result O
in O
the O
exact O
same O
sentence O
. O
We O
use O
this O
observation O
to O
develop O
an O
approach O
to O
automatically O
generate O
distractors O
for O
cloze O
exercises O
. O

We O
focus O
on O
exercises O
aimed O
at O
advanced O
English O
as O
a O
Second O
Language O
( O
ESL O
) O
learners O
. O
A O
carrier O
sentence O
is O
translated O
from O
English O
into O
another O
pivot O
language O
, O
where O
the O
top O
n O
translation O
hypotheses O
are O
generated O
. O
For O
each O
hypothesis O
, O
the O
top O
m O
back O
- O
translations O
into O
English O
are O
generated O
. O
Each O
back O
- O
translation O
is O
aligned O
with O
the O
original O
sentence O
, O
and O
the O
back O
- O
translated O
word O
aligned O
to O
the O
target O
is O
treated O
as O
a O
potential O
distractor O
. O

The O
intuition O
behind O
the O
approach O
is O
that O
word O
choice O
errors O
are O
commonly O
affected O
by O
the O
learner O
's O
first O
language O
. O
In O
particular O
, O
the O
different O
meanings O
( O
or O
contextual O
uses O
) O
of O
an O
ambiguous O
word O
in O
the O
learner O
's O
native O
language O
may O
lead O
to O
different O
word O
choices O
in O
English O
. O
The O
assumption O
thus O
is O
that O
lexical O
challenges O
that O
are O
common O
with O
non O
- O
native O
speakers O
will O
also O
manifest O
themselves O
in O
the O
round B-MethodName
- I-MethodName
trip I-MethodName
machine I-MethodName
translation I-MethodName
as O
backtranslated O
words O
that O
are O
semantically O
close O
to O
the O
target O
. O
Such O
words O
should O
therefore O
serve O
as O
challenging O
distractors O
for O
advanced O
learners O
. O
Unlike O
previous O
work O
, O
this O
method O
also O
opens O
up O
a O
possibility O
of O
customizing O
the O
cloze O
task O
for O
speakers O
of O
different O
languages O
. O

This O
work O
builds O
on O
a O
pilot O
study O
( O
Panda O
et O
al O
. O
, O
2022 O
) O
that O
made O
use O
of O
five O
round B-MethodName
- I-MethodName
trip I-MethodName
MT I-MethodName
systems O
. O
However O
, O
the O
pivots O
used O
in O
the O
study O
were O
closely O
related O
languages O
spoken O
in O
Europe O
. O
In O
addition O
, O
the O
study O
did O
not O
evaluate O
the O
difficulty O
of O
the O
automatic O
distractors O
and O
did O
not O
test O
these O
with O
language O
learners O
. O

In O
this O
paper O
, O
we O
use O
16 O
language O
pivots O
from O
a O
diverse O
set O
of O
linguistic O
families O
and O
conduct O
a O
thorough O
evaluation O
of O
the O
proposed O
method O
, O
using O
a O
dataset O
of O
real O
cloze O
exercises O
for O
advanced O
learners O
. O
Our O
contributions O
are O
as O
follows O
: O
( O
1 O
) O
We O
use O
MT B-MethodName
systems O
of O
varying O
levels O
of O
quality O
. O
We O
show O
that O
, O
while O
poor O
MT B-MethodName
systems O
generate O
a O
larger O
pool O
of O
candidate O
distractors O
, O
high O
quality O
systems O
tend O
to O
produce O
more O
challenging O
distractors O
that O
are O
semantically O
close O
to O
the O
target O
word O
; O
( O
2 O
) O
We O
evaluate O
the O
approach O
as O
a O
function O
of O
using O
pivots O
from O
different O
language O
families O
and O
show O
that O
pivot O
languages O
that O
are O
typologically O
distant O
contribute O
more O
diverse O
distractors O
; O
( O
3 O
) O
We O
conduct O
a O
human O
study O
with O
32 O
advanced O
language O
learners O
and O
show O
that O
the O
generated O
distractors O
are O
of O
the O
same O
difficulty O
as O
distractors O
created O
by O
experts O
. O

The O
rest O
of O
the O
paper O
is O
organized O
as O
follows O
. O
The O
next O
section O
presents O
related O
work O
. O
Section O
3 O
describes O
the O
dataset O
of O
cloze O
exercises O
. O
Section O
4 O
describes O
our O
approach O
. O
Section O
5 O
presents O
the O
evaluation O
of O
the O
approach O
along O
several O
dimensions O
. O
Section O
6 O
describes O
the O
human O
study O
. O
Section O
7 O
concludes O
, O
by O
outlining O
avenues O
for O
future O
work O
and O
discussing O
the O
limitations O
of O
the O
study O
. O

Related O
work O

Previous O
work O
on O
distractor B-TaskName
generation I-TaskName
made O
use O
of O
word O
frequency O
, O
phonetic O
and O
morphological O
similarity O
, O
and O
grammatical O
fit O
( O
Hoshino O
and O
Nakagawa O
, O
2005 O
; O
Pino O
and O
Eskénazi O
, O
2009 O
; O
Goto O
et O
al O
. O
, O
2010 O
) O
. O
For O
advanced O
speakers O
, O
distractors O
should O
be O
selected O
more O
carefully O
, O
so O
that O
they O
are O
reasonably O
hard O
to O
distinguish O
from O
the O
target O
. O
Consider O
, O
for O
example O
, O
the O
target O
word O
" O
error O
" O
in O
the O
carrier O
sentence O
: O
" O
It O
is O
often O
only O
through O
long O
experiments O
of O
trial O
and O
error O
that O
scientific O
progress O
is O
made O
. O
" O
The O
word O
" O
mistake O
" O
is O
semantically O
close O
to O
it O
but O
is O
not O
appropriate O
in O
the O
sentence O
, O
and O
thus O
could O
serve O
as O
a O
valid O
distractor O
. O
However O
, O
note O
that O
" O
mistake O
" O
can O
be O
substituted O
for O
" O
error O
" O
in O
the O
context O
of O
" O
He O
made O
a O
lot O
of O
mistakes O
in O
his O
test O
. O
" O
and O
would O
therefore O
not O
be O
a O
valid O
distractor O
in O
that O
context O
. O
Thus O
, O
challenging O
distractors O
should O
be O
semantically O
close O
to O
the O
target O
word O
, O
yet O
, O
a O
valid O
distractor O
should O
not O
produce O
an O
acceptable O
sentence O
. O

Most O
of O
the O
approaches O
to O
generating B-TaskName
challenging I-TaskName
distractors I-TaskName
rely O
on O
semantic O
relatedness O
, O
computed O
through O
n O
- O
grams O
and O
collocations O
( O
Liu O
et O
al O
. O
, O
2005 O
; O
Hill O
and O
Simha O
, O
2016 O
) O
, O
thesauri O
( O
Sumita O
et O
al O
. O
, O
2005 O
) O
, O
or O
WordNet O
( O
Brown O
et O
al O
. O
, O
2005 O
) O
. O
Zesch O
and O
Melamud O
( O
2014 O
) O
use O
semantic O
context O
- O
sensitive O
inference O
rules O
. O
Sakaguchi O
et O
al O
. O
( O
2013 O
) O
propose O
generating B-TaskName
distractors I-TaskName
using O
errors O
mined O
from O
a O
learner O
corpus O
. O
The O
approach O
, O
however O
, O
assumes O
an O
annotated O
learner O
corpus O
, O
and O
both O
the O
choice O
of O
the O
target O
word O
and O
of O
the O
distractors O
are O
constrained O
by O
the O
errors O
in O
the O
corpus O
. O
Several O
studies O
showed O
that O
word O
embeddings O
are O
effective O
in O
distractor B-TaskName
generation I-TaskName
( O
Jiang O
and O
Lee O
, O
2017 O
; O
Susanti O
et O
al O
. O
, O
2018 O
; O
Mikolov O
et O
al O
. O
, O
2013 O
) O
. O

Our O
work O
builds O
on O
a O
study O
that O
employed O
five O
pivot O
languages O
( O
Panda O
et O
al O
. O
, O
2022 O
) O
, O
showing O
that O
the O
round B-MethodName
- I-MethodName
trip I-MethodName
MT I-MethodName
approach O
outperforms O
two O
strong O
baselines O
-word2vec B-MethodName
and O
BERT B-MethodName
( O
Section O
5.4 O
and O
Appendix O
B O
provide O
more O
detail O
on O
the O
comparison O
of O
the O
MT B-MethodName
approach O
with O
these O
methods O
) O
. O
The O
present O
study O
focuses O
on O
an O
in O
- O
depth O
evaluation O
of O
the O
MT B-MethodName
approach O
to O
distractor O
generation O
along O
several O
dimensions O
. O

Data O

We O
obtain O
cloze O
exercises O
from O
a O
reputable O
test O
preparation O
website O
, O
ESL B-DatasetName
Lounge I-DatasetName
. O
2 O
The O
website O
contains O
study O
materials O
and O
preparatory O
exercises O
for O
ESL O
tests O
, O
such O
as O
FCE B-DatasetName
First I-DatasetName
Certificate I-DatasetName
, O
TOEFL B-DatasetName
, O
and O
International B-DatasetName
English I-DatasetName
Language I-DatasetName
Testing I-DatasetName
System I-DatasetName
( O
IELTS B-DatasetName
) O
. O
There O
was O
significant O
effort O
put O
into O
the O
development O
of O
the O
exercises O
, O
which O
were O
manually O
curated O
for O
ESL O
students O
, O
and O
the O
exercises O
are O
of O
high O
quality O
. O
This O
is O
the O
first O
dataset O
that O
can O
be O
used O
by O
researchers O
working O
on O
the O
task O
. O
3 O
Previous O
studies O
thus O
evaluate O
either O
on O
artificially O
created O
items O
or O
on O
proprietary O
data O
. O

We O
use O
the O
advanced O
level O
multiple O
choice O
cloze O
exercises O
, O
which O
includes O
142 O
cloze O
items O
. O
4 O
Each O
item O
consists O
of O
a O
carrier O
sentence O
with O
the O
target O
word O
removed O
and O
is O
accompanied O
by O
four O
word O
choices O
that O
include O
the O
target O
word O
and O
three O
distractors O
provided O
by O
human O
experts O
. O
We O
refer O
to O
these O
distractors O
as O
gold O
distractors O
. O

Generating B-TaskName
Distractors I-TaskName
with O
Neural B-MethodName
MT I-MethodName

Round B-MethodName
- I-MethodName
trip I-MethodName
machine I-MethodName
translation I-MethodName
Given O
a O
carrier O
sentence O
X O
with O
the O
target O
word O
, O
a O
forward O
machine O
translation O
system O
from O
English O
to O
a O
pivot O
language O
trg O
and O
a O
backward O
MT B-MethodName
system O
from O
trg O
to O
English O
, O
we O
can O
generate O
a O
round B-MethodName
- I-MethodName
trip I-MethodName
translation I-MethodName
for O
X. O
Importantly O
, O
we O
generate O
multiple O
hypotheses O
in O
each O
direction O
. O

We O
first O
translate O
the O
sentence O
X O
from O
English O
using O
a O
forward O
MT B-MethodName
system O
S O
en−trg O
to O
obtain O
a O
set O
of O
top O
N O
f O
translation O
hypotheses O
Y O
= O
{ O
Y O
1 O
, O
Y O
2 O
, O
. O
. O
. O
, O
Y O
N O
f O
} O
in O
the O
target O
language O
trg O
. O
We O
then O
translate O
the O
sentences O
in O
Y O
using O
a O
backward O
MT B-MethodName
system O
S O
trg−en O
and O
obtain O
a O
set O
of O
top O
N O
b O
translation O
hypotheses O
for O
Y O
i O
∈ O
Y O
. O
Finally O
, O
we O
obtain O
the O
set O
of O
round B-MethodName
- I-MethodName
trip I-MethodName
translations I-MethodName

X O
RT O
= O
{ O
X O
RT O
1 O
, O
X O
RT O
2 O
, O
. O
. O
. O
, O
X O
RT O
N O
f O
×N O
b O
} O
. O

Our O
earlier O
study O
included O
five O
Indo O
- O
European O
languages O
: O
German O
, O
Russian O
, O
Italian O
, O
French O
, O
and O
Czech O
. O
Presently O
, O
we O
include O
16 O
languages O
from O
a O
diverse O
set O
of O
language O
families O
. O
For O
all O
language O
pairs O
, O
we O
use O
competitive O
neural B-MethodName
MT I-MethodName
systems O
of O
Tiedemann O
and O
Thottingal O
( O
2020 O
) O
. O
Table O
2 O
lists O
the O
16 O
languages O
, O
and O
includes O
BLEU B-MetricName
scores O
in O
both O
directions O
and O
the O
averaged O
BLEU B-MetricName
scores O
on O
the O
Tatoeba B-DatasetName
Machine I-DatasetName
Translation I-DatasetName
dataset O
from O
the O
Tatoeba B-DatasetName
Translation I-DatasetName
Challenge I-DatasetName
( O
Tiedemann O
, O
2020 O
) O
. O
Tatoeba B-DatasetName
is O
a O
crowd O
- O
sourced O
collection O
of O
user O
- O
provided O
translations O
in O
a O
large O
number O
of O
languages O
. O
We O
split O
the O
languages O
into O
four O
groups O
, O
organized O
by O
the O
averaged O
BLEU B-MetricName
scores O
. O
We O
assume O
higher O
BLEU B-MetricName
scores O
correspond O
to O
back O
- O
translations O
of O
higher O
quality O
. O
Appendix O
A O
provides O
detail O
on O
the O
pivot O
grouping O
. O

Alignment O
computation O
Given O
a O
round B-MethodName
- I-MethodName
trip I-MethodName
translation I-MethodName
X O
RT O
i O
for O
carrier O
sentence O
X O
, O
we O
compute O
the O
alignment O
between O
the O
two O
sentences O
. O
The O
word O
in O
X O
RT O
i O
that O
is O
aligned O
to O
the O
target O
word O
in O
X O
is O
considered O
to O
be O
the O
back O
- O
translation O
of O
the O
target O
and O
can O
be O
a O
potential O
distractor O
. O
We O
use O
Simalign O
5 O
( O
Sabet O
et O
al O
. O
, O
2020 O
) O
that O
employs O
contextual O
word O
embeddings O
( O
Devlin O
et O
al O
. O
, O
2018 O
) O
to O
produce O
an O
alignment O
model O
for O
a O
pair O
of O
sentences O
. O
Given O
the O
original O
sentence O
X O
and O
a O
round O
- O
trip O
translation O
X O
RT O
i O
, O
the O
similarity O
between O
each O
token O
in O
X O
with O
each O
token O
in O
X O
RT O
i O
is O
computed O
, O
using O
contextual O
embeddings O
from O
multilingual O
BERT B-MethodName
. O

Candidate O
filtering O

In O
line O
with O
previous O
studies O
, O
we O
remove O
candidates O
that O
are O
of O
a O
different O
part O
- O
ofspeech O
( O
POS O
) O
than O
the O
target O
word O
, O
and O
those O
that O
might O
fit O
the O
carrier O
sentence O
. O
While O
the O
first O
group O
of O
candidates O
would O
make O
the O
item O
too O
easy O
for O
advanced O
learners O
, O
the O
second O
group O
would O
make O
the O
exercise O
item O
invalid O
, O
as O
an O
item O
must O
have O
only O
one O
correct O
option O
. O
To O
rule O
out O
candidates O
that O
might O
fit O
the O
context O
, O
we O
use O
WordNet B-DatasetName
synonyms I-DatasetName
( O
Fellbaum O
, O
1998 O
) O
. O
We O
use O
the O
NLTK B-DatasetName
POS I-DatasetName
tagger I-DatasetName
( O
Bird O
et O
al O
. O
, O
2009 O
) O
to O
remove O
candidates O
that O
have O
a O
different O
tag O
than O
the O
target O
word O
in O
the O
carrier O
sentence O
. O
The O
tagger O
is O
applied O
to O
the O
carrier O
sentence O
with O
the O
target O
position O
filled O
by O
the O
appropriate O
word O
. O
Filtering O
removes O
about O
50 O
% O
of O
generated O
candidates O
. O
All O
results O
are O
shown O
with O
the O
filtering O
applied O
. O

Evaluation O

We O
evaluate O
the O
MT B-MethodName
approach O
to O
distractor O
generation O
along O
4 O
dimensions O
: O
( O
1 O
) O
comparing O
the O
effect O
of O
using O
typologically O
diverse O
language O
pivots O
; O
( O
2 O
) O
using O
MT B-MethodName
systems O
of O
various O
quality O
; O
( O
3 O
) O
using O
different O
number O
of O
translation O
hypotheses O
in O
the O
forward O
and O
backward O
direction O
; O
( O
4 O
) O
evaluating O
the O
diversity O
of O
distractors O
produced O
with O
linguistically O
related O
versus O
linguistically O
unrelated O
pivots O
. O

Evaluation O
for O
the O
distractor B-TaskName
generation I-TaskName
task O
is O
not O
straightforward O
, O
since O
the O
set O
of O
valid O
distractors O
for O
a O
given O
exercise O
item O
is O
not O
uniquely O
defined O
. O
For O
this O
reason O
, O
automatic B-MetricName
evaluation I-MetricName
against O
the O
set O
of O
distractors O
proposed O
by O
human O
experts O
does O
not O
provide O
a O
full O
picture O
of O
the O
quality O
of O
the O
generated O
distractors O
. O
Thus O
, O
we O
conduct O
several O
types O
of O
evaluation O
. O
First O
, O
we O
compare O
the O
generated O
distractors O
against O
the O
set O
of O
gold O
distractors O
for O
each O
item O
, O
making O
the O
assumption O
that O
a O
method O
that O
retrieves O
a O
higher O
percentage O
of O
gold O
distractors O
among O
its O
automatic O
candidates O
is O
better O
. O
Second O
, O
we O
conduct O
manual O
annotation O
with O
native O
English O
speakers O
to O
determine O
the O
percentage O
of O
valid O
distractors O
among O
the O
candidates O
proposed O
by O
MT B-MethodName
: O
although O
filtering O
removes O
a O
majority O
of O
invalid O
candidates O
, O
there O
are O
still O
candidates O
that O
remain O
due O
to O
filtering O
errors O
. O
Third O
, O
we O
evaluate O
the O
difficulty B-MetricName
of O
the O
generated O
distractors O
by O
annotating O
the O
distractors O
for O
their O
semantic O
similarity O
to O
the O
target O
. O
Our O
final O
test O
with O
language O
learners O
in O
Section O
6 O
assesses O
the O
difficulty B-MetricName
of O
the O
automatic O
distractors O
generated O
using O
the O
best O
settings O
for O
MT B-MethodName
, O
as O
compared O
to O
the O
difficulty B-MetricName
of O
gold O
distractors O
. O

Diversity O
and O
quality O
of O
distractors O
by O
pivot O
language O

With O
each O
of O
the O
16 O
pivot O
language O
systems O
, O
we O
generate O
900 B-HyperparameterValue
back B-HyperparameterName
- I-HyperparameterName
translations I-HyperparameterName
for O
a O
single O
exercise O
item O
. O
We O
use O
30 B-HyperparameterValue
hypotheses B-HyperparameterName
in O
each O
direction O
. O
The O
carrier O
sentence O
is O
aligned O
with O
each O
of O
the O
backtranslations O
, O
and O
the O
back O
- O
translated O
word O
that O
is O
aligned O
to O
the O
target O
in O
the O
original O
sentence O
is O
selected O
as O
a O
candidate O
distractor O
. O
Note O
that O
many O
of O
the O
hypotheses O
are O
similar O
and O
result O
in O
the O
same O
round O
- O
trip O
translation O
of O
the O
target O
word O
. O

How O
many O
distractors O
are O
generated O
? O
In O
Figure O
1 O
, O
we O
show O
the O
average O
number O
of O
unique O
candidate O
distractors O
per O
exercise O
item O
, O
retrieved O
with O
each O
pivot O
language O
system O
and O
with O
the O
union O
of O
all O
the O
pivot O
systems O
. O
The O
average O
number O
of O
candidates O
generated O
per O
exercise O
item O
varies O
widely O
, O
from O
6.6 O
( O
Spanish O
) O
to O
72.3 O
( O
Malayalam O
) O
. O
Notably O
, O
the O
union O
produces O
an O
average O
of O
234 O
distractors O
per O
target O
word O
, O
suggesting O
that O
round O
- O
trip O
translations O
from O
different O
pivot O
languages O
contribute O
unique O
distractor O
candidates O
. O

Gold O
distractor O
retrieval O

Our O
assumption O
is O
that O
a O
better O
method O
should O
generate O
, O
among O
its O
candidates O
, O
more O
gold O
distractors O
. O
Given O
a O
cloze O
item O
with O
its O
set O
of O
3 O
gold O
distractors O
D O
gold O
, O
and O
an O
automatic O
distractor O
d O
generated O
for O
this O
cloze O
item O
, O
we O
compute O
the O
distractor O
retrieval B-MetricName
score O
as O
follows O
: O

Figure O
2 O
: O
The O
total O
number O
and O
percentage O
of O
gold O
distractors O
retrieved O
for O
the O
142 O
exercise O
items O
with O
different O
pivot O
systems O
, O
using O
30 O
translation O
hypotheses O
in O
each O
direction O
. O

r O
( O
d O
, O
D O
gold O
) O
= O
1 O
if O
d O
∈ O
D O
gold O
0 O
otherwise O
( O
1 O
) O

We O
compute O
cumulative O
retrieval B-MetricName
score O
r O
( O
d O
, O
D O
gold O
) O
across O
all O
cloze O
items O
( O
the O
total O
number O
of O
gold O
distractors O
is O
426 O
, O
since O
we O
have O
142 O
cloze O
items O
, O
each O
containing O
3 O
gold O
distractors O
) O
. O
Figure O
2 O
shows O
the O
cumulative O
retrieval B-MetricName
score O
( O
and O
percentage O
of O
gold O
distractors O
retrieved O
) O
by O
pivot O
and O
for O
the O
union O
of O
all O
languages O
: O
44.8 B-MetricValue
% I-MetricValue
of O
gold O
distractors O
are O
retrieved O
with O
the O
automatic O
approach O
. O
Compared O
to O
the O
results O
over O
5 O
language O
pivots O
in O
Panda O
et O
al O
. O
( O
2022 O
) O
, O
gold O
retrieval B-MetricName
score O
is O
increased O
from O
31.9 B-MetricValue
% I-MetricValue
to O
44.8 B-MetricValue
% I-MetricValue
when O
using O
16 O
pivot O
languages O
. O
The O
union O
of O
the O
pivot O
languages O
is O
able O
to O
retrieve O
3 O
to O
4 O
times O
as O
many O
gold O
distractors O
as O
the O
individual O
languages O
, O
indicating O
that O
multiple O
pivots O
produce O
diverse O
candidate O
distractors O
. O

Performance O
comparison O
by O
the O
quality O
of O
MT B-MethodName
systems O
Table O
3 O
shows O
gold O
retrieval B-MetricName
( O
column O
A O
) O
and O
the O
number B-MetricName
of I-MetricName
generated I-MetricName
candidates I-MetricName
( O
column O
B O
) O
, O
averaged O
over O
the O
systems O
in O
each O
pivot O
group O
. O
Top O
MT B-MethodName
systems O
( O
group O
1 O
) O
retrieve O
almost O
as O
many O
gold O
distractors O
as O
low O
- O
quality O
systems O
, O
but O
they O
generate O
substantially O
fewer O
candidates O
. O
Overall O
, O
better O
MT B-MethodName
systems O
generate O
significantly O
fewer O
distractor O
candidates O
. O

Manual B-MetricName
evaluation I-MetricName
of O
distractors O
for O
validity O

Although O
filtering O
removes O
a O
substantial O
number O
of O
invalid O
distractor O
candidates O
, O
there O
are O
still O
invalid O
candidates O
( O
contextual O
synonyms O
) O
that O
are O
not O
filtered O
out O
. O
To O
determine O
how O
many O
invalid O
candidates O
are O
generated O
, O
a O
set O
of O
100 O
distractors O
produced O
with O
each O
pivot O
system O
, O
is O
evaluated O
for O
validity O
independently O
by O
3 O
native O
English O
speakers O
. O
We O
then O
compute O
the O
percentage O
of O
candidates O
judged O
as O
valid O
( O
averaged O
over O
the O
3 O
raters O
) O
, O
shown O
in O
Table O
3 O
( O
column O
C O
) O
by O
pivot O
group O
. O
Overall O
, O
languages O
in O
pivot O
group O
1 O
with O
better O
MT B-MethodName
systems O
produce O
the O
smallest O
percentage O
of O
valid B-MetricName
candidates I-MetricName
, O
while O
the O
languages O
with O
the O
poorest O
MT B-MethodName
systems O
produce O
the O
highest O
percentage O
of O
valid B-MetricName
candidates I-MetricName
. O
We O
compute O
inter B-MetricName
- I-MetricName
annotator I-MetricName
agreement I-MetricName
for O
the O
3 O
native O
speakers O
, O
as O
described O
in O
Appendix O
C O
. O

Manual O
evaluation O
of O
the O
difficulty B-MetricName
of O
the O
automatic O
distractors O
by O
pivot O
group O
To O
evaluate O
the O
difficulty B-MetricName
of O
distractors O
, O
a O
trained O
linguist O
is O
presented O
with O
an O
exercise O
item O
together O
with O
the O
target O
word O
and O
a O
proposed O
distractor O
and O
is O
asked O
to O
judge O
whether O
the O
distractor O
has O
semantic B-MetricName
similarity I-MetricName
to O
the O
context O
and O
to O
the O
target O
word O
( O
distractors O
that O
have O
semantic O
similarity O
are O
more O
difficult O
for O
a O
language O
learner O
to O
rule O
out O
and O
thus O
are O
more O
appropriate O
for O
advanced O
language O
learners O
) O
. O
Only O
candidates O
judged O
as O
valid O
by O
all O
three O
raters O
are O
evaluated O
for O
semantic B-MetricName
similarity I-MetricName
. O
10 O
pivot O
languages O
are O
selected O
: O
4 O
from O
group O
1 O
, O
and O
2 O
from O
each O
other O
group O
. O
Results O
averaged O
by O
pivot O
group O
are O
shown O
in O
Table O
4 O
. O
Better O
quality O
MT B-MethodName
systems O
generate O
a O
higher O
percentage O
of O
challenging O
distractors O
among O
their O
candidates O
. O
Thus O
, O
although O
the O
pivots O
with O
better O
MT B-MethodName
systems O
produce O
fewer O
candidates O
overall O
, O
there O
is O
a O
substantially O
higher O
proportion O
of O
difficult O
distractors O
among O
the O
candidates O
, O
compared O
to O
pivots O
with O
low O
- O
quality O
MT B-MethodName
systems O
. O
Results O
by O
individual O
pivot O
are O
shown O
in O
Table O
D4 O
. O
sentence O
and O
the O
target O
word O
, O
and O
those O
that O
do O
not O
. O

Varying O
the O
number O
of O
generated O
hypotheses O
by O
translation O
direction O

So O
far O
, O
we O
have O
evaluated O
our O
approach O
, O
using O
30 O
translation O
hypotheses O
in O
each O
direction O
. O
We O
now O
compare O
three O
settings O
, O
generating O
900 O
backtranslations O
with O
30.30 O
, O
900.1 O
, O
and O
1.900 O
, O
where O
the O
first O
value O
is O
the O
number O
of O
hypotheses O
in O
the O
forward O
direction O
, O
while O
the O
second O
value O
is O
the O
number O
of O
hypotheses O
in O
the O
backward O
direction O
for O
each O
forward O
translation O
. O
6 O
Table O
6 O
summarizes O
gold O
retrieval B-MetricName
results O
and O
the O
average O
number B-MetricName
of I-MetricName
candidates I-MetricName
generated O
per O
exercise O
item O
, O
by O
pivot O
group O
. O
The O
highest O
retrieval B-MetricName
score O
is O
obtained O
in O
the O
900.1 O
setting O
( O
64.8 B-MetricValue
% I-MetricValue
of O
gold O
distractors O
are O
retrieved O
) O
, O
whereas O
the O
30.30 O
setting O
produces O
the O
smallest O
number O
of O
gold O
distractors O
( O
44.8 B-MetricValue
% I-MetricValue
) O
. O
The O
30.30 O
setting O
also O
produces O
the O
smallest O
number B-MetricName
of I-MetricName
candidates I-MetricName
( O
234 B-MetricValue
) O
, O
while O
the O
other O
two O
settings O
generate O
a O
similar O
number B-MetricName
of I-MetricName
candidates I-MetricName
( O
946 B-MetricValue
and O
868 B-MetricValue
) O
. O
Results O
by O
pivot O
group O
show O
similar O
trends O
across O
the O
3 O
settings O
and O
are O
shown O
in O
Appendix O
Table O
D5 O
. O
Performance O
of O
select O
individual O
pivots O
for O
the O
3 O
hypothesis O
settings O
can O
be O
viewed O
in O
Appendix O
Figures O
D3 O
and O
D4 O
. O

Manual O
evaluation O
of O
distractors O
for O
validity O
, O
by O
hypothesis O
setting O
We O
compute O
the O
percentage O
of O
valid B-MetricName
candidates I-MetricName
generated O
in O
each O
setting O
. O
We O
use O
six O
pivot O
languages O
: O
German O
and O
Russian O
( O
group O
1 O
) O
, O
Indonesian O
( O
group O
2 O
) O
, O
Malayalam O
( O
group O
3 O
) O
, O
and O
Chuukese O
and O
Hindi O
( O
group O
4 O
) O
. O
For O
each O
pivot O
, O
we O
generate O
3 O
sets O
of O
distractors O
( O
1 O
set O
of O
100 O
candidates O
for O
each O
of O
the O
3 O
direction O
settings O
) O
. O
Each O
candidate O
distractor O
is O
judged O
for O
validity B-MetricName
by O
the O
three O
annotators O
. O
Results O
are O
shown O
in O
Table O
7 O
: O
the O
900.1 O
setting O
generates O
the O
highest O
percentage O
of O
valid B-MetricName
candidates I-MetricName
( O
91.1 B-MetricValue
% I-MetricValue
) O
. O

Manual B-MetricName
evaluation I-MetricName
of O
the O
difficulty B-MetricName
of O
the O
automatic O
distractors O
by O
hypothesis O
setting O
As O
in O
previous O
section O
, O
we O
evaluate O
the O
difficulty B-MetricName
of O
the O
generated O
distractors O
, O
as O
a O
function O
of O
the O
translation O
hypotheses O
used O
in O
each O
direction O
. O
For O
each O
of O
the O
6 O
pivot O
systems O
annotated O
for O
validity B-MetricName
, O
the O
same O
linguist O
judged O
, O
for O
each O
candidate O
considered O
as O
valid O
by O
all O
3 O
raters O
, O
whether O
the O
candidate O
has O
semantic B-MetricName
similarity I-MetricName
to O
the O
target O
and O
to O
the O
carrier O
sentence O
context O
. O
Results O
are O
shown O
in O
Table O
8 O
. O
In O
groups O
1 O
and O
2 O
, O
the O
30.30 O
setting O
produces O
the O
highest O
percentage O
of O
candidates O
with O
semantic B-MetricName
similarity I-MetricName
. O
Overall O
, O
the O
30.30 O
setting O
with O
languages O
in O
group O
1 O
produces O
the O
highest O
percentage O
of O
difficult O
distractors O
. O
This O
is O
followed O
by O
the O
30.30 O
setting O
group O
2 O
( O
51.5 B-MetricValue
% I-MetricValue
) O
. O
This O
suggests O
that O
using O
the O
30.30 O
setting O
and O
good O
MT B-MethodName
systems O
is O
preferred O
for O
generating O
challenging O
distractors O
. O

Adding O
other O
language O
pivots O
might O
still O
be O
beneficial O
to O
obtain O
a O
more O
diverse O
set O
of O
distractors O
, O
however O
, O
more O
human O
feedback O
would O
be O
required O
to O
identify O
challenging O
candidates O
. O

Distractor O
Diversity O
for O
Related O
vs. O
Unrelated O
Language O
Pivots O

Section O
5.1 O
has O
shown O
that O
the O
union O
of O
16 O
pivot O
systems O
generates O
a O
diverse O
set O
of O
distractors O
. O
However O
, O
some O
of O
the O
pivots O
are O
more O
closely O
related O
than O
others O
. O
Here O
, O
we O
verify O
the O
claim O
that O
languages O
that O
are O
more O
closely O
related O
, O
tend O
to O
contribute O
similar O
distractors O
, O
whereas O
unrelated O
languages O
generate O
more O
diverse O
distractors O
. O
If O
this O
is O
true O
, O
this O
would O
also O
support O
the O
idea O
of O
customizing O
distractors O
to O
the O
native O
language O
of O
the O
learner O
. O
We O
identify O
several O
pairs O
of O
most O
closely O
related O
languages O
among O
the O
16 O
pivots O
used O
: O
French O
and O
Italian O
; O
Urdu O
and O
Hindi O
; O
Italian O
and O
Spanish O
; O
German O
and O
Dutch O
; O
Czech O
and O
Russian O
. O
For O
each O
language O
pair O
, O
we O
compute O
the O
gold O
retrieval O
score O
using O
the O
union O
of O
the O
candidates O
that O
the O
pivot O
pair O
generates O
. O
Let O
the O
first O
and O
second O
pivot O
in O
the O
pair O
be O
r O
1 O
and O
r O
2 O
, O
respectively O
. O
We O
then O
identify O
for O
each O
pair O
another O
pivot O
u O
1 O
that O
is O
unrelated O
to O
r O
1 O
, O
and O
compute O
gold O
retrieval B-MetricName
score O
for O
the O
union O
of O
r O
1 O
and O
u O
1 O
. O
We O
then O
compare O
the O
retrieval B-MetricName
scores O
for O
the O
union O
of O
r O
1 O
and O
r O
2 O
, O
and O
for O
the O
union O
of O
r O
1 O
and O
u O
1 O
. O

We O
compute O
the O
gold O
distractor O
retrieval B-MetricName
for O
each O
group O
using O
the O
30.30 O
setting O
. O
Since O
each O
language O
Sentence O
: O
We O
paid O
the O
lawyer O
to O
_ O
_ O
_ O
_ O
_ O
_ O
up O
a O
totally O
new O
will O
. O
Target O
word O
: O
draw O
; O
candidate O
: O
realize O
; O
semantic B-MetricName
similarity I-MetricName
: O
yes O
Sentence O
: O
Due O
to O
the O
fact O
you O
were O
n't O
listening O
, O
you O
understood O
_ O
_ O
_ O
_ O
_ O
_ O
nothing O
of O
what O
I O
said O
. O
Target O
word O
: O
virtually O
; O
candidate O
: O
barely O
; O
semantic B-MetricName
similarity I-MetricName
: O
yes O
Sentence O
: O
Despite O
past O
good O
performances O
, O
the O
actor O
was O
fired O
when O
the O
studio O
decided O
he O
had O
become O
a O
_ O
_ O
_ O
_ O
_ O
_ O
. O
Target O
word O
: O
liability O
; O
candidate O
: O
decision O
; O
semantic B-MetricName
similarity I-MetricName
: O
no O
Sentence O
: O
It O
was O
the O
child O
's O
history O
teacher O
that O
first O
realised O
she O
was O
being O
_ O
_ O
_ O
_ O
_ O
_ O
at O
home O
. O
Target O
word O
: O
neglected O
; O
candidate O
: O
aware O
; O
semantic B-MetricName
similarity I-MetricName
: O
no O
produces O
a O
different O
number O
of O
gold O
distractors O
, O
for O
a O
fair O
comparison O
, O
we O
select O
a O
u O
1 O
, O
such O
that O
the O
gold O
retrieval B-MetricName
score O
of O
u O
1 O
on O
its O
own O
is O
the O
same O
as O
or O
close O
to O
the O
score O
of O
r O
2 O
. O
Our O
hypothesis O
is O
that O
since O
r O
1 O
and O
u O
1 O
are O
unrelated O
, O
their O
candidates O
should O
have O
less O
of O
an O
overlap O
than O
the O
candidates O
of O
r O
1 O
and O
r O
2 O
. O
Therefore O
, O
the O
gold O
retrieval B-MetricName
score O
of O
the O
union O
of O
r O
1 O
with O
an O
unrelated O
language O
should O
be O
higher O
than O
for O
the O
union O
of O
r O
1 O
and O
r O
2 O
. O
Indeed O
, O
we O
confirm O
our O
hypothesis O
in O
Table O
9 O
. O

We O
further O
analyze O
the O
distractors O
proposed O
by O
various O
pivots O
and O
find O
that O
52 O
/ O
191 O
gold O
distractors O
in O
the O
30.30 O
setting O
( O
27 O
% O
) O
are O
proposed O
by O
a O
single O
pivot O
and O
not O
proposed O
by O
the O
other O
15 O
pivots O
. O

Comparison O
with O
baseline O
methods O

Our O
earlier O
study O
( O
Panda O
et O
al O
. O
, O
2022 O
) O
compared O
the O
round B-MethodName
- I-MethodName
trip I-MethodName
MT I-MethodName
against O
word2vec B-MethodName
and O
BERT B-MethodName
, O
two O
approaches O
that O
showed O
competitive O
results O
for O
distractor B-TaskName
generation I-TaskName
( O
Mikolov O
et O
al O
. O
, O
2013 O
; O
Gao O
et O
al O
. O
, O
2020 O
) O
. O
the O
three O
methods O
when O
generating O
the O
same O
number O
of O
candidates O
( O
51 O
) O
with O
each O
method O
. O
Table O
11 O
shows O
the O
percentage O
of O
valid B-MetricName
distractors I-MetricName
among O
the O
proposed O
candidates O
for O
each O
method O
, O
demonstrating O
the O
superiority O
of O
the O
MT B-MethodName
approach O
over O
word2vec B-MethodName
and O
BERT B-MethodName
. O
Further O
, O
neither O
word2vec B-MethodName
nor O
BERT B-MethodName
are O
effective O
at O
ranking O
the O
candidates O
, O
because O
word2vec B-MethodName
and O
BERT B-MethodName
tend O
to O
prefer O
words O
that O
are O
synonymous O
with O
the O
target O
and O
thus O
fit O
the O
context O
. O
Appendix O
B O
provides O
more O
detail O
on O
the O
two O
baseline O
methods O
and O
how O
comparisons O
are O
performed O
. O

Study O
with O
Language O
Learners O

To O
evaluate O
the O
difficulty O
of O
automatically O
generated O
distractors O
, O
we O
conduct O
a O
cloze O
exercise O
test O
with O
English O
learners O
. O
We O
use O
a O
pool O
of O
manually O
validated O
items O
from O
the O
30.30 O
setting O
and O
the O
pivots O
in O
group O
1 O
to O
create O
a O
cloze O
test O
for O
participants O
. O

Manual B-MetricName
validation I-MetricName
ensured O
that O
all O
of O
the O
automatically O
generated O
candidates O
are O
valid O
. O
We O
sample O
32 O
exercise O
items O
uniformly O
at O
random O
from O
the O
pool O
. O

Participants O
Our O
participants O
are O
adult O
nonnative O
English O
speakers O
of O
diverse O
language O
backgrounds O
. O
To O
ensure O
that O
the O
participants O
are O
advanced O
learners O
, O
we O
asked O
them O
to O
provide O
their O
Gold O
distractors B-MetricName
retrieved I-MetricName
Word2vec B-MethodName
BERT B-MethodName
MT B-MethodName
39 B-MetricValue
( O
9.2 B-MetricValue
% I-MetricValue
) O
97 B-MetricValue
( O
22.8 B-MetricValue
% I-MetricValue
) O
136 B-MetricValue
( O
31.9 B-MetricValue
% I-MetricValue
) O
TOEFL O
or O
IELTS O
scores O
. O
We O
also O
gave O
them O
a O
sample O
test O
to O
complete O
to O
exclude O
those O
whose O
English O
was O
too O
good O
or O
not O
good O
enough O
. O
Participants O
were O
informed O
that O
the O
results O
of O
their O
tests O
would O
be O
used O
to O
collect O
statistics O
for O
research O
, O
without O
disclosing O
personal O
information O
. O
Participants O
were O
provided O
with O
$ O
25 O
gift O
cards O
. O

Cloze O
exercise O
setup O
We O
create O
two O
versions O
of O
a O
cloze O
test O
with O
the O
same O
set O
of O
32 O
carrier O
sentences O
. O
Each O
version O
contains O
16 O
sentences O
with O
gold O
distractors O
and O
16 O
sentences O
with O
automatic O
distractors O
. O
The O
sentences O
that O
come O
with O
gold O
distractors O
in O
the O
first O
version O
, O
come O
with O
automatic O
distractors O
in O
the O
second O
version O
of O
the O
test O
, O
and O
vice O
versa O
. O
The O
order O
of O
the O
cloze O
items O
in O
each O
version O
is O
randomized O
. O
Additionally O
, O
we O
ensure O
that O
for O
each O
item O
the O
target O
always O
appears O
in O
the O
same O
position O
with O
both O
gold O
and O
automatic O
distractors O
on O
the O
multiple O
- O
choice O
list O
. O

Each O
version O
of O
the O
test O
was O
completed O
by O
exactly O
16 O
participants O
, O
so O
each O
cloze O
item O
was O
completed O
by O
16 O
learners O
who O
were O
given O
gold O
distractors O
, O
and O
by O
another O
group O
of O
16 O
learners O
who O
received O
automatic O
distractors O
. O
We O
use O
the O
first O
2 B-HyperparameterValue
cloze O
items O
as O
training B-HyperparameterName
items O
, O
to O
help O
the O
test O
takers O
familiarize O
themselves O
with O
the O
task O
. O
The O
statis- O
tics O
are O
computed O
using O
the O
remaining O
30 B-HyperparameterValue
cloze O
items O
. O
These O
remaining O
30 O
cloze O
items O
contain O
an O
equal O
number O
( O
15 O
) O
of O
items O
with O
gold O
distractors O
and O
automatic O
distractors O
. O

We O
set O
up O
the O
test O
in O
a O
user O
interface O
setting O
, O
where O
a O
participant O
can O
see O
the O
carrier O
sentence O
and O
the O
four O
choices O
on O
the O
screen O
and O
has O
to O
pick O
one O
choice O
. O
As O
part O
of O
the O
test O
instructions O
, O
the O
participants O
were O
asked O
not O
to O
leave O
the O
response O
blank O
. O
We O
asked O
the O
participants O
not O
to O
get O
help O
from O
external O
resources O
to O
solve O
the O
exercise O
. O
The O
participants O
took O
between O
20 O
to O
30 O
minutes O
to O
complete O
the O
test O
. O

Paired O
t O
- O
test O
A O
paired O
t O
- O
test O
was O
used O
to O
compare O
the O
human O
performance O
on O
cloze O
items O
with O
gold O
and O
automatic O
distractors O
. O
For O
computing O
the O
paired O
t O
- O
test O
statistics O
, O
we O
use O
the O
30 O
cloze O
items O
that O
were O
not O
used O
as O
training O
items O
, O
and O
compare O
scores O
of O
gold O
vs. O
automatic O
distractors O
used O
, O
where O
the O
score O
is O
defined O
as O
proportion O
of O
participants O
that O
correctly O
solved O
the O
item O
. O
There O
was O
no O
significant O
difference O
in O
the O
scores O
of O
gold O
distractors O
( O
with O
mean O
9.57 O
, O
standard O
deviation O
3.83 O
) O
and O
automatic O
distractors O
( O
with O
mean O
10.23 O
, O
standard O
deviation O
3.47 O
) O
. O
The O
two O
- O
tailed O
P O
value O
is O
0.2884 O
. O
These O
results O
suggest O
that O
the O
scores O
on O
cloze O
items O
using O
gold O
distractors O
and O
automatic O
distractors O
are O
not O
significantly O
different O
. O
Specifically O
, O
our O
results O
show O
that O
when O
automatic O
distractors O
are O
used O
in O
the O
cloze O
items O
instead O
of O
gold O
distractors O
, O
the O
difficulty O
of O
the O
cloze O
items O
remains O
the O
same O
. O

Conclusion O

We O
present O
a O
novel O
approach O
to O
generate B-TaskName
challenging I-TaskName
distractors I-TaskName
for O
cloze O
exercises O
with O
round B-MethodName
- I-MethodName
trip I-MethodName
neural I-MethodName
MT I-MethodName
. O
We O
show O
that O
using O
multiple O
pivot O
systems O
and O
a O
large O
set O
of O
round B-MethodName
- I-MethodName
trip I-MethodName
translations I-MethodName
produces O
diverse O
candidates O
, O
and O
each O
pivot O
contributes O
unique O
distractors O
. O
The O
latter O
opens O
up O
a O
possibility O
of O
customizing O
the O
cloze O
task O
for O
speakers O
of O
different O
languages O
, O
by O
tying O
the O
pivot O
choice O
to O
the O
learner O
's O
native O
language O
, O
an O
interesting O
promise O
that O
BERT B-MethodName
- O
based O
and O
other O
models O
can O
not O
do O
. O
We O
conduct O
a O
thorough O
evaluation O
of O
the O
distractors O
, O
using O
a O
set O
of O
real O
cloze O
exercises O
for O
advanced O
ESL O
learners O
. O
Finally O
, O
we O
conduct O
a O
study O
with O
language O
learners O
that O
demonstrates O
that O
the O
automatic O
distractors O
produced O
with O
our O
approach O
result O
in O
cloze O
items O
of O
the O
same O
difficulty O
as O
those O
that O
use O
gold O
distractors O
. O
For O
future O
work O
, O
we O
will O
focus O
on O
customizing O
distractors O
based O
on O
the O
learner O
's O
native O
language O
, O
by O
prioritizing O
that O
language O
as O
pivot O
for O
MT B-MethodName
. O

Limitations O

A O
qualitative O
analysis O
of O
distractors O
generated O
via O
MT B-MethodName
shows O
that O
this O
method O
can O
produce O
some O
inadequate O
candidates O
( O
and O
so O
do O
word2vec B-MethodName
and O
BERT B-MethodName
- O
based O
methods O
) O
. O
Thus O
, O
a O
human O
- O
in O
- O
the O
- O
loop O
is O
needed O
to O
ensure O
the O
validity O
of O
the O
generated O
distractors O
. O
However O
, O
human O
- O
in O
- O
the O
- O
loop O
is O
standard O
practice O
, O
when O
producing O
language O
exercises O
and O
tests O
( O
Attali O
et O
al O
. O
, O
2022 O
) O
. O
We O
therefore O
believe O
that O
the O
proposed O
approach O
does O
not O
need O
to O
be O
fully O
automatic O
to O
be O
useful O
, O
as O
it O
can O
still O
help O
speed O
up O
distractor O
generation O
to O
create O
advanced O
vocabulary O
exercises O
. O
The O
MT B-MethodName
method O
can O
thus O
be O
of O
huge O
help O
to O
human O
test O
developers O
. O

The O
MT B-MethodName
approach O
can O
be O
computationally O
more O
expensive O
than O
the O
methods O
proposed O
in O
prior O
work O
such O
as O
BERT B-MethodName
and O
word2vec B-MethodName
. O
Although O
we O
make O
use O
of O
pre O
- O
trained O
MT B-MethodName
systems O
, O
the O
approach O
can O
be O
still O
costly O
, O
as O
it O
requires O
running O
two O
MT B-MethodName
systems O
( O
forward O
and O
backward O
) O
with O
each O
pivot O
, O
and O
a O
BERT B-MethodName
- O
based O
word O
alignment O
model O
to O
align O
the O
carrier O
sentence O
with O
each O
of O
its O
900 O
back O
- O
translations O
. O
In O
terms O
of O
cost O
comparison O
, O
it O
takes O
1 O
- O
2 O
hours O
in O
a O
single O
Nvidia O
Tesla O
A100 O
GPU O
to O
generate O
900 O
translations O
and O
produce O
candidate O
distractors O
for O
a O
single O
pivot O
, O
versus O
0.5 O
hour O
with O
BERT B-MethodName
and O
word2vec B-MethodName
. O
However O
, O
the O
MT B-MethodName
approach O
can O
potentially O
offer O
advantages O
that O
other O
methods O
can O
not O
, O
such O
as O
producing O
a O
more O
diverse O
pool O
of O
distractors O
and O
, O
importantly O
, O
relating O
the O
native O
language O
of O
the O
learner O
to O
the O
pivot O
systems O
used O
to O
produce O
distractors O
. O
As O
our O
analyses O
show O
, O
each O
pivot O
system O
generates O
unique O
distractors O
. O
We O
stress O
that O
, O
while O
we O
show O
that O
using O
multiple O
pivots O
generates O
diverse O
distractors O
, O
we O
leave O
the O
question O
of O
whether O
using O
a O
pivot O
based O
on O
learner O
's O
first O
language O
is O
useful O
, O
to O
future O
work O
. O
We O
do O
hypothesize O
, O
however O
, O
that O
using O
pivots O
tied O
to O
the O
first O
language O
might O
be O
useful O
, O
however O
, O
but O
verifying O
this O
claim O
is O
left O
for O
future O
work O
. O
This O
is O
because O
verifying O
whether O
tying O
the O
pivot O
to O
learner O
's O
native O
language O
would O
be O
useful O
would O
require O
a O
human O
study O
with O
a O
relatively O
large O
group O
of O
learners O
of O
at O
least O
20 O
- O
30 O
students O
( O
all O
of O
advanced O
level O
) O
that O
all O
share O
the O
same O
first O
language O
. O
In O
fact O
, O
we O
would O
need O
to O
have O
several O
groups O
of O
learners O
, O
such O
that O
students O
in O
each O
group O
have O
the O
same O
first O
language O
background O
. O
This O
would O
be O
a O
large O
- O
scale O
study O
that O
is O
out O
of O
the O
scope O
of O
the O
paper O
. O
Note O
that O
the O
current O
work O
already O
presents O
a O
human O
study O
with O
32 O
students O
that O
demonstrates O
that O
the O
automatically O
generated O
pivots O
are O
of O
the O
same O
difficulty O
as O
those O
created O
manually O
. O

We O
also O
note O
that O
the O
method O
requires O
relatively O
good O
MT B-MethodName
systems O
for O
generating O
more O
difficult O
distractors O
. O
Finally O
, O
our O
study O
is O
limited O
to O
cloze O
items O
that O
include O
single O
words O
as O
targets O
and O
does O
not O
consider O
fixed O
expressions O
, O
such O
as O
phrasal O
verbs O
and O
idioms O
. O
In O
the O
language O
testing O
community O
, O
such O
expressions O
are O
typically O
tested O
separately O
from O
the O
generic O
cloze O
items O
. O
The O
basic O
approach O
is O
to O
detect O
them O
before O
the O
carrier O
sentence O
is O
cleared O
to O
be O
used O
for O
cloze O
exercises O
. O
Our O
current O
work O
is O
not O
focused O
on O
carrier O
sentence O
selection O
. O
But O
it O
makes O
sense O
to O
include O
this O
consideration O
in O
a O
larger O
suite O
of O
tools O
for O
cloze O
item O
generation O
. O
ciated O
with O
probability O
; O
top O
k O
candidates O
with O
the O
highest O
scores O
are O
selected O
. O
The O
candidates O
are O
filtered O
out O
using O
the O
same O
filtering O
algorithm O
applied O
in O
round B-MethodName
- I-MethodName
trip I-MethodName
MT I-MethodName
( O
see O
Section O
4 O
) O
. O
Comparing O
generated O
distractors O
with O
BERT B-MethodName
and O
word2vec B-MethodName
on O
gold O
distractor O
retrieval O
Using O
word2vec B-MethodName
and O
BERT B-MethodName
, O
a O
list O
of O
n O
nearest O
neighbors O
for O
each O
target O
word O
is O
generated O
. O
Since O
the O
roundtrip B-MethodName
MT I-MethodName
method O
produces O
a O
different O
number O
of O
candidate O
distractors O
per O
target O
, O
whereas O
word2vec B-MethodName
and O
BERT B-MethodName
generate O
a O
long O
list O
of O
candidates O
, O
the O
average O
number O
of O
candidates O
produced O
with O
roundtrip O
MT B-MethodName
with O
the O
union O
of O
5 O
pivot O
languages O
is O
used O
, O
to O
generate O
104 O
neighbors O
without O
filtering O
and O
51 O
neighbors O
with O
filtering O
applied O
. O
Results O
are O
shown O
in O
Table O
B2 O
before O
and O
after O
filtering O
is O
applied O
. O
Round B-MethodName
- I-MethodName
trip I-MethodName
MT I-MethodName
retrieves O
significantly O
more O
gold O
distractors O
compared O
to O
word2vec B-MethodName
and O
BERT B-MethodName
. O
Word2vec B-MethodName
performs O
the O
worst O
among O
the O
three O
methods O
. O
Manual O
evaluation O
of O
distractor O
validity O
for O
the O
three O
methods O
For O
each O
carrier O
sentence O
, O
5 O
sets O
of O
automatically O
- O
generated O
distractors O
are O
compared O
: O

( O
1 O
) O
round B-MethodName
- I-MethodName
trip I-MethodName
MT I-MethodName
( O
without O
ranking O
) O
; O
7 O
( O
2 O
) O
roundtrip B-MethodName
MT I-MethodName
with O
word2vec B-MethodName
ranking O
; O
( O
3 O
) O
round B-MethodName
- I-MethodName
trip I-MethodName
MT I-MethodName
with O
BERT B-MethodName
ranking O
; O
( O
4 O
) O
using O
word2vec B-MethodName
for O
generation O
; O
( O
5 O
) O
using O
BERT B-MethodName
for O
generation O
. O
BERT B-MethodName
and O
word2vec B-MethodName
can O
be O
used O
to O
rank O
candidates O
produced O
with O
MT B-MethodName
by O
using O
the O
semantic O
similarity O
of O
the O
candidate O
to O
the O
target O
. O
The O
most O
similar O
candidates O
would O
rank O
as O
the O
highest O
. O

The O
manual B-MetricName
evaluation I-MetricName
was O
performed O
by O
three O
annotators O
who O
are O
college O
students O
and O
native O
English O
speakers O
. O
The O
annotators O
were O
presented O
with O
a O
carrier O
sentence O
, O
the O
target O
word O
, O
and O
the O
manually O
evaluated O
five O
sets O
of O
distractors O
. O
The O
annotator O
's O
task O
was O
to O
mark O
each O
distractor O
as O
valid O
or O
invalid O
. O
Results O
are O
presented O
in O
Table O
11 O
in O
the O
main O
text O
and O
demonstrate O
that O
MT B-MethodName
without O
ranking O
produces O
the O
highest O
percentage O
of O
valid B-MetricName
candidates I-MetricName
with O
all O
three O
annotators O
. O
7 O
Five O
distractors O
are O
selected O
uniformly O
at O
random O
. O

Method O

Annotators O
Avg O
. O
1,2 O
1,3 O
2,3 O
MT B-MethodName
- O
all O
- O
pivots O
0.805 B-MetricValue
0.816 B-MetricValue
0.861 B-MetricValue
0.827 B-MetricValue
Figure O
D4 O
: O
Average O
number O
of O
candidates B-MetricName
generated I-MetricName
per O
exercise O
item O
, O
using O
900 B-HyperparameterValue
hypotheses B-HyperparameterName
with O
the O
different O
number O
of O
hypotheses O
in O
each O
direction O
. O

Acknowledgments O

The O
authors O
would O
like O
to O
thank O
the O
anonymous O
ARR O
reviewers O
for O
their O
insightful O
comments O
. O
This O
work O
was O
partly O
supported O
by O
the O
PSC O
- O
CUNY O
grant O
64487 O
- O
00 O
52 O
. O

Appendix O
A O
: O
Grouping O
Pivot O
Languages O
by O
Machine O
Translation O
Quality O

Using O
BLEU B-MetricName
scores O
on O
Tatoeba B-DatasetName
dataset O
To O
evaluate O
the O
contribution O
of O
the O
quality O
of O
MT B-MethodName
systems O
to O
the O
problem O
of O
distractor O
generation O
, O
we O
use O
BLEU B-MetricName
scores O
of O
the O
MT B-MethodName
systems O
on O
the O
Tatoeba B-DatasetName
dataset O
( O
since O
Bislama O
and O
Chuukese O
are O
not O
part O
of O
Tatoeba O
, O
for O
these O
languages O
we O
report O
BLEU B-MetricName
score O
results O
on O
the O
JW300 B-DatasetName
corpus O
for O
low O
- O
resource O
languages O
( O
Agić O
and O
Vulić O
, O
2019 O
) O
) O
. O

We O
then O
split O
the O
pivot O
languages O
into O
four O
groups O
, O
organized O
by O
the O
averaged O
BLEU B-MetricName
scores O
. O
We O
assume O
higher O
BLEU B-MetricName
scores O
correspond O
to O
back O
- O
translations O
of O
higher O
quality O
. O
Generally O
speaking O
, O
higher O
BLEU B-MetricName
scores O
correspond O
to O
language O
pairs O
with O
more O
training O
data O
( O
high O
- O
resource O
) O
, O
whereas O
lower O
scores O
correspond O
to O
language O
pairs O
that O
are O
low O
- O
resource O
. O
Table O
A1 O
shows O
the O
averaged O
number O
of O
parallel O
sentences O
per O
pivot O
group O
, O
supporting O
this O
claim O
. O
Although O
the O
training O
size O
varies O
by O
language O
, O
languages O
in O
group O
1 O
have O
substantially O
more O
training O
data O
than O
languages O
in O
other O
groups O
. O
The O
number O
of O
parallel O
sentences O
is O
between O
141 O
- O
905 O
M O
in O
group O
1 O
, O
66 O
- O
105 O
M O
in O
group O
2 O
, O
1.9K-126 O
M O
in O
group O
3 O
, O
and O
9.2 O
- O
28 O
M O
in O
group O
4 O
. O
Another O
factor O
that O
might O
be O
contributing O
to O
the O
BLEU B-MetricName
score O
levels O
is O
the O
typological O
distance O
of O
the O
pivot O
and O
English O
( O
all O
languages O
in O
group O
1 O
are O
Indo O
- O
European O
languages O
more O
closely O
related O
to O
English O
, O
compared O
to O
languages O
in O
other O
groups O
. O
) O

Using O
BLEU B-MetricName
scores O
of O
the O
carrier O
sentences O
Since O
BLEU B-MetricName
is O
dependent O
on O
the O
n O
- O
grams O
in O
the O
reference O
, O
we O
also O
perform O
the O
following O
experiment O
: O

1 O
. O
Calculate O
the O
BLEU B-MetricName
score O
for O
every O
carrier O
sentence O
and O
its O
900 O
round O
- O
trip O
translations O
. O
We O
use O
the O
carrier O
sentence O
as O
the O
reference O
and O
the O
round O
- O
trip O
translation O
as O
the O
hypothesis O
. O

2 O
. O
Average O
the O
resulting O
BLEU B-MetricName
scores O
to O
get O
the O
overall O
BLEU B-MetricName
score O
for O
each O
language O
pair O
. O

We O
find O
that O
the O
resulting O
BLEU B-MetricName
scores O
are O
drastically O
small O
, O
ranging O
between O
1.5 B-MetricValue
and O
2.30 B-MetricValue
, O
making O
it O
hard O
to O
provide O
a O
ranking O
between O
the O
language O
pairs O
. O
This O
is O
because O
lower O
- O
ranked O
hypotheses O
tend O
to O
diverge O
from O
the O
original O
sentence O
. O
We O
thus O
perform O
the O
same O
experiment O
by O
including O
only O
top O
10 O
hypotheses O
. O
BLEU B-MetricName
scores O
are O
slightly O
higher O
but O
still O
low O
. O
We O
obtained O
the O
following O
BLEU B-MetricName
scores O
, O
averaged O
by O
language O
group O
: O
6.9 B-MetricValue
( O
group O
1 O
) O
; O
6.4 B-MetricValue
( O
group O
2 O
) O
; O
5.0 B-MetricValue
( O
group O
3 O
) O
; O
2.2 B-MetricValue
( O
group O
4 O
) O
. O

While O
the O
averaged O
BLEU B-MetricName
scores O
are O
all O
very O
small O
, O
they O
do O
support O
the O
ranking O
based O
on O
the O
BLEU B-MetricName
scores O
on O
the O
Tatoeba B-DatasetName
dataset O
. O

Appendix O
B O
: O
Comparison O
with O
Other O
Approaches O

Below O
, O
we O
compare O
the O
MT B-MethodName
approach O
with O
word2vec B-MethodName
and O
BERT B-MethodName
, O
two O
methods O
that O
showed O
competitive O
results O
on O
the O
task O
of O
distractor O
generation O
. O
This O
comparison O
was O
carried O
out O
in O
our O
earlier O
study O
( O
Panda O
et O
al O
. O
, O
2022 O
) O
, O
and O
is O
presented O
here O
for O
convenience O
. O

Using O
word2vec B-MethodName
, O
candidate O
distractors O
are O
generated O
by O
producing O
a O
list O
of O
words O
that O
have O
the O
highest O
similarity O
to O
the O
target O
word O
. O
300 O
- O
dimensional O
word2vec B-MethodName
embeddings O
trained O
on O
Google B-DatasetName
News I-DatasetName
are O
used O
. O
For O
a O
given O
target O
word O
, O
k O
nearest O
neighboring O
words O
based O
on O
cosine O
similarity O
in O
the O
word O
embedding O
space O
are O
considered O
as O
candidates O
. O
With O
BERT B-MethodName
, O
the O
carrier O
sentence O
is O
passed O
to O
the O
model O
, O
with O
the O
target O
word O
replaced O
by O
a O
masked O
token O
. O
BERT B-MethodName
returns O
a O
list O
of O
words O
that O
best O
fit O
the O
context O
of O
the O
carrier O
sentence O
at O
the O
position O
of O
the O
masked O
token O
. O
Each O
word O
is O
asso- O

Appendix O
C O
: O
Inter O
- O
Annotator O
Agreement O

The O
annotators O
made O
a O
binary O
decision O
on O
each O
distractor O
, O
determining O
whether O
the O
distractor O
is O
valid O
. O
We O
compute O
pairwise O
agreement O
using O
Cohen B-MetricName
kappa I-MetricName
's I-MetricName
( O
Cohen O
, O
1960 O
) O
and O
present O
the O
results O
in O
Table O
C3 O
. O
Our O
average O
pairwise B-MetricName
agreement I-MetricName
values O
are O
shown O
in O
the O
last O
column O
. O
These O
values O
are O
better O
than O
those O
obtained O
by O
Yeung O
et O
al O
. O
( O
2019 O
) O
, O
although O
their O
annotation O
task O
included O
3 O
classes O
. O
Cohen B-MetricName
's I-MetricName
kappa I-MetricName
results O
indicate O
strong O
agreement O
in O
all O
cases O
. O
The O
numbers O
in O
the O
table O
indicate O
excellent O
agreement O
( O
Landis O
and O
Koch O
, O
1977 O
) O
. O

Appendix O
D O
: O
Additional O
Results O

Manual B-MetricName
evaluation I-MetricName
of O
the O
difficulty O
of O
the O
automatic O
distractors O
by O
pivot O
group O
Table O
D4 O
shows O
the O
number O
and O
percentage O
of O
candidate O
distractors O
that O
are O
judged O
as O
semantically O
similar O
to O
the O
target O
word O
and O
the O
carrier O
sentence O
. O

Varying O
the O
number O
of O
generated O
hypotheses O
by O
translation O
direction O
Table O
D5 O

