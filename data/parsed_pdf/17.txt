RASAT : Integrating Relational Structures into Pretrained Seq2Seq Model for Text - to - SQL
Relational structures such as schema linking and schema encoding have been validated as a key component to qualitatively translating natural language into SQL queries . However , introducing these structural relations comes with prices : they often result in a specialized model structure , which largely prohibits using large pretrained models in text - to - SQL . To address this problem , we propose RASAT : a Transformer seq2seq architecture augmented with relation - aware self - attention that could leverage a variety of relational structures while inheriting the pretrained parameters from the T5 model effectively . Our model can incorporate almost all types of existing relations in the literature , and in addition , we propose introducing co - reference relations for the multi - turn scenario . Experimental results on three widely used text - to - SQL datasets , covering both singleturn and multi - turn scenarios , have shown that RASAT could achieve state - of - the - art results across all three benchmarks ( 75.5 % EX on Spider , 52.6 % IEX on SParC , and 37.4 % IEX on
Introduction
Text - to - SQL is the task that aims at translating natural language questions into SQL queries . Since it could significantly break down barriers for nonexpert users to interact with databases , it is among the most important semantic parsing tasks that are of practical importance ( Kamath and Das , 2018;Deng et al . , 2021 ) .
Various types of relations have been introduced for this task since Zhong et al . ( 2017 ) collected the first large - scale text - to - SQL dataset , which has resulted in significant boosts in the performance through recent years . For example , Bogin et al . ( 2019b ) introduced schema encoding to represent the schema structure of the database , and the resulting augmented LSTM encoder - decoder architecture was able to generalize better towards unseen database schema . Lin et al . ( 2020a ) introduced relations between the entity mentioned in the question and the matched entries in the database to utilize database content effectively . Their BERT - based encoder is followed by an LSTM - based pointer network as the decoder , which generalizes better between natural language variations and captures corresponding schema columns more precisely . RAT - SQL ( Wang et al . , 2020a ) introduced schema linking , which aligns mentions of entity names in the question to the corresponding schema columns or tables . Their augmented Transformer encoder is coupled with a specific tree - decoder . SADGA ( Cai et al . , 2021 ) introduced the dependency structure of the natural language question and designed a graph neural network - based encoder with a tree - decoder . On the other hand , a tree - decoder that can generate grammatically correct SQL queries is usually needed to better decode the encoder output , among which Yin and Neubig ( 2017 ) is one of the most widely used .
Although integrating various relational structures as well as using a tree - decoder have been shown to be vital to generating qualitative SQL queries and generalizing better towards unseen database schema , the dev of various specifically designed model architectures significantly deviate from the general sequential form , which has made it hard if one considers leveraging large pre - trained models for this task . Existing methods either use BERT output as the input embedding of the specifically designed model ( Cao et al . , 2021;Choi et al . , 2021;Wang et al . , 2020a;Guo et al . , 2019 ) , or stack a specific decoder on top of BERT ( Lin et al . , 2020a ) .
In another thread , pretrained seq2seq models just have unveiled their powerful potential for this task . Recent attempts by Shaw et al . ( 2021 ) show that directly fine - tuning a T5 model ( Raffel et al . , 2020 ) on this task without presenting any relational structures could achieve satisfying results . Moreover , PICARD ( Scholak et al . , 2021 ) presents a way to prune invalid beam search results during inference time , thus drastically improving the grammatical correctness of the SQL queries generated by the autoregressive decoder that comes with T5 .
In this work , different from the more common approach of fine - tuning the original pretrained model or using prompt tuning , we propose to augment the self - attention modules in the encoder and introduce new parameters to the model while still being able to leverage the pre - trained weights . We call the proposed model RASAT 2 . Our model can incorporate almost all existing types of relations in the literature , including schema encoding , schema linking , syntactic dependency of the question , etc . , into a unified relation representation . In addition to that , we also introduce coreference relations to our model for multi - turn text - to - SQL tasks . Experimental results show that RASAT could effectively leverage the advantage of T5 . It achieves the stateof - art performance in question execution accuracy ( EX / IEX ) on both multi - turn ( SParC and CoSQL ) and single - turn ( Spider ) text - to - SQL benchmarks . On SParC , RASAT surpasses all previous methods in interaction execution accuracy ( IEX ) and improves state - of - the - art performance from 21.6 % to 52.6 % , 31 % absolute improvements . On CoSQL , we improve state - of - the - art IEX performance from 8.4 % to 37.4 % , achieving 29 % absolute improvements . Moreover , on Spider , we improve state - ofthe - art execution accuracy from 75.1 % to 75.5 % , achieving 0.4 % absolute improvements .
Related Work
Early works usually exploit a sketch - based slotfilling method that uses different modules to predict the corresponding part of SQL . These methods decompose the SQL generation task into several independent sketches and use different classifiers to predict corresponding part , such as SQLNet ( Xu et al . , 2017 ) , SQLOVA ( Hwang et al . , 2019 ) , X - SQL ( He et al . , 2019 ) , RYANSQL ( Choi et al . , 2021 ) , et.al , . However , most of these methods only 2 RASAT : Relation - Aware Self - Attention - augmented T5 handle simple queries while failing to generate correct SQL in a complex setting such as on Spider .
Faced with the multi - table and complex SQL setting , using graph structures to encode various complex relationships is a major trend in the text - to - SQL task . For example , Global - GNN ( Bogin et al . , 2019a ) represents the complex database schema as a graph , RAT - SQL ( Wang et al . , 2020a ) introduces schema encoding and linking and assigns every two input items a relation , LGESQL ( Cao et al . , 2021 ) further distinguishes local and non - local relations by exploiting a line graph enhanced hidden module , SADGA ( Cai et al . , 2021 ) uses contextual structure and dependency structure to encode question - graph while database schema relations are used in schema graph , S 2 SQL ( Hui et al . , 2022 ) adds syntactic dependency information in relational graph attention network ( RGAT ) ( Wang et al . , 2020b ) .
For the conversational context - dependent textto - SQL task that includes multiple turns of interactions , such as SParC and CoSQL , the key challenge is how to take advantage of historical interaction context . Edit - SQL   edits the last turn 's predicted SQL to generate the newly predicted SQL at the token level . IGSQL ( Cai and Wan , 2020 ) uses cross - turn and intra - turn schema graph layers to model database schema items in a conversational scenario . Tree - SQL ( Wang et al . , 2021b ) uses a tree - structured intermediate representation and assigns a probability to reuse subtree of historical Tree - SQLs . IST - SQL ( Wang et al . , 2021a ) proposes an interaction state tracking method to predict the SQL query . RAT - SQL - TC adds two auxiliary training tasks to explicitly model the semantic changes in both turn grain and conversation grain . R 2 SQL ( Hui et al . , 2021 ) and HIE - SQL ( Zheng et al . , 2022 ) introduce a dynamic schema - linking graph by adding the current utterance , interaction history utterances , database schema , and the last predicted SQL query .
Recently , Shaw et al . ( 2021 ) showed that finetuning a pre - trained T5 - 3B model could yield results competitive to the then - state - of - the - art . Based on this discovery , Scholak et al . ( 2021 ) proposed to constrain the autoregressive decoder through incremental parsing during inference time , effectively filtering out grammatically incorrect sequences on the fly during beam search , which significantly improved the qualities of the generated SQL .
Preliminaries
Task Formulation
Given a natural language question Q and database schema S = < T , C > , our goal is to predict the SQL query Y. Here Q = { q i } |Q| i=1 is a sequence of natural language tokens , and the schema S consists of a series of tables T = { t i }
|T | i=1 with their corresponding columns C = { C i } |T | i=1 . The content of database S is noted as V. For each table t i , the columns in this table is denoted as C i = { c ij } |C i | j=1 . For each table t i , the table name contains |t i | tokens t i = t i,1 , • • • , t i,|t i |
, and the same holds for column names . In this work , we present the predicted SQL query as a sequence of tokens ,
Y = { y i } |Y| i=1 .
In the multi - turn setting , our notations adapt correspondingly . i.e. , Q = { Q i } |Q| i=1 denotes a sequence of questions in the interaction , with Q i denoting each question . Also , the target to be predicted is a sequence of SQL queries , Y = { Y i } |Y| i=1 , with each Y i denoting the corresponding SQL query for the i - th question Q i . Generally , for each question , there is one corresponding SQL query , such that |Q| = |Y| . While predicting Y i , only the questions in the interaction history are available , i.e. , { Q 1 , • • • , Q i } .
Relation - aware Self - Attention
Relation - aware self - attention ( Shaw et al . , 2018 ) augments the vanilla self - attention ( Vaswani et al . , 2017 ) by introducing relation embeddings into the key and value entries . Assume the input to the self attention is a sequence of n embeddings X = { x i } n i=1 where x i ∈ R dx , then it calculates its output z as ( || means concatenate operation ):
α ( h ) ij = softmax    x i W ( h ) Q x j W ( h ) K + r K ij ⊤ dz / H    zi = H h=1 n j=1 α ( h ) ij x j W ( h ) V + r V ij ( 1 )
where H is the number of heads , and
W ( h ) Q , W ( h ) K , W ( h ) V
are learnable weights . The r K ij , r V ij are two different relation embeddings used to represent the relation r between the i - th and j - th token .
RASAT
Model Overview
The overall structure of our RASAT model is shown in Figure 1 . Architecture - wise it is rather simple : the T5 model is taken as the base model , with its self - attention modules in the encoder substituted as relation - aware self - attentions .
The input to the encoder is a combination of question(s ) Q , database schema S = < T , C > with the database name S , as well as database content mentions and necessary delimiters . We mostly follow Shaw et al . ( 2021 ) and Scholak et al . ( 2021 ) to serialize the inputs . Formally ,
X = Q|S|t 1 : c 11 [ v ] , • • • , c 1|T 1 | |t 2 : c 21 , • • •
( 2 ) where t i is the table name , c ij is the j - th column name of the i - th table . The v ∈ V showing after column c 11 is the database content belonging to the column that has n - gram matches with the tokens in the question . As for delimiters , we use | to note the boundaries between Q , S , and different tables in the schema . Within each table , we use : to separate between table name and its columns . Between each column , , is used as the delimiter .
As for the multi - turn scenario , we add the questions in the history at the start of the sequence and truncate the trailing tokens in the front of the sequence when the sequence length reaches 512 . i.e. ,
X = Q 1 |Q 2 | • • • |Q t |S|t 1 : c 11 [ v ] , • • • ( 3 )
where | are the corresponding delimiters . Next , we add various types of relations as triplets , linking between tokens in the serialized input , which naturally turns the input sequence into a graph ( Figure 1 ) . We will elaborate on this in Subsection 4.2 . Moreover , since almost all relation triplets , its head and tail correspond to either a word or a phrase , while the T5 model is at subword level , we also introduce relation propagation to map these relations to subword level , which is detailed in Subsection 4.3 .
To fine - tune this model , we inherit all the parameters from T5 and randomly initialize the extra relation embeddings introduced by relation - aware self - attention . The overall increase of parameters is less than 0.01 % ( c.f . Appendix A ) .
Interaction Graph
Equipped with relation - aware self - attention , we can incorporate various types of relations into the
INPUT : q 1,1 q 1,2 | … | q t-1,1 q t-1,2 | q t,1 q t,2 q t,3 q t,4 | S | t 1 : c 11 [ v 1 ] , c
Dis1 D is 1 Dis 1 D is 1 Dep Co re f Match H a sD h a sC hasC hasC M at ch F k e y k r v r Interaction Graph K V Figure 1 :
The overview of our model . Our model inherits the seq2seq architecture of T5 , consisting of N layers of encoders and decoders . The self - attention modules in the encoder are substituted with relation - aware self - attention , introducing two additional relation embedding lookup tables R K and R V . We convert the sequential input into an interaction graph by introducing various types of relations and adapting them to the subword level through relation propagation . During the forward process , the relation - aware self - attention modules read out the relations of each token through the interaction graph and retrieve the corresponding relations embeddings from the lookup tables R K and R V .
Type Head H Tail T Edge Label Description   T5 model , as long as the relation can be presented as a triplet , with its head and tail being the tokens in the input sequence X. Formally , we present the triplet as
Schema Encoding T C PRIMARY - KEY T is the primary - key for H BELONGS - TO T is a column in H C C FOREIGN - KEY H is the foreign key for T Schema Linking Q T /C EXACT - MATCH H is part of T ,
< H , r , T > ( 4 )
where H , T are the head and tail items in the triplet , and r represents the relation . Given the input sequence X of length |X| , we assume that for each direction of a given pair of tokens , there only exists up to one relation . Thus , if we consider the tokens in X as vertices of a graph , it could have up to |X| 2 directed edges , with each edge corresponding to an entry in the adjacency matrix of the graph . In this paper , we call this graph , containing tokens from the whole input sequence as its vertices and the incorporated relations as its edges , as interaction graph . We assign two relation embeddings for each type of introduced relation . Thus the Transformer encoder comes with two trainable lookup tables storing relations embeddings to compute the key and value in the self - attention ( c.f . Figure 1 ) . Formally , we denote them as R K , R V ∈ R µ×d kv where µ is the kinds of relations and d kv is the dimension of each attention head in the key and value states . Note that we share the relation embedding between different heads and layers but untie them between key and value . During forward computation , for all the layers , r K ij and r V ij in Equation 1 are retrieved from the two trainable look - up tables .
We reserve a set of generic relations for serving as mock relations for token pairs that do not have a specific edge . In total , we have used 51 different relations in the model ( c.f . Appendix D ) . Apart from the mock generic relations , there are generally 5 types of relations , which are : schema encoding , schema linking , question dependency structure , coreference between questions , and database content mentions . Please refer to Table 1 for some representative examples for each type . We will describe each of them in the following paragraphs .
Schema Encoding . Schema encoding relations refer to the relation between schema items , i.e. , H , T ∈ S. These relations describe the structure information in a database schema . For example , PRIMARY - KEY indicates which column is the primary key of a table , BELONGS - TO shows which table a column belongs to , and FORIGN - KEY connects the foreign key in one table , and the primary key in another table .
Schema Linking . Schema linking relations refer to the relations between schema and question items , i.e. , H ∈ S , T ∈ Q or vice versa . We follow the settings in RAT - SQL ( Wang et al . , 2020a ) , which uses n - gram matches to indicate question mentions of the schema items . Detecting these relations is shown to be challenging in previous works ( Guo et al . , 2019;Deng et al . , 2021 ) due to the common mismatch between natural language references and their actual names in the schema . Thus , we also discriminate between exact matches and partial matches to suppress the noise caused by imperfect matches .
Question Dependency Structure . This type of relation refers to the edges of a dependency tree of the question , i.e. , H , T ∈ Q. Unlike the previous two relation types , it is less explored in the literature on text - to - SQL . Since it reflects the grammatical structure of the question , we believe it should also be beneficial for the task . In our work , to control the total number of relations and avoid unnecessary overfitting , we do not discriminate between different dependency relations . Figure 2 shows an example of dependency relations in one of its questions .
Coreference Between Questions . This type of relation is unique to the multi - turn scenario . In a dialog with multiple turns , it is important for the model to figure out the referent of the pronouns correctly . Figure 2 shows a typical case of coreference resolution . The question item " their " in Turn 1 , " they " in Turn 2 , and " they " in Turn 3 all refer to the question item " students " in Turn 1 . i.e. ,   1 and is also widely used in many graph - structured models ( Wang et al . , 2020a;Cao et al . , 2021 ) .
Relation Propagation
The various aforementioned types of relations are between types of items , with their H and T being either words or phrases . However , almost all pretrained models take input tokens at the subword level , resulting in a difference in the granularity between the relations and the input tokens . Previous works use an extra step to aggregate multiple subword tokens to obtain a single embedding for each item in the interaction graph , such as mean pooling , attentive pooling , or with BiLSTMs ( Wang et al . , 2020a;Cao et al . , 2021 ) . However , these aggregation methods are detrimental to inheriting the pre - trained knowledge in the pretrained models .
In this work , we adopt the other way : we propagate the relations into the subword level by cre-   
Experiments
In this section , we will show our model 's performance on three common text - to - SQL datasets : Spider ( Yu et al . , 2018 ) , SParC ( Yu et al . , 2019b ) and CoSQL ( Yu et al . , 2019a ) . Besides , we experiment on a more realistic setting of the Spider dataset : Spider - Realistic ( Deng et al . , 2021 ) to test the gen - eralizability of our model . The statistics of these datasets are shown in Table 3 . We also present a set of ablation studies to show the effect of our method on different sized models , as well as the relative contribution of different relations . In addition , we put 2 case studies in Appendix C.
Experiment Setup
Datasets Spider is a large - scale , multi - domain , and cross - database benchmark . SparC and CoSQL are multi - turn versions of Spider on which the dialogue state tracking is required . All test data is hidden to ensure fairness , and we submit our model to the organizer of the challenge for evaluation .     ) , Bridge ( Lin et al . , 2020b , GAZP ( Zhong et al . , 2020 ) , NatSQL ( Gan et al . , 2021 ) , SmBoP ( Rubin and Berant , 2021 ) , LGESQL ( Cao et al . , 2021 ) , S 2 SQL ( Hui et al . , 2022 ) , T5 and PICARD ( Scholak et al . , 2021 ) .
coreference links . In total , 51 types of relations are used ( c.f . Appendix D for a detailed list ) . For dependency parsing , stanza ( Qi et al . , 2020 ) is used .
The batch size we used is 2048 . We use Adafactor ( Shazeer and Stern , 2018 ) as optimizer and the learning rate is 1e-4 . We set " parse with guards " mode for PICARD and beam size is set to 8 . The max tokens to check for PICARD is 2 . Experiments are run on NVIDIA A100 - SXM4 - 80 GB GPUs .
Results on SParC
The results on SParC are shown in   achieves state - of - the - art results on all four evaluation metrics .
Compared with the previous state - of - the - art RAT - SQL - TC + GAP , RASAT + PICARD brings the QEM from 65.7 % to 67.7 % and IEM from 43.2 % to 45.2 % on the test set . In addition , our model can produce executable SQLs ( with values ) , whereas many of the models listed in the table do not provide value predictions .
Among the models that can predict with values , the fine - tuned T5 - 3B model from UNIFIEDSKG ( Xie et al . , 2022 ) is currently the state - of - the - art . We did comparison of QEX / IEX on the dev set since they did not report their performance on the test set . RASAT + PICARD surpasses all previous methods and improves the state - of - art QEX and IEX from 67.3 % and 46.4 % to 73.3 % and 54.0 % , with 6 % and 7.6 % absolute improvements , respectively .
Furthermore , on the official leaderboard of SParc which reports over test set , our proposed RASAT + PICARD brings the IEX from 21.6 % to 52.6 % , achieving 31 % absolute improvements .
Results on CoSQL
Compared with SParC , CoSQL is labeled in a Wizard - of - Oz fashion , forming a more realistic and challenging testbed . Nevertheless , our proposed model could still achieve state - of - the - art results ( Table 4 ) on all four evaluation metrics .
By comparing to the previous state - of - the - art HIE - SQL + GraPPa ( Zheng et al . , 2022 ) and T5 - 3B+PICARD ( Scholak et al . , 2021 ) , RASAT + PI - CARD brings the QEM from 54.6 % to 55.7 % and IEM from 24.6 % to 26.5 % on the test set .
For the same reason as on SParC , we mainly compare QEX / IEX performance on the dev set , and RASAT + PICARD surpasses all models that can predict executable SQLs ( with values ) . Especially for IEX , our model surpasses the previous state - of - the - art from 26.2 % to 39.6 % , with 13.4 % absolute improvement . Moreover , on the official leaderboard of CoSQL which reports over test set , RASAT + PICARD brings the IEX from 8.4 % to 37.4 % , with 29 % absolute improvements .
Results on Spider and Spider - Realistic
The results on the Spider is provided in Table 5 . Our proposed RASAT model achieves state - of - theart performance in EX and competitive results in EM . On the dev set , compared with T5 - 3B , which also does not use the PICARD during beam search , our model 's EX increases from 74.4 % to 76.6 % , achieving 2.2 % absolute improvement . When augmented with PICARD , RASAT+PICARD brings the EX even higher to 80.5 % , with 1.2 % absolute improvement compared to T5 - 3B + PICARD . Furthermore , on the official leaderboard of Spider , our proposed RASAT + PICARD brings the EX from 75.1 % to 75.5 % , achieving new state - of - the - art .
Furthermore , we also evaluate our model on a more challenging Spider variant , Spider - Realistic ( Deng et al . , 2021 ) . It is a evaluation dataset that has modified the user questions by removing or paraphrasing explicit mentions of column names to present a realistic and challenging setting . Our model also achieves a new state - of - the - art performance ( Table 6 ) , which suggests strong ability of our model to generalize to unseen data .
Ablation Study
In this subsection , we conduct a set of ablation studies to examine various aspects of the proposed model . Due to the limited availability of the test sets , all numbers in this subsection are reported on    the dev set .
Effect on SQL difficulty . One might conjecture that the introduced relations are only effective for more difficult , longer SQL query predictions , while for predicting short SQL queries , the original T5 model could handle equally well . Thus , we evaluate our model according to the difficulty of the examples , where the question / SQL pairs in the dev set are categorized into four subsets , i.e. , easy , medium , hard , and extra hard , according to their level of difficulty . In Table 7 we provide a comparison between T5 - 3B + PICARD ( Scholak et al . , 2021 ) and RASAT + PICARD on the EX metric on the four subsets . RASAT + PICARD surpasses T5 - 3B + PICARD across all subsets , validating the effectiveness of the introduced relational structures for all SQL sequences .   .0(+0.5 ) 45.5(-0.2 ) 69.9(+0.7 ) 50.7(+0.3 ) w/o Cf 65.0(+0.5 ) 45.0(-0.7 ) 69.4(+0.2 ) 50.0(-0.4 ) w/o Db 64.1(-0.4 ) 45.3(-0.4 ) 67.9(-1.3 ) 48.5(-1.9 ) w/o SL 64.5 45.5(-0.2 ) 68.8(-0.4 ) 49.4(-1.0 ) w/o SE 63.9(-0.6 ) 44.6(-1.1 ) 68.6(-0.6 ) 48.9(-1.5 ) Table 10 : Ablation study on the relative contribution of different relation types . Experiment are conducted using RASAT(-3B ) on the SParC dataset . " Dp " is short for dependency relation , " Cf " for coreference relation , " SL " for schema linking relation , " SE " for schema encoding relation and " Db " means database content .
Model Size Impact . To test the effectiveness of the introduced relational structures on pretrained models with different sizes , we implant RASAT into four T5 models of different sizes ( T5 - small , T5 - base , T5 - large , T5 - 3B ) and test it on Spider ( Table 8) . Interestingly , for smaller pretrained models , our RASAT model could bring even larger performance gaps between its T5 - 3B counterpart . This suggests that the larger T5 model might have learned some of the relational structures implicitly . We believe this is consistent with the findings on other fine - tuning tasks , where larger pretrained models are more capable of capturing the abundant implicit dependencies in the raw text .
Relation Types . We conducted additional experiments to analyze the relative contribution of different relation types . The experimental results on Spider is shown in Table 9 while result on SParC is shown in Table 10 ( since CoSQL has similar conversational modality with SParC , the experiments are only conducted on SParC ) . We find that both T5 and RASAT models can benefit from leveraging database content . Another important finding is that the performance has increased obviously by adding dependency relationship to RASAT(-small ) on Spider . As for SParC , the database content plays a more important role by looking at EX results ; from what we can see , IEX will decrease by 1.9 % after removing database content from the input .
Conclusion
In this work , we propose RASAT , a Relation - Aware Self - Attention - augmented T5 model for the textto - SQL generation . Compared with previous work , RASAT can introduce various structural relations into the sequential T5 model . Different from the more common approach of fine - tuning the origi - nal model or using prompt tuning , we propose to augment the self - attention modules in the encoder and introduce new parameters to the model while still being able to leverage the pre - trained weights . RASAT had achieved state - of - the - art performances , especially on execution accuracy , in the three most common text - to - SQL benchmarks .
Limitation
Our method consumes plenty of computational resources since we leverage the large T5 - 3B model . We train our models on 8 A100 GPUs ( 80 G ) for around 2 days . Our model truncates the source sequences to 512 , this may lead to information loss when a sample has long input . We find that about 3 % of training data in CoSQL will be affected . We only work with English since it has richer analytical tools and resources than other language .
A Model Size
Compared with the original T5 model , only two embedding matrices are added to the encoder in our model , with 2 × µ × d kv parameters . These embedding matrices are shared in each encoder layer and each head . Here µ = 51 is the total number of relations and d kv is the dimension of the key / value states in self - attention ( 64 in T5 - small / base / large and 128 in T5 - 3B ) . The overall increase of parameters is less than 0.01 % .  
Approach
B Output Comparation between T5 and Tree - based Decoder Model
Here we show the output difference between T5 and most AST - based models . As it shown in Table 12 , most of these models which exploited ASTtree - based decoder ( such as RAT - SQL ( Wang et al . , 2020a ) , LGESQL ( Cao et al . , 2021 ) ) usually use a place holder ( i.e. " value " ) to represent the real value("France " in this example ) . These outputs can not be executed in a real database and they fail to evaluate in the EXecution Accuracy(EX / QEX / IEX ) metric .
C Case Study
In
D Relations Used in Experiment
Table 14 shows all relations used in our experiment while most of these are consistant with RAT - SQL ( Wang et al . , 2020a ) and LGESQL ( Cao et al . , 2021 ) . There are total 51 kinds relation used .    
Acknowledgement
This work was sponsored by the National Natural Science Foundation of China ( NSFC ) grant ( No . 62106143 ) , and Shanghai Pujiang Program ( No . 21PJ1405700 ) . We would like to thank Tao Yu , Hongjin Su , and Yusen Zhang for running evaluations on our submitted models .
