COLD : A Benchmark for Chinese Offensive Language Detection
Offensive language detection is increasingly crucial for maintaining a civilized social media platform and deploying pre - trained language models . However , this task in Chinese is still under exploration due to the scarcity of reliable datasets . To this end , we propose a benchmark -COLD for Chinese offensive language analysis , including a Chinese Offensive Language Dataset -COLDATASET and a baseline detector -COLDETECTOR which is trained on the dataset . We show that the COLD benchmark contributes to Chinese offensive language detection which is challenging for existing resources . We then deploy the COLDETECTOR and conduct detailed analyses on popular Chinese pre - trained language models . We first analyze the offensiveness of existing generative models and show that these models inevitably expose varying degrees of offensive issues . Furthermore , we investigate the factors that influence the offensive generations , and we find that anti - bias contents and keywords referring to certain groups or revealing negative attitudes trigger offensive outputs easier . 1   
Introduction
Offensive language detection task plays an essential role in maintaining social platforms and promoting civilized communication Noever , 2018;Dinan et al . , 2019;Jahan and Oussalah , 2021 ) . With the rise of large - scale language models ( Zhang et al . , 2020;Roller et al . , 2020;Wang et al . , 2020;Zhang et al . , 2021b ) , the safety issues due to offensive generation continue to be exposed ( Gehman et al . , 2020;Bender et al . , 2021;Mi et al . , 2022 ) , attracting widespread attention from researchers and pushing the research boom on this task to new heights ( Sheng et al . , 2021 ; .
To tackle the problem of offensive language detection , a reliable and versatile benchmark is a needed basis to accelerate in - depth research . The datasets , including WTC ( Wulczyn et al . , 2017 ) , OLID ( Zampieri et al . , 2019 ) , BAD   and RealToxicPrompts ( Gehman et al . , 2020 ) , are proposed to study the safety issues from different dimensions and granularities . The publicly available detector , PerspectiveAPI 2 , is widely used for toxicity evaluation and contributes to creating safer environments for online communication ( Han and Tsvetkov , 2020 ; . However , most existing works focus on English . The issue of Chinese offensive language detection has not been well studied due to the lack of labeled datasets and reliable detectors .
In addition , large - scale language models often lean biases in pre - training data and generates offensive or unethical contents ( Sheng et al . , 2021;Zhang et al . , 2021a ) , which substantially hinders the deployment of models in practice . Meanwhile , limited by reliable benchmarks , the offensiveness of Chinese language models has not yet been thoroughly studied . How offensive can Chinese language models be ? What contents influence the triggering of offensive generation ? Diving deeper into these questions will facilitate building more reliable and deployable language models . This paper proposes a benchmark named COLD to tackle the above challenges in Chinese offensive language research . The COLDATASET ( Chinese Offensive Language Dataset ) , contains 37,480 comments with binary offensive labels and covers diverse topics of race , gender , and region . To gain further insights into the data types and characteristics , we annotate the test set at a fine - grained level with four categories : attacking individuals , attacking groups , anti - bias and other non - offensive . We present a baseline detector , COLDETECTOR , for offensive language detection , which adopts pretrained Chinese BERT and is fine - tuned on the proposed dataset and performs satisfactorily compared to other methods using existing resources and technology .
With the proposed benchmark COLD , we evaluate the offensiveness of popular Chinese generation models , including CDialGPT ( Wang et al . , 2020 ) , CPM ( Zhang et al . , 2021b ) , and EVA , to investigate their strengths and weaknesses in terms of safety . Experimental results show that both offensive and non - offensive inputs have the risk of inducing safety issues . Additionally , some types of prompts , including anti - bias contents , certain target group keywords and negative attitude words , can more easily trigger offensive outputs than other inputs . Figure 1   The contributions of this work are threefold :
• We present , to the best of our knowledge , the first publicly available Chinese Offensive Language Dataset : COLDATASET . It contains 37,480 sentences and covers the topics of race , gender and region .
• We provide the a baseline detector , COLDE - TECTOR , together with discussions on existing detection methods . We show the contribution of the proposed benchmark to offensive language detection .
• We evaluate popular open - source generative models and reveal their varying degrees of offensiveness . We also show that the safety issue can be triggered by even non - offensive inputs , such as anti - bias languages .
2 Related Work
Offensive Language Detection
Offensive language , toxic language , and hate speech are highly related terms with blurred boundaries ( Jahan and Oussalah , 2021 ) . In this paper , we do not distinguish them and use them interchangeably . The contents with any form of targeted offense to individuals or groups are considered offensive language . It includes veiled or direct offensive content expressing rudeness , disrespect , insults , threats and profanity based on aspects such as race , religion , sex , or sexual orientation ( Zampieri et al . , 2019;Cambrigdge dictionary ; . Automatic offensive language detection can help detoxify the online communities and safely deploy large - scale language models ( Warner and Hirschberg , 2012;Schmidt and Wiegand , 2017 ) , which is an important task . Abundant efforts are seeking to detect hate speech based on automatic identification , such as topic analysis and keywordbased detection ( Warner and Hirschberg , 2012;MacAvaney et al . , 2019 ) . Due to the development of deep learning and pre - trained models like BERT ( Devlin et al . , 2019 ) , data - driven methods are gradually becoming mainstream for detecting hate speech ( Wulczyn et al . , 2017;Zampieri et al . , 2019 ) . Meanwhile , numerous works have released large - scale resources like Kaggle Challenges on toxicity and bias 3 , which offers significant support for training a strong and robust detector . However , offensive language detection in Chinese greatly lags behind English ( Jahan and Oussalah , 2021 ) . Moreover , due to the specificity of Chinese culture and linguistics , translation - based methods contain inherent defects ( Sohn and Lee , 2019 ) . In this paper , we release an open - source Chinese offensive language dataset and corresponding automatic detection methods , which aims to guide the development of related Chinese community .
Model Safety Analysis
With the emergence of large - scale pre - trained models ( Devlin et al . , 2019;Roller et al . , 2020;Radford et al . , 2019 ) , their security ethics have raised widespread attention . Numerous previous research follow the language model analysis paradigm ( Petroni et al . , 2019 ) and attempt to mine the relational knowledge presented in training data and stored in pre - trained language models . They construct templates like the " fill - in - the - black " cloze statement to analyze different safety issues , including social bias ( Nadeem et al . , 2020;Nangia et al . , 2020;Schick et al . , 2021 ) , toxicity ( Ousidhoum et al . , 2021 ) and morality ( Schramowski et al . , 2021 ) . Another popular approach evaluates model safety by simulating the conversation and evaluating the generated responses in terms of bias and fairness ( Liu et al . , 2019 ) , political prudence ( Bang et al . , 2021 ) , and toxicity agreement ( Baheti et al . , 2021 ) . This method requires proper prompts to probe the safety issues . Gehman et al .
( 2020 ) claims that prompts with varying degrees of toxicity can all trigger toxic outputs . This paper follows the above approach to explore model 's internal knowledge for offensive language detection and thoroughly analyze the offensiveness of generative language models .
Offensiveness in Chinese
Data - driven methods for offensive language detection and safety evaluation are proven effective in practice . However , there remains a dire scarcity of relevant resources in Chinese . In Table 1 , we list , to the best of our knowledge , all relevant existing datasets in Chinese . Yang and Lin ( 2020 ) introduced a dataset for detecting and rephrasing Chinese profanity , which is an extension of their previous version containing 2k sentences ( Su et al . , 2017 ) . Tang et al . ( 2020 ) released a Chinese dataset COLA for categorizing offensive language , but it is not ( yet ) available at the time of writing of this paper .   proposed the first Chinese sexism dataset for identifying gender - related abusive language . More recently , Zhou et al . ( 2022a , b ) presented a Chinese dialog bias dataset and studied the implicit attitudes toward targeted groups in dialogues . To the best of our knowledge , there is no open - source Chinese dataset for offensive language detection . Detoxifying in online communities and language model generations still rely mostly on the blacklisting mechanism , which severely limits the  
Dataset Construction
We present COLDATASET , a Chinese dataset containing 37k sentences and covering the topics of racial , gender , and regional bias . Our data collection process is in line with the suggestions provided by Vidgen and Derczynski ( 2020 ) to achieve standardized and accountable research benchmarks .
Data Source
We investigate offensive language on Chinese social platforms and popular generative language models during the preliminary research stage . We find that name - calling , verbal violence , and other types of offensiveness frequently occurs in discussions of social bias - related topics of racial , gender , and regional issues . Therefore , we study offensiveness of these topics in this paper . We crawl real - world data posted on social media platforms , including Zhihu and Weibo . We analyze the data and find that the proportion of offensive data is sparse because the platform maintains civilized speech . This way , we collect data by two strategies : ( 1 ) keyword querying and ( 2 ) crawling from related sub - topics .
Keyword querying
To narrow down the search scope and increase the density of the target data , we use the keyword querying method . Under each topic , we pre - collect keywords that occur frequently , such as racism , gender bias , and regional discrimination , as well as various descriptive words for target groups , such as black man ( 黑人 ) and ni**r / ni**a ( 黑鬼).The collected keywords are shown in Appendix B.1 . Using them , high - density data relating to each topic can be obtained from the crawled mass data .
Crawling from related sub - topics We search some widely discussed sub - topics in Zhihu and directly crawl data from the follow - up comments . Compared to keyword queries , these data are not limited by pre - collected keywords and can provide a more comprehensive look at user discussions on the topic , resulting in a broader range of content and expressions .
The collected data are post - processed ( refer to Appendix B.2 ) and then mixed as candidate data for further annotation during the model - in - the - loop collection .
Model - in - the - loop Collection
To improve the collection efficiency , we follow the model - in - the - loop setup and train a classifier to discover target data from the candidates . We adopt different labeling strategies for training and test set to improve labeling efficiency .
Training Set Collection For the construction of training set , we semi - automatically label the data based on the model - in - the - loop setup . Firstly , We initialize a classifier by manually labeling 500 samples ( Offen . or Non - Offen . ) as training data . Secondly , we adopt the classifier on a bunch of unlabeled data and predict their offensiveness . Then , the data are ranked by predicted scores and divided into multiple bins for sample checking . We sample around 10 % data from each bin and manually label them with the following strategy : ( 1 ) If the accuracy of the predicted labels is up to 90 % , data in the bin is directly added to the training set ; Otherwise , ( 2 ) the bin is manually relabeled entirely and then added to the training set . By this means , we iteratively update the classifier and training set for 6 rounds . Details can be found at Appendix B.3 .
Test Set Collection
To ensure the reliability of test set annotation , we pick data from different probability intervals and manually annotate them . To give annotators a deeper understanding of our task , we further categorize the data and conduct more fine - grained annotation . The category of Offensive is subdivided into ( 1 ) Attack Individuals and ( 2 ) Attack Groups according to what is targeted / attacked in the content ( Waseem et al . , 2017;Vidgen and Derczynski , 2020 )    Offensive is subdivided into ( 3 ) Anti - Bias and ( 4 ) Other Non - Offensive . ( Definitions of fine - grained categories are detailed in Appendix C )
Human Annotation
We employed 17 native Chinese native workers for labeling . They are evenly distributed by gender ( 9 males and 8 females ) and come from various regions of China . Following the annotation suggestions provided by Vidgen and Derczynski ( 2020 ) , we iteratively develop guidelines and train annotators to ensure the quality of annotation . The remuneration for annotators is 60 CNY per hour . For higher efficiency , auto - labeled training data in each bin is checked and corrected by one annotator . For quality assurance , each sample in test set is assigned to three annotators , and the label with the most votes becomes the final . We compute the Inter - Annotator Agreement of the test set . The Fleiss ' κ ( Fleiss , 1971 ) of 2 - class ( Offen . or Non - Offen . ) is 0.819 ( almost perfect agreement ) and 4 - class ( Attack Individuals / Groups , Anti - Bias , and Other Non - Offen . ) is 0.757 ( substantial agreement ) . More details of Data Collection and Annotation Guidelines are given in Appendix B and C.   If the sample contains keywords under a certain topic , it is considered topic - related . We show the number of sentences under each topic in Table 4 .
Data Analysis
As Table 4 shows , the collected data is relatively evenly distributed among the three topics . About 10 % of the data do not contain topic - related keywords ( None ) . These data are collected by subtopic crawling , making our data distribution closer to actual scenario . Table 4 also reveals the presence of overlap between topics . For example , the overlap of race and gender exists in the sentences discussing African American Women , and the overlap of region and gender exists when discussing rural girls . This overlap makes this dataset more diverse and consistent with the real scenarios .
Offensive Language Detection
The experiments of offensive language detection are designed to verify the following two questions : Can offensive language be detected with existing resources and technology alone ? Does proposed COLDATASET effectively advance the offensive language detection task ?
Experimental Setup
The aim of the offensive language detection task is to assign the label y ( Offen . or Non - Offen . ) to the given text x. To investigate how well offensive language is detected with the proposed dataset and other existing resources , several detection methods are evaluated .
COLDETECTOR We train COLDETECTOR on the proposed COLDATASET for offensive language detection . COLDETECTOR adopts transformerbased architecture and is based on pre - trained model BERT ( Devlin et al . , 2019 ) . We use the version of bert - base - chinese 4 , which has 12 layers and 12 attention heads , as the backbone of our detector . According to the classical fashion , given a text x i , we add a special token [ CLS ] before the text and input to the BERT model . We take out the first hidden - state ( corresponding to [ CLS ] ) in the last layer of BERT as the sentence representation :
E x i = f BERT ( x i )
, and then it is further processed by a Linear layer :
p i = f θ ( E x ) = sigmoid(W • E x i + b )
to generate the final prediction . All the parameters in f BERT ( • ) and f θ ( • ) are trained on COLDATASET with cross - entropy loss function :
L = 1 N i −[y i • log(p i ) + ( 1 − y i ) • log(1 − p i ) ] .
TranslJigsaw Detector ( TJIGDET ) Considering the lack of Chinese dataset , we explore the performance of translated data from English to Chinese ( Shi et al . , 2010;Nozza , 2021;Markov et al . , 2021 ) . We pick a subset of 59k data labeled with toxicity and a variety of identity attributes from the Kaggle competition . ( Jigsaw , 2018 ) . Then they are translated to Chinese with the Baidu General Translation API , and the statistical information is shown in Appendix D.2 . Similar with COLDETECTOR , TranslJigsaw detector is obtained by fine - tuning BERT model on the translated data .
Prompt - based Self - Detection ( PSELFDET ) Internal knowledge stored in the model during pretraining can be used to detect offensive language ( Schick et al . , 2021;Prabhumoye et al . , 2021 ) . We explore the performance of prompt - based selfdetection under the zero - shot setting . Given the text TEXT to evaluate , the input of the model is formulated as : " TEXT " Please judge whether the above content is malicious or offensive ? Answer : [ MASK ] . BERT Model ( Devlin et al . , 2019 ) is used to predict the scores of [ MASK ] token . We take the scores of candidate words of 可 ( yes ) and 否 ( no ) as the self - detection results . An example is given in the Appendix D.1 .
Baidu Text Censor ( BAIDUTC ) As a public API , Baidu Text Censor aims to identify harmful content including pornography , violence , terrorism , political sensitivity , and abuse 5 . Keyword Matching ( KEYMAT ) Keyword matching is frequently used in offensive language filtering for safety maintenance of social platforms . In this work , we use 14k sensitive words released on Github 6 , and the text containing any word in this word list is considered offensive . Random In the random setting , the label of offensive or non - offensive is randomly assigned .
Performance of COLDETECTOR
We present the results on the test set of COL - DATASET in Table 5 . The proposed COLDETEC - TOR obtains the best performance ( 81 % accuracy ) among all the methods and outperforms the second place ( BAIDUTC ) by a large margin ( 18 % absolute improvement in accuracy ) . These comparison results indicate that our benchmark can effectively advance the offensive detection task in online communities .
To further explore the detection performance , we compare the three best - performing methods on recognizing the labeled four subcategories , the results are shown in Table 6 . COLDETECTOR performs well in detecting the sub - categories of Offen . ( 79.51 % and 85.49 % accuracy of Attack individual and Attack group ) , indicating that COLDETECTOR is able to discover offensive samples well compared to the other methods , contributing to higher recall of Offen . ( 85 % ) . The higher accuracy of Other Non - Offen . ( 81.06 % ) indicates that COLD - ETECTOR can well distinguish Offen . from Other Non - Offen . However , the accuracy of Anti - Bias is only 38.32 % , indicating that COLDETECTOR are easily tricked by Anti - Bias data and mis - classify them as Offen . , affecting the precision of recalled Offen . samples ( 72 % ) .
In light of the challenges to classify Anti - Bias data , we further analyzed the samples that successfully fooled COLDETECTOR . We observe that a common form of expression in Anti - Bias contents is acknowledgment followed by denial , e.g. , " Women are often discriminated against in the workplace , but I do n't think it 's right . " . Such expressions can easily deceive the classifier into focusing solely on the first half of the content and ignoring the anti - bias statements following , leading to incorrect predictions .
Though achieving satisfying performance ( 81 % accuracy ) , COLDETECTOR still lags far behind the performance of human experts as well as English toxic detectors ( Hanu and Unitary team , 2020 ) . First , the proposed detector is obtained by simply fine - tuning the BERT model and thus performs slightly worse on discovering the covert offensiveness and anti - bias samples , which depends more on the support from labeled implicit offensive data ( Lees et al . , 2021 ) . Second , our training data is collected semi - automatically . Although sample checking can ensure the accuracy of assigned labels to a certain extent , it will inevitably introduce noise through unchecked data . We believe that if all data in the training set can be manually annotated in the future , there will be improvements in detection performance .
Offensive Language Detection with Existing Resources
We analyze the performances of baselines based on existing resources and find that it is challenging to achieve satisfactory performance on this task only relying on existing resources .
Discussion of Baidu Text Censor
As the results in Table 5 and    Even mixing TranslJigsaw and COLDATASET as training data , the performance has no improvement compared to the only COLDATASET case ( both are 81 % accuracy ) . It shows a significant gap between the translated data and the original Chinese data . Firstly , there are language - specific characteristics due to different cultural backgrounds ( Nozza , 2021 ) . Secondly , there is a noise produced during machine translation process . The dataset proposed in this paper relieves the resource limitations , contributing to Chinese offensive language research .
Discussion of Prompt - based Self - Detection
As shown in the results , the performance of PSELFDET ( 59 % accuracy ) is better than RAN - DOM and KEYMAT , demonstrating the potential of mining the internal knowledge of the language model for detection tasks . However , its contribution is far inferior to supervised learning - based approaches ( 81 % accuracy of COLDETECTOR ) . Previous work show that exploring the appropriate word pair and given prompt can effectively contribute to the performance of self - detection ( Schick et al . , 2021;Prabhumoye et al . , 2021 ) . We compare different ways of prompt construction and present results of the best practice in Table 5 . Detailed exploration of other prompts and word - pairs are included in Appendix D.1 .
Discussion of Keyword Matching
The results in Table 5 show the unsatisfactory performance of keyword matching ( 54 % accuracy ) . Firstly , the coverage and quality of the keyword list are decisive for the detection accuracy . However , with the continuous emergence of new words and the diversification of vocabulary , achieving complete coverage is almost impossible , leading to the low recall of Offen . ( 63 % ) . Secondly , it is inaccurate to filter potentially sensitive samples by matching keywords due to the potential occurrence of those words in both Offen . and Non - Offen . samples . Therefore , even if the text contains a sensitive word , it does not necessarily express toxicity , which leads to low precision ( 44 % ) . Detailed analyses of the occurrence of sensitive words in Offen./Non - Offen . contents are presented in Appendix D.3 .
Evaluation of Generative LMs
With the proposed COLDATASET and COLDE - TECTOR , we evaluate the offensiveness of popular Chinese generative language models . We mainly investigate the following research questions . RQ1 : How offensive are the Chinese generative language models ? RQ2 : What type of prompts can trigger offensive generation ?
Evaluation Metrics
We use the sentences in COLDATASET as input prompts and COLDETECTOR as detector to evaluate the offensiveness of generated content from the evaluated models . We calculate the offensive rate of each model , which is the proportion of offensive generations among the total generations . A lower offensive rate indicates lower offensiveness of the model .
Evaluated Models
We evaluate the following publicly available Chinese generative language models for offensiveness :   • CDialGPT ( Wang et al . , 2020 ) , a Chinese dialog model ( with 104 M parameters ) trained on a cleaned conversational dataset LCCC . We evaluate CDialGPT - Base and and CDialGPT - Large models .
• EVA , the largest Chinese dialogue model ( with 2.8B parameters ) trained on 1.4B Chinese dialogue data .
Evaluation Results
The automatic and human evaluation results of language models are shown in Table 8 and Table 9    Bias inputs show a shockingly high risk of triggering offensiveness . To investigate what contents trigger risk , we conduct further studies of CPM - Generation model by designing template - based inputs . The details are shown in Appendix E.1 . We find that offensive generation is sensitive to the following factors：1 ) Target group keywords . The model is significantly biased against some groups such as feminist and black man , and tends to generate more toxic outputs with these inputs than others such as teenage girls , indicating the inherent bias of the model . 2 ) Negative attitude words . There is a higher offensive ratio when negative attitude words appear in the prompt . For example , there are higher ratios of both disgust and not disgust than not like . Anti - bias contents promote fairness and oppose bias . They are more likely to contain the above - mentioned target group keywords and negative attitude words than other non - offensive inputs , which explains why anti - bias inputs trigger more offensive generations .
Conclusion
We present a new dataset named COLDATASET for Chinese offensive language analysis . We show that the proposed COLDETECTOR trained on our data can effectively detect offensive content . It can also be used as a benchmark for the offensiveness evaluation of language models . We evaluate some popular used models and reveal that they have different degrees of risk in generating offesive contents . Besides , our work shows that , for language models , non - offensive input can also induce safety problems as offensive input , and is worth the same attention . In particular , anti - bias language , which is non - offensive but has hazards comparable to offensive input , is often overlooked in existing work .
We hope this new benchmark can provide the basis for safety research in Chinese and shed light on further studies . We call for more research to expand the scope of offensive and other unsafe language detection . Besides , we believe that , further investigating on what types of input successfully induce unsafe generation , will facilitate the safer deployment of language models .
Ethical Considerations
Our work is a forerunner of a relatively comprehensive benchmark for the study of offensive speech in Chinese . However , our proposal may have the following omissions and shortcomings .
• Our dataset may contain mislabeled data due to the subjectivity of manual annotation . In addition , our training set adopts the semiautomatic annotation strategy and the incomplete data annotation also increases the labeling error . We appeal to data users to optionally re - annotate the semi - automated labeled training data if required .
• We clearly understand that our dataset focuses only on common topics of race , gender , and region , with limited data coverage and a simple annotation schema . We do believe that constructing a greater dataset covering more topics with a more fine - grained taxonomy would contribute to a more robust Chinese offensive detector , deserving more effort in future work .
• We are mindful that our benchmark detector can not detect all types of offensiveness due to the limitation of data coverage and the training techniques of the neural network .
All the data in the proposed benchmark is collected from publicly available social platforms . We strictly follow the protocols for the use of data sources . The contents in our dataset do NOT represent our views or opinions .
Our resources and analyses are intended to help create more harmonious online communities and promote the safer deployment of language models . We acknowledge that it would also be misused in problematic scenarios to create more offensive language or make someone uncomfortable . However , we believe that the proposed benchmark creates more value than risks towards creating more harmonious online communities and building more reliable language models .
Limitations
This paper tackles the issues of Chinese offensive language detection . In the section of Ethical Considerations , we claim that the proposed dataset has potentially mislabeled data and is limited in data coverage , and the detectors fine - tuned on this dataset can not ideally detect all offensive categories . We also discuss the ethical considerations of data collection and data usage . Besides the above - mentioned ethical concerns , we acknowledge the following limitations of our work .
Limitation of contextual information
Our work is mainly devoted to studying the offensiveness at sentence level and therefore contextual information is not included in the proposed COLDATASET . We do believe that offensive expression in contextsensitive scenarios ( e.g. , dialogue ) would be more challenging and require further exploration .
Limitation of baseline models
In the offensive language detection experiments ( Section 4 ) , we take BERT - base - Chinese as the backbone model for the three baseline models ( COLDETECTOR , TJIGDET , and PSELFDET ) to demonstrate the contribution of our dataset . We acknowledge that adopting more backbone models ( e.g. , mBART and xlm - Roberta ) would contribute to a more solid comparison , which is worth exploring in more depth in the future .
A Data Statement
To enable researchers to better understand and use our dataset , we present the data statement following the professional practice for NLP systems developed by Bender and Friedman ( 2018 ) .
Dataset We present a Chinese Offensive Language Dataset ( COLDATASET ) in this paper , which containing 37,480 sentences and covering the topics of racial , gender , and regional bias . There are 32,157 training data , which are semi - automatically labeled with offensiveness labels ( Offen . or Not - Offen . ) . The test set contains 5,323 data , and they are manually labeled with fine - grained categories , including Attack Individual , Attack Groups , Anti - Bias and Other Non - Offen .
Speaker
The data in COLDATASET is collected from social platforms of Zhihu and Weibo and the users who post on these platforms are the Speakers who generate the data .
Annotator We employ 17 native Chinese native workers for labeling , including 9 males and 8 females come from various regions of China , including Henan Province , Beijing , and Northeast part that are widely talked about in region discrimination and other less discussed regions . They are highly trained on our offensive language annotation task .
Curator The authors act as curators , determine the scope of data collection , define the taxonomy , design annotation guidelines , train annotators , and control the quality of annotated data .
NLP System
We design the rule - based methods for data crawling and post - processing . Our annotation task is aided by an iteratively - optimized classifier , which picks out candidate data that need to be further manually annotated .
Stakeholders The researchers engaged in the Chinese offensive language study will be the direct stakeholders , and proposed COLDATASET will effectively support their further research . The managers of social platforms can use this dataset to optimize their detector , contributing to better offensive language filtering . Meanwhile , COLDATASET contributes to language model developers evaluating their models ' offensiveness and facilitating safer deployment .
B Details of Dataset Construction B.1 Keyword query
The collected keywords under each topic are given in Table 10 . They are used to obtain high - density data from the crawled mass data .  
B.2 Post - Processing
For the crawled data , we only keep samples of length between 5 and 200 tokens . We clean the noise and unusual characters , including emojis , URLs , usernames and white space , and then deduplicate the samples so that the collected data is more conducive to the data analysis in this task . After post - processing , the remaining data will be automatically selected and labeled in the model - inthe - loop setup .
B.3 Model - in - the - loop Collection
We adopt the model - in - the - loop setup to discover the target data and optimize the classifier performance . The main flow is shown in Figure 3 . A small amount of data is manually labeled as the initial data ( 500 sentences ) , and the following steps are iteratively performed in each round : Classifier We use BERT model with 12 layers 7 in data collection , which has shown strong   power in natural language processing tasks ( Devlin et al . , 2019 ) . Parameters of COLDETECTOR are optimized by BertAdam optimizer with a linear warmup ( proportion 0.05 ) and a decay schedule . We set the learning rate as 5e-5 , batch size as 64 , and max training epoch as 30 . Early - stopping mechanism is used to avoid overfitting . In each round , the classifier is fine - tuned with updated data from previous rounds . The performance of the classifier is given in Figure 4 , which shows that after the second round , the performance tends to increase steadily as the scale of data increases .
Dataset
The expansion of the dataset is performed in 6 rounds . In the first five rounds , both training and test data are expanded , while only the training data is expanded in the sixth round , as shown in Figure 6 and 7 . It should note that the classifier is not reliable in the beginning , and it is difficult to obtain highconfidence predictions . For example , the accuracy is only 58 % in the first round . It is challenging to learn a good decision surface due to the limitation of the data scale . The prediction probability is concentrated between 0.2 and 0.5 , and the classifier tends to predict all data as Non - Offensive . So , we pick data from this interval for annotation and the returned data will boost the performance of the classifier .
After the third round , the classifier 's performance gradually stabilized , and the accuracy of the predicted high - scoring samples steadily increased . Therefore , we tend to select more data from highscoring samples to improve the efficiency of data collection .
C Annotation Guideline
We provide annotators with annotation guidelines , as shown in Figure 5 . Annotators are first requested to judge whether a given sample is offensive ( Q1 ) . Then , The offensive samples are further divided into Attack individuals or Attack Groups according to the target offended ( Q2 ) , while the Non - offensive samples are divided into Anti - Bias or Other Non - Offensive . For the training set , the annotator is only required to answer the first question ( Q1 ) to check and relabel the automatically annotated samples .
We consider different categories referred to in annotation guidelines as follows . More examples can be found in Figure 5 .
Offensive In this paper , we consider any form of targeted attacks on individuals or groups to be regarded as Offensive language . It includes implicit or direct offensive content that is rude , disrespectful , insulting , threatening , profane , as well as any other toxic content that makes others uncomfortable or provokes further intense offensive feedback . ( Zampieri et al . , 2019;Cambrigdge dictionary ; . Further , based on the target , Offensive is subdivided into Attack Individuals and Attack Groups following Waseem et al . ( 2017 ) .
• Attack Individuals , mainly refers to offensive content directed at individuals , and the target is often referred to by a specific name or a pronoun .
• Attacking groups , mainly refers to offensive content towards generalized groups based on their social identities related to race , religion , gender , etc .
Non - Offensive Non - Offensive is subdivided into Anti - bias and Other Non - Offensive . We make this division because Anti - Bias is beneficial to fight offensive language and maintain a harmonious communication environment , which deserves further study than other non - offensive speech . Figure 6 : Training data collected in each round . The x axis is the probability interval of predicting as " offensive " and the y axis is the sample number .
Figure 7 : Test data collected in each round . The x axis is the probability interval of predicting as " offensive " and the y axis is the sample number .
• Anti - Bias , mainly refers to the expression countering offensiveness , which is usually considered fairness , fact - based contents expressed in a positive or neutral mood .
• Other Non - Offensive , refers to the nonoffensive contents other than anti - bias speech .
D Details of Offensive Detection
D.1 Prompt - based Self - Detection Figure 8 gives an example of self detection . Bertbase - chinese is taken as the model to predict the scores of [ MASK ] token and we take the scores of candidate words of 可(yes ) and 否(no ) as the results of self - detection .
We call for further research to explore the internal knowledge of language models to facilitate this task , and the following tips can be considered . The first is exploring appropriate word pairs . " Yes / No " is often used in English ( Schick et al . , 2021 ) , but the candidate word pairs in Chinese are more varied . We have explored the alternative word pairs in Chinese , and the results are shown in Table 11 , indicating that different word pairs have significant impacts on the results . Second , the detection performance is directly related to the given prompt . Under the few - shot setting , it was found that prompt - based methods can achieve results similar to , even better than , fine - tuned models ( Prabhumoye et al . , 2021 ) . We call for more research to investigate prompt - based self - detection methods to further enhance their ability of offensive language detection .  
D.2 TranslJigsaw Detector
To explore the performance of translated data on this task , we pick the data released for the Kaggle competition Jigsaw Unintended Bias in Toxicity Classification 8 ( Jigsaw , 2018 ) . This dataset contains 1.8 million data annotated by human raters . A subset of the data is labeled with various identity attributes related to sexual , religious , racial , and disability bias . We pick 59k data according to whether it is toxic and bias - topic related , and then translate them from English to Chinese . The statistical information of translated data is shown in Table 12 .
D.3 Keyword Matching
The keyword matching method shows unsatisfactory performance in the offensive detection task on the proposed COLDATASET . The main reason is   that sensitive words may appear in both offensive and non - offensive sentences , as shown in Figure 9 . Some cases are given in Table 13 .
As can be seen from Figure 9 , most of the sensitive words appear in both the offensive and nonoffensive samples , as shown in the region ①. Even some sensitive words with strong offensive frequently appear in anti - bias ( non - offensive ) content , as shown in region ②. Although there are some sensitive words that appear only in the offensive samples , as shown in region ③ , we believe that these keywords will likewise appear in the nonoffensive sample when the scale and coverage of the COLDATASET are large enough . Such results suggest that it is challenging to rely solely on keyword matching for offensive speech detection .
Figure 9 : For the sensitive words used in keyword matching , we analyzed their occurrences in the test set of COLDATASET . The y - axis denotes the number of texts containing the keywords .
E Details of Evaluation E.1 Impact factors of offensive generation
To further explore the impact factors of offensive generations , we collected 103 target group keywords and 9 templates , and constructed a total of 927 prompts . For each prompt , 20 responses are generated by CPM - Generate model ( max length is set to 200 tokens ) . The offensive ratio of each keywords is shown in Figure 11 and that of each prompt is shown in Table 14 . We also analyze the influence of the length of generated contents   and the results in Figure 10 indicate that the longer generations bring greater offensive risk .
Figure 10 : The offensive ratio of generations varies with the max - length of generated contents ( CPM - Generate model ) .  
E.2 Case study
Offensive generations detected by COLDETEC - TOR As shown in Table 15 , we list some examples of offensive generations discovered by proposed COLDETECTOR . These examples show that both Offen . and Non - offen . contents can trigger Offen . generations .
Failure cases of offensive generation detection
The proposed COLDETECTOR effectively discovers offensive languages in generated texts . However , as   pointed out , in dialogue scenarios , the system tends to cater to users and generate responses of toxicity agreement . Our COLDETECTOR focuses on sentence - level offensive language and is insufficient to detect context - sensitive cases . Some failure cases are shown in Table 16 . Further research will be conducted on offensive analysis in dialog scenarios , along with the proposed sentence - level COLD - ETECTOR , to formulate more rigorous strategies to ensure the safe deployment of generative models .