Rethinking the Role of Scale for In - Context Learning : An Interpretability - based Case Study at 66 Billion Scale
Language models have been shown to perform better with an increase in scale on a wide variety of tasks via the in - context learning paradigm . In this paper , we investigate the hypothesis that the ability of a large language model to in - context learn - perform a task is not uniformly spread across all of its underlying components . Using a 66 billion parameter language model ( OPT-66B ) across a diverse set of 14 downstream tasks , we find this is indeed the case : ∼70 % of the attention heads and ∼20 % of the feed forward networks can be removed with minimal decline in task performance . We find substantial overlap in the set of attention heads ( un)important for incontext learning across tasks and number of in - context examples . We also address our hypothesis through a task - agnostic lens , finding that a small set of attention heads in OPT-66B score highly on their ability to perform primitive induction operations associated with incontext learning , namely , prefix matching and copying . These induction heads overlap with task - specific important heads , reinforcing arguments by Olsson et al . ( 2022 ) regarding induction head generality to more sophisticated behaviors associated with in - context learning . Overall , our study provides several insights that indicate large language models may be undertrained for in - context learning and opens up questions on how to pre - train language models to more effectively perform in - context learning .
Introduction
In recent years , large language models ( LLMs ) Rae et al . , 2021;Lieber et al . , 2021;Black et al . , 2022;Zhang et al . , 2022;Chowdhery et al . , 2022;Hoffmann et al . , 2022;Smith et al . , 2022 ) based on the Transformer architecture ( Vaswani et al . , 2017 ) pre - trained using self - supervision on web - scale textual corpora have revolutionized the field of natural language processing ( NLP ) . At larger scales , these models demonstrate remarkable emergent ( Wei et al . , 2022 ) prowess in performing a wide variety of tasks without any form of fine - tuning , via the zero / few - shot incontext learning paradigm . How in - context learning works has been an open question since its advent and recent studies ( Xie et al . , 2021;Garg et al . , 2022;Olsson et al . , 2022;Min et al . , 2022b ) have begun scratching the surface toward better understanding the paradigm . In this paper , we empirically address the following key question : Are all LLM components really needed to perform in - context learning ?
The first way we address the aforementioned question is through the lens of task - specific importance scores and structured pruning ( Li et al . , 2016;Molchanov et al . , 2016;Anwar et al . , 2017 ) of components underlying modern LLMs , which are primarily stacks composed of multiple highdimensional self - attention blocks that form multiheaded attention and densely activated feed forward networks ( FFNs ) . We pick the Open Pretrained Transformer ( OPT ) ( Zhang et al . , 2022 ) model with 66B parameters for our analyses , which yield several surprising observations . We find that important attention heads are primarily clustered in the intermediate layers and important FFNs are primarily in later layers of the model ( § 4 ) . We find that the ability to perform zero / few - shot in - context learning on a variety of 14 NLP datasets / tasks stays nearly intact when up to 70 % ( ∼15.7B parameters in OPT-66B ) of the attention heads are removed ( § 5.1 ) . The attention heads that are ( un)important for in - context learning also seem to overlap across tasks ( § 6.1 ) and shots ( § 6.2 ) , and pruning attention heads based on a " universal " importance order computed using all 14 datasets generalizes to varying degrees on out - of - distribution datasets ( § 6.1.2 ) . These observations indicate that a common taskagnostic subset of the attention heads are responsible for in - context learning . We also find that only up to 20 % of the FFNs ( ∼8.5B parameters ) can be removed with minimal decline in zero / few - shot in - context learning performance ( § 5.2 ) , indicating the importance of FFNs toward in - context learning .
The second way we address the aforementioned question is by quantifying the capacity of all attention heads in OPT-66B to perform a subset of task - agnostic primitive operations associated with in - context learning , namely , prefix matching and copying : explicitly searching for a prior occurrence of the current token in - context and copying over its suffix . Elhage et al . ( 2021 ) and Olsson et al . ( 2022 ) developed a mathematical framework to reverse - engineer a Transformer and also find such heads , termed induction heads , and explored the hypothesis that such heads drive in - context learning with model sizes up to 13B parameters in a mostly task - agnostic fashion . Using this framework , we compute task - agnostic scores for prefix matching and copying for each attention head and find that a small set of heads in OPT-66B have nontrivial scores for both primitives ( § 6.3 ) . Qualitative inspection and quantitative analyses show that these heads overlap ( to varying degrees ) with the ones identified earlier to be important for in - context learning via our set of 14 NLP datasets / tasks , indicating that induction heads are capable of more sophisticated behaviors associated with in - context learning such as latent concept matching but are not the only heads with such capabilities ( § 6.3.1 ) .
Overall , our study provides several insights about in - context learning at massive scale using both task - specific and task - agnostic settings . In a world of ever increasing language model sizes , we believe these insights serve as a strong foundation for researchers and practitioners in language modeling to build and leverage compact language models that can also demonstrate emergent abilities .
Background & Methods
In this section , we establish notation and methods with the Open Pre - trained Transformer ( OPT ) ( Zhang et al . , 2022 ) model used for our study , provide background on in - context learning and the mathematical formulation of induction heads by Olsson et al . ( 2022 ) that we build on , and describe our adaptation of oracle and gradient - based importance score formulations for in - context learning .
Open Pre - trained Transformer ( OPT )
OPT is a suite of language models of varying sizes aimed at serving as open replicas of GPT-3 . The largest openly accessible model from this suite is OPT-66B with 66 billion parameters .
Architecture : Consider a tokenized input sentence to OPT , X ∈ R N ×de , where N is the number of tokens in the sentence and d e is the embedding dimension . The input is processed by multiple decoder layers consisting of multi - headed attention ( MHA ) blocks , layer norm ( LN ) and feed forward networks ( FFN ) , followed by a linear layer to produce logits over the vocabulary . The decoder layers can be formally expressed as follows :
t ( ℓ+1 ) = z ℓ + MHA ℓ ( LN ℓ ( z ℓ ) )
( 1 )
z ( ℓ+1 ) = t ( ℓ+1 ) + FFN ℓ ( t ( ℓ+1 ) ) ( 2 )
where z 1 = X , and ( 1 ) & ( 2 ) are the residual connections corresponding to the MHA and FFN in layer ℓ > = 1 respectively . OPT-66B was pre - trained with a maximum sequence length of 2048 and embedding dimension d e = 9216 .
MHA :
In an MHA block , H attention heads are applied in parallel to the input and their outputs are concatenated . In OPT-66B , there are H = 72 attention heads of dimension d h = 128 in every layer ℓ. An individual attention head h in layer ℓ consists of three learnable matrices , W h k , W h q , W h v ∈ R de×d h , all unique to the head , such that it applies selfattention A h ( . ) on the input , where d h = d e /H. Formally , for input M in layer ℓ :
MHA ℓ ( M ) = [ A 1 ( M ) ; • • • ; A H ( M)]W ℓ o ( 3 ) A h ( M ) = s h ( M)MW h v ( 4 ) s h ( M ) = σ MW h q ( W h k ) T M T √ d h ( 5
)
where σ is the softmax function and W ℓ o ∈ R de×de is a learnable output matrix unique to the MHA block in layer ℓ. To ensure OPT is auto - regressive , the output of s h ( . ) is masked to prevent the dependence of the hidden state of the token i , z ℓ i ∈ R de , on future tokens in indices { i + 1 , . . . , N } .
To remove a head h in layer ℓ in practice , we set A h ( M ) to be the zero matrix in Equation ( 3 ) . This implies that W h k , W h q , W h v can be entirely removed , and the corresponding d h rows in W ℓ o can also be removed . In total , there are 4608 attention heads across 64 layers in OPT-66B that constitute 21.7B of the total 66B parameters .
FFN : Each layer ℓ consists of a feed forward network ( FFN ) parameterized by a high - dimensional projection matrix , W ℓ 1 ∈ R de×d followed by a low - dimensional projection matrix , W ℓ 2 ∈ R d×de where d = 36864 for OPT-66B. Formally , for input M in layer ℓ :
FFN ℓ ( M ) = ReLU(LN ℓ ( M)W ℓ 1 ) W ℓ 2 ( 6 )
where ReLU is the rectified linear unit activation function and LN is the layer norm .
To remove an FFN in layer ℓ in practice , we set FFN ℓ ( M ) to be the zero matrix in Equation ( 6 ) . This implies W ℓ 1 , W ℓ 2 and the layer norm LN ℓ ( . ) for the FFN can be entirely removed . In total , FFNs constitute 43.4B parameters in OPT-66B.
In - Context Learning & Induction Heads
With increasingly larger language models being trained in recent years , a new paradigm of learning termed in - context learning   has become popular . In this paradigm , language models perform tasks by being prompted to generate output text conditioned on a few ( or zero ) incontext training examples that form solved " inputoutput " pairs for the task along with a query input . Figure 1 illustrates the paradigm for the task of identifying the sound that an animal makes . In some cases , tasks can also be accompanied by task descriptions / templates to help prime the language model better , e.g. , zero - shot translating from English to German using the prompt : While these examples involve learning and relying on latent concepts during inference , few - shot in - context learning can additionally involve explicit primitive interactions between the in - context examples . For example , with the prompt : the model may rely on prior in - context translations of the tokens I and like when performing the task for the query input . Olsson et al . ( 2022 ) developed a mathematical framework toward better understanding such mechanics , starting off with a task - agnostic formulation of in - context learning as the ability of a model to better predict tokens later in the context than the tokens earlier . They define a set of task - agnostic primitive operations that reflect the kind of interactions we refer to in the above example , namely , prefix matching and copying . These operations are defined in a simplistic fashion on a repeated sequence of randomly generated tokens : explicitly searching for a prior occurrence of the current token in - context and copying over its suffix . The heads that are capable of performing these operations are termed induction heads . Figure 2 depicts these operations for a repeated sequence of tokens . While these operations are intertwined in practice , the capacity of attention heads to independently perform them is computed with the scoring algorithms described in detail in Appendix A.8 .
Importance Scores
Consider a model M and a dataset D = { X , Y } , where
X = { x 1 , • • • , x L } and Y = { y 1 , • • • , y L } such that x i represents a prompt with few ( or zero ) in - context
training examples along with a query input and y i represents the corresponding target output sequence . We define and compute importance scores for model components using such datasets to quantify their relative contributions to the model 's ability to perform in - context learning .
Oracle
Let P M ( D ) denote a dataset / task - specific performance metric , e.g. , accuracy . Given dataset D , the oracle importance score of a component C in M is computed as follows :
IS C ( D ) = P M ( D ) − P M \C ( D)(7 )
where M \C denotes the resultant model when C is pruned from M. Clearly , if pruning a component leads to poor model performance on the task , it must be important for the task . Similarly , if there is no difference or an improvement in performance upon pruning a component , it must be unimportant . Computing oracle importance scores for K model components requires us to perform O(K ) evaluations for each dataset D.
Gradient - based
Given dataset D , the gradient - based importance score ( Molchanov et al . , 2016;Michel et al . , 2019 ) of an attention head h captures the expected sensitivity of the model to h and is computed as follows :
IS h ( D ) = E ( x , y ) A h ( [ x ; y ] ) T ∂L(y|x ) ∂A h ( [ x ; y])(8 )
where ; is the concatenation operator , ( x , y ) ∼ D such that x is a sequence of T x tokens x 1 : Tx , y is a sequence of T y tokens y 1 : Ty , A h is the output of head h defined in ( 4 ) and the loss term in ( 8) is computed using the auto - regressive decomposition of the log - likelihood :
L(y|x ) = − 1 T y j = Ty j=1
log(p(y j |x , y 1 : j−1 ) ) ( 9 )
These importance scores can be efficiently computed for all heads by simply performing a single forward and backward pass over the model with D.
We also define the aggregated importance score of an attention head on a set of datasets S = { D 1 , • • • , D K } as follows :
IS h ( S ) = E D∼S [ IS h ( D)](10 )
3 Experimental Setup
We conducted our experiments on OPT-66B , which was the largest publicly available dense decoderonly language model at the time of our experiments . We efficiently compute gradient - based importance scores for the 4608 attention heads and oracle importance scores for the 64 feed forward networks ( FFNs ) in OPT-66B. We experiment with a variety of 14 NLP datasets / tasks . For consistency in the evaluation metric , we report accuracy on all tasks . Our choice of datasets and metric is in line with Zhang et al . ( 2022 ) . The datasets include ARC Easy and Challenge   and OpenBookQA ( Mihaylov et al . , 2018 ) for advanced question - answering , Hel - laSwag ( Zellers et al . , 2019 ) , PIQA ( Bisk et al . , 2020 ) and Winogrande ( Sakaguchi et al . , 2021 ) for various forms of commonsense reasoning , and the following datasets from the standard SuperGLUE benchmark ( Wang et al . , 2019 ): BoolQ , CB , COPA , MultiRC , ReCoRD , RTE , WiC , and WSC . For a subset of experiments involving evaluation of outof - distribution generalization , we also use 2 additional datasets : MathQA ( Amini et al . , 2019 ) and LAMBADA ( Paperno et al . , 2016 ) . We use a modified version of the lm - evaluation - harness framework ( Gao et al . , 2021 ) for our experiments . The default framework samples in - context examples at random , which we use without modification .
4 Importance Scores for OPT-66B Figure 3 depicts a heatmap of the head importance scores averaged across all tasks ( as described in § 2.3.2 ) in the 5 - shot setting . Task - averaged heatmaps for the 0 - shot and 1 - shot settings and all task - specific heatmaps are provided in Appendix A.1 . We observe that the important attention heads are primarily clustered in the intermediate layers of OPT-66B in both the task - averaged and taskspecific cases . We also observe overlap in the important heads across the different zero / few - shot settings , confirmed in follow - up analysis in § 6.2 .
Attention Heads
Feed Forward Networks
We compute oracle importance scores ( both taskspecific and averaged across tasks ) for each FFN as described in § 2.3.1 in the zero / few - shot settings . 1 4 7 10 13 16 19 22 25 28 31 34 37 40 43 46 49 52 55 58 61    We observe in the 0/1 - shot settings that the removal of any FFN in the early ( 1 - 30 ) layers of OPT-66B either gives comparable or better performance for a vast majority of tasks . In the 5 - shot setting however , both the early and later layers seem to have important FFNs for most tasks . We also generally observe high variance in FFN importance scores in later layers . We particularly note high variance for WSC and MultiRC , observing that removal of some individual FFNs can lead to absolute accuracy improvements / degradation of up to 20 % ! We leave further investigation into the cause for this variance for future work .
Iterative Pruning
We now assess to what extent we can remove multiple attention heads and/or FFNs with minimal decline in task performance . For each task in each ( 0/1/5 - shot ) in - context learning setting , we create separate rankings of attention heads and FFNs in OPT-66B by separately sorting them in ascending order by their importance scores ( § 4.1 and § 4.2 ) . We then remove unimportant attention heads or FFNs in an iterative fashion using these rankings , 10 % at a time , and re - evaluate task performance after each removal . 1
Removing Attention Heads
Figure 5 depicts the resulting task - specific and task - averaged accuracy trends in the 5 - shot setting . Corresponding 0/1 - shot trends are depicted in Appendix A.3 . We observe that the average accuracy across tasks does not change much up until 1 We do not remove attention heads one at a time and reevaluate given the number of heads and evaluation cost . ∼70 % of the attention heads are removed . A finegrained look at the individual tasks also mostly shows similar trends , with accuracy staying fairly intact until a large proportion of the heads are removed . Some oddities include tasks such as WSC and CB , wherein we see that the 0 - shot accuracy actually increases after removal of 70 % of the heads . Figure 6 depicts the resulting task - specific and task - averaged accuracy trends in the 0 - shot setting . Corresponding 1/5 - shot trends are depicted in Appendix A.4 . We observe that in the 0 - shot setting , the average accuracy across tasks does not change up until ∼20 % of the FFNs are removed . For some tasks such as PIQA , Winogrande and RTE , the accuracy does not change even if 30 % of the FFNs ( ∼13B of the 66B parameters ) are removed . We also observe that the inflection point after which we observe a sharp decline in accuracy changes to 10 % for the few - shot settings . Overall , these observations indicate that FFNs play a critical role toward in - context learning . We now investigate whether the inflection points to in - context learning performance when removing either attention heads or FFNs in an iterative fashion still hold when removing them in tandem . Figure 7 depicts the average 5 - shot accuracy of all tasks on joint iterative removal of attention heads and FFNs . Corresponding 0/1 - shot trends are depicted in Appendix A.5 . We observe that the removal of 70 % of the attention heads ( ∼15.7B parameters ) and 20 % of the FFNs ( ∼8.5B parameters ) leads to a mere 5 % absolute drop in the average 0 - shot accuracy . In the 1 - shot setting , the drop in accuracy is 6 % on removing 70 % of the attention heads and 10 % of the FFNs . In the 5 - shot setting , the drop in accuracy is 4 % on removing 60 % of the attention heads and 20 % of the FFNs . Overall , these new inflection points have deviated by at most 10 % absolute , which may be attributed to the interplay between heads and FFNs .
Removing FFNs
Combined Removal of Heads & FFNs
Detailed Analysis of Attention Heads
In this section , we perform a detailed analysis of the attention heads in OPT-66B , given that in - context learning is auto - regressive in nature and attention heads explicitly encode cross - token interactions . Michel et al . ( 2019 ) found preliminary empirical evidence of the existence of " universally " important attention heads in trained task - specific Transformer and BERT models via evaluating on out - of - domain test sets for machine translation and natural language inference respectively . With similar motivation , we study if the ( un)important attention heads identified in various in - context learning settings for OPT-66B are shared across tasks .
Cross - Task Analysis
Spearman 's Rank Correlation
We assess overlap in ( un)important attention heads across tasks by sorting task - specific head importance scores to get head importance rankings and computing the Spearman 's rank correlation coefficient ( SRCC ) between the rankings for every pair of tasks in the zero - shot and few - shot settings . We also sort the task - aggregate head importance scores to get the aggregate ranking and compute the SRCC against the ranking for every constituent task . All correlations are depicted in Figure 8 for the 5 - shot setting and Appendix A.6 for the 0/1 - shot settings .
In both zero and few - shot settings , we observe statistically significant ( p < 0.01 ) positive correlations in the head importance rankings for every pair of tasks , as well as between every task 's ranking and the aggregate ranking . This indicates that the set of ( un)important attention heads are clustered together across tasks . We also observe seemingly lower magnitude SRCC values between every task and ReCoRD , a long reading comprehension task which requires commonsense reasoning , indicating the amount of head overlap is proportionally lower .  
Generalization Trends
To understand how well head importance rankings generalize across tasks , we study accuracy trends for tasks when pruning using various head importance rankings . We study two sets of tasks .
The first set of tasks we study were used to compute the aggregate ranking : COPA , Winogrande and ReCoRD . For each of these 3 tasks , we consider the impact of pruning based on the self - ranking , aggregate ranking and the rankings from the tasks which share the highest and lowest SRCC with them . Figures 9a , 9b and 9c de - pict the accuracy trends for these 3 tasks in the 5 - shot setting . Corresponding trends in the 0/1shot settings are in Appendix A.7 . In the 0 - shot setting , we observe that the accuracy on all 3 tasks when pruning using the rankings described is almost unaffected up to the 50 % mark . We then observe a sharp decline in accuracy on COPA and Winogrande when the model is pruned to the 70 % mark using the ranking identified via ReCoRD , the task with the lowest SRCC ( 0.13 ) with both COPA and Winogrande . This indicates that even if the rankings vary between ReCoRD and COPA / Winogrande ( as reflected in the low magnitude of the SRCC score ) , the set of attention heads important for 0 - shot learning with ReCoRD are important for COPA / Winogrande too . To further verify this , we calculated and found 71 % and 76 % overlap between the top 30 % important attention heads for ReCoRD - COPA and ReCoRD - Winogrande respectively . Comparing the zero - shot setting against the few - shot settings , we note that the decline / divergence in accuracy beyond the 50 % pruning mark using the ReCoRD ranking is less sharp for COPA and Winogrande in the 1 - shot setting and fades away in the 5 - shot setting , indicating a convergence of important heads across tasks .
The second set of tasks we study are unseen , i.e. , not used to compute the aggregate ranking : MathQA and LAMBADA . For these tasks , we analyze accuracy trends when pruning using the selfranking and aggregate ranking . Figures 9d and   9e depict their accuracy trends in the 5 - shot setting . Corresponding trends in the 0/1 - shot settings are in Appendix A.7 . As expected , we observe that the self - ranking accuracy curves are somewhat higher than the aggregate ranking accuracy curves in general across both tasks . For MathQA , we also observe that the absolute difference in accuracy for both cases is within 1 - 2 % . These indicate that the aggregate rankings generalize well to MathQA but not as much to LAMBADA .
Cross - Shot Analysis
To see if the attention heads identified to be ( un)important for a task are shared across the different zero and few - shot settings , we compute Spearman 's rank correlation coefficient ( SRCC ) between the cross - shot head importance rankings for each task and compute the mean and variance across all 14 tasks . We observe that the mean SRCC is higher for rankings within the few - shot setting ( 0.41 for 1 - shot vs. 5 - shot ) than for rankings across the zero and few - shot settings ( 0.39 for 0 - shot vs. 1 - shot and 0.37 for 0 - shot vs. 5 - shot ) , with low variance ( 0.001 ) and p - value < 0.01 . This matches the intuition that a similar set of heads must be important within the different few - shot settings than across the zero - shot and any of the few - shot settings . However , we also see that the SRCC magnitudes for the latter are not very far off . In totality , these indicate non - trivial overlap in the ( un)important attention heads for tasks across shots .
Induction Heads in OPT-66B
We look for induction heads in OPT-66B by quantifying the capacity of all attention heads to perform prefix matching and copying using random input sequences in a task - agnostic fashion , following the definition and algorithms by Olsson et al . ( 2022 ) discussed in § 2.2 and Appendix A.8 .
Figures 10a and 10b depict the prefix matching and copying score heatmaps respectively for OPT-66B. We observe that a small subset of attention heads in OPT-66B have high prefix matching scores , located in the upper layers ( 31 + ) of the model . On the other hand , there are a relatively larger number of attention heads with high copying scores , although the vast majority of these are also located in the upper layers ( 41 + ) . When seen in conjunction , these observations indicate that there is a sparse set of attention heads that are capable of performing both primitive operations and thus can be deemed plausible induction heads .
Are Induction Heads Important ?
We now study whether induction heads ( which encode the basic in - context learning primitives of explicit prefix matching and copying ) overlap with attention heads identified to be important ( and consequently capable of sophisticated and latent behaviors associated with in - context learning ) for our chosen downstream tasks .
A qualitative comparison of the heatmaps in Figure 10 against the heatmaps referenced in § 4.1 indicates that induction heads do overlap with taskaggregated important attention heads . To better facilitate this comparison , we first formalize the total capacity of a model to perform prefix matching ( or copying ) to be the sum of the respective scores for individual attention heads in the model . We then investigate how much of this capacity is retained when attention heads are pruned in the order of least important heads first . Figure 11    picts this comparison . We observe that much of the total prefix matching score is retained when 20 % of the least important heads are removed , with the slope of decline becoming sharp only after the 40 % pruning mark . This indicates that unimportant heads also have low prefix matching scores . We also observe that the prefix matching scores are generally higher for heads important for few - shot in - context learning than for heads important for zero - shot learning . On the other hand , we observe across the zero - shot and few - shot settings that the total copying score retained on pruning attention heads rapidly and consistently declines , indicating that even unimportant heads have a non - trivial capacity to perform copying . When seen in conjunction , these observations indicate that induction heads in OPT-66B are capable of sophisticated behaviors associated with in - context learning popular downstream NLP tasks and reinforce the induction head generality arguments Olsson et al . ( 2022 ) make in the context of smaller models with stylized and synthetic tasks . We also provide per - task plots in Appendix A.9 which showcase that some tasks rely on induction heads more than other tasks .
Related Work
There has been an interest in effectively leveraging the in - context learning paradigm ( Zhao et al . , 2021;Holtzman et al . , 2021;Min et al . , 2022a;Lu et al . , 2022;Rubin et al . , 2022 ; ever since its introduction by , but there have been relatively fewer studies toward better understanding the paradigm itself . Xie et al . ( 2021 ) cast in - context learning as implicit Bayesian inference where the language model implicitly infers a shared concept among in - context examples when making a prediction . Min et al . ( 2022b ) study the role of the in - context examples themselves , finding that the ground - truth labels are not needed in the examples and that the more important drivers are provision of the label space , the distribution of the input text and the overall format of the sequence . Garg et al . ( 2022 ) showcase that Transformer models trained from scratch can in - context learn the class of linear functions with performance comparable to the optimal least squares estimator even under distribution shifts . Razeghi et al . ( 2022 ) showcase that incontext learning performance is correlated strongly with term frequencies in the pre - training corpora used . Olsson et al . ( 2022 ) consider an alternate framing of in - context learning as the ability of a language model to better predict tokens later in the context than tokens earlier and hypothesize the existence of induction heads that are responsible for in - context learning . Chan et al . ( 2022 ) show that Transformers exhibit striking differences in generalizing from in - context vs. in - weights information .
Several works have also focused on analyzing and interpreting how attention works . Vig and Belinkov ( 2019 ) performed a study on GPT-2 , finding that attention targets different parts of speech at different layer depths and aligns with dependency relations most strongly in the middle layers . Tenney et al . ( 2019 ) showcase that BERT encodes the classical NLP pipeline in an interpretable way across layers . There are works relying on different formulations for head importance , such as layerwise relevance propagation ( Voita et al . , 2019 ) , gradient - based importance and oracle knock - off importance ( Michel et al . , 2019 ) , with small taskspecific trained models and report the existence of specialized heads . Given the recent trend of increasing model scale ( Lieber et al . , 2021;Chowdhery et al . , 2022;Smith et al . , 2022;Rae et al . , 2021 ) toward tuning - free general - purpose language models that exhibit emergent in - context learning abilities , we draw and build on prior work to understand just how much scale is really needed and/or used for in - context learning downstream , an aspect somewhat eclipsed by the focus on the pre - training loss curve in scaling laws ( Hoffmann et al . , 2022 ) . It is also worth noting that some of our empirical observations rely on a simple greedy approach to training - free pruning since our focus was not to optimally prune a language model with respect to performing in - context learning . Li et al . ( 2021 ) show the greedy approach is sub - optimal and produces under - estimates and Halabi et al . ( 2022 ) account for the need to re - compute importance scores after removal of each attention head or FFN by formulating pruning as weakly sub - modular maximization .
Conclusion & Future Work
In this paper , we studied the efficacy of attention heads and feed forward networks ( FFNs ) in a large language model ( OPT-66B ) in performing in - context learning in both task - specific and task - agnostic settings . We observed that while in - context learning may have emerged via selfsupervised pre - training at scale , only a core nucleus of attention heads and FFNs seem to be important for in - context learning across a wide variety of downstream tasks . We observed that a small set of attention heads have the capacity to perform task - agnostic primitive induction operations associated with in - context learning , namely , prefix matching and copying . We also saw that these induction heads overlap with task - specific important attention heads , indicating that induction heads are capable of more sophisticated forms of incontext learning and reinforcing arguments ( Olsson et al . , 2022 ) about their generality . Overall , our incontext learning - centric observations complement recent work ( Hoffmann et al . , 2022 ) in indicating that large language models may be under - trained and motivate several interesting directions for future work . While induction heads are formed naturally during self - supervised pre - training in its current form , we believe it may be possible to increase the number and strength of induction heads formed by defining auxiliary pre - training objectives for primitives like prefix matching and copying . More generally , it may also be prudent to investigate and improve ( pre-)training regimes to increase the number of important model components to in - context learn - perform a wide variety of downstream tasks . Multi - task instruction - tuning likely belongs to this category and it would be interesting to replicate our study with now increasingly accessible instructiontuned model variants ( such as OPT 's instruction meta - learned variant OPT - IML ) .
Limitations
Our work is a comprehensive empirical study of a popular large language model 's capacity to perform in - context learning , relying on both task - specific ( via a wide variety of challenging and practically relevant downstream tasks ) and task - agnostic ( via looking for induction heads ) analyses and connecting the two via correlation / overlap investigations . We do not claim a causal link , i.e. , we do not claim that an attention head that acquires the capacity to be an induction head will become capable of more sophisticated in - context learning associated with our downstream tasks . Making this claim will require a more deeper investigation that is outside the scope of this paper . We also do not fully understand why most attention heads seem to be unimportant for in - context learning and why there is an overlap in ( un)important attention heads across tasks and shots , which warrant further investigation . Other more obvious limitations to our work include our use of only up to 5 in - context examples , random selection of in - context examples for a query input and our choice of all monolingual downstream tasks .
Impact Statement
The findings in our work have significant implications for the design , development and deployment of large language models , known to have a very high carbon footprint as well as training and inference costs . Having identified that a core nucleus of model parameters seem to be important for incontext learning , it may be possible to reduce these models ' carbon footprint and mitigate these costs . Our findings provide architectural transparency and may also be helpful in identifying targeted improvements for downstream tasks as well as for more broader facets such as bias and fairness .
A Appendix
A.1 Head Importance Scores Figure 12 depicts the attention head aggregate importance score heatmaps in the 0 - shot and 1 - shot settings . Figures 14 , 15 and 16 depict the attention head importance scores for each task in the 0 - shot , 1 - shot and 5 - shot settings respectively .
A.2 FFN Importance Scores
Figure 13 depicts the task - specific and taskaveraged importance scores for feed forward networks in the 0 - shot and 1 - shot settings .
A.3 Removing Attention Heads
Figure 17 depicts the task - specific and taskaveraged accuracy trends on iterative removal of attention heads in the order of least important first in the 0 - shot and 1 - shot settings .
A.4 Removing FFNs
Figure 18 depicts the task - specific and taskaveraged accuracy trends on iterative removal of feed forward networks in the order of least important first in the 1 - shot and 5 - shot settings .
A.5 Combined Removal of Heads & FFNs
Figure 19 depicts the average accuracy of all tasks on joint iterative removal of attention heads and feed forward networks in the order of least important first in the 0 - shot and 1 - shot settings .
A.6 Cross - Task Analysis : Spearman 's Rank Correlation
Figure 20 depicts the Spearman 's rank correlation coefficients ( SRCC ) between the attention head importance rankings for every pair of tasks in the 0 - shot and 1 - shot settings . It also depicts the SRCC between the aggregate ranking and the ranking for each constituent task .
A.7 Cross - Task Analysis : Generalization Trends
Figures 21 and 22 depict the cross - task head importance ranking generalization plots in the 0 - shot and 1 - shot settings .
A.8 Details of Prefix Matching and Copying Scores
Algorithms 1 and 2 contain pseudo - code to compute prefix matching and copying scores respectively for each attention head in OPT-66B. We follow the approach described by Olsson et al . ( 2022 ) , but instead of computing scores using 10 sequences with fixed length of 25 , we compute these scores using 100 sequences with varying lengths to account for OPT-66B 's large maximum sequence length . Each FFN is knocked off independently to compute these scores , i.e. , the curves are discrete and not cumulative .
As in Olsson et al . ( 2022 ) , we exclude a small fraction of the most and least common tokens from the model 's vocabulary and randomly sample tokens for these sequences to strip out the effects of pretraining corpora memorization from our scores and inductive behavior analyses .
For prefix matching , the high - level approach is the following : take a random sequence , repeat it 4 times , perform a forward pass and then for each head , compute the attention pattern and take the average of all attention pattern entries attending from a given token back to tokens that succeeded the same token in earlier repeats .
For copying , the high - level approach is the following : take a random sequence , directly feed the sequence through each head and compute the contribution of the head to the output logits , and then measure how much the head increased the logit of the maximally attended to token over increasing the logits of other attendable tokens at each timestep . Unlike Olsson et al . ( 2022 ) , we do not scale the raw scores to be in the range of -1 to 1 .
A.9 Importance of Induction Heads to Each Task
Figures 23 and 24 showcase the importance of induction heads to each task via measuring the percentage of the total prefix matching and copying capacities retained as a function of percentage of attention heads pruned , where heads are pruned based on each task 's head importance ranking for each in - context learning setting ( zero - shot , oneshot and five - shot ) in the order of least important first . A small initial slope of decline implies that unimportant heads also have low prefix matching or copying scores while a steep initial slope of decline implies unimportant heads also have high prefix matching or copying scores . We observe differ - ences in the slopes of decline across different tasks , with tasks like HellaSwag and ReCoRD ( which have high accuracies in Figure 5 ) having smaller initial slopes than a task like OpenBookQA ( which has relatively lower accuracy in Figure 5 ) . When seen in conjunction , these plots not only point to the generality of induction heads to more sophisticated behaviors associated with in - context learning but also indicate that some tasks rely on induction heads more than others .         

The OPT-66B model is open - sourced by Meta under an unrestricted license for academic research .