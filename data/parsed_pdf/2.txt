PVGRU : Generating Diverse and Relevant Dialogue Responses via Pseudo - Variational Mechanism
We investigate response generation for multiturn dialogue in generative chatbots . Existing generative models based on RNNs ( Recurrent Neural Networks ) usually employ the last hidden state to summarize the history , which makes models unable to capture the subtle variability observed in different dialogues and can not distinguish the differences between dialogues that are similar in composition . In this paper , we propose Pseudo - Variational Gated Recurrent Unit ( PVGRU ) . The key novelty of PVGRU is a recurrent summarizing variable that aggregates the accumulated distribution variations of subsequences . We train PVGRU without relying on posterior knowledge , thus avoiding the training - inference inconsistency problem . PVGRU can perceive subtle semantic variability through summarizing variables that are optimized by two objectives we employ for training : distribution consistency and reconstruction . In addition , we build a Pseudo - Variational Hierarchical Dialogue ( PVHD ) model based on PVGRU . Experimental results demonstrate that PVGRU can broadly improve the diversity and relevance of responses on two benchmark datasets .
Introduction
The structure of natural language discourse is complex and highly variable ( Gormley and Tong , 2015;Chung et al . , 2015;Nie et al . , 2022 ) ; this is especially true for dialogue . As shown in Figure 1 , examples ( a ) and ( b ) have the same dialogue history but they end with different responses : utterances u a 6 vs. u b 6 . On the other hand , two dialogues with semantically similar utterances may express quite different context meanings . Because of this variability , there is no simple one - to - one mapping between dialogue context and response . The mapping can be one - to - many -as in Figure 1 , i.e. , different responses to the same dialogue context -as well as many - to - one , i.e. , different context histories requiring the same response . We observe that the distribution of a dialogue context ( e.g. , N a 6 and N b 6 in the figure ) is composed of the distribution of its utterances and the distribution of each utterance is composed of the distribution of its words . A good model of word level and utterance level variation is a key requirement for improving the quality of responses in dialogue .
One line of research ( Henderson et al . , 2014;Shang et al . , 2015;Luo et al . , 2018 ) employs recurrent neural networks ( RNNs ) to model dialogue context . However , standard RNNs are not well suited for dialogue context variability ( Chung et al . , 2015 ) . This is because the internal transition structure of RNNs is deterministic . Thus , RNNs can not effectively model randomness and variability in dialogue context ( Chung et al . , 2015 ) .
Variational mechanism has been shown to be well suited for modeling variability -from both theoretical and practical perspectives ( Kingma and Welling , 2014 ) . Methods based on variational mechanism Gu et al . , 2019;Khan et al . , 2020;Sun et al . , 2021 ) introduce latent variables into RNNs to model one - to - many and many - to - one phenomena in dialogue . Although these approaches achieve promising results , they still have defects . First , these methods face the dilemma that latent variables may vanish because of the posterior collapse issue ( Zhao et al . , 2017(Zhao et al . , , 2018Shi et al . , 2020 ) . Variational mechanism can work only when latent variables with intractable posterior distributions exist ( Kingma and Welling , 2014 ) . Second , the sampled latent variables may not correctly reflect the relationship between dialogue context and response due to the one - tomany and many - to - one phenomena observed in dialogue ( Sun et al . , 2021 ) . Third , posterior knowledge is employed in training while prior knowledge is used in inference ; this causes an inconsistency problem between training and inference ( Shang et al . , 2015;Zhao et al . , 2017;Shi et al . , 2020 ) . a man is making a sandwich while sitting at a dresser he gets up and brings the sandwich to … can you see the guy ? yes , i see one man what is he doing ? he was sitting on a chair and applying jam on a bread .
1 : To tackle these problems , we propose a Pseudo - Variational Gated Recurrent Unit ( PVGRU ) component based on pseudo - variational mechanism . PVGRU introduces a recurrent summarizing variable into the GRU . This summarizing variable can aggregate the accumulated distribution variations of subsequences . The methods based on PVGRU can model the subtle semantic differences between different sequences . First , pseudovariational mechanism adopts the idea of latent variables but does not adopt posterior mechanism ( Serban et al . , 2017;Zhao et al . , 2017;Park et al . , 2018;Sun et al . , 2021 ) . Therefore , PVGRU does not suffer from the posterior collapse issue ( Zhao et al . , 2017(Zhao et al . , , 2018Shi et al . , 2020 ) . Second , we design consistency and reconstruction objectives to optimize the recurrent summarizing variable in PV - GRU ; this ensures that the recurrent variable can reflect the semantics of dialogue context on both the word level and the utterance level . The consistency objective makes the distribution of the incremental information consistent with the corresponding input at each time step . Third , we guarantee the consistency between training and inference since we do not employ posterior knowledge when optimizing the summarizing variable .
Our proposed method avoids the problems caused by variational optimization and can model the diversity problem in dialogue . For instance in Figure 1 , examples ( a ) and ( b ) have the same dialogue history but different responses . N a 6 and N b 6 can learn the distribution differences caused by u a 6 and u b 6 . Simultaneously , semantic reconstruction can enhance the model 's perception of semantic changes , which in turn can strengthen the distribution differences caused by semantic changes . Although the example only shows diversity at the utterance level , similar diversity issues exist at the word level . Therefore , we build a Pseudo - Variational Hierarchical Dialogue model ( PVHD ) based on PVGRU to model both word level and utterance level variation .
To summarize , we make the following contributions :
• We analyze the reasons for one - to - many and many - to - one issues from high variability of dialogue corpus and propose PVGRU with a recurrent summarizing variable to model the variability of dialogue sequences . • We propose to optimize the recurrent summarizing variable using consistency and reconstruction objectives , which guarantees that the summarizing variable can reflect the semantics of the dialogue context and maintain the consistency between training and inference processes . • We propose the PVHD model based on PVGRU .
PVHD significantly outperforms strong baselines with RNN and Transformer architectures on two benchmark datasets . The code including baselines for comparison is available on Github 1 .
RELATED WORK
Dialogue Generation
As an important task in Natural Language Processing , dialogue generation systems aim to generate fluent and informative responses based on the dialogue context ( Ke et al . , 2018 ) . Early dialogue generation models ( Henderson et al . , 2014;Shang et al . , 2015;Luo et al . , 2018 ) usually adopt the simple seq2seq ( Sutskever et al . , 2014 ) framework to model the relationship between dialogue context and response in the manner of machine translation . However , the vanilla seq2seq structure tends to generate dull and generic responses . To generate informative responses , hierarchical structures Song et al . , 2021;Liu et al . , 2022 ) and pre - training techniques ( Radford et al . , 2019;Lewis et al . , 2020;Zhang et al . , 2020 ) are employed to capture the hierarchical dependencies of dialogue context . The results of these methods do not meet expectations ( Wei et al . , 2019 ) .
The main reason is that there are one - to - many and many - to - one relationships between dialogue context and responses . Modeling the multimapping relationship is crucial for improving the quality of the dialog generation . In this paper , we propose a PVGRU component by introducing recurrent summarizing variables into GRU , which can model the varieties of dialogue context .
Variational Mechanism
Variational mechanisms enable efficient working in directed probabilistic models when latent variables with intractable posterior distributions existing ( Kingma and Welling , 2014 ) . Variational mechanisms can learn the latent relationship between dialogue context and responses by introducing latent variables . Most existing methods ( Serban et al . , 2017;Zhao et al . , 2017;Bao et al . , 2020 ) based on variational mechanisms employ prior to approximate true posterior probability . These methods not only encounter the problem of posterior collapse issue but also the problem of inconsistency between training and inference ( Zhao et al . , 2018;Shi et al . , 2020 ) . In this paper , we employ consistency and reconstruction objectives to optimize the summarizing variable different from variational mechanism , which can model the multi - mapping phenomena in dialogues .
Preliminary
In this paper , we employ GRU ( Gated Recurrent Unit ) ( Cho et al . , 2014 ) as the implementation of recurrent neural network ( RNN ) . The reset gate r t is computed by :
rt = σ(Wrxt + Urht−1 ) ( 1 )
where σ is the logistic sigmoid function . x t represents the input at time step t and h t−1 denotes the hidden state at time step t-1 . W r and U r are parameter matrices which are learned . Similarly , the updated gate z t is defined as :
zt = σ(Wzxt + Uzht−1)(2 )
The hidden state h t at the time step t is then computed by :
ht = ztht−1 + ( 1 − zt)ht ( 3 ) ht = ϕ(W xt + U ( rt ⊙ ht−1))(4 )
where ϕ(• ) is the tanh function , W and U are weight matrices which are learned . GRU is considered as a classic implementation of RNN , which is widely employed in generative tasks .   2 ( a ) , PVGRU introduces a recurrent summarizing variable v based on GRU . The recurrent summarizing variable v is obtained based on the incremental information of hidden state h and the previous state of summarizing variable . Specially , the summarizing variable v 0 is initialized with standard Gaussian distribution ( i.e. , Figure 3 ( a ) ) . We assume the input is x t at the time step t , the reset gate r t is rewrited as :
rt = σ(Wrxt + Urht−1 + Vrvt−1)(5 )
where W r , U r and V r are parameter matrices , and v t−1 is the previous summarizing variable state .
Similarly , the update gate z t is computed by :    We introduce a gate g t for summarizing variable factor , which is defined as follows :
zt = σ(Wzxt + Uzht−1 + Vzvt−1 ) ( 6 ) 1- tanh 1- RE sam 1- ℎ ℎ −1 −1 ℎ −1 ℎ ( 0,1 ) 0 1 2 … 1 2 Encoder PVGRU … ( 0,1 ) 0 … 1 2 Context PVGRU … 1 2 … ℎ 1 ℎ 2 ℎ 0 … 1 2 … Decoder ℎ 0 0 1 1 2 +1 ( a ) ( b )
b ) Recurrence ( a ) Init ( 0,1 ) 0 1 ℎ 0 ℎ 1 ( c ) Training ℎ −1 ℎ −1 RE ≈ ℎ −1 ℎ −1 +1
gt = σ(Wgxt + Ught−1 + Vgvt−1)(7 )
The updated gate of summarizing factor controls how much information from the previous variable will carry over to the current summarizing variable state . Under the effect of g t , theh t follows the equation :
ht = ϕ(W xt + U ( rt ⊙ ht−1 ) + V ( gt ⊙ vt−1 ) ) ( 8)
Then the PVGRU updates its hidden state h t using the same recurrence equation as GRU . The summarizing variable v t at the time step t is defined as : ṽ
where φ(• ) represents a nonlinear neural network approximator andṽ t denotes the variations between time t and time t − 1 . The variations across subsequent up to time t is defined as :
vt = gt ⊙ṽt + ( 1 − gt ) ⊙ vt−1 ( 10 )
Figure 3 ( b ) demonstrates the schematic diagram of the recurrent process of PVGRU described above . We can observe that PVGRU does not adopt posterior knowledge , which can guarantee the consistency between training and inference .
Optimization Summarizing Variable
Based on but different from traditional variational mechanism , we design the consistency and reconstruction objectives to optimize the summarizing variable . The consistency objective ensures that the distribution of the information increment of hidden state at each time step is consistent with the input .
For example , we will keep the distribution of information increment h t − h t−1 at time t consistent with x t . The consistency objective function at time step t is denoted as :
ℓ t c = KL(p(xt)||p(ht − ht−1 ) ) = KL(p(xt)||ṽt)(11 )
where KL(• ) represents Kullback - Leibler divergence ( Barz et al . , 2018 ) and p(• ) represents the distribution of the vector . We employ " sam " to represent this process of distribution sampling in Figure 2 ( a ) . The reconstruction optimization objective ensures that the summarizing variable can correctly reflect the semantic of the dialogue context from the whole perspective , which requires PVGRU reconstructs the sequence information from the accumulated distribution variable . The reconstruction loss at time step t is described as :
ℓ t r ( vt , ht ) = 1 2 |f ( vt ) − ht| , |vt − ht| ≤ δ δ|f ( vt ) − ht| − 1 2 δ 2 , |vt − ht| > δ ( 12
)
where f ( • ) stands for decoder using MLP , δ is a hyperparameter and | • | represents the absolute value . We employ " RE " to represent the reconstruction process in Figure 2 ( a ) . Figure 3 ( c ) demonstrates the schematic diagram of optimizing summarizing variable . Reconstruction and consistency objectives ensure that summarizing variable can correctly reflect the semantics of the dialogue context .
Hierarchical Pseudo - variational Model
As shown in Figure 1 , the dialogues contain word - level and sentence - level variability . We follow previous studies ( Serban et al . , , 2017Huang et al . , 2021 )
, u 2 , ... , u m } to utterance vec- tors { h u 1 , h u 2 , ... , h u m } .
At the same time , v t records the accumulated distribution variations of the subsequence at time step t. The context PVGRU takes charge of capturing the utterance - level variabilities . The last hidden state of the context PVGRU represents a summary of the dialogue . The last summarizing variable state of the context PVGRU stands for the distribution of dialogue . The decoder PVGRU takes the last states of context PVGRU and produces a probability distribution over the tokens in the response { y 1 , y 2 , ... , y n } . The generation process of training and inference can be formally described as :
p(y ≤T , v ≤n ) = n t=1 p(yt|y < t , v < t)(13 )
The log - likelihood loss of predicting reponse is formalized as :
ℓ t ll = logp(yt|y < t , v < t)(14 )
The total loss can be written as :
ℓ total = E T t=1 ( ℓ t ll + ℓ t r + ℓ t c ) ( 15 )
5 Experiments
For descriptions of the datasets , please refer to the Appendix A.1 . Please refer to Appendix A.2 for implementation details . In Appendix A.5 we show the ablation results of two objective functions , showing the effectiveness of the objective functions . In order to evaluate the effectiveness of experimental results , we performed a significance test in Appendix A.6 . We can observe that the pvalues of PVHD are less than 0.05 compared with other models . In addition , we present case studies in Appendix A.7 and discuss model limitations in Appendix 7 , respectively .
Baselines
The automatic evaluation metrics is employed to verify the generality of PVGRU , we select the following RNN - based dialogue generation models as baselines : seq2seq : sequence - to - sequence model GRU - based with attention mechanisms ( Bahdanau et al . , 2015 ) . HRED : hierarchical recurrent encoder - decoder on recurrent neural network ( Serban et al . , 2016 ) for dialogue generation . HRAN : hierarchical recurrent neural network dialogue generation model based on attentiom mechanism ( Xing et al . , 2018 ) . CSG : hierarchical recurrent neural network model using static attention for contextsensitive generation of dialogue responses ( Zhang et al . , 2018 ) .
To evaluate the performance of the PVHD , we choose dialogue generation model based on variational mechanism as baselines : HVRNN : VRNN ( Variational Recurrent Neural Network ) ( Chung et al . , 2015 ) is a recurrent version of the VAE . We combine VRNN ( Chung et al . , 2015 ) and HRED   to construct the HVRNN . CVAE : hierarchical dialogue generation model based on conditional variational autoencoders ( Zhao et al . , 2017 ) . We implement CVAE with bag - of - word loss and KL annealing technique . VAD : hierarchical dialogue generation model introducing a series of latent variables ( Du et al . , 2018 ) . VHCR : hierarchical dialogue generation model using global and local latent variables ( Park et al . , 2018 ) . SepaCVAE : self - separated conditional variational autoencoder introducing group information to regularize the latent variables ( Sun et al . , 2021 ) . SVT : sequential variational transformer augmenting deocder with a sequence of fine - grained latent variables ( Lin et al . , 2020 ) . GVT : global variational transformer modeling the discourselevel diversity with a global latent variable ( Lin et al . , 2020 et al . , 2020 ) . Different from original implementation , we do not use knowledge on the DSTC7 - AVSD . DialogVED : a pre - trained latent variable encoder - decoder model for dialog response generation ( Chen et al . , 2022 ) . We initialize the model with the large version of DialogVED .
Automatic & Human Evaluation
Please refer to Appendix A.3 and Appendix A.4 for details of automatic evaluation metrics . Some differences from previous works are emphasized here . We employ improved versions of BLEU and ROUGE - L , which can better correlate n - gram overlap with human judgment by weighting the relevant n - gram compared with original BLEU ( Chen and Cherry , 2014 ) . Although using the improved versions of BLEU and ROUGE - L will result in lower literal values on the corresponding metrics , this does not affect the fairness of the comparison . We adopt the implementation of distinct-1/2 metrics following previous study ( Bahuleyan et al . , 2018 ) .
The source code for the evaluation method can be found on the anonymous GitHub .
Generality of PVGRU
Table 1 reports the automatic evaluation performance comparison of the models using GRU and PVGRU . We can observe that the performance of the models based on PVGRU is higher than that based on GRU . Specifically , on DailyDialog dataset , the average performance of models based on PVGRU is 0.63 % to 16.35 % higher on PPL , 1.40 % to 1.92 % higher on BLEU-1 , 1.08 % to 2.02 % higher on Rouge - L , 1.10 % to 2.33 % higher on Dist-1 and 1.36 % to 1.62 % higher on average embedding compared with models based on GRU . On DSTC7 - AVSD dataset , the performance of models based on PVGRU is 0.45 % to 5.47 % higher on PPL , 1.14 % to 2.57 % higher on BLEU-1 , 1.38 % to 2.7 % higher on Rouge - L , 0.69 % to 2.06 % higher on Dist-1 and 0.69 % to 2.69 % higher on average embedding compared with models based on GRU .    GRU introduces a recurrent summarizing variable , which records the accumulated distribution variations of sequences . The recurrent summarizing variable brings randomness to the internal transition structure of PVGRU , which makes model perceive the subtle semantic variability .
( a ) ( b ) ( c ) ( d )
Automatic Evaluation Results & Analysis
Table 2 reports the results of automatic evaluation of PVHD and other baselines on DailyDialog and DSTC7 - AVSD datasets . Compared to RNNbased baselines based on variational mechanism , PVHD enjoys an advantage in performance . On DailyDialog datasets , the performance of PVHD is 1.16 % higher on BLEU-1 , 0.45 % higher on Rouge - L , 1.01 % higher on Dist-1 and 2.22 % higher on average embedding compared to HVRNN . As compared to the classic variational mechanism models CVAE , VAD and VHCR , PVHD has a advantage of 0.02 % to 22.75 % on PPL , 1.87 % to 6.88 % higher on BLEU-1 , 1.48 % to 3.25 % higher on Dist-1 , 0.43 % to 13.37 % higher on Dist-2 and 0.80 % to 2.76 % higher on average embedding . We can observe similar results on DSTC7 - AVSD . PVHD enjoys the advantage of 1.3 % to 18.22 % on PPL , 3.00 % to 3.40 % higher on BLEU-1 , 0.54 % to 1.19 % higher on Dist-1 , 1.31 % to 5.76 % higher on Dist-2 and 0.11 % to 2.22 % higher on average embedding compared with these classic variational mechanism models . The main reason for the unimpressive performance of RNN - based baselines is that these models suffer from latent variables vanishing observed in experiments . As shown in Figure 4 , the Kullback - Leibler term of these models losses close to zero means that variational posterior distribution closely matches the prior for a subset of latent variables , indicating that failure of the variational mechanism ( Lucas et al . , 2019 ) . The performance of SepaCVAE is unimpressive . In fact , the performance of SepaCVAE depends on the quality of context grouping ( referring to dialogue augmentation in original paper ( Sun et al . , 2021 ) ) . Sepa - CVAE will degenerate to CVAE model if context grouping fails to work well , and even which will introduce wrong grouping noise information result - ing in degrade performance . As shown in Figure 4 , the Kullback - Leibler term of SepaCVAE losses is at a high level , which demonstrates that the prior for a subset of latent variables can not approximate variational posterior distribution .
Compared with Transformer - based baselines , PVHD still enjoys an advantage on most metrics , especially the distinct metric . GVT introduces latent variables between the whole dialogue history and response , which faces the problem of latent variables vanishing . SVT introduces a sequence of latent variables into the decoder to model the diversity of responses . But it is debatable whether latent variables will destroy the fragile sequence perception ability of the transformer , which will greatly reduce the quality of the responses . Training the transformer from scratch instead of using a pretrained model is another reason for the inferior performance of SVT and GVT . Compared to DialogVED and PLATO , PVHD achieves the best performance on most metrics . The main reason is that pseudo - variational approaches do not depend on posteriors distribution avoiding optimization problems and the recurrent summarizing variable can model the diversity of sequences . Overall , PVHD has the most obvious advantages in diversity , which demonstrates the effectiveness of the recurrent summarizing variable . Another reason is that Transformer - based baselines including SVT , GVT , PLATO and DialogVED connect all the dialogue history utterances into a consecutive sequence . They can only model the diversity between entire dialogue histories and responses . Coarse - grained modeling is the reason for poor model performance .
Although transformers are popular for generation task , our research is still meritorious . First , transformer models usually require pre - training on large - scale corpus while RNN - based models usually do not have such limitations . It is debatable whether transformer models training from scratch under conditions where pre - training language models are unavaliable can achieve the desired performance if downstream task does not have enough corpus . Second , the parameter amount of the RNNbased model is usually smaller than that of the transformer - based model . The parameter sizes of PVHD on the DailyDialog and DSTC7 - AVSD are 29 M and 21 M , respectively . The number of parameters for PLATO and DialogVED is 132 M and 1143 M on two datasets , respectively . Compared to PLATO and DialogVED , the average number of parameters of PVHD is 5.28x and 45.72x smaller , respectively .
Human Evaluation Results & Analysis
We conduct human evaluation to further confirm the effectiveness of the PVHD . To evaluate the consistency of the results assessed by annotators , we employ Pearson 's correlation coefficient ( Sedgwick , 2012 ) . This coefficient is 0.35 on diversity , 0.65 on relevance , and 0.75 on fluency , with p < 0.0001 and below 0.001 , which demonstrates high correlation and agreement . The results of the human evaluation are shown in Table 3 . Compared to RNN - based baselines , PVHD has a significant advantage in relevance and diversity . Specifically , PVHD enjoys the advantage of 11.40 % on diversity and 16.00 % on relevance compared to SepaCVAE on DailyDialog . On DSTC7 - AVSD , PVHD has a advantage of 10.50 % on diversity and 73.00 % on relevance compared to SepaCVAE . Compared to transformer - based baselines , although PVHD is sub - optimal in some metrics , it enjoys the advantage in most metrics , especially diversity . In terms of fluency , PVHD is only 1.00 % lower than HVRNN and is much better that other baselines on DailyDialog . However , the fluency of PVHD is 26.50 % lower compared with HVRNN and 8.00 % lower compared with VHCR on DSTC7 - AVSD . We argue that introducing a recurrent summary variable in the decoder increases the randomness of word generation , which will promote the diversity of the responses with a side effect of fluency reduction .
Effectiveness of Summarizing Variables
We further analyze the effectiveness of PVHD on summarizing variables . Figure 5 demonstrates the visualization of word - level and utterance - level summarizing variables on test set of DailyDialog and DSTC7 - AVSD datasets . We can observe that both datasets exhibit high variability characteristic on word - level and utterance - level . Specifically , the summarizing variables on word - level show obvious categorical features , which indicates that a subsequence may have multiple suitable candidate words . Moreover , the summarizing variables on utterancelevel also exhibit impressive categorical features , which confirms that there is a one - to - many issue in the dialogue . These phenomena make dialogue generation different from machine translation where unique semantic mapping exists between source and target language .
Conclusion
We analyze the reasons for one - to - many and manyto - one issues from high variability of dialogue . We build PVHD based on proposed PVGRU component to model the word - level and utterance - level variation in dialogue for generating relevant and diverse responses . The results demonstrate that PVHD even outperforms pre - trained language models on diversity metrics .
Limitations
Although our work can effectively model the variability issue in dialogue , we acknowledge some limitations of our study . Firstly , our study can work well on the approaches based on RNN , but can not be employed to sequence models based on Transformer , which limits the generality of our approach . The reasons we analyze are as follows .
Transformer is not a good architecture for finegrained diversity . The diversity of dialogue includes three granularities of discourse level , utterance level and word level . To model diversity , models will be required to utilize the representation at time t and the relationship between the representation at time t and time t+1 to determine the representation at time t+1 . Relationships are computed step by step . If we only consider discourse - level diversity , our approach and variational mechanisms are easily transferable to Transformer architectures . Because we can use the Transformer model to encode the entire historical dialogue sequence . Latent variables or summarizing variables only exist between the entire historical sequence and the responses . This will not destroy the parallel structure of the Transformer . if we employ a Transformer to model diversity at the utterance and word granularity , this will seriously damage the parallelism of the Transformer .
There are great limitations in the variational transformer models . The transformer and variational thinking is not a good match , which leads to less relevant research . The Transformer baselines we compared in the manuscript ( i.e. SVT , GVT , PLATO and DialogVED ) cover most of the current transformer models that combine variations . Although SVT , GVT , PLATO and DialogVED incorporate variational ideas , these models connect all the dialogue history utterances into a consecutive sequence . It is inadvisable to model the finegrained diversity relationship in a parallel structure .
Secondly , although our methods can improve the diversity and relevence of responses , there are still gaps in fluency compared with other baselines .   Gold : honey , cross my heart , i'v never looked at another woman since the first day i set my eyes on you ! believe me , that 's the truth . SVT : Honey , cross my heart , I'v never looked at another woman since the truth . GVT : I'v never looked at another woman since the first day UNK . SepaCVAE : i know how you really well . DialogVED : i 'm sorry , but i ca n't let you do that . PVHD : actually , i'v not looked at another woman at all because the first day i set my eyes on you ! believe me .
Context : he turns his cellphone light on to help him see to screw the knob back into the dresser drawer .
does he end up fixing it correctly ? yes , he screws the knob back on correctly . does he take anything our of the drawer ? no he does not open the drawer , only fixes it . Gold : he interacts with the computers after fixing the knob he simply stands up and begins to leave the room . SVT : he appears to be carrying something . GVT : no , he does not go to the computer . SepaCVAE : no , he does not move from his computer . DialogVED : no , he does not touch the computer . PVHD : no , he does not interact with the computer at all .  
A.2 Implementation Details
We implement our model and baselines using Tensorflow 2 and train baselines on a server with RTX 8000 GPU ( 48 G ) . The dimension of word embeddings is set 512 . We consider at most 10 turns of dialogue context and 50 words for each utterance . The encoder adopts bidirectional structure and the decoder uses unidirectional structure .
A.3 Automatic Evaluation Metrics
We employ both automatic and human evaluations to assess the performance of compared methods . The automatic evaluation mainly includes the following metrics : BLEU ( Yang et al . , 2018 ) evaluates the n - gram co - occurrence between generated response and target response . ROUGE - L ( Yang et al . , 2018 ) evaluates the overlap of the longest common subsequences between generated response and the target response . Distinct-1/2 ( Li et al . , 2016 ) measures the generated response diversity , which is defined as the number of distinct uni - grams / bi - grams divided by the total amount of generated words . PPL ( Perplexity ) evaluates the confidence of the generated response . The lower PPL score , the higher confidence for generating responses . Embedding - based metrics ( Average , Exterma and Greedy ) measure the semantic relevance between generated response and target response ( Liu et al . , 2016;Sedoc et al . , 2019;Xu et al . , 2018b ) .
A.4 Human Evaluation
Following the work of ( Sun et al . , 2021;Li et al . , 2017a;Xu et al . , 2018a ) , we divide six crowdsourced graduate students into two groups to evaluate the quality of generated responses for 100 randomly sampled input contexts , respectively . We request annotators to rank the generated responses with respect to three aspects : fluency , diversity , and relevance . Fluency measures whether the generated responses are smooth or grammatically correct . Diversity evaluates whether the generated responses are informative , rather than generic and repeated information . Relevance evaluates whether the generated responses are relevant to the dialogue context . The average scores of the two groups is taken as the final score .
A.5 Ablation Study
We conduct ablation experiments on the proposed loss modules . Table 4 reports the results of the ablation experiments of PVHD on DailyDialog and DSTC7 - AVSD . -RE removes the reconstruction loss . -CO removes the consistency loss . The results demonstrate that our optimization objectives are effective . We can observe that the reconstruction loss can improve the BLEU-1/2 and Rouge - L. The consistency loss can improve Dist-1/2 metrics at the the expense of BLEU-1/2 and Rouge - L metrics . We believe that the consistency loss can ensure the consistency between the incremental information and the input at each time step . There may be multiple candidate tokens following the same distribution , which increases the diversity of generated responses . The reconstruction loss can make the summarizing variable recording the accumulated distribution of subsequence reflect the semantic information of dialogue context correctly , which will reduce the randomness of the generation process by limiting candidates that do not conform to sequence semantics .
A.6 Significance Testing
To evaluate the reliability of the PVHD results , we performe multiple significance tests . Table 6 ( in Appendix A ) reports the results of the significance test for automatic evaluation . We can observe that the p - values of PVHD are less than 0.05 compared with other models . Although the results of PVHD is not optimal in some metrics , the significance test demonstrates that results of PVHD are statistically significantly different from other models . In other words , the performance advantage of PVHD is statistically reliable and not an accident caused by random factors .
3307
A.7 Case Study
To further dissect the quality of PVHD , several examples of generated responses are provided in Table 5 . Although DialogVED , SVT , GVT can generate relevant responses , PVHD can produce higher quality responses in comparison . Specifically , for the first example , the responses generated by other models are contextual except for Sepa - CVAE . The response generated by DialogVED is more diffuse than gold response , but response generated by PVHD is more informative and possesses a different sentence pattern and different wording than gold response to some extent . We can observe the similar case for the second example . We believe that this is mainly due to the capture of variability of corpus by summarizing variable , which enables the model to identify similar sentence patterns and words , and generate diverse responses .
Acknowledgement
We would like to thank the reviewers for their constructive comments . The project is supported by the National Natural Science Foundation of China ( 62272092,62172086 ) and the European Research Council ( grant # 740516 ) . The project is also supported by the Fundamental Research Funds for the Central Universities of China under Grant No . N2116008 and China Scholarship Council .