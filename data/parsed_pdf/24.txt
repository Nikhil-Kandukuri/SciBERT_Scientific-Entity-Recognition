VGNMN : Video - grounded Neural Module Networks for Video - Grounded Dialogue Systems
Neural module networks ( NMN ) have achieved success in image - grounded tasks such as Visual Question Answering ( VQA ) on synthetic images . However , very limited work on NMN has been studied in the video - grounded dialogue tasks . These tasks extend the complexity of traditional visual tasks with the additional visual temporal variance and language cross - turn dependencies . Motivated by recent NMN approaches on image - grounded tasks , we introduce Videogrounded Neural Module Network ( VGNMN ) to model the information retrieval process in video - grounded language tasks as a pipeline of neural modules . VGNMN first decomposes all language components in dialogues to explicitly resolve any entity references and detect corresponding action - based inputs from the question . The detected entities and actions are used as parameters to instantiate neural module networks and extract visual cues from the video . Our experiments show that VGNMN can achieve promising performance on a challenging video - grounded dialogue benchmark as well as a video QA benchmark .
Introduction
Vision - language tasks have been studied to build intelligent systems that can perceive information from multiple modalities , such as images , videos , and text . Extended from image - grounded tasks , e.g. ( Antol et al . , 2015 ) , recently Jang et al . ( 2017 ) ; Lei et al . ( 2018 ) propose to use video as the grounding features . This modification poses a significant challenge to previous image - based models with the additional temporal variance through video frames .
Recently   further develop videogrounded language research into the dialogue domain . In the proposed task , video - grounded dialogues , the dialogue agent is required to answer questions about a video over multiple dialogue turns . Using Figure 1 as an example , to answer Video Caption : a boy and a man walk to the room .
The boy carries his backpack while the man … Visual : ... Audio : ...
Question
Dialogue Understanding Video Understanding
Figure 1 : A sample video - grounded dialogue with a demonstration of a reasoning process questions correctly , a dialogue agent has to resolve references in dialogue context , e.g. " he " and " it " , and identify the original entity , e.g. " a boy " and " a backpack " . Besides , the agent also needs to identify the actions of these entities , e.g. " carrying a backpack " to retrieve information from the video .
Current state - of - the - art approaches to videogrounded dialogue tasks , e.g. ( Le et al . , 2019b;Fan et al . , 2019 ) have achieved remarkable performance through the use of deep neural networks to retrieve grounding video signals based on language inputs . However , these approaches often assume the reasoning structure , including resolving references of entities and detecting the corresponding actions to retrieve visual cues , is implicitly learned . An explicit reasoning structure becomes more beneficial as the tasks complicate in two scenarios : video with complex spatial and temporal dynamics , and language inputs with sophisticated semantic dependencies , e.g. questions positioned in a dialogue context . These scenarios often challenge researchers to interpret model hidden layers , identify errors , and assess model reasoning capability .
Similar challenges have been observed in imagegrounded tasks in which deep neural networks exhibit shallow understanding capability as they exploit superficial visual cues ( Agrawal et al . , 2016;Goyal et al . , 2017;Feng et al . , 2018;Serrano and Smith , 2019 ) . Andreas et al . ( 2016b ) propose neural module networks ( NMNs ) by decomposing a question into sub - sequences called program and assembling a network of neural operations . Motivated by this line of research , we propose a new approach , VGNMN , to video - grounded language tasks . Our approach benefits from integrating neural networks with a compositional reasoning structure to exploit low - level information signals in video . An example of the reasoning structure can be seen on the right side of Figure 1 .
Video - grounded Neural Module Network ( VGNMN ) tackles video understanding through action and entity - paramterized NMNs to retrieve video features . We first decompose question into a set of entities and extract video features related to these entities . VGNMN then extracts the temporal steps by focusing on relevant actions that are associated with these entities . VGNMN is analogous to how human processes information by gradually retrieving signals from input modalities using a set of discrete subjects and their actions .
To tackle dialogue understanding , VGNMN is trained to resolve any co - reference in language inputs , e.g. questions in a dialogue context , to identify the unique entities in each dialogue . Previous approaches to video - grounded dialogues often obtain question global representations in relation to dialogue context . These approaches might be suitable to represent general semantics in open - domain dialogues ( Serban et al . , 2016 ) . However , they are not ideal to detect fine - grained information in a video - grounded dialogue which frequently entails dependencies between questions and past dialogue turns in the form of entity references .
In summary , our contributions include :
• VGNMN , a neural module network - based approach for video - grounded dialogues .
• The approach includes a modularized system that creates a reasoning pipeline parameterized by entity and action - based representations from both dialogue and video contexts .
• Our experiments are conducted on the challenging benchmark for video - grounded dialogues , Audio - visual Scene - Aware Dialogues ( AVSD )   as well as TGIF - QA ( Jang et al . , 2017 ) for video QA task .
• Our results indicate strong performance of VGNMN as well as improved model inter - pretability and robustness to difficult scenarios of dialogues , videos , and question structures .
2 Related Work
Video - Language Understanding
The research of video - language understanding aims to develop a model 's joint understanding capability of language , video , and their interactions . Jang et al . ( 2017 ) ; Gao et al . ( 2018 ) ;   propose to learn attention guided by question global representation to retrieve spatial - level and temporal - level visual features . Li et al . ( 2019 ) ; Fan et al . ( 2019 ) ; Jiang and Han ( 2020 ) model interaction between all pairs of question tokenlevel representations and temporal - level features of the input video through similarity matrix , memory networks , and graph networks respectively . ; Le et al . ( 2019cLe et al . ( , 2020b ; Lei et al . ( 2020 ) ; Huang et al . ( 2020 ) extends the previous approach by dividing a video into equal segments , sub - sampling video frames , or considering objectlevel representations of input video . We propose to replace token - level and global question representations with question representations composed of specific entities and actions . Recently , we have witnessed emerging techniques in video - language systems that exploit deep transformer - based architectures such as BERT ( Devlin et al . , 2019 ) for pretraining multimodal representations ( Li et al . , 2020a;Yang et al . , 2020;Kim et al . , 2021;Tang et al . , 2021;Zellers et al . , 2021 ) in very large - scale videolanguage datasets . While these systems can achieve impressive performance , they are not straightforward to apply in domains with limited data such as video - grounded dialogues . Moreover , as we shown in our qualitative examples , our approach facilitates better interpretability through the output of decoded functional programs .
Video - grounded Dialogues
Extended from video QA , video - grounded dialogue is an emerging task that combines dialogue response generation and video - language understanding research . This task entails a novel requirement for models to learn dialogue semantics and decode entity co - references in questions . Nguyen et al . ( 2018 ) ; ; ; Sanabria et al . ( 2019 ) ; Le et al . ( 2019a , b ) extend traditional QA models by adding dialogue history neural encoders . Kumar et al . ( 2019 ) en - hances dialogue features with topic - level representations to express the general topic in each dialogue . Schwartz et al . ( 2019 ) treats each dialogue turn as an independent sequence and allows interaction between questions and each dialogue turn . Le et al . ( 2019b ) encodes dialogue history as a sequence with embedding and positional representations . Different from prior work , we dissect the question sequence and explicitly detect and decode any entities and their references . Our approach also enables insights on how models extract deductive bias from dialogues to extract video information .
Neural Module Network
Neural Module Network ( NMN ) ( Andreas et al . , 2016b , a ) is introduced to address visual QA by decomposing questions into linguistic sub - structures , known as programs , to instantiate a network of neural modules . NMN models have achieved success in synthetic image domains where a multi - step reasoning process is required ( Johnson et al . , 2017b;Hu et al . , 2018;Han et al . , 2019 ) . Yi et al . ( 2018 ) ; Han et al . ( 2019 ) ;   improve NMN models by decoupling visual - language understanding and visual concept learning . Our work is related to the recent work ( Kottur et al . , 2018;Jiang and Bansal , 2019;Gupta et al . , 2020 ) that extended NMNs to image reasoning in dialogues and reading comprehension reasoning . Our approach follows the previous approaches that learn to generate program structure and require no parser at evaluation time . Compared to prior work , we use NMN to learn dependencies between the composition in language inputs and the spatio - temporal dynamics in videos . Specifically , we propose to construct a reasoning structure from text , from which detected entities are used to extract visual information in the spatial space and detected actions are used to find visual information in the temporal space .
Method
In this section , we present the design of our model . An overview of the model can be seen in Figure 2 .
Task Definition
The input to the model consists of a dialogue D which is grounded on a video V. The input components include the question of current dialogue turn Q , dialogue history H , and the features of the input video , including visual and audio input . The output is a dialogue response , denoted as R. Each text input component is a sequence of words w 1 , ... , w m ∈ V in , the input vocabulary . Similarly , the output response R is a sequence of tokens w 1 , ... , w n ∈ V out , the output vocabulary . The objective of the task is the generation objective that output answers of the current dialogue turn t :
R t = arg max Rt P ( R t |V , H t , Q t ; θ ) = arg max Rt L R n=1 P m ( w n |R t,1 : n−1 , V , H t , Q t ; θ )
where L R is the length of the sequence R. In a Video - QA task , the dialogue history H is simply absent and the output response is typically collapsed to a single - token response .
Encoders
Text Encoder . A text encoder is shared to encode text inputs , including dialogue history , questions , and captions . The text encoder converts each text sequence X = w 1 , ... , w m into a sequence of embeddings X ∈ R m×d . We use a trainable embedding matrix to map token indices to vector representations of d dimensions through a mapping function φ . These vectors are then integrated with ordering information of tokens through a positional encoding function with layer normalization ( Ba et al . , 2016;Vaswani et al . , 2017 1 : Description of the modules and their functionalities . We denote P as the parameter to instantiate each module , H as the dialogue history , Q as the question of the current dialogue turn , and V as video input .
corresponding coordinates projected to d vis dimensions . We also make use of a CNN - based pretrained model to obtain features of temporal dimension Z cnn ∈ R F ×d vis . The audio feature is obtained through a pretrained audio model , Z aud ∈ R F ×d aud . We passed all video features through a linear transformation layer with ReLU activation to the same embedding dimension d.
Neural Modules
We introduce neural modules that are used to assemble an executable program constructed by the generated sequence from question parsers . We provide an overview of neural modules in Table 1 and demonstrate dialogue understanding and video understanding modules in Figure 3 and 4 respectively . Each module parameter , e.g. " a backpack " , is extracted from the parsed program ( See Section 3.4 ) .
For each parameter , we denote P ∈ R d as the average pooling of component token embeddings . find(P , H)→H ent . This module handles entity tracing by obtaining a distribution over tokens in the dialogue history . We use an entity - todialogue - history attention mechanism applied from an entity P i to all tokens in the dialogue history . Any neural network that learn to generate attention between two tensors is applicable .e.g . ( Bahdanau et al . , 2015;Vaswani et al . , 2017 ) . The attention matrix normalized by softmax , A find , i ∈ R L H , is used to compute the weighted sum of dialogue history token representations . The output is combined with entity embedding P i to obtain contextual entity representation H ent , i ∈ R d .
summarize(H ent , Q)→Q ctx . For each contextual entity representation H ent , i , i = 1 , ... , N ent , it is projected to L Q dimensions and is combined with question token embeddings through elementwise summation to obtain entity - aware question representation Q ent , i ∈ R L Q ×d . It is fed to a onedimensional CNN with max - pooling layer ( Kim , 2014 ) to obtain a contextual entity - aware question representation . We denote the final output
as Q ctx ∈ R Nent×d .
While previous models usually focus on global or token - level dependencies Le et al . , 2019b ) to encode question features , our modules compress fine - grained question representations at the entity level . Specifically , find and summarize modules can generate entitydependent local and global representations of question semantics . We show that our modularized approach can achieve better performance and transparency than traditional approaches to encode dialogue context ( Serban et al . , 2016;Vaswani et al . , 2017 ) ( Section 4 ) .
where(P , V)→V ent . Similar to the find module , this module handles entity - based attention to the video input . However , the entity representation P , in this case , is parameterized by the original entity in dialogue rather than in question ( See Section 3.4 for more description ) . Each entity P i is stacked to match the number of sampled video frames / clips F . An attention network is used to obtain entity - to - object attention matrix A where , i ∈ R F ×O . The attended feature are compressed through weighted sum pooling along the spatial dimension , resulting in V ent , i ∈ R F ×d , i = 1 , ... , N ent . when(P , V ent ) →V ent+act . This module follows a similar architecture as the where module . However , the action parameter P i is stacked to match N ent dimensions . The attention matrix A when , i ∈ R F is then used to compute the visual entity - action representations through weighted sum along the temporal dimension . We denote the output for all actions P i as V ent+act ∈ R Nent×Nact×d describe(P , V ent+act ) →V ctx . This module is a linear transformation to compute V ctx = W desc T [ V ent+act ; P stack ] ∈ R Nent×Nact×d where W desc ∈ R 2d×d , P stack is the stacked representations of parameter embedding P to N ent × N act dimensions , and [ ; ] is the concatenation operation . Note that the parameter P here is extracted from questions , often as the type of questions e.    and " how " . This eliminates the need to have different modules for different question types . However , we noted the current design may be challenged in rare cases in which an utterance contain numerous questions ( refer to Figure 5 ) .
The exist module is used when the questions are " yes / no " questions . This module is a special case of describe module where the parameter P is simply the average pooled question embeddings . The above where module is applied to object - level features . For temporal - based features such as CNN - based and audio features , the same neural operation is applied along the temporal dimension . Each resulting entity - aware output is then incorporated to frame - level features through element - wise summation .
An advantage of our architecture is that it separates dialogue and video understanding . We adopt a transparent approach to solve linguistic entity references during the dialogue understanding phase . The resolved entities are fed to the video understanding phase to learn entity - action dynamics in the video . We show that our approach is robust when dialogue evolves to many turns and video extends over time ( Please refer to Section 4 ) .
Question Parsers
To learn compositional programs , we follow ( Johnson et al . , 2017a;Hu et al . , 2017 ) and consider program generation as a sequence - tosequence task . We adopt a simple template " param 1 module 1 param 2 module 2 ... " as the target sequence . The resulting target sequences for dialogue and video understanding programs are sequences P dial and P vid respectively .
The parsers decompose questions into sub - sequences to construct compositional reasoning programs for dialogue and video understanding . Each parser is a vanilla Transformer decoder , including multi - head attention layers on questions and past dialogue turns ( Please refer to Appendix A.1 for more technical details ) .
Response Decoder
System response is decoded by incorporating the dialogue context and video context outputs from the corresponding reasoning programs to target token representations . We follows a vanilla Transformer decoder architecture ( Le et al . , 2019b ) , which consists of 3 attention layers : self - attention to attend on existing tokens , attention to Q ctx from dialogue understanding program execution , and attention to V ctx from video understanding program execution .
A ( 1 ) res = Attention(R| j−1 0 , R| j−1 0 , R| j−1 0 ) ∈ R j×d A ( 2 ) res = Attention(A ( 1 ) res , Q ctx , Q ctx ) ∈ R j×d A ( 3 ) res = Attention(A ( 2 ) res , V ctx , V ctx ) ∈ R j×d Multimodal Fusion .
For video features come from multiple modalities , visual and audio , the contextual features , denoted V ctx , is obtained through a weighted sum of component modalities , e.g. contextual visual features V vis ctx and contextual audio features V aud ctx . The scores S fusion to compute the weighted sum is defined as :
S fusion = Softmax(W T fusion [ Q stack ; V vis ctx ; V aud ctx ]
) where Q stack is the mean pooling output of question embeddings Q which is then stacked to N ent + N act dimensions , and W fusion ∈ R 3d×2 are trainable model parameters . The resulting S fusion has a dimension of ∈ R ( Nent+Nact)×2 .
Response Generation . To generate response sequences , a special token " _ sos " is concatenated as the first token w 0 . The decoded token w 1 is then appended to w 0 as input to decode w 2 and so on . Similarly to input source sequences , at decoding time step j , the input target sequence is encoded to obtain representations of system response R| j−1 0 . We combine vocabulary of input and output sequences and share the embedding matrix E ∈ R |V|×d where V = V in ∩ V out . During training time , we directly use the ground - truth responses as input to the decoder and optimize VGNMN with a cross - entropy loss to decode the next ground - truth tokens . During test time , responses are generated auto - regressively through beam search with beam size 5 . Note that we apply the same procedure to generate reasoning programs from question parsers .
Experiments
Datasets . We use the AVSD benchmark from the Dialogue System Technology Challenge 7 ( DSTC7 ) . The benchmark consists of dialogues grounded on the Charades videos ( Sigurdsson et al . , 2016 ) . Each dialogue contains up to 10 dialogue turns , each turn consists of a question and expected response about a given video . For visual features , we use the 3D CNN - based features from a pretrained I3D model ( Carreira and Zisserman , 2017 ) and object - level features from a pretrained FasterRNN model ( Ren et al . , 2015b ) . The audio features are obtained from a pretrained VGGish model ( Hershey et al . , 2017 ) . In the experiments with AVSD , we consider two settings : one with video summary and one without video summary as input . In the setting with video summary , the summary is concatenated to the dialogue history before the first dialogue turn . We also adapt VGNMN to the video QA benchmark TGIF - QA ( Jang et al . , 2017   For the TGIF - QA benchmark , we use the extracted features from a pretrained ResNet model ( He et al . , 2016 ) . Table 2 shows a summary of the AVSD and TGIF - QA benchmarks .
Training Details . We follow prior approaches ( Hu et al . , 2017(Hu et al . , , 2018Kottur et al . , 2018 ) by obtaining the annotations of the programs through a language parser ( Hu et al . , 2016 ) and a reference resolution model ( Clark and Manning , 2016 ) .
During training , we directly use these as groundtruth labels of programs to train our models . The ground - truth responses are augmented with label smoothing technique ( Szegedy et al . , 2016 ) . During inference time , we generate all programs and responses from given dialogues and videos . We run beam search to enumerate programs for dialogue and video understanding and dialogue responses . We use a training batch size of 32 and embedding dimension d = 128 in all experiments . Where Transformer attention is used , we fix the number of attention heads to 8 in all attention layers . In neural modules with MLP layers , the MLP network is fixed to 2 linear layers with a ReLU activation in between . In neural modules with CNN , we adopt a vanilla CNN architecture for text classification ( without the last MLP layer ) where the number of input channels is 1 , the kernel sizes are { 3 , 4 , 5 } , and the number of output channels is d. We initialize models with uniform distribution ( Glorot and Bengio , 2010 ) . During training , we adopt the Adam optimizer ( Kingma and Ba , 2015 ) and a decaying learning rate ( Vaswani et al . , 2017 ) where we fix the warm - up steps to 15 K training steps . We employ dropout ( Srivastava et al . , 2014 ) of 0.2 at all networks except the last linear layers of question parsers and response decoder . We train models up to 50 epochs and select the best models based on the average loss per epoch in the validation set .
All models are trained in a V100 GPU with a capacity of 16 GB . We approximated each training epoch took about 20 minutes to run . For each model experiment with VGNMN , we obtained at least 2 runs and reported the average results . We implemented models in Pytorch and released the code and model checkpoints 1 .
Optimization . We optimize models by joint training to minimize the cross - entropy losses to generate responses and functional programs .
L = αL dial + βL vid + L res = α j − log(P dial ( P dial , j ) ) + β l − log(P video ( P video , l ) ) + n − log(P res ( R n ) )
where P is the probability distribution of an output token . The probability is computed by passing output representations from the parsers and decoder to a linear layer W ∈ R d×V with softmax activation . We share the parameters between W and embedding matrix E. AVSD Results . We evaluate model performance by the objective metrics , including BLEU ( Papineni et al . , 2002 ) , METEOR ( Banerjee and Lavie , 2005 ) , ROUGE - L ( Lin , 2004 ) , and CIDEr ( Vedantam et al . , 2015 ) , between each generated response and 6 reference gold responses . As seen in Table 3 , our models outperform most of existing approaches . We observed that our approach did not outperform the GPT - based baselines ( Li et al . , 2020b;Le and Hoi , 2020 ) in the setting that allows video summary / caption input . However , the performance of our model in the setting without video summary / caption input is on par with the GPT - based baseline ( Li et al . , 2020b ) , even though our model did not rely on deep pretrained representations on large - scale text data . These observations imply that GPT - based models can better capture video context from video caption / summary through rich pretrained representations . However , without access to video caption / summary , these models may fail to understand video from visual - only representations . In this setting , GPT - based models may be inferior to VGNMN , which explicitly exploits the compositional structures from textual inputs to integrate visual features . We also found that VGNMN applied to object - level features is competitive to the model applied to CNN - based features . The    as well as model ( 1 ) . We observed that related factors might affect the discrepancy , such as the complexity of the questions for these short and long - range videos . Potentially , our question parser for the video understanding program needs to be improved ( e.g. for tree - based programs ) to retrieve information in these ranges . Robustness to dialogue turn : In Table 4b , we observed that model ( 1 ) performs better than model ( 2 ) overall , especially in higher turn positions , i.e. from the 4 th turn to 8 th turn . Interestingly , we noted some mixed results in very low turn position , i.e. the 2 nd and 3 rd turn , and very high turn position , i.e. the 10 th turn . Potentially , with a large dialogue turn position , the neural - based approach such as hierarchical RNN can better capture the global dependencies within dialogue context than the entity - based compositional NMN method .
Robustness to question structure : Finally , we compared performance of VGNMN with the no - NMN variant ( 1 ) in different cases of question structures : single - question vs. multiple - part structure . In single - question structures , we examined by the question types ( e.g. yes / no , wh - questions ) . In multi - part structures , we further classified whether there are sentences preceding the question ( e.g. " 1Sent+Que " ) or there are smaller ( sub-)questions ( e.g. " 2SubQue " ) within the question . In Table 4c , we observed that VGNMN has clearer performance gains in multi - part structures than singlequestion structures . In multi - part structures , we observed higher gaps between VGNMN and model ( 1 ) in highly complex cases e.g. " 2Sent+Que " vs. " 1Sent+Que " . These observations indicate the robustness of VGNMN and the underlying compositionality principle to deal with complex question structures . We also noted that VGNMN is still susceptible to extremely long questions ( " > 2Sent+Que " ) and future work is needed to ad - dress these scenarios .   Interpretability . In Figure 5 , we show both success and failure cases of generated responses and corresponding generated functional programs . In each example , we marked predicted outputs as incorrect if they do not match the ground - truth completely ( even though the outputs might be partially correct ) . From Figure 5 , we observe that in cases where generated dialogue programs and video programs match or are close to the gold labels , the model can generate generally correct responses . For cases where some module parameters do not exactly match but are closed to the gold labels , the model can still generate responses with the correct visual information ( e.g. the 4 th turn in example B ) . In cases of wrong predicted responses , we can further look at how the model understands the questions based on predicted programs . In the 3 rd turn of example A , the output response is missing a minor detail as compared to the label response because the video program fails to capture " rooftop " as a where parameter . These subtle yet important details can determine whether output responses can fully address user queries . In the 3 rd turn of example B , the model wrongly identifies " what room " as a where parameter and subsequently generates a wrong response that it is " a living room " .
TGIF - QA Results . We report the result using the L2 loss in Count task and accuracy in other tasks . From Table 5 , VGNMN outperforms the majority of the baseline models in all tasks by a large margin . Compared to AVSD experiments , the TGIF - QA experiments emphasize the video understanding ability of the models , removing the requirement for dialogue understanding and natural language generation . Since TGIF - QA questions follow a very specific question type distribution ( count , action , transition , and frameQA ) , the question structures are simpler and easier to learn than AVSD . Using exact - match accuracy of parsed programs vs. label programs as a metric , our question parser can achieve a performance 81 % to 94 % accuracy in TGIF - QA vs. 41 - 45 % in AVSD . The higher accuracy in decoding a reasoning structure translates to better adaptation between training and test time , resulting in higher performance gains .   Cascading Errors . Compared to prior approaches , we noted that VGNMN is a modularized system which may result in cascading errors to downstream modules . One major error is the error of generated programs which is used as parameters in neural modules . To gauge this error , we compare the performance of VGNMN between 2 cases : with generated programs and with groundtruth programs . From Table 6 , we noticed some performance gaps between these cases . These observations imply that : ( 1   For additional experiment results , qualitative samples , and analysis between model variants , refer to Appendix B and C.
Conclusion
In this work , we introduce Video - grounded Neural Module Network ( VGNMN ) . VGNMN consists of dialogue and video understanding neural modules , each of which performs entity and action - level operations on language and video components . Our comprehensive experiments on AVSD and TGIF - QA benchmarks show that our models can achieve competitive performance while promoting a compositional and interpretable learning approach .
Broader Impacts
During the duration of this work , there have been no ethical concerns regarding the model implementation , training , and testing . The data used in this work has been carefully reviewed and accordingly to the description from the original authors , we did not find any concerns on any significant biases . For any potential application or extension of this work , we would like to highlight some specific concerns . First , as the work is developed to build an intelligent dialogue agents , models should not be used with the intention to create fake human profiles for any harmful purposes ( e.g. fishing or spreading fake news ) . For wider use of dialogue systems , the application of work might result in certain impacts to some stakeholders whose jobs may be affected by this application ( e.g. customer service call agents ) . We hope any application should be carefully considered against these potential risks .
Each attention is followed by a feed - forward network applied to each position identically . We exploit the multi - head and feed - forward architecture , which show good performance in NLP tasks such as NMT and QA ( Vaswani et al . , 2017;Dehghani et al . , 2019 ) , to efficiently incorporate contextual cues from dialogue components to parse question into reasoning programs . At decoding step 0 , we simply use a special token _ sos as the input to the parser . In each subsequent decoding step , we concatenate the prior input sequence with the generated token to decode in an auto - regressive manner . We share the vocabulary sets of input and output components and thus , use the same embedding matrix . Given the encoded question Q , to decode the program for dialogue understanding , the contextual signals are integrated through 2 attention layers : one attention on previously generated tokens , and the other on question tokens . At time step j , we denote the output from an attention layer as A dial , j .
A
( 1 ) dial = Attention(P dial | j−1 0 , P dial | j−1 0 , P dial | j−1 0 ) A
( 2 ) dial = Attention(A ( 1 ) dial , Q , Q ) ∈ R j×d To generate programs for video understanding , the contextual signals are learned and incorporated in a similar manner . However , to exploit dialogue contextual cues , the execution output of dialogue understanding neural modules Q ctx is incorporated to each vector in P dial through an additional attention layer . This layer integrates the resolved entity information to decode the original entities for video understanding . It is equivalent to a reasoning process that converts the question from its original multi - turn semantics to single - turn semantics .
A
( 1 ) vid = Attention(P vid | j−1 0 , P vid | j−1 0 , P vid | j−1 0 ) A Noted that in the neural modules described in Section 3.3 , during training , we simply feed the ground - truth programs to optimize these modules . For instance , the neural module where received the ground truth entities P which is then used to instantiate the neural network and retrieve from video V . During test time , we decode the programs token by token through the question parsers , and feed the predicted entitiesP to neural modules . Note that we do not assume , and hence not train model to retrieve ground - truth locations of visual entities in videos . This strategy enables the applicability of VGNMN as we consider these entity annotations mostly unavailable in real - world systems .
B Additional Experimental Results
B.1 Non - NMN Models
We experiment with several Non - NMN based variants of our models . As can be seen in Table 7 , our approach to video and dialogue understanding through compositional reasoning programs exhibits better performance than non - compositional approaches . Compared to the approaches that directly process frame - level features in videos ( Row B ) or token - level features in dialogues ( Row C , D ) , our full VGNMN ( Row A ) considers entitylevel and action - level information extraction and thus , avoids unnecessary and possibly noisy extraction . Compared to the approaches that obtain dialogue contextual cues through a hierarchical encoding architecture ( Row E , F ) such as ( Serban et al . , 2016 ; , VGNMN directly addresses the challenge of entity references in dialogues . As mentioned , we hypothesize that the hierarchical encoding architecture is more appropriate for less entity - sensitive dialogues such as chit - chat and open - domain dialogues .
B.2 Dialogue context integration
Experimenting with different ways to integrate dialogue context representations , we observe that adding an attention layer attending to question during response decoding ( Row G ) is not necessary . This can be explained as the representation Q ctx obtained from dialogue understanding program already contains contextual information of both dialogue history and question and question input is no longer needed in the decoding phase . Furthermore , we investigate the model sensitivity to natural language generation through its ability to construct linguistically correct programs and responses . To generate responses that are linguistically appropriate , VGNMN needs dialogue context representation Q ctx as input to the response decoder ( Row H ) . The model also needs encoded question Q as input to the video understanding program parser to be able to decompose this sequence to entity and action module parameters ( Row I ) .
A Additional Model Details
A.1 Question Parsers
To learn compositional programs , we follow ( Johnson et al . , 2017a;Hu et al . , 2017 ) and consider program generation as a sequence - tosequence task . We adopt a simple template " param 1 module 1 param 2 module 2 ... " as the target sequence . The resulting target sequences for dialogue and video understanding programs are sequences P dial and P vid respectively .
The parsers decompose questions into subsequences to construct compositional reasoning programs for dialogue and video understanding . Each parser is an attention - based Transformer decoder . The Transformer attention is a multi - head attention on query q , key k , and value v tensors , denoted as Attention(q , k , v ) . For each token in the q sequence , the distribution over tokens in the k sequence is used to obtain the weighted sum of the corresponding representations in the v sequence .  
C Interpretability
We extract the predicted programs and responses for some example dialogues in Figure 6 , 7 , 8 , and 9 and report our observations :
• We observe that when the predicted programs are correct , the output responses generally match the ground - truth ( See the 1 st and 2 nd turn in Figure 6 , and the 1 st and 4 th turn in Figure 8) or close to the ground - truth responses ( 1 st turn in Figure 7 ) .
• When the output responses do not match the ground truth , we can understand the model mistakes by interpreting the predicted programs . For example , in the 3 rd turn in Figure 6 , the output response describes a room because the predicted video program focuses on the entity " what room " instead of the entity " an object " in the question . Another example is the 3 rd turn in Figure 8 where the entity " rooftop " is missing in the video program . These mismatches can deviate the information retrieved from the video during video program execution , leading to wrong output responses with wrong visual contents .
• We also note that in some cases , one or both of the predicted programs are incorrect , but the predicted responses still match the groundtruth responses . This might be explained as the predicted module parameters are still close enough to the " gold " labels ( e.g. 4 th turn in Figure 6 ) . Sometimes , our model predicted programs that are more appropriate than the ground truth . For example , in the 2 nd turn in Figure 7 , the program is added with a where module parameterized by the entity " the shopping bag " which was solved from the reference " them " mentioned in the question .
• We observe that for complex questions that involve more than one queries ( e.g. the 3 rd turn in Figure 8) , it becomes more challenging to decode an appropriate video understanding program and generate responses that can address all queries .
• In Figure 9 , we demonstrate some output examples of VGNMN and compare with two baselines : Baseline   and MTN ( Le et al . , 2019b ) . We noted that VGNMN can include important entities relevant to the current dialogue turn to construct output responses while other models might miss some entity details , e.g. " them / dishes " in example A and " the magazine " in example B. These small yet important details can determine the correctness of dialogue responses .    Figure 9 : Interpretability of example outputs from VGNMN and baselines models Le et al . , 2019b )
