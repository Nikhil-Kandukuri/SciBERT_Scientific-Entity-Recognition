How Gender Debiasing Affects Internal Model Representations , and Why It Matters
Common studies of gender bias in NLP focus either on extrinsic bias measured by model performance on a downstream task or on intrinsic bias found in models ' internal representations . However , the relationship between extrinsic and intrinsic bias is relatively unknown . In this work , we illuminate this relationship by measuring both quantities together : we debias a model during downstream fine - tuning , which reduces extrinsic bias , and measure the effect on intrinsic bias , which is operationalized as bias extractability with information - theoretic probing . Through experiments on two tasks and multiple bias metrics , we show that our intrinsic bias metric is a better indicator of debiasing than ( a contextual adaptation of ) the standard WEAT metric , and can also expose cases of superficial debiasing . Our framework provides a comprehensive perspective on bias in NLP models , which can be applied to deploy NLP systems in a more informed manner . 1 * Supported by the Viterbi Fellowship in the Center for Computer Engineering at the Technion .
Introduction
Efforts to identify and mitigate gender bias in Natural Language Processing ( NLP ) systems typically target one of two notions of bias . Extrinsic evaluation methods and debiasing techniques focus on the bias reflected in a downstream task Zhao et al . , 2018 ) , while intrinsic methods focus on a model 's internal representations , such as word or sentence embedding geometry ( Caliskan et al . , 2017;Bolukbasi et al . , 2016;Guo and Caliskan , 2021 ) . Despite an abundance of evidence pointing towards gender bias in pretrained language models ( LMs ) , the extent of harm caused by these biases is not clear when it is not reflected in a specific downstream task ( Barocas Figure 1 : Our proposed framework . Black arrows mark forward passes , red arrows mark things we measure . We first ( a ) train a model on a downstream task , then ( b ) train another model on the same task using a debiased dataset , and finally ( c ) measure intrinsic bias in both models and compare . Kate Crawford , 2017;Blodgett et al . , 2020;Bommasani et al . , 2021 ) . For instance , while the word embedding proximity of " doctor " to " man " and " nurse " to " woman " is intuitively normatively wrong , it is not clear when such phenomena would lead to downstream predictions manifesting in social biases . Recently , Goldfarb - Tarrant et al . ( 2021 ) have shown that debiasing static embeddings intrinsically is not correlated with extrinsic gender bias measures , but the nature of the reverse relationship is unknown : how are extrinsic interventions reflected in intrinsic representations ? Furthermore , Gonen and Goldberg ( 2019a ) demonstrated that a number of intrinsic debiasing methods applied to static embeddings only partially remove the bias and that most of it is still hidden within the embed - ding . Complementing their view , we examine extrinsic debiasing methods , as well as demonstrate the possible harm this could cause . Contrary to their conclusion , we do not claim that these debiasing methods should not be trusted , as long as they are utilized with care .
Our goal is to gain a better understanding of the relationship between a model 's internal representations and its extrinsic gender bias by examining the effects of various debiasing methods on the model 's representations . Specifically , we fine - tune models with and without gender debiasing strategies , evaluate their external bias using various bias metrics , and measure intrinsic bias in the representations . We operationalize intrinsic bias via two metrics : First , we use CEAT ( Guo and Caliskan , 2021 ) , a contextual adaptation of the widely used intrinsic bias metric WEAT ( Caliskan et al . , 2017 ) . Second , we propose to use an information - theoretic probe to quantify the degree to which gender can be extracted from the internal model representations . Then , we examine how these intrinsic metrics correlate with a variety of extrinsic bias metrics that we measure on the model 's downstream performance . Our approach is visualised in Figure 1 .
We perform extensive experiments on two downstream tasks ( occupation prediction and coreference resolution ) ; several debiasing strategies that involve alterations to the training dataset ( such as removing names and gender indicators , or balancing the data by oversampling or downsampling ) ; and a multitude of extrinsic bias metrics . Our analysis reveals new insights into the way language models encode and use information on gender :
• The effect of debiasing on internal representations is reflected in gender extractability , while not always in CEAT . Thus , gender extractability is a more reliable indicator of gender bias in NLP models .
• In cases of high gender extractability but low extrinsic bias metrics , the debiasing is superficial , and the internal representations are a good indicator for this : The bias is still present in internal representations and can be restored by retraining the classification layer . Therefore , our proposed measuring method can help in detecting such cases before deploying the model .
• The two tasks show different patterns of correlation between intrinsic and extrinsic bias .
The coreference task exhibits a high correlation . The occupation prediction task exhibits a lower correlation , but it increases after retraining ( a case of superficial debiasing ) . Gender extractability shows higher correlations with extrinsic metrics than CEAT , increasing the confidence in this metric as a reliable measure for gender bias in NLP models .
Methodology
In this study , we investigate the relationship between extrinsic bias metrics of a task and a model 's internal representations , under various debiasing conditions , for two datasets in English . We perform extrinsic debiasing , evaluate various extrinsic and intrinsic bias metrics before and after debiasing , and examine correlations .
Dataset . Let D = { X , Y , Z } be a dataset consisting of input data X , labels Y and protected attributes Z. 2 This work focuses on gender as the protected attribute z. In all definitions , F and M indicate female and male gender , respectively , as the value of the protected attribute z.
Trained Model . The model is optimized to solve the downstream task posed by the dataset . It can be formalized as f • g : X → R |Y| , where g(• ) is the feature extractor , implemented by a language model , e.g. , RoBERTa ( Liu et al . , 2019 ) , f ( • ) is the classification function , and Y is the set of the possible labels for the task .
Bias Metrics
Each bias evaluation method described in the literature can be categorized as extrinsic or intrinsic . In all definitions , r indicates the model 's output probabilities .
Extrinsic Metrics
Extrinsic methods involve measuring the bias of a model solving a downstream problem . The extrinsic metric is a function :
E(X , Y , R , Z ) ∈ R
The output represents the quantity of bias measured ; the further from 0 the number is , the larger the bias is . Our analysis comprises a wide range of extrinsic metrics , including some that have been measured in the past on the analyzed tasks ( Zhao et al . , 2018;Ravfogel et al . , 2020;Goldfarb - Tarrant et al . , 2021 ) and some that have never been measured before , and shows our results apply to many of them . For illustration , we will consider occupation prediction , a common task in research on gender bias Ravfogel et al . , 2020 ; . The input x is a biography and the prediction y is the profession of the person described in it . The protected attribute z is the gender of that person .
Performance gap . This is the difference in performance metric for two different groups , for instance two groups of binary genders , or a group of pro - stereotypical and a group of anti - stereotypical examples . We measure the following metrics : True Positive Rate ( TPR ) , False Positive Rate ( FPR ) , and Precision . In occupation prediction , for instance , the TPR gap for each profession y expresses the difference in the percentage of women and men whose profession is y and are correctly classified as such . We also measure F1 of three standard clustering metrics for coreference resolution . Each such performance gap captures a different facet of gender bias , and one might be more interested in one of the metrics depending on the application . We compute two types of performance gap metrics : ( 1 ) the sum of absolute gap values over all classes ; ( 2 ) the Pearson correlation between the performance gap for a class and the percentage of women in that class . For instance , if y is a profession , we measure the correlation between performance gaps and percentages of women in each profession . 3 The two metrics are closely related but answer slightly different questions : the sum quantifies how a model behaves differently on different genders , and the correlation shows the relation of model behaviour to social biases ( in the world or the data ) without regard to actual gap size .
Statistical metrics . For breadth of analysis , we examine three additional statistical metrics ( Barocas et al . , 2019 ) , which correspond to different notions of bias . All three are measured as differences ( d ) between two probability distributions , and we then obtain a single bias quantity per metric by summing all computed distances .
• Independence : d P ( r|z = z ) , P ( r ) ∀z ∈ { F , M } . For instance , we measure the difference between the distribution of model 's predictions on women and the distribution of all predictions . Independence is stronger as the prediction r is less correlated with the protected attribute z. It is measured with no relation to the gold labels .
• Separation : d P ( r|y = y , z = z ) , P ( r|y = y ) ∀y ∈ Y , z ∈ { F , M } . For instance , we measure the difference between the distribution of a model 's predictions on women who are teachers and the distribution of predictions on all teachers . It encapsulates the TPR and FPR gaps discussed previously , and can be seen as a more general metric .
• Sufficiency : d P ( y|r = r , z = z ) , P ( y|r = r ) . For instance , we measure the difference between the distribution of gold labels on women classified as teachers by the model and the distribution of gold labels on all individuals classified as teachers by the model . Sufficiency relates to the concept of calibration in classification . A difference in the classifier 's scores for men and for women indicates that it might be penalizing or over - promoting one of the genders .
Intrinsic Metrics
Intrinsic methods are applied to the representation obtained from the feature extractor . These methods are independent of any downstream task . The intrinsic metric is a function :
I(g(X ) , Z ) ∈ R Compression .
Our main intrinsic metric is the compression of gender information evaluated by a minimum description length ( MDL ) probing classifier ( Voita and Titov , 2020 ) , trained to predict gender from the model 's representations . Probing classifiers are widely used for predicting various properties of interest from frozen model representations ( Belinkov and Glass , 2019 ) . MDL probes were proposed because a probe 's accuracy may be misleading due to memorization and other issues ( Hewitt and Liang , 2019;Belinkov , 2021 ) . We use the MDL online code , where the probe is trained in timesteps , on increasing subsets of the training set , then evaluated against the rest of it . Higher compression indicates greater gender extractability .
CEAT . We also measure CEAT ( Guo and Caliskan , 2021 ) , which is a contextualized version of WEAT ( Caliskan et al . , 2017 ) , a widely used bias metric for static word embeddings . WEAT defines sets X and Y of target words , and sets A and B of attribute words . For instance , A and B contain males and females names , while X and Y contain career and family related words , respectively . The bias is operationalized as the geometric proximity between the target and attribute word embeddings , and is quantified in CEAT by the Combined Effect Size ( CES ) and a p - value for the null hypothesis of having no biased associations . For more information on CEAT refer to Appendix A.4.3 .
Debiasing Techniques
We debias models by modifying the downstream task 's training data before fine - tuning . Scrubbing ( De - Arteaga et al . , 2019 ) removes first names and gender - specific terms ( " he " , " she " , " husband " , " wife " , " Mr " , " Mrs " , etc . ) . Balancing subsamples or oversamples examples such that each gender is equally represented in the resulting dataset w.r.t each label . Anonymization ( Zhao et al . , 2018 ) removes named entities . Counterfactual Augmentation ( Zhao et al . , 2018 ) involves replacing male entities in an example with female entities , and adding the modified example to the training set . As some of these are dataset / task - specific , we give more details in the following section .
Experiments
In each experiment , we fine - tune a model for a downstream task . For training , we use either the original dataset or a dataset debiased with one of the methods from Section 2.2 . Figure 2 presents examples of debiasing methods for the two downstream tasks . We measure two intrinsic metrics by probing that model 's inner representations for gender extractability ( as measured by MDL ) and by CEAT , and test various extrinsic metrics . The relation between one intrinsic and one extrinsic metric becomes one data point , and we repeat over many random seeds ( for both the model and the probe ) . Further implementation details are in appendix A.
Occupation Prediction
The task of occupation prediction is to predict a person 's occupations ( from a closed set ) , based on their biography . We use the Bias in Bios dataset . Regardless of the training method , the test set is subsampled such that each profession has equal gender representation .
Britney currently works on CNN 's newest primetime show . She has also written for the New York Times .
_ currently works on CNN 's newest primetime show . _ has also written for the New York Times .
Scrubbing
My sister is taking a painting class this summer , so she has been sharing the latest lesson with me .
My brother is taking a painting class this summer , so he has been sharing the latest lesson with me .
Counterfactual augmentation Occupation Classification Coreference Resolution Original dataset
Original dataset Model . Our main model is a RoBERTa model ( Liu et al . , 2019 ) topped with a linear classifier , which receives the [ CLS ] token embedding as input and generates a probability distribution over the professions . In addition , we experiment with training a baseline classifier layer on top of a frozen , non - finetuned RoBERTa . We also replicate our RoBERTa experiments with a DeBERTa model ( He et al . , 2020 ) , to verify that our results are are not model specific and hold more broadly .
Debiasing Techniques . Following De - Arteaga et al . ( 2019 ) we experiment with scrubbing the training dataset . Figure 2 shows an example biography snippet and its scrubbed version . We also conduct balancing ( per profession , subsampling and oversampling to ensure an equal number of males and females per profession ) , which has not previously been used on this dataset and task .
Metrics . We measure all bias metrics from Section 2.1 except for F1 .
Probing . The probing dataset for this task is the test set , and the gender label of a single biography is the gender of the person described in it . We probe the [ CLS ] token representation of the biography . In addition to the models described above , we measure baseline extractability of gender information from a randomly initialized RoBERTa model .
Coreference Resolution
The task of coreference resolution is to find all textual expressions referring to the same real - world entities . We train on Ontonotes 5.0 ( Weischedel et al . , 2013 ) and test on the Winobias challenge dataset ( Zhao et al . , 2018 ) . Winobias consists of sentence pairs , pro - and anti - stereotypical variants , with individuals referred to by their profession . For example , " The physician hired the secretary be-   cause he / she was busy . " is pro / anti - stereotypical , based on US labor statistics . 4 A coreference system is measured by the performance gap between the pro - and anti - stereotypical subsets .
Model . We use the model presented in Lee et al . ( 2018a ) with RoBERTa as a feature extractor .
Debiasing Techniques . Following Zhao et al . ( 2018 ) , we apply anonymization ( denoted as Anon ) and counterfactual augmentation ( CA ) on the training set . These techniques were used jointly in previous work ; we examine each individually as well .
Metrics . Following Zhao et al . ( 2018 ) , we measure the F1 difference between anti - and prostereotypical examples . 5 We also interpret the task as a classification problem , and measure all metrics from Section 2.1 . For more details refer to Appendix A.4.2 .
Probing . We probe the representation of a profession word as extracted from Winobias sentences ,
Results
Tables 1a and 1b present intrinsic and extrinsic metrics for RoBERTa models on the occupation prediction and coreference resolution tasks , respectively . We present a representative subset of the measured metrics that demonstrate the observed phenomena ; full results are found in Appendix B. The DeBERTa model results are consistent with the RoBERTa model trends .
Compression Reflects Debiasing Effects
As shown in the tables , compression captures differences in models that were debiased differently . CEAT , however , can not differentiate between occupation prediction models . For example , in occupation prediction ( Table 1a ) the compression rate varies significantly between a non - debiased and a debiased model via scrubbing and oversampling , while CEAT detects no difference between the models . In coreference resolution ( Table 1b ) , both compression and CEAT are able to identify differences between the non - debiased model and the others , such as CA , which has both a lower compression and CEAT effect . But the CEAT effect sizes are small ( below 0.5 ) , which implies no bias , in contrast to the extrinsic metrics .
High Gender Extractability Implies Superficial Debiasing
Extrinsic and intrinsic effects of debiasing . In occupation classification (   1a and 1b also compare extrinsic metrics before and after retraining . All models show bias restoration , due to the classification layer being trained on the biased dataset . 6 The amount of bias restored varies between models in a way that is predictable by the compression metric .
In the occupation prediction task , comparing Before and After numbers in Table 1a , the model fine - tuned using a scrubbed dataset - which has the lowest compression rate - displays the least bias restoration , confirming that the LM absorbed the process of debiasing . The model fine - tuned on subsampled data has higher extrinsic bias after retraining . Hence , the debiasing was primarily cosmetic , and the representations within the LM were not debiased . The model fine - tuned on oversampled data - which has the highest compression - has the highest extrinsic bias ( except for FPR ) , even though this was not true before retraining .
In coreference resolution , comparing Before and After numbers in Table 1b , models with the least extrinsic bias ( CA and CA+Anon ) are also least biased after retraining . Compression rate predicted this ; these models also had lower compression rates than non - debiased models . Interestingly , the model fine - tuned with an anonymized dataset is the most biased after retraining , consistent with its high compression rate relative to the other models . As with subsampling and oversampling in occupation prediction , anonymization 's ( lack of ) effect on extrinsic metrics was cosmetic ( compare None and Anon in Before block , Table 1b ) . Anonymization actually had a biasing effect on the LM , which was realized after retraining .
We conclude that compression rate is a useful indicator of superficial debiasing , and can potentially be used to verify and gain confidence in attempts to debias an NLP model , especially when there is little or no testing data .     retraining . In occupation prediction , certain extrinsic metrics have a weak correlation with compression rate , while others do not . Except one metric ( FPR gap sum ) , the compression rate and the extrinsic metric correlate more after retraining . Figure 3 illustrates this for TPR - gap ( Pearson ) . The increase is due to superficial debiasing , especially by subsampling data , which prior to retraining had low extrinsic metrics and relatively high intrinsic metrics . This shows that correlation between extrinsic metrics and compression rate for certain metrics is stronger than it appeared before retraining . It is unsurprising that CEAT does not correlate with any extrinsic metrics , since CEAT could not distinguish between different models .
Correlation between Extrinsic and Intrinsic Metrics
Coreference resolution shows stronger correlations between compression rate and extrinsic met - rics , but low correlations between Pearson metrics . We further discuss cases of no correlation in appendix D. Correlations decrease after retraining , but metrics that were highly correlated remain so ( > 0.7 after retraining ) . The correlations are visualized for F1 difference metrics in Figure 4 . CEAT and extrinsic metrics correlate much less than compression rate ( Table 2 ) . Our results are in line with those of Goldfarb - Tarrant et al . ( 2021 ) , who found a lack of correlation between extrinsic metrics and WEAT , the static - embedded version of CEAT .
Given that recent work ( Goldfarb - Tarrant et al . , 2021;Cao et al . , 2022 ) questions the validity of intrinsic metrics as a reliable indicator for gender bias , the compression rate provides a reliable alternative to current intrinsic metrics , by offering correlation to many extrinsic bias metrics .  
Related Work
There are few studies that examine both intrinsic and extrinsic metrics . Previous work by Goldfarb - Tarrant et al . ( 2021 ) showed that debiasing static embeddings intrinsically is not correlated with extrinsic bias , challenging the assumption that intrinsic metrics are predictive of bias . We examine the other direction , exploring how extrinsic debiasing affects intrinsic metrics . We also extend beyond their work to contextualized embeddings , a wider range of extrinsic metrics , and a new , more effective intrinsic metric based on information - theoretic probing . A contemporary work by Cao et al . ( 2022 ) measured the correlations between intrinsic and extrinsic metrics in contextualized settings across different language models . In contrast , our work examines the correlations across different versions of the same language model by fine - tuning it using various debiasing techniques . Studies that inspect extrinsic metrics include either a challenge dataset curated to expose differences in model behavior by gender , or a test dataset labelled by gender . Among these datasets are Winobias ( Zhao et al . , 2018 ) , Winogender ( Rudinger et al . , 2018 ) and GAP ( Webster et al . , 2018 ) for coreference resolution , WinoMT ( Stanovsky et al . , 2019 ) for machine translation , EEC ( Kiritchenko and Mohammad , 2018 ) for sentiment analysis , BOLD ( Dhamala et al . , 2021 ) for language generation , gendered NLI ( Sharma et al . , 2020 ) for natural language inference and Bias in Bios   for occupation prediction .
Studies that measure gender bias intrinsically in static word or sentence embeddings measure characteristics of the geometry , such as the prox - imity between female - and male - related words to stereotypical words , or how embeddings cluster or relate to a gender subspace ( Bolukbasi et al . , 2016;Caliskan et al . , 2017;Gonen and Goldberg , 2019b;Ethayarajh et al . , 2019 ) . However , metrics and debiasing methods for static embeddings do not apply directly to contextualized ones . Several studies use sentence templates to adapt to contextual embeddings ( May et al . , 2019;Kurita et al . , 2019;Tan and Celis , 2019 ) . This templated approach is difficult to scale , and lacks the range of representations that a contextual embedding offers . Other work extracts embedding representations of words from natural corpora ( Zhao et al . , 2019;Guo and Caliskan , 2021;Basta et al . , 2019 ) . These studies often adapt the WEAT method ( Caliskan et al . , 2017 ) , which measures embedding geometry . None measure the effect of the presumably found " bias " on a downstream task .
There is a growing conversation in the field ( Barocas et al . , 2017;Kate Crawford , 2017;Blodgett et al . , 2020;Bommasani et al . , 2021 ) about the importance of articulating the harms of measured bias . In general , extrinsic metrics have clear , interpretable impacts for which harm can be defined . Intrinsic metrics have an unclear effect . Without evidence from a concrete downstream task , a found intrinsic bias is only theoretically harmful . Our work is a step towards understanding whether intrinsic metrics provide valuable insights about bias in a model .
Discussion and Conclusions
This study examined whether bias in internal representations is related to extrinsic bias . We designed a new framework in which we debias a model on a downstream task , and measure its intrinsic bias . We found that gender extractability from internal representations , measured by compression rate via MDL probing , reflects bias in a model . Compression was much more reliable than an alternative intrinsic metric for contextualised representations , CEAT . Compression correlated well - to varying degrees - with many extrinsic metrics . We thus encourage NLP practitioners to use compression as an intrinsic indicator for gender bias in NLP models . When comparing two alternative models , a lower compression rate provides confidence in a model 's superiority in terms of gender bias . The relative success of compression over CEAT may be because the compression rate was calculated on the same dataset as the extrinsic metrics , whereas CEAT was measured on a different dataset not necessarily aligned with a specific downstream task . The use of a non - task - aligned dataset is a common strategy among other intrinsic metrics ( May et al . , 2019;Kurita et al . , 2019;Basta et al . , 2021 ) . Another possible explanation is that compression rate measures a more focused concept , namely the gender information within the internal representations . CEAT measures proximity among embeddings of general terms that may include other social contexts that do not directly relate to gender ( e.g. a female term like ' lady ' or ' Sarah ' contains information about not just gender but class , culture , formality , etc , and it can be hard to isolate just one of these from the rest ) .
Our results show that when a debiasing method reduces extrinsic metrics but not compression , it indicates that the language model remains biased . When such superficial debiasing occurs , the debiased language model may be reapplied to another task , as in Jin et al . ( 2021 ) , resulting in unexpected biases and nullifying the supposed debiasing . Our findings suggest that practitioners of NLP should take special care when adopting previously debiased models and inspect them carefully , perhaps using our framework . Our results differ from those of Mendelson and Belinkov ( 2021a ) , who found that the debiasing increases bias extractability as measured by compression rate . However , they studied different , non - social biases , that arise from spurious or unintended correlations in training datasets ( often called dataset biases ) . In our case , some debiasing strategies increase intrinsic bias while others decrease it . Future work could investigate why debiasing affects extractability differently for these two types of biases .
Our work also highlighted the importance of the classification layer . Using a debiased objective , such as a balanced dataset , the classification layer can provide significant debiasing . This holds even if the internal representations are biased and the classifier is a single linear layer , as shown in the occupation prediction task . Bias stems in part from internal LM bias and in part from classification bias . Practitioners should focus their efforts on both parts when attempting to debias a model .
We used a broader set of extrinsic metrics than is typically used , and found that the bias metrics behaved differently : some decreased more than others after debiasing , and they correlated differently with compression rate . Debiasing efforts may not be fully understood by testing only a few extrinsic metrics . However , compression as an intrinsic bias metric can indicate meaningful debiasing of internal model representations even when not all metrics are easily measurable , since it correlates well with many extrinsic metrics .
A major limitation of this study is the use of gender as a binary variable , which is trans - exclusive . Cao and Daumé III ( 2020 ) made the first steps towards inclusive gender bias evaluation in NLP , revealing that coreference systems fail on genderinclusive text . Further work is required to adjust our framework to non - binary genders , potentially revealing insights about the poor performance of NLP systems in that area .
A Implementation Details
We used RoBERTa in all models ( base size , 120 M parameters ) . We use following random seeds in all repeated experiments : 0 , 5,11,26,42,46,50,63,83,90 . Our code was implemented mainly using the Python libraries Pytorch ( Paszke et al . , 2019 ) , Transformers ( Wolf et al . , 2020 ) , Sklearn ( Pedregosa et al . , 2011 ) , and the experiments were logged using Wandb ( Biewald , 2020 ) .
A.1 Occupation Classification
We fine - tuned a RoBERTa - base model with a linear classification layer on top . Training was done for 10 epochs at a learning rate of 5e-5 , batch size of 64 . The input to RoBERTa was the biography tokens , which is limited to the first 128 tokens . The resulting [ CLS ] token embedding is fed to the classifier to predict the occupation . The probing task involves using the same [ CLS ] token and training the probing classifier to predict the gender of the person in the biography . The experiments without fine - tuning included either a pre - trained or a previously fine - tuned RoBERTa . We first extracted the pre - trained RoBERTa 's embeddings of tokens from the [ CLS ] and then trained a linear classifier on them . The learning rate was 0.001 and the batch size was 64 . We trained the classification layer with pre - trained RoBERTa on 300 epochs , but with fine - tuned RoBERTa , 10 epochs were sufficient . For all training processes , the epoch with the greatest validation accuracy was saved . Finetuning took 7 hours on a GeForce RTX 2080 Ti GPU . Bias in Bios contains almost 400k biographies , and we obtain validation ( 10 % ) and test set ( 25 % ) by splitting with Scikit - learn 's ( Pedregosa et al . , 2011 ) test_train_split with our random seeds .
A.2 Coreference Resolution
We use the implementation of Xu and Choi ( 2020 ) , a model that was introduced by Lee et al . ( 2018b ) and has been adopted by many coreference resolution models . Coreference resolution is the process of clustering different mentions in a text that refer to the same real - world entities . The task is solved by detecting mentions through text spans and then predicting for each pair of spans if they represent the same entity . The span representations were extracted with a RoBERTa model , which is fine - tuned throughout the training process , except in the retraining experiment . Fine - tuning took 3 hours on an NVIDIA RTX A6000 GPU . Ontonotes 5.0 has 625k sentences and we use the standard validation and test splits .
A.3 Probing Classifier
We use the MDL probe ( Voita and Titov , 2020 ) implementation by Mendelson and Belinkov ( 2021b ) . In all experiments , we use a linear probe and train it with a batch size of 16 and a learning rate of 1e-3 . The timestamps used , meaning the accumulating fractions of data that the probe is trained on , are 2.0 % , 3.0 % , 4.4 % , 6.5 % , 9.5 % , 14.0 % , 21.0 % , 31.0 % , 45.7 % , 67.6 % , 100 % .
A.4 Metrics
A.4.1 Fairness - Based Metrics Implementation
All three statistical fairness metrics measure the difference between two probability distributions , where this difference describes a notion of bias . We calculate Independence and Separation via Kullback - Leibler ( KL ) divergence , using the Al - lenNLP implementation ( https://github.com/ allenai / allennlp ) . We calculate Sufficiency via Wasserstein distance instead , which is motivated by Kwegyir - Aggrey et al . ( 2021 ) . In this case , we can not use KL divergence , since there are some classes that do not occur in model predictions for both male and female genders . This causes the probability distributions to not have the same support , and KL divergence is unbounded . Wasserstein distance lacks the requirement for equal support .
A.4.2 Classification Metrics Interpretation in Winobias
Winobias datasets contain pairs of stereotypical and anti - stereotypical sentences . The stereotypes are derived from the US labor statistics ( for instance , a profession with a majority of males is stereotypically male ) . Since coreference resolution is viewed as a clustering problem , it is usually measured via clustering evaluation metrics . Coreference resolution is commonly measured as the average F1 score of these , and the same is true for Winobias . Nevertheless , coreference resolution is accomplished by making a prediction for each pair of mentions , so it can be seen as a classification task . Winobias can be viewed as a simpler task than general coreference resolution , as it contains exactly two mentions of professions and one pronoun , which refers to exactly one profession . Therefore , we reframe it as a classification problem . In a Winobias sentence with two professions x and y , as well as a pronoun p , where p is referring to x , a true positive would be to cluster x and p together , while a false positive would be to cluster y and p together . Our classification metrics are derived based on these definitions . For instance , the TPR gap for profession " teacher " , which is a stereotypical female occupation , is the TPR rate on pro - stereotypical sentences ( with a female pronoun ) minus the TPR rate on anti - stereotypical sentences ( with a male pronoun ) .
A.4.3 CEAT
The Word Embedding Association Test ( WEAT ) developed by ( Caliskan et al . , 2017 ) is a method for evaluating bias in static word embeddings . The test is defined as follows : given two sets of target words X , Y ( e.g. , ' executive ' , ' management ' , ' professional ' and ' home ' , ' parents ' , ' children ' ) and two sets of attribute words ( e.g. , male names and female names ) , and using ⃗ w to represent the word embedding for word w , the effect size is : In essence , the effect size measures how different are the distances between the embedding vectors of each target group and the attribute groups . Specifically , if s(x , A , B ) > 0 , ⃗
x is more similar to attribute words B and vice versa . For instance , a larger effect size is observed if target words X are more similar to attribute words A and target words Y are more similar to attribute words B. |ES| > 0.5 and |ES| > 0.8 are considered medium and large effect sizes , respectively ( Rice and Harris , 2005 ) . The null hypothesis holds that there is no difference between the two sets of target words in terms of their relative similarity to the two sets of attribute words , indicating that there are no biased associations . Statistical significance is defined by the p - value of WEAT , which reflects the probability of observing the effect size under the null hypothesis .
Since a word can take on a great variety of vector representations in a contextual setting , ES varies according to the sentences used to extract word representation . Thus , to adopt WEAT to contextualized representations , the Combined Effect Size ( CES ) ( Guo and Caliskan , 2021 ) is derived as the distribution of WEAT effect sizes over many possible contextual word representations :
CES(X , Y , A , B ) = N i=1 v i ES i N i=1 v i
where ES i denotes the WEAT effect size of the i'th choice of word representations from a large corpus , and v i is the inverse of the sum of in - sample variance V i and between - sample variance in the distribution of random - effects . As in Guo and Caliskan ( 2021 ) , the representation for each word is derived from 10,000 random sentences extracted from a corpus of Reddit comments .
The combined effect size of each of the models is examined on WEAT stimulus 6 , which contains target words of career / family and attribute words of male / female names . This was the only one that detected bias on a pre - trained RoBERTa ( CES close to 0.5 and p < 0.05 ) . The points that we kept in our analysis are those where p < 0.05 , which make up 90 % of the points in occupation prediction and 95 % of the points in coreference resolution .
B Full Results
In this section we provide the full results of a RoBERTa model trained on the downstream task .
Table 3 presents results for the occupation prediction task after fine - tuning , Table 4 presents the retrained model results .
Figure 5 illustrates the correlations between extrinsic metrics and compression rate before and after retraining .
Table 5 presents the complete results for the occupation prediction task of the model trained without fine - tuning , meaning that the RoBERTa model is the pretrained version from Liu et al . ( 2019 ) and only the classification layer was updated . Subsampling the dataset has significant debiasing effects , which suggests that this debiasing method can achieve low extrinsic bias even when internal bias exists . The Pearson correlation on precision exhibits a different behavior . It makes sense nonetheless : precision is computed as T P \(T P + F P ) . A biased model will assign more examples of a specific profession to a specific gender ( which aligns with the percentage of biographies of this profession with this gender on the training set ) , increasing both T P and F P and decreasing precision . The results on the coreference resolution task align with the results of occupation prediction .
Table 6 presents the results using a DeBERTa model ( He et al . , 2020 ) for the occupation classification task . The trends are similar to those of RoBERTa , with the same metrics showing an increase , no change , or decrease in correlation after re - training , suggesting a general trend in the behavior of these metrics in relation to internal model representations .
Table 7 displays the results on a finetuned model for the coreference resolution task and Table 8 displays the retraining results .
Figure 6 shows the correlations between compression rate and extrinsic metrics before and after the retraining .    C Why is scrubbing not as effective as subsampling ?
The debiasing method of subsampling significantly reduced external biases in the occupation prediction task . Although compression rates show that scrubbing reduced more gender information , subsampling outperforms it as a debiasing method . We find that in spite of the scrubbing , a probe is able to correctly identify the gender from an internal representation with 68.8 % accuracy compared to 90.7 % on the original , non - scrubbed data . This means that although the scrubbing process reduces extrinsic bias significantly , gender information is still embedded in the [ CLS ] token embeddings .
To investigate the source of gender information after scrubbing , we use logistic regression ( LR ) model to predict the gender from the Bag - of - Words of the scrubbed biographies . We perform an iterative process for automatic extra scrubbing : in each iteration we ( 1 ) train a LR model for gender prediction ( 2 ) scrub the n most significant words for each gender according to the LR weights . The most relevant words among 5 seeds of training with n=10 words scrubbed per iteration are displayed in Table 9 . The model learns indirect correlations to gender in the absence of explicit gendered words . Because the significant words are related to male - or femaledominated professions , we conducted the process on a specific profession . Table 10 presents the most significant words for biographies of nurses . There are differences in wording even between females and males in the same profession . The results of this study are in line with the results of other studies that have been conducted on the way biographies are written for men and women ( Wagner et al . , 2016;Sun and Peng , 2021 ) .
Subsampling is therefore more effective even when gender information is present since it prevents the model from learning correlations between gender information and a profession whereas scrubbing only attempts to remove gender indicators without removing correlations . On the other hand , it is possible that oversampling is less effective for debiasing since seeing more non - unique examples an unrepresented group encourages learning correlations .
D A closer look into no - correlation cases D.1 Occupation Prediction
Although compression has the ability to identify bias in most cases , some metrics still show little or no correlation with compression rate . These results suggest that gender information comprises only one facet of embedded bias in the representations . Other factors that may influence these metrics are not considered or measured , such as the connection between a name and a profession .
For example , as can be see in Tables 3 and 4 , LMs finetuned on subsampled data have the largest FPR gaps after retraining , despite being the least biased before retraining , while those finetuned on oversampled data have the next - to - lowest FPR gaps after retraining . The information encoded in the internal representations may have been encoded in a manner that allowed the classification layer to exhibit a smaller FPR gap when trained on a balanced dataset . However , when the classification Figure 7 : Occupation prediction : Before ( left ) and after ( right ) plots of compression rate versus Pearson metrics as computed from real - world statistics ( as opposed to statistics derived from the training dataset ) . This shows the unrealiability of using real world statistics to draw conclusions , as they may not be reflected in the data .
Acknowledgements
This research was supported by the ISRAEL SCI - ENCE FOUNDATION ( grant No . 448/20 )   and by an Azrieli Foundation Early Career Faculty Fellowship . We also thank Kate McCurdy and Andreas Grivas for comments on early drafts , the members of the Technion CS NLP group for their valuable feedback , and the anonymous reviewers for their useful suggestions .
Debiasing Strategy
Metric
None Oversampling Subsampling Scrubbing Compression 4.121 ± 1.238 8.522 * ± 2.354 3.568 ± 1.516 1.699 * ± 0.138 Accuracy 0.861 ± 0.005 0.852 * ± 0.004 0.861 ± 0.003 0.851 * ± 0.003 TPR gap ( P ) 0.763 ± 0.071 0.729 ± 0.067 0.319 * ± 0.114 0.704 * ± 0.068 TPR gap ( S ) 2.391 ± 0.257 2.145 * ± 0.220 1.598 * ± 0.273 2.019 * ± 0.262 FPR gap ( P ) 0.591 ± 0.052 0.491 * ± 0.059 0.087 * ± 0.094 0.552 ± 0.063 FPR gap ( S ) 0.075 ± 0.010 0.085 * ± 0.011 0.030 * ± 0.006 0.057 * ± 0.007 Precision gap ( P )
-0.880 ± 0.031 -0.855 ± 0.115 -0.299 * ± 0.215 -0.815 * ± 0.040 Precision gap ( S )
3.621 ± 0.337 3.401 ± 0.667 1.549 * ± 0.229 2.590 * ± 0.279 Independence gap ( S ) 0.009 ± 0.002 0.008 ± 0.002 0.001 * ± 0.000 0.005 * ± 0.001 Separation gap ( S ) 0.327 ± 0.051 0.305 ± 0.030 0.204 * ± 0.032 0.296 ± 0.053 Sufficiency gap ( S ) 9.451 ± 1.945 8.324 * ± 1.537 1.218 * ± 0.330 4.930 * ± 0.927       layer was retrained on biased training data , it used the same features to make biased predictions .
D.2 Coreference Resolution
The cases where there is no correlation between our intrinsic metric and an extrinsic metric are the cases where the metric is based on Pearson correlation . Unlike occupation prediction , coreference resolution seems to exhibit no correlation between those metrics and compression rate . These metrics are computed as the Pearson correlation between a performance gap for a specific profession and the percentage of women in that profession , however the percentages are computed differently in each task : in occupation prediction , the percentages are computed from the train set , focusing on the representation each gender has in the data . In Winobias , the percentages are taken from the US labor statistics , and are unrelated to the training dataset statistics . We note that the two statistics can be different -the real - world representation of women in a profession does not have to be equal to their representation in written text ( Suresh and Guttag , 2021 ) . We thus decided to test what happens if we change the statistics used in Winobias to dataset statistics , but Ontonotes 5.0 has very little representation to each profession and the statistics extracted from it would not be reliable . We thus took a different approach and computed the Pearson correlations for occupation prediction with real world statistics instead of dataset statistics . To do this , we mapped the professions appearing in this dataset to professions from the US labor statistics , and dropped those who could no be mapped ( 6 out of 29 of the professions which is 21.4 % ) . We then repeated all experiments on the Pearson metrics using these statistics . Figure 7 shows the results . Correlations are very different when computed with respect to real - world statistics . TPR - gap has no correlation at all although it had with training data statistics , the correlation for FPR - gap after retraining exists but is negative , and the correlation with precision - gap does not exist after retraining . We thus conclude that the Pearson metrics are less reliable as they are heavily dependent on the statistics with respect to which they are calculated .
